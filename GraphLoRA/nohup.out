Running: year=2015 â†’ downstream_year=2016, seed=0
Random seed set to 42

==============================
PRE-TRAINING
==============================
create PreTrain instance...
pre-training...
(T) | Epoch=001, loss=7.3009, this epoch 0.0441, total 0.0441
+++model saved ! 2015.pth
(T) | Epoch=002, loss=6.3445, this epoch 0.0331, total 0.0772
+++model saved ! 2015.pth
(T) | Epoch=003, loss=6.3445, this epoch 0.0358, total 0.1130
(T) | Epoch=004, loss=6.3444, this epoch 0.0236, total 0.1366
+++model saved ! 2015.pth
(T) | Epoch=005, loss=6.3451, this epoch 0.0227, total 0.1593
(T) | Epoch=006, loss=6.3445, this epoch 0.0208, total 0.1801
(T) | Epoch=007, loss=6.3472, this epoch 0.0214, total 0.2015
(T) | Epoch=008, loss=6.8267, this epoch 0.0287, total 0.2301
(T) | Epoch=009, loss=6.3447, this epoch 0.0215, total 0.2516
(T) | Epoch=010, loss=6.3446, this epoch 0.0289, total 0.2806
(T) | Epoch=011, loss=6.7118, this epoch 0.0219, total 0.3024
(T) | Epoch=012, loss=6.3444, this epoch 0.0264, total 0.3289
(T) | Epoch=013, loss=6.3447, this epoch 0.0299, total 0.3587
(T) | Epoch=014, loss=6.3439, this epoch 0.0226, total 0.3814
+++model saved ! 2015.pth
(T) | Epoch=015, loss=6.3438, this epoch 0.0360, total 0.4174
+++model saved ! 2015.pth
(T) | Epoch=016, loss=6.3444, this epoch 0.0413, total 0.4586
(T) | Epoch=017, loss=6.3436, this epoch 0.0373, total 0.4959
+++model saved ! 2015.pth
(T) | Epoch=018, loss=6.3435, this epoch 0.0380, total 0.5340
+++model saved ! 2015.pth
(T) | Epoch=019, loss=6.5342, this epoch 0.0358, total 0.5698
(T) | Epoch=020, loss=6.3434, this epoch 0.0291, total 0.5989
+++model saved ! 2015.pth
(T) | Epoch=021, loss=6.3474, this epoch 0.0317, total 0.6306
(T) | Epoch=022, loss=6.3466, this epoch 0.0220, total 0.6526
(T) | Epoch=023, loss=6.4797, this epoch 0.0334, total 0.6860
(T) | Epoch=024, loss=6.3427, this epoch 0.0304, total 0.7164
+++model saved ! 2015.pth
(T) | Epoch=025, loss=6.3428, this epoch 0.0299, total 0.7463
(T) | Epoch=026, loss=6.3444, this epoch 0.0262, total 0.7725
(T) | Epoch=027, loss=6.3420, this epoch 0.0208, total 0.7933
+++model saved ! 2015.pth
(T) | Epoch=028, loss=6.3464, this epoch 0.0414, total 0.8347
(T) | Epoch=029, loss=6.3437, this epoch 0.0301, total 0.8648
(T) | Epoch=030, loss=6.4340, this epoch 0.0392, total 0.9039
(T) | Epoch=031, loss=6.3460, this epoch 0.0309, total 0.9348
(T) | Epoch=032, loss=6.3407, this epoch 0.0321, total 0.9670
+++model saved ! 2015.pth
(T) | Epoch=033, loss=6.3406, this epoch 0.0224, total 0.9893
+++model saved ! 2015.pth
(T) | Epoch=034, loss=6.3411, this epoch 0.0396, total 1.0289
(T) | Epoch=035, loss=6.3453, this epoch 0.0237, total 1.0527
(T) | Epoch=036, loss=6.3412, this epoch 0.0249, total 1.0776
(T) | Epoch=037, loss=6.3867, this epoch 0.0311, total 1.1086
(T) | Epoch=038, loss=6.4135, this epoch 0.0274, total 1.1360
(T) | Epoch=039, loss=6.3379, this epoch 0.0363, total 1.1723
+++model saved ! 2015.pth
(T) | Epoch=040, loss=6.3458, this epoch 0.0250, total 1.1973
(T) | Epoch=041, loss=6.3442, this epoch 0.0214, total 1.2187
(T) | Epoch=042, loss=6.3398, this epoch 0.0216, total 1.2402
(T) | Epoch=043, loss=6.3361, this epoch 0.0218, total 1.2620
+++model saved ! 2015.pth
(T) | Epoch=044, loss=6.3448, this epoch 0.0297, total 1.2917
(T) | Epoch=045, loss=6.3339, this epoch 0.0283, total 1.3199
+++model saved ! 2015.pth
(T) | Epoch=046, loss=6.3378, this epoch 0.0357, total 1.3556
(T) | Epoch=047, loss=6.3318, this epoch 0.0302, total 1.3858
+++model saved ! 2015.pth
(T) | Epoch=048, loss=6.3725, this epoch 0.0384, total 1.4243
(T) | Epoch=049, loss=6.3294, this epoch 0.0355, total 1.4598
+++model saved ! 2015.pth
(T) | Epoch=050, loss=6.3329, this epoch 0.0244, total 1.4842
(T) | Epoch=051, loss=6.3295, this epoch 0.0285, total 1.5126
(T) | Epoch=052, loss=6.3292, this epoch 0.0215, total 1.5342
+++model saved ! 2015.pth
(T) | Epoch=053, loss=6.3262, this epoch 0.0227, total 1.5569
+++model saved ! 2015.pth
(T) | Epoch=054, loss=6.3243, this epoch 0.0348, total 1.5917
+++model saved ! 2015.pth
(T) | Epoch=055, loss=6.3177, this epoch 0.0244, total 1.6161
+++model saved ! 2015.pth
(T) | Epoch=056, loss=6.3145, this epoch 0.0299, total 1.6460
+++model saved ! 2015.pth
(T) | Epoch=057, loss=6.3199, this epoch 0.0358, total 1.6818
(T) | Epoch=058, loss=6.3309, this epoch 0.0284, total 1.7102
(T) | Epoch=059, loss=6.3194, this epoch 0.0269, total 1.7372
(T) | Epoch=060, loss=6.2954, this epoch 0.0291, total 1.7663
+++model saved ! 2015.pth
(T) | Epoch=061, loss=6.2872, this epoch 0.0373, total 1.8036
+++model saved ! 2015.pth
(T) | Epoch=062, loss=6.2811, this epoch 0.0315, total 1.8351
+++model saved ! 2015.pth
(T) | Epoch=063, loss=6.2604, this epoch 0.0226, total 1.8577
+++model saved ! 2015.pth
(T) | Epoch=064, loss=6.2564, this epoch 0.0350, total 1.8927
+++model saved ! 2015.pth
(T) | Epoch=065, loss=6.2311, this epoch 0.0319, total 1.9246
+++model saved ! 2015.pth
(T) | Epoch=066, loss=6.2235, this epoch 0.0295, total 1.9541
+++model saved ! 2015.pth
(T) | Epoch=067, loss=6.2264, this epoch 0.0370, total 1.9912
(T) | Epoch=068, loss=6.1711, this epoch 0.0319, total 2.0231
+++model saved ! 2015.pth
(T) | Epoch=069, loss=6.2524, this epoch 0.0227, total 2.0458
(T) | Epoch=070, loss=6.0517, this epoch 0.0213, total 2.0671
+++model saved ! 2015.pth
(T) | Epoch=071, loss=5.9781, this epoch 0.0226, total 2.0897
+++model saved ! 2015.pth
(T) | Epoch=072, loss=5.9220, this epoch 0.0287, total 2.1184
+++model saved ! 2015.pth
(T) | Epoch=073, loss=5.8787, this epoch 0.0413, total 2.1597
+++model saved ! 2015.pth
(T) | Epoch=074, loss=5.8249, this epoch 0.0371, total 2.1967
+++model saved ! 2015.pth
(T) | Epoch=075, loss=5.7711, this epoch 0.0241, total 2.2209
+++model saved ! 2015.pth
(T) | Epoch=076, loss=6.6189, this epoch 0.0344, total 2.2552
(T) | Epoch=077, loss=5.7260, this epoch 0.0241, total 2.2793
+++model saved ! 2015.pth
(T) | Epoch=078, loss=5.6917, this epoch 0.0220, total 2.3013
+++model saved ! 2015.pth
(T) | Epoch=079, loss=5.7709, this epoch 0.0411, total 2.3424
(T) | Epoch=080, loss=5.6984, this epoch 0.0356, total 2.3780
(T) | Epoch=081, loss=5.6744, this epoch 0.0307, total 2.4086
+++model saved ! 2015.pth
(T) | Epoch=082, loss=5.7215, this epoch 0.0349, total 2.4435
(T) | Epoch=083, loss=5.6127, this epoch 0.0293, total 2.4728
+++model saved ! 2015.pth
(T) | Epoch=084, loss=5.9439, this epoch 0.0236, total 2.4964
(T) | Epoch=085, loss=5.6125, this epoch 0.0215, total 2.5180
+++model saved ! 2015.pth
(T) | Epoch=086, loss=5.5824, this epoch 0.0284, total 2.5463
+++model saved ! 2015.pth
(T) | Epoch=087, loss=5.5724, this epoch 0.0298, total 2.5761
+++model saved ! 2015.pth
(T) | Epoch=088, loss=5.9507, this epoch 0.0296, total 2.6057
(T) | Epoch=089, loss=6.1384, this epoch 0.0281, total 2.6338
(T) | Epoch=090, loss=5.6252, this epoch 0.0263, total 2.6602
(T) | Epoch=091, loss=6.0038, this epoch 0.0304, total 2.6906
(T) | Epoch=092, loss=5.6440, this epoch 0.0336, total 2.7242
(T) | Epoch=093, loss=5.6387, this epoch 0.0351, total 2.7592
(T) | Epoch=094, loss=5.8832, this epoch 0.0313, total 2.7906
(T) | Epoch=095, loss=5.7851, this epoch 0.0293, total 2.8199
(T) | Epoch=096, loss=5.5782, this epoch 0.0281, total 2.8480
(T) | Epoch=097, loss=5.6243, this epoch 0.0332, total 2.8813
(T) | Epoch=098, loss=5.7081, this epoch 0.0298, total 2.9110
(T) | Epoch=099, loss=5.6286, this epoch 0.0334, total 2.9444
(T) | Epoch=100, loss=5.6361, this epoch 0.0325, total 2.9769
(T) | Epoch=101, loss=5.9126, this epoch 0.0272, total 3.0041
(T) | Epoch=102, loss=5.6857, this epoch 0.0346, total 3.0387
(T) | Epoch=103, loss=5.6386, this epoch 0.0350, total 3.0737
(T) | Epoch=104, loss=5.6401, this epoch 0.0302, total 3.1039
(T) | Epoch=105, loss=5.6428, this epoch 0.0284, total 3.1323
(T) | Epoch=106, loss=5.7198, this epoch 0.0331, total 3.1653
(T) | Epoch=107, loss=6.5084, this epoch 0.0309, total 3.1962
(T) | Epoch=108, loss=5.6058, this epoch 0.0275, total 3.2237
(T) | Epoch=109, loss=6.0109, this epoch 0.0302, total 3.2539
(T) | Epoch=110, loss=5.7566, this epoch 0.0254, total 3.2793
(T) | Epoch=111, loss=5.7794, this epoch 0.0292, total 3.3085
(T) | Epoch=112, loss=5.8127, this epoch 0.0391, total 3.3475
(T) | Epoch=113, loss=5.8358, this epoch 0.0354, total 3.3830
(T) | Epoch=114, loss=6.3422, this epoch 0.0291, total 3.4120
(T) | Epoch=115, loss=5.9372, this epoch 0.0293, total 3.4413
(T) | Epoch=116, loss=5.8214, this epoch 0.0216, total 3.4629
(T) | Epoch=117, loss=6.2301, this epoch 0.0338, total 3.4967
(T) | Epoch=118, loss=5.8718, this epoch 0.0308, total 3.5275
(T) | Epoch=119, loss=5.7465, this epoch 0.0334, total 3.5609
(T) | Epoch=120, loss=5.9975, this epoch 0.0273, total 3.5882
(T) | Epoch=121, loss=5.7267, this epoch 0.0294, total 3.6176
(T) | Epoch=122, loss=5.7174, this epoch 0.0321, total 3.6497
(T) | Epoch=123, loss=5.6807, this epoch 0.0364, total 3.6860
(T) | Epoch=124, loss=5.6803, this epoch 0.0332, total 3.7192
(T) | Epoch=125, loss=5.7367, this epoch 0.0276, total 3.7469
(T) | Epoch=126, loss=6.0703, this epoch 0.0346, total 3.7814
(T) | Epoch=127, loss=5.8209, this epoch 0.0296, total 3.8110
(T) | Epoch=128, loss=5.6466, this epoch 0.0278, total 3.8388
(T) | Epoch=129, loss=5.6444, this epoch 0.0276, total 3.8664
(T) | Epoch=130, loss=5.6630, this epoch 0.0347, total 3.9011
(T) | Epoch=131, loss=5.6858, this epoch 0.0311, total 3.9322
(T) | Epoch=132, loss=5.6466, this epoch 0.0276, total 3.9598
(T) | Epoch=133, loss=5.7401, this epoch 0.0287, total 3.9885
(T) | Epoch=134, loss=6.0010, this epoch 0.0332, total 4.0217
(T) | Epoch=135, loss=5.8345, this epoch 0.0283, total 4.0499
(T) | Epoch=136, loss=5.6735, this epoch 0.0286, total 4.0786
(T) | Epoch=137, loss=5.5890, this epoch 0.0308, total 4.1094
(T) | Epoch=138, loss=5.5860, this epoch 0.0213, total 4.1307
(T) | Epoch=139, loss=5.6898, this epoch 0.0277, total 4.1583
(T) | Epoch=140, loss=5.6042, this epoch 0.0376, total 4.1959
(T) | Epoch=141, loss=6.1395, this epoch 0.0375, total 4.2334
(T) | Epoch=142, loss=5.6056, this epoch 0.0387, total 4.2721
(T) | Epoch=143, loss=5.6725, this epoch 0.0296, total 4.3017
(T) | Epoch=144, loss=5.6118, this epoch 0.0288, total 4.3304
(T) | Epoch=145, loss=5.6213, this epoch 0.0272, total 4.3576
(T) | Epoch=146, loss=5.7486, this epoch 0.0345, total 4.3921
(T) | Epoch=147, loss=5.6208, this epoch 0.0355, total 4.4275
(T) | Epoch=148, loss=5.6559, this epoch 0.0379, total 4.4654
(T) | Epoch=149, loss=5.6870, this epoch 0.0304, total 4.4958
(T) | Epoch=150, loss=5.6693, this epoch 0.0264, total 4.5221
(T) | Epoch=151, loss=5.5899, this epoch 0.0327, total 4.5548
(T) | Epoch=152, loss=5.7057, this epoch 0.0275, total 4.5823
(T) | Epoch=153, loss=5.5816, this epoch 0.0350, total 4.6173
(T) | Epoch=154, loss=5.5361, this epoch 0.0287, total 4.6461
+++model saved ! 2015.pth
(T) | Epoch=155, loss=5.5700, this epoch 0.0317, total 4.6778
(T) | Epoch=156, loss=5.9975, this epoch 0.0293, total 4.7071
(T) | Epoch=157, loss=5.5145, this epoch 0.0213, total 4.7284
+++model saved ! 2015.pth
(T) | Epoch=158, loss=5.6933, this epoch 0.0223, total 4.7507
(T) | Epoch=159, loss=5.6869, this epoch 0.0336, total 4.7843
(T) | Epoch=160, loss=5.5327, this epoch 0.0227, total 4.8070
(T) | Epoch=161, loss=5.6936, this epoch 0.0219, total 4.8289
(T) | Epoch=162, loss=5.5443, this epoch 0.0217, total 4.8506
(T) | Epoch=163, loss=5.5563, this epoch 0.0216, total 4.8722
(T) | Epoch=164, loss=5.6716, this epoch 0.0212, total 4.8934
(T) | Epoch=165, loss=5.5633, this epoch 0.0305, total 4.9239
(T) | Epoch=166, loss=5.6526, this epoch 0.0274, total 4.9513
(T) | Epoch=167, loss=5.5458, this epoch 0.0223, total 4.9736
(T) | Epoch=168, loss=5.6109, this epoch 0.0272, total 5.0008
(T) | Epoch=169, loss=6.0887, this epoch 0.0364, total 5.0372
(T) | Epoch=170, loss=5.6300, this epoch 0.0384, total 5.0755
(T) | Epoch=171, loss=5.5377, this epoch 0.0265, total 5.1020
(T) | Epoch=172, loss=5.6407, this epoch 0.0289, total 5.1309
(T) | Epoch=173, loss=5.6512, this epoch 0.0236, total 5.1545
(T) | Epoch=174, loss=5.6599, this epoch 0.0263, total 5.1808
(T) | Epoch=175, loss=5.5950, this epoch 0.0303, total 5.2112
(T) | Epoch=176, loss=5.6222, this epoch 0.0211, total 5.2323
(T) | Epoch=177, loss=5.5519, this epoch 0.0262, total 5.2584
(T) | Epoch=178, loss=5.6009, this epoch 0.0303, total 5.2887
(T) | Epoch=179, loss=5.8427, this epoch 0.0357, total 5.3244
(T) | Epoch=180, loss=5.5160, this epoch 0.0303, total 5.3546
(T) | Epoch=181, loss=5.5264, this epoch 0.0213, total 5.3759
(T) | Epoch=182, loss=5.5991, this epoch 0.0206, total 5.3964
(T) | Epoch=183, loss=5.5318, this epoch 0.0210, total 5.4175
(T) | Epoch=184, loss=5.6236, this epoch 0.0262, total 5.4437
(T) | Epoch=185, loss=5.5202, this epoch 0.0316, total 5.4753
(T) | Epoch=186, loss=5.5019, this epoch 0.0220, total 5.4974
+++model saved ! 2015.pth
(T) | Epoch=187, loss=5.5239, this epoch 0.0441, total 5.5415
(T) | Epoch=188, loss=5.4774, this epoch 0.0311, total 5.5726
+++model saved ! 2015.pth
(T) | Epoch=189, loss=5.6433, this epoch 0.0224, total 5.5950
(T) | Epoch=190, loss=5.5375, this epoch 0.0288, total 5.6238
(T) | Epoch=191, loss=5.4963, this epoch 0.0279, total 5.6517
(T) | Epoch=192, loss=5.6377, this epoch 0.0369, total 5.6886
(T) | Epoch=193, loss=5.7552, this epoch 0.0310, total 5.7195
(T) | Epoch=194, loss=5.4859, this epoch 0.0318, total 5.7513
(T) | Epoch=195, loss=5.8543, this epoch 0.0219, total 5.7731
(T) | Epoch=196, loss=5.8948, this epoch 0.0296, total 5.8027
(T) | Epoch=197, loss=5.4898, this epoch 0.0412, total 5.8440
(T) | Epoch=198, loss=5.5865, this epoch 0.0315, total 5.8755
(T) | Epoch=199, loss=5.5759, this epoch 0.0228, total 5.8983
(T) | Epoch=200, loss=5.6207, this epoch 0.0354, total 5.9336
(T) | Epoch=201, loss=5.6858, this epoch 0.0342, total 5.9679
(T) | Epoch=202, loss=5.5701, this epoch 0.0427, total 6.0106
(T) | Epoch=203, loss=5.7228, this epoch 0.0301, total 6.0407
(T) | Epoch=204, loss=5.6139, this epoch 0.0299, total 6.0706
(T) | Epoch=205, loss=5.5377, this epoch 0.0228, total 6.0934
(T) | Epoch=206, loss=5.7066, this epoch 0.0312, total 6.1246
(T) | Epoch=207, loss=5.6557, this epoch 0.0218, total 6.1464
(T) | Epoch=208, loss=5.7168, this epoch 0.0294, total 6.1758
(T) | Epoch=209, loss=5.5084, this epoch 0.0295, total 6.2053
(T) | Epoch=210, loss=5.4506, this epoch 0.0293, total 6.2346
+++model saved ! 2015.pth
(T) | Epoch=211, loss=5.5602, this epoch 0.0390, total 6.2737
(T) | Epoch=212, loss=5.5185, this epoch 0.0389, total 6.3126
(T) | Epoch=213, loss=5.5186, this epoch 0.0292, total 6.3418
(T) | Epoch=214, loss=5.5161, this epoch 0.0362, total 6.3780
(T) | Epoch=215, loss=5.5392, this epoch 0.0309, total 6.4089
(T) | Epoch=216, loss=5.6644, this epoch 0.0415, total 6.4505
(T) | Epoch=217, loss=5.5933, this epoch 0.0407, total 6.4912
(T) | Epoch=218, loss=5.4884, this epoch 0.0229, total 6.5140
(T) | Epoch=219, loss=5.4725, this epoch 0.0356, total 6.5496
(T) | Epoch=220, loss=5.5151, this epoch 0.0322, total 6.5818
(T) | Epoch=221, loss=5.5940, this epoch 0.0270, total 6.6088
(T) | Epoch=222, loss=5.7623, this epoch 0.0303, total 6.6392
(T) | Epoch=223, loss=5.5258, this epoch 0.0375, total 6.6767
(T) | Epoch=224, loss=5.5260, this epoch 0.0312, total 6.7078
(T) | Epoch=225, loss=5.5467, this epoch 0.0291, total 6.7370
(T) | Epoch=226, loss=5.5767, this epoch 0.0307, total 6.7677
(T) | Epoch=227, loss=5.5687, this epoch 0.0364, total 6.8040
(T) | Epoch=228, loss=5.5500, this epoch 0.0311, total 6.8351
(T) | Epoch=229, loss=5.4830, this epoch 0.0270, total 6.8621
(T) | Epoch=230, loss=5.4849, this epoch 0.0309, total 6.8930
(T) | Epoch=231, loss=5.6051, this epoch 0.0362, total 6.9292
(T) | Epoch=232, loss=5.5024, this epoch 0.0232, total 6.9524
(T) | Epoch=233, loss=5.4967, this epoch 0.0285, total 6.9808
(T) | Epoch=234, loss=5.5081, this epoch 0.0315, total 7.0123
(T) | Epoch=235, loss=5.4733, this epoch 0.0213, total 7.0336
(T) | Epoch=236, loss=5.4702, this epoch 0.0364, total 7.0700
(T) | Epoch=237, loss=5.5384, this epoch 0.0264, total 7.0964
(T) | Epoch=238, loss=5.4866, this epoch 0.0311, total 7.1275
(T) | Epoch=239, loss=5.4858, this epoch 0.0321, total 7.1596
(T) | Epoch=240, loss=5.4585, this epoch 0.0320, total 7.1916
(T) | Epoch=241, loss=5.4662, this epoch 0.0325, total 7.2241
(T) | Epoch=242, loss=5.4382, this epoch 0.0384, total 7.2625
+++model saved ! 2015.pth
(T) | Epoch=243, loss=5.5176, this epoch 0.0315, total 7.2939
(T) | Epoch=244, loss=5.4797, this epoch 0.0289, total 7.3228
(T) | Epoch=245, loss=5.4400, this epoch 0.0293, total 7.3522
(T) | Epoch=246, loss=5.7701, this epoch 0.0300, total 7.3822
(T) | Epoch=247, loss=5.5373, this epoch 0.0274, total 7.4096
(T) | Epoch=248, loss=5.5813, this epoch 0.0288, total 7.4383
(T) | Epoch=249, loss=5.5397, this epoch 0.0304, total 7.4688
(T) | Epoch=250, loss=5.6056, this epoch 0.0303, total 7.4990
(T) | Epoch=251, loss=5.5702, this epoch 0.0351, total 7.5341
(T) | Epoch=252, loss=5.7220, this epoch 0.0309, total 7.5650
(T) | Epoch=253, loss=5.5406, this epoch 0.0278, total 7.5929
(T) | Epoch=254, loss=5.5306, this epoch 0.0309, total 7.6238
(T) | Epoch=255, loss=5.4585, this epoch 0.0283, total 7.6521
(T) | Epoch=256, loss=5.4566, this epoch 0.0358, total 7.6879
(T) | Epoch=257, loss=5.8813, this epoch 0.0293, total 7.7172
(T) | Epoch=258, loss=5.5537, this epoch 0.0311, total 7.7483
(T) | Epoch=259, loss=5.4958, this epoch 0.0274, total 7.7757
(T) | Epoch=260, loss=5.4418, this epoch 0.0337, total 7.8094
(T) | Epoch=261, loss=5.4479, this epoch 0.0236, total 7.8330
(T) | Epoch=262, loss=5.4576, this epoch 0.0343, total 7.8673
(T) | Epoch=263, loss=5.5335, this epoch 0.0208, total 7.8880
(T) | Epoch=264, loss=5.4604, this epoch 0.0297, total 7.9177
(T) | Epoch=265, loss=5.5186, this epoch 0.0272, total 7.9448
(T) | Epoch=266, loss=5.4479, this epoch 0.0295, total 7.9744
(T) | Epoch=267, loss=5.4676, this epoch 0.0386, total 8.0129
(T) | Epoch=268, loss=5.8167, this epoch 0.0300, total 8.0429
(T) | Epoch=269, loss=5.6987, this epoch 0.0408, total 8.0837
(T) | Epoch=270, loss=5.4679, this epoch 0.0310, total 8.1147
(T) | Epoch=271, loss=6.1941, this epoch 0.0280, total 8.1428
(T) | Epoch=272, loss=5.4472, this epoch 0.0311, total 8.1738
(T) | Epoch=273, loss=5.6609, this epoch 0.0321, total 8.2060
(T) | Epoch=274, loss=6.1314, this epoch 0.0325, total 8.2385
(T) | Epoch=275, loss=6.0612, this epoch 0.0300, total 8.2685
(T) | Epoch=276, loss=6.0916, this epoch 0.0212, total 8.2897
(T) | Epoch=277, loss=6.2599, this epoch 0.0366, total 8.3264
(T) | Epoch=278, loss=6.5025, this epoch 0.0347, total 8.3611
(T) | Epoch=279, loss=6.5513, this epoch 0.0293, total 8.3904
(T) | Epoch=280, loss=5.4721, this epoch 0.0371, total 8.4275
(T) | Epoch=281, loss=6.2545, this epoch 0.0290, total 8.4565
(T) | Epoch=282, loss=6.2623, this epoch 0.0366, total 8.4932
(T) | Epoch=283, loss=6.6019, this epoch 0.0291, total 8.5223
(T) | Epoch=284, loss=6.2044, this epoch 0.0225, total 8.5448
(T) | Epoch=285, loss=6.2058, this epoch 0.0283, total 8.5731
(T) | Epoch=286, loss=6.2458, this epoch 0.0303, total 8.6034
(T) | Epoch=287, loss=6.6400, this epoch 0.0269, total 8.6302
(T) | Epoch=288, loss=6.5835, this epoch 0.0369, total 8.6672
(T) | Epoch=289, loss=6.2874, this epoch 0.0297, total 8.6969
(T) | Epoch=290, loss=6.2720, this epoch 0.0391, total 8.7360
(T) | Epoch=291, loss=6.5008, this epoch 0.0376, total 8.7736
(T) | Epoch=292, loss=6.2123, this epoch 0.0367, total 8.8103
(T) | Epoch=293, loss=6.2381, this epoch 0.0313, total 8.8416
(T) | Epoch=294, loss=6.2386, this epoch 0.0353, total 8.8769
(T) | Epoch=295, loss=6.2049, this epoch 0.0306, total 8.9075
(T) | Epoch=296, loss=6.2041, this epoch 0.0292, total 8.9367
(T) | Epoch=297, loss=6.2023, this epoch 0.0282, total 8.9649
(T) | Epoch=298, loss=6.1916, this epoch 0.0290, total 8.9939
(T) | Epoch=299, loss=6.1928, this epoch 0.0221, total 9.0160
(T) | Epoch=300, loss=6.2349, this epoch 0.0302, total 9.0462
(T) | Epoch=301, loss=6.1909, this epoch 0.0211, total 9.0672
(T) | Epoch=302, loss=6.2020, this epoch 0.0347, total 9.1020
(T) | Epoch=303, loss=6.6337, this epoch 0.0234, total 9.1254
(T) | Epoch=304, loss=6.2454, this epoch 0.0289, total 9.1543
(T) | Epoch=305, loss=6.1594, this epoch 0.0282, total 9.1824
(T) | Epoch=306, loss=6.2184, this epoch 0.0282, total 9.2106
(T) | Epoch=307, loss=6.2316, this epoch 0.0228, total 9.2334
(T) | Epoch=308, loss=6.1389, this epoch 0.0349, total 9.2682
(T) | Epoch=309, loss=6.1474, this epoch 0.0310, total 9.2992
(T) | Epoch=310, loss=6.1200, this epoch 0.0215, total 9.3207
(T) | Epoch=311, loss=6.2501, this epoch 0.0209, total 9.3417
(T) | Epoch=312, loss=6.3235, this epoch 0.0290, total 9.3707
(T) | Epoch=313, loss=6.0877, this epoch 0.0318, total 9.4025
(T) | Epoch=314, loss=6.1825, this epoch 0.0210, total 9.4235
(T) | Epoch=315, loss=6.1606, this epoch 0.0348, total 9.4583
(T) | Epoch=316, loss=6.1346, this epoch 0.0212, total 9.4795
(T) | Epoch=317, loss=6.0499, this epoch 0.0355, total 9.5150
(T) | Epoch=318, loss=6.1340, this epoch 0.0360, total 9.5510
(T) | Epoch=319, loss=6.1094, this epoch 0.0312, total 9.5822
(T) | Epoch=320, loss=5.9998, this epoch 0.0258, total 9.6080
(T) | Epoch=321, loss=6.0074, this epoch 0.0205, total 9.6286
(T) | Epoch=322, loss=6.1328, this epoch 0.0207, total 9.6493
(T) | Epoch=323, loss=6.2367, this epoch 0.0249, total 9.6742
(T) | Epoch=324, loss=6.0144, this epoch 0.0227, total 9.6970
(T) | Epoch=325, loss=5.9449, this epoch 0.0279, total 9.7248
(T) | Epoch=326, loss=6.0434, this epoch 0.0261, total 9.7509
(T) | Epoch=327, loss=5.9020, this epoch 0.0293, total 9.7803
(T) | Epoch=328, loss=6.1169, this epoch 0.0344, total 9.8147
(T) | Epoch=329, loss=5.8791, this epoch 0.0372, total 9.8519
(T) | Epoch=330, loss=5.8667, this epoch 0.0305, total 9.8824
(T) | Epoch=331, loss=5.8375, this epoch 0.0211, total 9.9035
(T) | Epoch=332, loss=5.8661, this epoch 0.0304, total 9.9339
(T) | Epoch=333, loss=6.2030, this epoch 0.0209, total 9.9548
(T) | Epoch=334, loss=5.8829, this epoch 0.0211, total 9.9759
(T) | Epoch=335, loss=5.8186, this epoch 0.0213, total 9.9972
(T) | Epoch=336, loss=5.8234, this epoch 0.0210, total 10.0182
(T) | Epoch=337, loss=5.8930, this epoch 0.0211, total 10.0393
(T) | Epoch=338, loss=5.7735, this epoch 0.0208, total 10.0601
(T) | Epoch=339, loss=6.1419, this epoch 0.0306, total 10.0907
(T) | Epoch=340, loss=5.7431, this epoch 0.0212, total 10.1118
(T) | Epoch=341, loss=5.7633, this epoch 0.0274, total 10.1392
(T) | Epoch=342, loss=5.8036, this epoch 0.0221, total 10.1613
(T) | Epoch=343, loss=5.7080, this epoch 0.0330, total 10.1944
(T) | Epoch=344, loss=5.7409, this epoch 0.0229, total 10.2173
(T) | Epoch=345, loss=5.7964, this epoch 0.0355, total 10.2528
(T) | Epoch=346, loss=5.7684, this epoch 0.0341, total 10.2869
(T) | Epoch=347, loss=5.7332, this epoch 0.0279, total 10.3148
(T) | Epoch=348, loss=5.8051, this epoch 0.0219, total 10.3366
(T) | Epoch=349, loss=5.6643, this epoch 0.0207, total 10.3574
(T) | Epoch=350, loss=5.6727, this epoch 0.0370, total 10.3943
(T) | Epoch=351, loss=5.5700, this epoch 0.0232, total 10.4175
(T) | Epoch=352, loss=5.6498, this epoch 0.0331, total 10.4507
(T) | Epoch=353, loss=5.8596, this epoch 0.0354, total 10.4861
(T) | Epoch=354, loss=5.5419, this epoch 0.0231, total 10.5092
(T) | Epoch=355, loss=5.5567, this epoch 0.0352, total 10.5445
(T) | Epoch=356, loss=5.6211, this epoch 0.0294, total 10.5739
(T) | Epoch=357, loss=5.6299, this epoch 0.0299, total 10.6038
(T) | Epoch=358, loss=5.6012, this epoch 0.0344, total 10.6382
(T) | Epoch=359, loss=5.5336, this epoch 0.0350, total 10.6732
(T) | Epoch=360, loss=5.6693, this epoch 0.0227, total 10.6959
(T) | Epoch=361, loss=5.4747, this epoch 0.0280, total 10.7239
(T) | Epoch=362, loss=5.4624, this epoch 0.0206, total 10.7444
(T) | Epoch=363, loss=5.4755, this epoch 0.0205, total 10.7649
(T) | Epoch=364, loss=5.9041, this epoch 0.0271, total 10.7920
(T) | Epoch=365, loss=5.8374, this epoch 0.0281, total 10.8201
(T) | Epoch=366, loss=5.6171, this epoch 0.0306, total 10.8507
(T) | Epoch=367, loss=5.9630, this epoch 0.0292, total 10.8798
(T) | Epoch=368, loss=5.5305, this epoch 0.0280, total 10.9079
(T) | Epoch=369, loss=5.5830, this epoch 0.0212, total 10.9291
(T) | Epoch=370, loss=5.7993, this epoch 0.0207, total 10.9498
(T) | Epoch=371, loss=5.7036, this epoch 0.0330, total 10.9828
(T) | Epoch=372, loss=5.8526, this epoch 0.0227, total 11.0055
(T) | Epoch=373, loss=5.6596, this epoch 0.0206, total 11.0261
(T) | Epoch=374, loss=5.8250, this epoch 0.0276, total 11.0537
(T) | Epoch=375, loss=6.0241, this epoch 0.0352, total 11.0889
(T) | Epoch=376, loss=5.6424, this epoch 0.0356, total 11.1245
(T) | Epoch=377, loss=5.6452, this epoch 0.0299, total 11.1544
(T) | Epoch=378, loss=5.7309, this epoch 0.0303, total 11.1847
(T) | Epoch=379, loss=5.7704, this epoch 0.0273, total 11.2119
(T) | Epoch=380, loss=5.6148, this epoch 0.0302, total 11.2421
(T) | Epoch=381, loss=5.6558, this epoch 0.0305, total 11.2726
(T) | Epoch=382, loss=5.6497, this epoch 0.0211, total 11.2937
(T) | Epoch=383, loss=5.6110, this epoch 0.0214, total 11.3151
(T) | Epoch=384, loss=5.7810, this epoch 0.0210, total 11.3361
(T) | Epoch=385, loss=5.9786, this epoch 0.0211, total 11.3572
(T) | Epoch=386, loss=5.6451, this epoch 0.0211, total 11.3783
(T) | Epoch=387, loss=5.5955, this epoch 0.0210, total 11.3994
(T) | Epoch=388, loss=5.6465, this epoch 0.0210, total 11.4203
(T) | Epoch=389, loss=5.4910, this epoch 0.0211, total 11.4415
(T) | Epoch=390, loss=5.6189, this epoch 0.0214, total 11.4629
(T) | Epoch=391, loss=5.4685, this epoch 0.0218, total 11.4847
(T) | Epoch=392, loss=5.4958, this epoch 0.0359, total 11.5206
(T) | Epoch=393, loss=5.5641, this epoch 0.0283, total 11.5489
(T) | Epoch=394, loss=5.4502, this epoch 0.0219, total 11.5707
(T) | Epoch=395, loss=5.4532, this epoch 0.0277, total 11.5984
(T) | Epoch=396, loss=5.4759, this epoch 0.0291, total 11.6275
(T) | Epoch=397, loss=5.4575, this epoch 0.0216, total 11.6491
(T) | Epoch=398, loss=5.6009, this epoch 0.0209, total 11.6700
(T) | Epoch=399, loss=5.5847, this epoch 0.0354, total 11.7054
(T) | Epoch=400, loss=5.4685, this epoch 0.0377, total 11.7431
(T) | Epoch=401, loss=5.5455, this epoch 0.0227, total 11.7658
(T) | Epoch=402, loss=5.6247, this epoch 0.0212, total 11.7870
(T) | Epoch=403, loss=5.4525, this epoch 0.0273, total 11.8143
(T) | Epoch=404, loss=5.4362, this epoch 0.0217, total 11.8360
+++model saved ! 2015.pth
(T) | Epoch=405, loss=5.4400, this epoch 0.0237, total 11.8598
(T) | Epoch=406, loss=5.4798, this epoch 0.0221, total 11.8818
(T) | Epoch=407, loss=5.5529, this epoch 0.0285, total 11.9103
(T) | Epoch=408, loss=5.4490, this epoch 0.0290, total 11.9393
(T) | Epoch=409, loss=5.4630, this epoch 0.0353, total 11.9746
(T) | Epoch=410, loss=5.5495, this epoch 0.0314, total 12.0060
(T) | Epoch=411, loss=5.5500, this epoch 0.0273, total 12.0333
(T) | Epoch=412, loss=5.6063, this epoch 0.0289, total 12.0622
(T) | Epoch=413, loss=5.4396, this epoch 0.0290, total 12.0912
(T) | Epoch=414, loss=5.4364, this epoch 0.0342, total 12.1254
(T) | Epoch=415, loss=5.6077, this epoch 0.0226, total 12.1480
(T) | Epoch=416, loss=5.5016, this epoch 0.0297, total 12.1777
(T) | Epoch=417, loss=5.4417, this epoch 0.0399, total 12.2176
(T) | Epoch=418, loss=5.4503, this epoch 0.0305, total 12.2481
(T) | Epoch=419, loss=5.4448, this epoch 0.0271, total 12.2752
(T) | Epoch=420, loss=5.6126, this epoch 0.0301, total 12.3053
(T) | Epoch=421, loss=5.4671, this epoch 0.0258, total 12.3312
(T) | Epoch=422, loss=5.4307, this epoch 0.0289, total 12.3600
+++model saved ! 2015.pth
(T) | Epoch=423, loss=5.5285, this epoch 0.0302, total 12.3903
(T) | Epoch=424, loss=5.5160, this epoch 0.0320, total 12.4222
(T) | Epoch=425, loss=5.5378, this epoch 0.0377, total 12.4599
(T) | Epoch=426, loss=5.4382, this epoch 0.0318, total 12.4918
(T) | Epoch=427, loss=5.4949, this epoch 0.0351, total 12.5269
(T) | Epoch=428, loss=5.4308, this epoch 0.0303, total 12.5572
(T) | Epoch=429, loss=5.4630, this epoch 0.0366, total 12.5938
(T) | Epoch=430, loss=5.4905, this epoch 0.0363, total 12.6301
(T) | Epoch=431, loss=5.4332, this epoch 0.0379, total 12.6680
(T) | Epoch=432, loss=5.4322, this epoch 0.0302, total 12.6983
(T) | Epoch=433, loss=5.4577, this epoch 0.0283, total 12.7266
(T) | Epoch=434, loss=5.4955, this epoch 0.0299, total 12.7565
(T) | Epoch=435, loss=5.4330, this epoch 0.0353, total 12.7917
(T) | Epoch=436, loss=5.4945, this epoch 0.0369, total 12.8287
(T) | Epoch=437, loss=5.4604, this epoch 0.0398, total 12.8684
(T) | Epoch=438, loss=5.4203, this epoch 0.0319, total 12.9004
+++model saved ! 2015.pth
(T) | Epoch=439, loss=5.5291, this epoch 0.0282, total 12.9285
(T) | Epoch=440, loss=5.4317, this epoch 0.0233, total 12.9518
(T) | Epoch=441, loss=5.5912, this epoch 0.0216, total 12.9734
(T) | Epoch=442, loss=5.4522, this epoch 0.0286, total 13.0019
(T) | Epoch=443, loss=5.4724, this epoch 0.0358, total 13.0377
(T) | Epoch=444, loss=5.5039, this epoch 0.0226, total 13.0603
(T) | Epoch=445, loss=5.4231, this epoch 0.0213, total 13.0816
(T) | Epoch=446, loss=5.7531, this epoch 0.0212, total 13.1029
(T) | Epoch=447, loss=5.7189, this epoch 0.0218, total 13.1247
(T) | Epoch=448, loss=5.4252, this epoch 0.0218, total 13.1465
(T) | Epoch=449, loss=5.5057, this epoch 0.0268, total 13.1733
(T) | Epoch=450, loss=5.5007, this epoch 0.0303, total 13.2036
(T) | Epoch=451, loss=5.4943, this epoch 0.0213, total 13.2249
(T) | Epoch=452, loss=5.4707, this epoch 0.0267, total 13.2516
(T) | Epoch=453, loss=5.4843, this epoch 0.0290, total 13.2806
(T) | Epoch=454, loss=5.4385, this epoch 0.0304, total 13.3109
(T) | Epoch=455, loss=5.4589, this epoch 0.0263, total 13.3372
(T) | Epoch=456, loss=5.5895, this epoch 0.0221, total 13.3593
(T) | Epoch=457, loss=5.4486, this epoch 0.0281, total 13.3874
(T) | Epoch=458, loss=5.5444, this epoch 0.0215, total 13.4090
(T) | Epoch=459, loss=5.4526, this epoch 0.0271, total 13.4361
(T) | Epoch=460, loss=5.4215, this epoch 0.0290, total 13.4651
(T) | Epoch=461, loss=5.4392, this epoch 0.0270, total 13.4921
(T) | Epoch=462, loss=5.5083, this epoch 0.0230, total 13.5151
(T) | Epoch=463, loss=5.4156, this epoch 0.0216, total 13.5367
+++model saved ! 2015.pth
(T) | Epoch=464, loss=5.4269, this epoch 0.0365, total 13.5732
(T) | Epoch=465, loss=5.4246, this epoch 0.0232, total 13.5964
(T) | Epoch=466, loss=5.4477, this epoch 0.0216, total 13.6180
(T) | Epoch=467, loss=5.4322, this epoch 0.0249, total 13.6429
(T) | Epoch=468, loss=5.6179, this epoch 0.0368, total 13.6797
(T) | Epoch=469, loss=5.4339, this epoch 0.0279, total 13.7076
(T) | Epoch=470, loss=5.5125, this epoch 0.0234, total 13.7310
(T) | Epoch=471, loss=5.4552, this epoch 0.0275, total 13.7585
(T) | Epoch=472, loss=5.4483, this epoch 0.0286, total 13.7871
(T) | Epoch=473, loss=5.4204, this epoch 0.0224, total 13.8095
(T) | Epoch=474, loss=5.4519, this epoch 0.0268, total 13.8363
(T) | Epoch=475, loss=5.4931, this epoch 0.0313, total 13.8676
(T) | Epoch=476, loss=5.5540, this epoch 0.0344, total 13.9020
(T) | Epoch=477, loss=5.4611, this epoch 0.0229, total 13.9249
(T) | Epoch=478, loss=5.4274, this epoch 0.0357, total 13.9606
(T) | Epoch=479, loss=5.4271, this epoch 0.0315, total 13.9921
(T) | Epoch=480, loss=5.4793, this epoch 0.0338, total 14.0260
(T) | Epoch=481, loss=5.5025, this epoch 0.0296, total 14.0555
(T) | Epoch=482, loss=5.4387, this epoch 0.0281, total 14.0837
(T) | Epoch=483, loss=5.5007, this epoch 0.0274, total 14.1110
(T) | Epoch=484, loss=5.4266, this epoch 0.0293, total 14.1403
(T) | Epoch=485, loss=5.4199, this epoch 0.0335, total 14.1738
(T) | Epoch=486, loss=5.5012, this epoch 0.0303, total 14.2040
(T) | Epoch=487, loss=5.4642, this epoch 0.0267, total 14.2308
(T) | Epoch=488, loss=5.4254, this epoch 0.0306, total 14.2613
(T) | Epoch=489, loss=5.4601, this epoch 0.0364, total 14.2977
(T) | Epoch=490, loss=5.4232, this epoch 0.0315, total 14.3291
(T) | Epoch=491, loss=5.4730, this epoch 0.0220, total 14.3511
(T) | Epoch=492, loss=5.5828, this epoch 0.0289, total 14.3800
(T) | Epoch=493, loss=5.6463, this epoch 0.0296, total 14.4096
(T) | Epoch=494, loss=5.4714, this epoch 0.0218, total 14.4314
(T) | Epoch=495, loss=5.4351, this epoch 0.0284, total 14.4598
(T) | Epoch=496, loss=5.5540, this epoch 0.0362, total 14.4960
(T) | Epoch=497, loss=5.4227, this epoch 0.0299, total 14.5259
(T) | Epoch=498, loss=5.4147, this epoch 0.0338, total 14.5597
+++model saved ! 2015.pth
(T) | Epoch=499, loss=5.4123, this epoch 0.0396, total 14.5993
+++model saved ! 2015.pth
(T) | Epoch=500, loss=5.4194, this epoch 0.0372, total 14.6365
(T) | Epoch=501, loss=5.5119, this epoch 0.0371, total 14.6736
(T) | Epoch=502, loss=5.5781, this epoch 0.0304, total 14.7039
(T) | Epoch=503, loss=5.4564, this epoch 0.0336, total 14.7375
(T) | Epoch=504, loss=5.4778, this epoch 0.0305, total 14.7680
(T) | Epoch=505, loss=5.5049, this epoch 0.0275, total 14.7955
(T) | Epoch=506, loss=5.4150, this epoch 0.0310, total 14.8266
(T) | Epoch=507, loss=5.4572, this epoch 0.0234, total 14.8500
(T) | Epoch=508, loss=5.4238, this epoch 0.0249, total 14.8749
(T) | Epoch=509, loss=5.4941, this epoch 0.0294, total 14.9043
(T) | Epoch=510, loss=5.5819, this epoch 0.0211, total 14.9254
(T) | Epoch=511, loss=5.4522, this epoch 0.0277, total 14.9532
(T) | Epoch=512, loss=5.4787, this epoch 0.0292, total 14.9824
(T) | Epoch=513, loss=5.4105, this epoch 0.0223, total 15.0047
+++model saved ! 2015.pth
(T) | Epoch=514, loss=5.4909, this epoch 0.0228, total 15.0275
(T) | Epoch=515, loss=5.4507, this epoch 0.0282, total 15.0557
(T) | Epoch=516, loss=5.4115, this epoch 0.0215, total 15.0772
(T) | Epoch=517, loss=5.4083, this epoch 0.0283, total 15.1054
+++model saved ! 2015.pth
(T) | Epoch=518, loss=5.4304, this epoch 0.0287, total 15.1341
(T) | Epoch=519, loss=5.4396, this epoch 0.0275, total 15.1617
(T) | Epoch=520, loss=5.4213, this epoch 0.0346, total 15.1962
(T) | Epoch=521, loss=5.6394, this epoch 0.0354, total 15.2316
(T) | Epoch=522, loss=5.4966, this epoch 0.0307, total 15.2623
(T) | Epoch=523, loss=5.4166, this epoch 0.0281, total 15.2904
(T) | Epoch=524, loss=5.5204, this epoch 0.0272, total 15.3176
(T) | Epoch=525, loss=5.4660, this epoch 0.0285, total 15.3461
(T) | Epoch=526, loss=5.4446, this epoch 0.0394, total 15.3855
(T) | Epoch=527, loss=5.5216, this epoch 0.0358, total 15.4213
(T) | Epoch=528, loss=5.4499, this epoch 0.0383, total 15.4595
(T) | Epoch=529, loss=5.4461, this epoch 0.0305, total 15.4901
(T) | Epoch=530, loss=5.7697, this epoch 0.0212, total 15.5112
(T) | Epoch=531, loss=5.4530, this epoch 0.0339, total 15.5451
(T) | Epoch=532, loss=5.4320, this epoch 0.0315, total 15.5766
(T) | Epoch=533, loss=5.4671, this epoch 0.0216, total 15.5982
(T) | Epoch=534, loss=5.4261, this epoch 0.0214, total 15.6196
(T) | Epoch=535, loss=5.4369, this epoch 0.0291, total 15.6487
(T) | Epoch=536, loss=5.4156, this epoch 0.0209, total 15.6696
(T) | Epoch=537, loss=5.4810, this epoch 0.0283, total 15.6979
(T) | Epoch=538, loss=5.4287, this epoch 0.0285, total 15.7263
(T) | Epoch=539, loss=5.4810, this epoch 0.0285, total 15.7548
(T) | Epoch=540, loss=5.4084, this epoch 0.0340, total 15.7888
(T) | Epoch=541, loss=5.4099, this epoch 0.0311, total 15.8200
(T) | Epoch=542, loss=5.4558, this epoch 0.0340, total 15.8539
(T) | Epoch=543, loss=5.4880, this epoch 0.0217, total 15.8756
(T) | Epoch=544, loss=5.3996, this epoch 0.0210, total 15.8966
+++model saved ! 2015.pth
(T) | Epoch=545, loss=5.4638, this epoch 0.0301, total 15.9267
(T) | Epoch=546, loss=5.4003, this epoch 0.0277, total 15.9544
(T) | Epoch=547, loss=5.4178, this epoch 0.0226, total 15.9770
(T) | Epoch=548, loss=5.4627, this epoch 0.0340, total 16.0110
(T) | Epoch=549, loss=5.4088, this epoch 0.0287, total 16.0397
(T) | Epoch=550, loss=5.4664, this epoch 0.0271, total 16.0668
(T) | Epoch=551, loss=5.4519, this epoch 0.0292, total 16.0960
(T) | Epoch=552, loss=5.4807, this epoch 0.0334, total 16.1295
(T) | Epoch=553, loss=5.4028, this epoch 0.0332, total 16.1626
(T) | Epoch=554, loss=5.4194, this epoch 0.0277, total 16.1903
(T) | Epoch=555, loss=5.4337, this epoch 0.0339, total 16.2242
(T) | Epoch=556, loss=5.4022, this epoch 0.0244, total 16.2486
(T) | Epoch=557, loss=5.4235, this epoch 0.0209, total 16.2695
(T) | Epoch=558, loss=5.3940, this epoch 0.0338, total 16.3033
+++model saved ! 2015.pth
(T) | Epoch=559, loss=5.4017, this epoch 0.0225, total 16.3259
(T) | Epoch=560, loss=5.4002, this epoch 0.0351, total 16.3609
(T) | Epoch=561, loss=5.4339, this epoch 0.0274, total 16.3884
(T) | Epoch=562, loss=5.4761, this epoch 0.0288, total 16.4171
(T) | Epoch=563, loss=5.4846, this epoch 0.0231, total 16.4402
(T) | Epoch=564, loss=5.4196, this epoch 0.0285, total 16.4688
(T) | Epoch=565, loss=5.3929, this epoch 0.0266, total 16.4954
+++model saved ! 2015.pth
(T) | Epoch=566, loss=5.4045, this epoch 0.0375, total 16.5328
(T) | Epoch=567, loss=5.3953, this epoch 0.0295, total 16.5624
(T) | Epoch=568, loss=5.4318, this epoch 0.0295, total 16.5919
(T) | Epoch=569, loss=5.4803, this epoch 0.0348, total 16.6267
(T) | Epoch=570, loss=5.4449, this epoch 0.0289, total 16.6556
(T) | Epoch=571, loss=5.3929, this epoch 0.0352, total 16.6908
(T) | Epoch=572, loss=5.4055, this epoch 0.0311, total 16.7219
(T) | Epoch=573, loss=5.5337, this epoch 0.0285, total 16.7504
(T) | Epoch=574, loss=5.4835, this epoch 0.0273, total 16.7776
(T) | Epoch=575, loss=5.3933, this epoch 0.0303, total 16.8080
(T) | Epoch=576, loss=5.4332, this epoch 0.0343, total 16.8422
(T) | Epoch=577, loss=5.4184, this epoch 0.0226, total 16.8648
(T) | Epoch=578, loss=5.4450, this epoch 0.0397, total 16.9045
(T) | Epoch=579, loss=5.4045, this epoch 0.0308, total 16.9353
(T) | Epoch=580, loss=5.4775, this epoch 0.0332, total 16.9685
(T) | Epoch=581, loss=5.5230, this epoch 0.0360, total 17.0045
(T) | Epoch=582, loss=5.3867, this epoch 0.0282, total 17.0327
+++model saved ! 2015.pth
(T) | Epoch=583, loss=5.4258, this epoch 0.0246, total 17.0573
(T) | Epoch=584, loss=5.4008, this epoch 0.0228, total 17.0802
(T) | Epoch=585, loss=5.4499, this epoch 0.0209, total 17.1010
(T) | Epoch=586, loss=5.3967, this epoch 0.0261, total 17.1271
(T) | Epoch=587, loss=5.3927, this epoch 0.0316, total 17.1587
(T) | Epoch=588, loss=5.3894, this epoch 0.0214, total 17.1800
(T) | Epoch=589, loss=5.3882, this epoch 0.0221, total 17.2021
(T) | Epoch=590, loss=5.4413, this epoch 0.0212, total 17.2233
(T) | Epoch=591, loss=5.4813, this epoch 0.0272, total 17.2505
(T) | Epoch=592, loss=5.4879, this epoch 0.0348, total 17.2853
(T) | Epoch=593, loss=5.4199, this epoch 0.0319, total 17.3172
(T) | Epoch=594, loss=5.4542, this epoch 0.0342, total 17.3514
(T) | Epoch=595, loss=5.4107, this epoch 0.0305, total 17.3819
(T) | Epoch=596, loss=5.4156, this epoch 0.0332, total 17.4151
(T) | Epoch=597, loss=5.4049, this epoch 0.0240, total 17.4391
(T) | Epoch=598, loss=5.4162, this epoch 0.0211, total 17.4602
(T) | Epoch=599, loss=5.4080, this epoch 0.0213, total 17.4815
(T) | Epoch=600, loss=5.3987, this epoch 0.0284, total 17.5099
(T) | Epoch=601, loss=5.4383, this epoch 0.0272, total 17.5371
(T) | Epoch=602, loss=5.4394, this epoch 0.0225, total 17.5596
(T) | Epoch=603, loss=5.4434, this epoch 0.0331, total 17.5927
(T) | Epoch=604, loss=5.3968, this epoch 0.0340, total 17.6267
(T) | Epoch=605, loss=5.4324, this epoch 0.0303, total 17.6570
(T) | Epoch=606, loss=5.3914, this epoch 0.0365, total 17.6935
(T) | Epoch=607, loss=5.4746, this epoch 0.0303, total 17.7238
(T) | Epoch=608, loss=5.4322, this epoch 0.0273, total 17.7510
(T) | Epoch=609, loss=5.4214, this epoch 0.0301, total 17.7811
(T) | Epoch=610, loss=5.3942, this epoch 0.0289, total 17.8101
(T) | Epoch=611, loss=5.3984, this epoch 0.0269, total 17.8370
(T) | Epoch=612, loss=5.4614, this epoch 0.0291, total 17.8661
(T) | Epoch=613, loss=5.3986, this epoch 0.0294, total 17.8955
(T) | Epoch=614, loss=5.3817, this epoch 0.0280, total 17.9235
+++model saved ! 2015.pth
(T) | Epoch=615, loss=5.4214, this epoch 0.0283, total 17.9519
(T) | Epoch=616, loss=5.3858, this epoch 0.0271, total 17.9790
(T) | Epoch=617, loss=5.4838, this epoch 0.0346, total 18.0136
(T) | Epoch=618, loss=5.4272, this epoch 0.0280, total 18.0416
(T) | Epoch=619, loss=5.4520, this epoch 0.0348, total 18.0765
(T) | Epoch=620, loss=5.3841, this epoch 0.0278, total 18.1043
(T) | Epoch=621, loss=5.4481, this epoch 0.0392, total 18.1435
(T) | Epoch=622, loss=5.3893, this epoch 0.0293, total 18.1727
(T) | Epoch=623, loss=5.4128, this epoch 0.0294, total 18.2022
(T) | Epoch=624, loss=5.4245, this epoch 0.0282, total 18.2303
(T) | Epoch=625, loss=5.4436, this epoch 0.0367, total 18.2670
(T) | Epoch=626, loss=5.4665, this epoch 0.0265, total 18.2935
(T) | Epoch=627, loss=5.4290, this epoch 0.0380, total 18.3316
(T) | Epoch=628, loss=5.4125, this epoch 0.0317, total 18.3632
(T) | Epoch=629, loss=5.5287, this epoch 0.0290, total 18.3922
(T) | Epoch=630, loss=5.4578, this epoch 0.0280, total 18.4202
(T) | Epoch=631, loss=5.4757, this epoch 0.0227, total 18.4429
(T) | Epoch=632, loss=5.4147, this epoch 0.0329, total 18.4758
(T) | Epoch=633, loss=5.4140, this epoch 0.0306, total 18.5064
(T) | Epoch=634, loss=5.3873, this epoch 0.0335, total 18.5399
(T) | Epoch=635, loss=5.3738, this epoch 0.0353, total 18.5752
+++model saved ! 2015.pth
(T) | Epoch=636, loss=5.4164, this epoch 0.0344, total 18.6095
(T) | Epoch=637, loss=5.4186, this epoch 0.0240, total 18.6335
(T) | Epoch=638, loss=5.3951, this epoch 0.0304, total 18.6639
(T) | Epoch=639, loss=5.3783, this epoch 0.0293, total 18.6932
(T) | Epoch=640, loss=5.4784, this epoch 0.0210, total 18.7142
(T) | Epoch=641, loss=5.3808, this epoch 0.0298, total 18.7440
(T) | Epoch=642, loss=5.4367, this epoch 0.0300, total 18.7739
(T) | Epoch=643, loss=5.3911, this epoch 0.0273, total 18.8013
(T) | Epoch=644, loss=5.4279, this epoch 0.0211, total 18.8224
(T) | Epoch=645, loss=5.4048, this epoch 0.0209, total 18.8433
(T) | Epoch=646, loss=5.3812, this epoch 0.0339, total 18.8772
(T) | Epoch=647, loss=5.3814, this epoch 0.0282, total 18.9054
(T) | Epoch=648, loss=5.3960, this epoch 0.0217, total 18.9271
(T) | Epoch=649, loss=5.3807, this epoch 0.0309, total 18.9580
(T) | Epoch=650, loss=5.3812, this epoch 0.0285, total 18.9865
(T) | Epoch=651, loss=5.3959, this epoch 0.0331, total 19.0197
(T) | Epoch=652, loss=5.3792, this epoch 0.0364, total 19.0561
(T) | Epoch=653, loss=5.4079, this epoch 0.0335, total 19.0895
(T) | Epoch=654, loss=5.4052, this epoch 0.0367, total 19.1263
(T) | Epoch=655, loss=5.4203, this epoch 0.0227, total 19.1490
(T) | Epoch=656, loss=5.4435, this epoch 0.0348, total 19.1838
(T) | Epoch=657, loss=5.4626, this epoch 0.0340, total 19.2178
(T) | Epoch=658, loss=5.3795, this epoch 0.0289, total 19.2467
(T) | Epoch=659, loss=5.4804, this epoch 0.0272, total 19.2739
(T) | Epoch=660, loss=5.4538, this epoch 0.0215, total 19.2954
(T) | Epoch=661, loss=5.4180, this epoch 0.0263, total 19.3218
(T) | Epoch=662, loss=5.4859, this epoch 0.0360, total 19.3578
(T) | Epoch=663, loss=5.3740, this epoch 0.0224, total 19.3802
(T) | Epoch=664, loss=5.4250, this epoch 0.0319, total 19.4121
(T) | Epoch=665, loss=5.4059, this epoch 0.0348, total 19.4469
(T) | Epoch=666, loss=5.3698, this epoch 0.0288, total 19.4756
+++model saved ! 2015.pth
(T) | Epoch=667, loss=5.4287, this epoch 0.0304, total 19.5060
(T) | Epoch=668, loss=5.4953, this epoch 0.0294, total 19.5354
(T) | Epoch=669, loss=5.3876, this epoch 0.0213, total 19.5567
(T) | Epoch=670, loss=5.3914, this epoch 0.0215, total 19.5782
(T) | Epoch=671, loss=5.3856, this epoch 0.0281, total 19.6063
(T) | Epoch=672, loss=5.3890, this epoch 0.0304, total 19.6368
(T) | Epoch=673, loss=5.3692, this epoch 0.0318, total 19.6686
+++model saved ! 2015.pth
(T) | Epoch=674, loss=5.4544, this epoch 0.0279, total 19.6965
(T) | Epoch=675, loss=5.3751, this epoch 0.0240, total 19.7205
(T) | Epoch=676, loss=5.4191, this epoch 0.0260, total 19.7465
(T) | Epoch=677, loss=5.3874, this epoch 0.0276, total 19.7740
(T) | Epoch=678, loss=5.3973, this epoch 0.0224, total 19.7964
(T) | Epoch=679, loss=5.3596, this epoch 0.0280, total 19.8244
+++model saved ! 2015.pth
(T) | Epoch=680, loss=5.4302, this epoch 0.0294, total 19.8538
(T) | Epoch=681, loss=5.4128, this epoch 0.0357, total 19.8896
(T) | Epoch=682, loss=5.3741, this epoch 0.0283, total 19.9178
(T) | Epoch=683, loss=5.3642, this epoch 0.0353, total 19.9531
(T) | Epoch=684, loss=5.3765, this epoch 0.0292, total 19.9823
(T) | Epoch=685, loss=5.4445, this epoch 0.0285, total 20.0109
(T) | Epoch=686, loss=5.3769, this epoch 0.0278, total 20.0386
(T) | Epoch=687, loss=5.3681, this epoch 0.0293, total 20.0679
(T) | Epoch=688, loss=5.3696, this epoch 0.0341, total 20.1020
(T) | Epoch=689, loss=5.4498, this epoch 0.0283, total 20.1303
(T) | Epoch=690, loss=5.4072, this epoch 0.0294, total 20.1597
(T) | Epoch=691, loss=5.4061, this epoch 0.0256, total 20.1853
(T) | Epoch=692, loss=5.3829, this epoch 0.0307, total 20.2159
(T) | Epoch=693, loss=5.3655, this epoch 0.0285, total 20.2444
(T) | Epoch=694, loss=5.3944, this epoch 0.0331, total 20.2775
(T) | Epoch=695, loss=5.5199, this epoch 0.0301, total 20.3077
(T) | Epoch=696, loss=5.4869, this epoch 0.0359, total 20.3435
(T) | Epoch=697, loss=5.3692, this epoch 0.0360, total 20.3796
(T) | Epoch=698, loss=5.3732, this epoch 0.0301, total 20.4097
(T) | Epoch=699, loss=5.4472, this epoch 0.0299, total 20.4396
(T) | Epoch=700, loss=5.4123, this epoch 0.0269, total 20.4665
(T) | Epoch=701, loss=5.4575, this epoch 0.0350, total 20.5015
(T) | Epoch=702, loss=5.5159, this epoch 0.0358, total 20.5373
(T) | Epoch=703, loss=5.4134, this epoch 0.0368, total 20.5740
(T) | Epoch=704, loss=5.4620, this epoch 0.0389, total 20.6130
(T) | Epoch=705, loss=5.4137, this epoch 0.0311, total 20.6440
(T) | Epoch=706, loss=5.4683, this epoch 0.0351, total 20.6791
(T) | Epoch=707, loss=5.5749, this epoch 0.0312, total 20.7103
(T) | Epoch=708, loss=5.5047, this epoch 0.0286, total 20.7389
(T) | Epoch=709, loss=5.3673, this epoch 0.0349, total 20.7738
(T) | Epoch=710, loss=5.3699, this epoch 0.0300, total 20.8038
(T) | Epoch=711, loss=5.4399, this epoch 0.0345, total 20.8383
(T) | Epoch=712, loss=5.3803, this epoch 0.0325, total 20.8708
(T) | Epoch=713, loss=5.3843, this epoch 0.0311, total 20.9019
(T) | Epoch=714, loss=5.3793, this epoch 0.0274, total 20.9293
(T) | Epoch=715, loss=5.3783, this epoch 0.0325, total 20.9618
(T) | Epoch=716, loss=5.4011, this epoch 0.0349, total 20.9967
(T) | Epoch=717, loss=5.3734, this epoch 0.0303, total 21.0270
(T) | Epoch=718, loss=5.4123, this epoch 0.0289, total 21.0558
(T) | Epoch=719, loss=5.4271, this epoch 0.0357, total 21.0915
(T) | Epoch=720, loss=5.4221, this epoch 0.0374, total 21.1289
(T) | Epoch=721, loss=5.3811, this epoch 0.0308, total 21.1597
(T) | Epoch=722, loss=5.4150, this epoch 0.0309, total 21.1906
(T) | Epoch=723, loss=5.4235, this epoch 0.0263, total 21.2170
(T) | Epoch=724, loss=5.4279, this epoch 0.0377, total 21.2546
(T) | Epoch=725, loss=5.3678, this epoch 0.0283, total 21.2829
(T) | Epoch=726, loss=5.4044, this epoch 0.0306, total 21.3135
(T) | Epoch=727, loss=5.3635, this epoch 0.0358, total 21.3493
(T) | Epoch=728, loss=5.5054, this epoch 0.0302, total 21.3795
(T) | Epoch=729, loss=5.3890, this epoch 0.0279, total 21.4074
(T) | Epoch=730, loss=5.4012, this epoch 0.0365, total 21.4439
(T) | Epoch=731, loss=5.4674, this epoch 0.0342, total 21.4781
(T) | Epoch=732, loss=5.3972, this epoch 0.0300, total 21.5080
(T) | Epoch=733, loss=5.4267, this epoch 0.0281, total 21.5361
(T) | Epoch=734, loss=5.4719, this epoch 0.0285, total 21.5646
(T) | Epoch=735, loss=5.4134, this epoch 0.0353, total 21.5999
(T) | Epoch=736, loss=5.3880, this epoch 0.0351, total 21.6351
(T) | Epoch=737, loss=5.3600, this epoch 0.0302, total 21.6653
(T) | Epoch=738, loss=5.3930, this epoch 0.0331, total 21.6984
(T) | Epoch=739, loss=5.3802, this epoch 0.0307, total 21.7291
(T) | Epoch=740, loss=5.3979, this epoch 0.0322, total 21.7613
(T) | Epoch=741, loss=5.4039, this epoch 0.0358, total 21.7971
(T) | Epoch=742, loss=5.3895, this epoch 0.0373, total 21.8344
(T) | Epoch=743, loss=5.4181, this epoch 0.0299, total 21.8643
(T) | Epoch=744, loss=5.3665, this epoch 0.0281, total 21.8925
(T) | Epoch=745, loss=5.4168, this epoch 0.0282, total 21.9207
(T) | Epoch=746, loss=5.3958, this epoch 0.0356, total 21.9563
(T) | Epoch=747, loss=5.4048, this epoch 0.0349, total 21.9912
(T) | Epoch=748, loss=5.4325, this epoch 0.0387, total 22.0299
(T) | Epoch=749, loss=5.3737, this epoch 0.0276, total 22.0575
(T) | Epoch=750, loss=5.4135, this epoch 0.0347, total 22.0922
(T) | Epoch=751, loss=5.3698, this epoch 0.0375, total 22.1297
(T) | Epoch=752, loss=5.4782, this epoch 0.0369, total 22.1667
(T) | Epoch=753, loss=5.4658, this epoch 0.0300, total 22.1967
(T) | Epoch=754, loss=5.3614, this epoch 0.0284, total 22.2251
(T) | Epoch=755, loss=5.4456, this epoch 0.0355, total 22.2606
(T) | Epoch=756, loss=5.4411, this epoch 0.0352, total 22.2958
(T) | Epoch=757, loss=5.3949, this epoch 0.0240, total 22.3198
(T) | Epoch=758, loss=5.4387, this epoch 0.0273, total 22.3471
(T) | Epoch=759, loss=5.4691, this epoch 0.0301, total 22.3772
(T) | Epoch=760, loss=5.4357, this epoch 0.0219, total 22.3991
(T) | Epoch=761, loss=5.3539, this epoch 0.0366, total 22.4357
+++model saved ! 2015.pth
(T) | Epoch=762, loss=5.4288, this epoch 0.0378, total 22.4735
(T) | Epoch=763, loss=5.3986, this epoch 0.0241, total 22.4976
(T) | Epoch=764, loss=5.3650, this epoch 0.0336, total 22.5312
(T) | Epoch=765, loss=5.3671, this epoch 0.0245, total 22.5557
(T) | Epoch=766, loss=5.4010, this epoch 0.0416, total 22.5973
(T) | Epoch=767, loss=5.3996, this epoch 0.0384, total 22.6357
(T) | Epoch=768, loss=5.4140, this epoch 0.0290, total 22.6647
(T) | Epoch=769, loss=5.3702, this epoch 0.0352, total 22.6999
(T) | Epoch=770, loss=5.4556, this epoch 0.0349, total 22.7348
(T) | Epoch=771, loss=5.3688, this epoch 0.0249, total 22.7596
(T) | Epoch=772, loss=5.3623, this epoch 0.0335, total 22.7931
(T) | Epoch=773, loss=5.4249, this epoch 0.0297, total 22.8228
(T) | Epoch=774, loss=5.3988, this epoch 0.0403, total 22.8632
(T) | Epoch=775, loss=5.3867, this epoch 0.0358, total 22.8990
(T) | Epoch=776, loss=5.3730, this epoch 0.0309, total 22.9299
(T) | Epoch=777, loss=5.4105, this epoch 0.0350, total 22.9649
(T) | Epoch=778, loss=5.3803, this epoch 0.0283, total 22.9932
(T) | Epoch=779, loss=5.3936, this epoch 0.0352, total 23.0284
(T) | Epoch=780, loss=5.3704, this epoch 0.0305, total 23.0590
(T) | Epoch=781, loss=5.4121, this epoch 0.0346, total 23.0935
(T) | Epoch=782, loss=5.3946, this epoch 0.0285, total 23.1221
(T) | Epoch=783, loss=5.3889, this epoch 0.0336, total 23.1557
(T) | Epoch=784, loss=5.3704, this epoch 0.0377, total 23.1934
(T) | Epoch=785, loss=5.4164, this epoch 0.0300, total 23.2234
(T) | Epoch=786, loss=5.4374, this epoch 0.0285, total 23.2519
(T) | Epoch=787, loss=5.3925, this epoch 0.0282, total 23.2801
(T) | Epoch=788, loss=5.3876, this epoch 0.0363, total 23.3164
(T) | Epoch=789, loss=5.3820, this epoch 0.0358, total 23.3522
(T) | Epoch=790, loss=5.3742, this epoch 0.0376, total 23.3898
(T) | Epoch=791, loss=5.3743, this epoch 0.0304, total 23.4202
(T) | Epoch=792, loss=5.4089, this epoch 0.0362, total 23.4563
(T) | Epoch=793, loss=5.3674, this epoch 0.0251, total 23.4814
(T) | Epoch=794, loss=5.3959, this epoch 0.0344, total 23.5158
(T) | Epoch=795, loss=5.4421, this epoch 0.0386, total 23.5544
(T) | Epoch=796, loss=5.4073, this epoch 0.0293, total 23.5837
(T) | Epoch=797, loss=5.3637, this epoch 0.0363, total 23.6201
(T) | Epoch=798, loss=5.3967, this epoch 0.0373, total 23.6574
(T) | Epoch=799, loss=5.4872, this epoch 0.0310, total 23.6884
(T) | Epoch=800, loss=5.3723, this epoch 0.0414, total 23.7298
(T) | Epoch=801, loss=5.3682, this epoch 0.0333, total 23.7631
(T) | Epoch=802, loss=5.4193, this epoch 0.0283, total 23.7914
(T) | Epoch=803, loss=5.4744, this epoch 0.0324, total 23.8238
(T) | Epoch=804, loss=5.3613, this epoch 0.0367, total 23.8605
(T) | Epoch=805, loss=5.4679, this epoch 0.0355, total 23.8960
(T) | Epoch=806, loss=5.3684, this epoch 0.0374, total 23.9334
(T) | Epoch=807, loss=5.3621, this epoch 0.0240, total 23.9574
(T) | Epoch=808, loss=5.3791, this epoch 0.0334, total 23.9907
(T) | Epoch=809, loss=5.4098, this epoch 0.0232, total 24.0140
(T) | Epoch=810, loss=5.4331, this epoch 0.0284, total 24.0424
(T) | Epoch=811, loss=5.4196, this epoch 0.0219, total 24.0643
(T) | Epoch=812, loss=5.3887, this epoch 0.0365, total 24.1008
(T) | Epoch=813, loss=5.4669, this epoch 0.0322, total 24.1330
(T) | Epoch=814, loss=5.3837, this epoch 0.0378, total 24.1708
(T) | Epoch=815, loss=5.4138, this epoch 0.0380, total 24.2088
(T) | Epoch=816, loss=5.3783, this epoch 0.0316, total 24.2404
(T) | Epoch=817, loss=5.4120, this epoch 0.0345, total 24.2748
(T) | Epoch=818, loss=5.3779, this epoch 0.0309, total 24.3057
(T) | Epoch=819, loss=5.4079, this epoch 0.0421, total 24.3478
(T) | Epoch=820, loss=5.3661, this epoch 0.0376, total 24.3854
(T) | Epoch=821, loss=5.3759, this epoch 0.0320, total 24.4174
(T) | Epoch=822, loss=5.3918, this epoch 0.0331, total 24.4505
(T) | Epoch=823, loss=5.3698, this epoch 0.0304, total 24.4810
(T) | Epoch=824, loss=5.3619, this epoch 0.0296, total 24.5105
(T) | Epoch=825, loss=5.4502, this epoch 0.0388, total 24.5493
(T) | Epoch=826, loss=5.3947, this epoch 0.0311, total 24.5804
(T) | Epoch=827, loss=5.3738, this epoch 0.0372, total 24.6177
(T) | Epoch=828, loss=5.3934, this epoch 0.0289, total 24.6466
(T) | Epoch=829, loss=5.3747, this epoch 0.0303, total 24.6768
(T) | Epoch=830, loss=5.4158, this epoch 0.0274, total 24.7042
(T) | Epoch=831, loss=5.3569, this epoch 0.0314, total 24.7356
(T) | Epoch=832, loss=5.3737, this epoch 0.0355, total 24.7711
(T) | Epoch=833, loss=5.4222, this epoch 0.0353, total 24.8063
(T) | Epoch=834, loss=5.3644, this epoch 0.0299, total 24.8363
(T) | Epoch=835, loss=5.6100, this epoch 0.0368, total 24.8731
(T) | Epoch=836, loss=5.3763, this epoch 0.0308, total 24.9039
(T) | Epoch=837, loss=5.4399, this epoch 0.0308, total 24.9347
(T) | Epoch=838, loss=5.4819, this epoch 0.0373, total 24.9720
(T) | Epoch=839, loss=5.4625, this epoch 0.0278, total 24.9998
(T) | Epoch=840, loss=5.4507, this epoch 0.0385, total 25.0383
(T) | Epoch=841, loss=5.4639, this epoch 0.0289, total 25.0672
(T) | Epoch=842, loss=5.5462, this epoch 0.0306, total 25.0979
(T) | Epoch=843, loss=5.4335, this epoch 0.0361, total 25.1340
(T) | Epoch=844, loss=5.4010, this epoch 0.0320, total 25.1660
(T) | Epoch=845, loss=5.4892, this epoch 0.0281, total 25.1941
(T) | Epoch=846, loss=5.3663, this epoch 0.0338, total 25.2280
(T) | Epoch=847, loss=5.4921, this epoch 0.0293, total 25.2573
(T) | Epoch=848, loss=5.3733, this epoch 0.0305, total 25.2878
(T) | Epoch=849, loss=5.3962, this epoch 0.0325, total 25.3203
(T) | Epoch=850, loss=5.3674, this epoch 0.0287, total 25.3490
(T) | Epoch=851, loss=5.4591, this epoch 0.0374, total 25.3864
(T) | Epoch=852, loss=5.4907, this epoch 0.0367, total 25.4230
(T) | Epoch=853, loss=5.4064, this epoch 0.0359, total 25.4589
(T) | Epoch=854, loss=5.4593, this epoch 0.0322, total 25.4911
(T) | Epoch=855, loss=5.3664, this epoch 0.0367, total 25.5278
(T) | Epoch=856, loss=5.4139, this epoch 0.0363, total 25.5641
(T) | Epoch=857, loss=5.4293, this epoch 0.0379, total 25.6019
(T) | Epoch=858, loss=5.3614, this epoch 0.0377, total 25.6397
(T) | Epoch=859, loss=5.3615, this epoch 0.0385, total 25.6782
(T) | Epoch=860, loss=5.4114, this epoch 0.0300, total 25.7082
(T) | Epoch=861, loss=5.3956, this epoch 0.0341, total 25.7423
(T) | Epoch=862, loss=5.4147, this epoch 0.0385, total 25.7808
(T) | Epoch=863, loss=5.4119, this epoch 0.0386, total 25.8194
(T) | Epoch=864, loss=5.4430, this epoch 0.0385, total 25.8579
(T) | Epoch=865, loss=5.4192, this epoch 0.0356, total 25.8935
(T) | Epoch=866, loss=5.4617, this epoch 0.0386, total 25.9321
(T) | Epoch=867, loss=5.3594, this epoch 0.0380, total 25.9702
(T) | Epoch=868, loss=5.3947, this epoch 0.0313, total 26.0015
(T) | Epoch=869, loss=5.3578, this epoch 0.0377, total 26.0393
(T) | Epoch=870, loss=5.3803, this epoch 0.0330, total 26.0722
(T) | Epoch=871, loss=5.4395, this epoch 0.0356, total 26.1078
(T) | Epoch=872, loss=5.5254, this epoch 0.0373, total 26.1451
(T) | Epoch=873, loss=5.4054, this epoch 0.0305, total 26.1756
(T) | Epoch=874, loss=5.3704, this epoch 0.0297, total 26.2052
(T) | Epoch=875, loss=5.3595, this epoch 0.0375, total 26.2428
(T) | Epoch=876, loss=5.3830, this epoch 0.0288, total 26.2716
(T) | Epoch=877, loss=5.3606, this epoch 0.0306, total 26.3021
(T) | Epoch=878, loss=5.3745, this epoch 0.0344, total 26.3366
(T) | Epoch=879, loss=5.4077, this epoch 0.0363, total 26.3729
(T) | Epoch=880, loss=5.4641, this epoch 0.0308, total 26.4036
(T) | Epoch=881, loss=5.3625, this epoch 0.0364, total 26.4400
(T) | Epoch=882, loss=5.5027, this epoch 0.0365, total 26.4765
(T) | Epoch=883, loss=5.4419, this epoch 0.0367, total 26.5132
(T) | Epoch=884, loss=5.4442, this epoch 0.0306, total 26.5437
(T) | Epoch=885, loss=5.4889, this epoch 0.0308, total 26.5745
(T) | Epoch=886, loss=5.3904, this epoch 0.0384, total 26.6129
(T) | Epoch=887, loss=5.3811, this epoch 0.0381, total 26.6510
(T) | Epoch=888, loss=5.3536, this epoch 0.0295, total 26.6805
+++model saved ! 2015.pth
(T) | Epoch=889, loss=5.3893, this epoch 0.0323, total 26.7128
(T) | Epoch=890, loss=5.3615, this epoch 0.0287, total 26.7415
(T) | Epoch=891, loss=5.3730, this epoch 0.0300, total 26.7715
(T) | Epoch=892, loss=5.4291, this epoch 0.0277, total 26.7992
(T) | Epoch=893, loss=5.4850, this epoch 0.0286, total 26.8278
(T) | Epoch=894, loss=5.3885, this epoch 0.0340, total 26.8617
(T) | Epoch=895, loss=5.4378, this epoch 0.0361, total 26.8978
(T) | Epoch=896, loss=5.4141, this epoch 0.0303, total 26.9281
(T) | Epoch=897, loss=5.4474, this epoch 0.0344, total 26.9625
(T) | Epoch=898, loss=5.3651, this epoch 0.0291, total 26.9916
(T) | Epoch=899, loss=5.3834, this epoch 0.0305, total 27.0221
(T) | Epoch=900, loss=5.3587, this epoch 0.0224, total 27.0445
(T) | Epoch=901, loss=5.4534, this epoch 0.0276, total 27.0721
(T) | Epoch=902, loss=5.3763, this epoch 0.0220, total 27.0940
(T) | Epoch=903, loss=5.3644, this epoch 0.0299, total 27.1239
(T) | Epoch=904, loss=5.4079, this epoch 0.0343, total 27.1582
(T) | Epoch=905, loss=5.4267, this epoch 0.0303, total 27.1885
(T) | Epoch=906, loss=5.3807, this epoch 0.0216, total 27.2101
(T) | Epoch=907, loss=5.4625, this epoch 0.0276, total 27.2378
(T) | Epoch=908, loss=5.3605, this epoch 0.0343, total 27.2721
(T) | Epoch=909, loss=5.3567, this epoch 0.0302, total 27.3023
(T) | Epoch=910, loss=5.4293, this epoch 0.0288, total 27.3310
(T) | Epoch=911, loss=5.3916, this epoch 0.0283, total 27.3593
(T) | Epoch=912, loss=5.3558, this epoch 0.0351, total 27.3944
(T) | Epoch=913, loss=5.3560, this epoch 0.0305, total 27.4249
(T) | Epoch=914, loss=5.3572, this epoch 0.0288, total 27.4537
(T) | Epoch=915, loss=5.3912, this epoch 0.0338, total 27.4875
(T) | Epoch=916, loss=5.3535, this epoch 0.0308, total 27.5183
+++model saved ! 2015.pth
(T) | Epoch=917, loss=5.3664, this epoch 0.0302, total 27.5485
(T) | Epoch=918, loss=5.3662, this epoch 0.0290, total 27.5775
(T) | Epoch=919, loss=5.4028, this epoch 0.0298, total 27.6073
(T) | Epoch=920, loss=5.4187, this epoch 0.0282, total 27.6355
(T) | Epoch=921, loss=5.3978, this epoch 0.0286, total 27.6641
(T) | Epoch=922, loss=5.3634, this epoch 0.0288, total 27.6930
(T) | Epoch=923, loss=5.4691, this epoch 0.0312, total 27.7242
(T) | Epoch=924, loss=5.3935, this epoch 0.0288, total 27.7531
(T) | Epoch=925, loss=5.3615, this epoch 0.0344, total 27.7875
(T) | Epoch=926, loss=5.4377, this epoch 0.0368, total 27.8243
(T) | Epoch=927, loss=5.3582, this epoch 0.0395, total 27.8638
(T) | Epoch=928, loss=5.4260, this epoch 0.0366, total 27.9004
(T) | Epoch=929, loss=5.3660, this epoch 0.0335, total 27.9340
(T) | Epoch=930, loss=5.3632, this epoch 0.0371, total 27.9711
(T) | Epoch=931, loss=5.3678, this epoch 0.0286, total 27.9997
(T) | Epoch=932, loss=5.4873, this epoch 0.0285, total 28.0282
(T) | Epoch=933, loss=5.3579, this epoch 0.0287, total 28.0569
(T) | Epoch=934, loss=5.4084, this epoch 0.0364, total 28.0933
(T) | Epoch=935, loss=5.3909, this epoch 0.0306, total 28.1239
(T) | Epoch=936, loss=5.5196, this epoch 0.0341, total 28.1580
(T) | Epoch=937, loss=5.3984, this epoch 0.0385, total 28.1966
(T) | Epoch=938, loss=5.3748, this epoch 0.0312, total 28.2277
(T) | Epoch=939, loss=5.3695, this epoch 0.0384, total 28.2661
(T) | Epoch=940, loss=5.4638, this epoch 0.0358, total 28.3019
(T) | Epoch=941, loss=5.3916, this epoch 0.0304, total 28.3323
(T) | Epoch=942, loss=5.4531, this epoch 0.0363, total 28.3686
(T) | Epoch=943, loss=5.3814, this epoch 0.0318, total 28.4004
(T) | Epoch=944, loss=5.4165, this epoch 0.0408, total 28.4412
(T) | Epoch=945, loss=5.4232, this epoch 0.0379, total 28.4792
(T) | Epoch=946, loss=5.3983, this epoch 0.0378, total 28.5169
(T) | Epoch=947, loss=5.3631, this epoch 0.0384, total 28.5553
(T) | Epoch=948, loss=5.3577, this epoch 0.0292, total 28.5845
(T) | Epoch=949, loss=5.4128, this epoch 0.0304, total 28.6149
(T) | Epoch=950, loss=5.4152, this epoch 0.0310, total 28.6459
(T) | Epoch=951, loss=5.5175, this epoch 0.0292, total 28.6751
(T) | Epoch=952, loss=5.4276, this epoch 0.0284, total 28.7035
(T) | Epoch=953, loss=5.3794, this epoch 0.0296, total 28.7331
(T) | Epoch=954, loss=5.3561, this epoch 0.0287, total 28.7618
(T) | Epoch=955, loss=5.5146, this epoch 0.0336, total 28.7954
(T) | Epoch=956, loss=5.4424, this epoch 0.0301, total 28.8254
(T) | Epoch=957, loss=5.4077, this epoch 0.0354, total 28.8608
(T) | Epoch=958, loss=5.3551, this epoch 0.0389, total 28.8997
(T) | Epoch=959, loss=5.3544, this epoch 0.0357, total 28.9355
(T) | Epoch=960, loss=5.3548, this epoch 0.0380, total 28.9735
(T) | Epoch=961, loss=5.3850, this epoch 0.0305, total 29.0040
(T) | Epoch=962, loss=5.3611, this epoch 0.0287, total 29.0327
(T) | Epoch=963, loss=5.3906, this epoch 0.0340, total 29.0666
(T) | Epoch=964, loss=5.4360, this epoch 0.0337, total 29.1003
(T) | Epoch=965, loss=5.3804, this epoch 0.0409, total 29.1412
(T) | Epoch=966, loss=5.3631, this epoch 0.0302, total 29.1714
(T) | Epoch=967, loss=5.3901, this epoch 0.0365, total 29.2080
(T) | Epoch=968, loss=5.3902, this epoch 0.0302, total 29.2382
(T) | Epoch=969, loss=5.7143, this epoch 0.0313, total 29.2695
(T) | Epoch=970, loss=5.3718, this epoch 0.0321, total 29.3016
(T) | Epoch=971, loss=5.3679, this epoch 0.0330, total 29.3346
(T) | Epoch=972, loss=5.3615, this epoch 0.0384, total 29.3730
(T) | Epoch=973, loss=5.3878, this epoch 0.0303, total 29.4033
(T) | Epoch=974, loss=5.4083, this epoch 0.0364, total 29.4396
(T) | Epoch=975, loss=5.3535, this epoch 0.0381, total 29.4777
(T) | Epoch=976, loss=5.4206, this epoch 0.0380, total 29.5157
(T) | Epoch=977, loss=5.4463, this epoch 0.0370, total 29.5527
(T) | Epoch=978, loss=5.3644, this epoch 0.0336, total 29.5864
(T) | Epoch=979, loss=5.4362, this epoch 0.0353, total 29.6216
(T) | Epoch=980, loss=5.3540, this epoch 0.0364, total 29.6581
(T) | Epoch=981, loss=5.4379, this epoch 0.0384, total 29.6964
(T) | Epoch=982, loss=5.3980, this epoch 0.0381, total 29.7346
(T) | Epoch=983, loss=5.3550, this epoch 0.0361, total 29.7706
(T) | Epoch=984, loss=5.3559, this epoch 0.0305, total 29.8012
(T) | Epoch=985, loss=5.3511, this epoch 0.0368, total 29.8379
+++model saved ! 2015.pth
(T) | Epoch=986, loss=5.3598, this epoch 0.0402, total 29.8781
(T) | Epoch=987, loss=5.4850, this epoch 0.0323, total 29.9104
(T) | Epoch=988, loss=5.4163, this epoch 0.0285, total 29.9389
(T) | Epoch=989, loss=5.3497, this epoch 0.0421, total 29.9810
+++model saved ! 2015.pth
(T) | Epoch=990, loss=5.3871, this epoch 0.0397, total 30.0207
(T) | Epoch=991, loss=5.4118, this epoch 0.0361, total 30.0568
(T) | Epoch=992, loss=5.4212, this epoch 0.0285, total 30.0854
(T) | Epoch=993, loss=5.4191, this epoch 0.0359, total 30.1213
(T) | Epoch=994, loss=5.4329, this epoch 0.0281, total 30.1494
(T) | Epoch=995, loss=5.3584, this epoch 0.0372, total 30.1866
(T) | Epoch=996, loss=5.3596, this epoch 0.0299, total 30.2164
(T) | Epoch=997, loss=5.4289, this epoch 0.0368, total 30.2532
(T) | Epoch=998, loss=5.3952, this epoch 0.0375, total 30.2907
(T) | Epoch=999, loss=5.4049, this epoch 0.0370, total 30.3277
(T) | Epoch=1000, loss=5.4135, this epoch 0.0279, total 30.3556
=== Final ===

==============================
LoRA FINE-TUNING
==============================
Random seed set to 0
Epoch: 0, loss: 33.6533, train_acc: 0.0000, train_recall: 0.0000, train_f1: 0.0000, val_acc: 0.000000, val_recall: 0.000000, val_f1: 0.000000
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed0.csv
Epoch: 1, loss: 34.9806, train_acc: 0.2977, train_recall: 0.1111, train_f1: 0.0510, val_acc: 0.332512, val_recall: 0.125000, val_f1: 0.062384
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed0.csv
Epoch: 2, loss: 60.6312, train_acc: 0.2193, train_recall: 0.1111, train_f1: 0.0400, val_acc: 0.226601, val_recall: 0.125000, val_f1: 0.046185
Epoch: 3, loss: 109.8171, train_acc: 0.3337, train_recall: 0.1111, train_f1: 0.0556, val_acc: 0.285714, val_recall: 0.125000, val_f1: 0.055556
Epoch: 4, loss: 172.0105, train_acc: 0.0572, train_recall: 0.1111, train_f1: 0.0120, val_acc: 0.081281, val_recall: 0.111111, val_f1: 0.016743
Epoch: 5, loss: 201.7050, train_acc: 0.0021, train_recall: 0.1111, train_f1: 0.0005, val_acc: 0.000000, val_recall: 0.000000, val_f1: 0.000000
Epoch: 6, loss: 137.0586, train_acc: 0.0021, train_recall: 0.1111, train_f1: 0.0005, val_acc: 0.000000, val_recall: 0.000000, val_f1: 0.000000
Epoch: 7, loss: 104.9867, train_acc: 0.3337, train_recall: 0.1111, train_f1: 0.0556, val_acc: 0.285714, val_recall: 0.125000, val_f1: 0.055556
Epoch: 8, loss: 85.7038, train_acc: 0.3337, train_recall: 0.1111, train_f1: 0.0556, val_acc: 0.285714, val_recall: 0.125000, val_f1: 0.055556
Epoch: 9, loss: 84.5444, train_acc: 0.2193, train_recall: 0.1111, train_f1: 0.0400, val_acc: 0.226601, val_recall: 0.125000, val_f1: 0.046185
Epoch: 10, loss: 89.7140, train_acc: 0.0233, train_recall: 0.1111, train_f1: 0.0051, val_acc: 0.012315, val_recall: 0.125000, val_f1: 0.003041
Epoch: 11, loss: 86.7712, train_acc: 0.0371, train_recall: 0.1111, train_f1: 0.0079, val_acc: 0.036946, val_recall: 0.125000, val_f1: 0.008907
Epoch: 12, loss: 70.6567, train_acc: 0.2977, train_recall: 0.1111, train_f1: 0.0510, val_acc: 0.332512, val_recall: 0.125000, val_f1: 0.062384
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed0.csv
Epoch: 13, loss: 76.5863, train_acc: 0.2977, train_recall: 0.1111, train_f1: 0.0510, val_acc: 0.332512, val_recall: 0.125000, val_f1: 0.062384
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed0.csv
Epoch: 14, loss: 68.8441, train_acc: 0.2193, train_recall: 0.1111, train_f1: 0.0400, val_acc: 0.226601, val_recall: 0.125000, val_f1: 0.046185
Epoch: 15, loss: 63.3357, train_acc: 0.3008, train_recall: 0.1127, train_f1: 0.0543, val_acc: 0.334975, val_recall: 0.126359, val_f1: 0.065544
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed0.csv
Epoch: 16, loss: 57.4720, train_acc: 0.2193, train_recall: 0.1111, train_f1: 0.0400, val_acc: 0.226601, val_recall: 0.125000, val_f1: 0.046185
Epoch: 17, loss: 75.0054, train_acc: 0.3337, train_recall: 0.1111, train_f1: 0.0556, val_acc: 0.285714, val_recall: 0.125000, val_f1: 0.055556
Epoch: 18, loss: 83.3402, train_acc: 0.3337, train_recall: 0.1111, train_f1: 0.0556, val_acc: 0.285714, val_recall: 0.125000, val_f1: 0.055556
Epoch: 19, loss: 80.9715, train_acc: 0.3337, train_recall: 0.1111, train_f1: 0.0556, val_acc: 0.285714, val_recall: 0.125000, val_f1: 0.055556
Epoch: 20, loss: 69.8858, train_acc: 0.3337, train_recall: 0.1111, train_f1: 0.0556, val_acc: 0.285714, val_recall: 0.125000, val_f1: 0.055556
Epoch: 21, loss: 60.9002, train_acc: 0.2987, train_recall: 0.1115, train_f1: 0.0517, val_acc: 0.334975, val_recall: 0.126078, val_f1: 0.064637
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed0.csv
Epoch: 22, loss: 61.7801, train_acc: 0.2977, train_recall: 0.1111, train_f1: 0.0510, val_acc: 0.332512, val_recall: 0.125000, val_f1: 0.062384
Epoch: 23, loss: 53.1361, train_acc: 0.3072, train_recall: 0.1162, train_f1: 0.0622, val_acc: 0.342365, val_recall: 0.130868, val_f1: 0.075343
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed0.csv
Epoch: 24, loss: 64.5587, train_acc: 0.2193, train_recall: 0.1111, train_f1: 0.0400, val_acc: 0.226601, val_recall: 0.125000, val_f1: 0.046185
Epoch: 25, loss: 63.1901, train_acc: 0.2193, train_recall: 0.1111, train_f1: 0.0400, val_acc: 0.226601, val_recall: 0.125000, val_f1: 0.046185
Epoch: 26, loss: 50.4475, train_acc: 0.2203, train_recall: 0.1115, train_f1: 0.0407, val_acc: 0.229064, val_recall: 0.126078, val_f1: 0.048414
Epoch: 27, loss: 57.6119, train_acc: 0.3347, train_recall: 0.1152, train_f1: 0.0636, val_acc: 0.285714, val_recall: 0.125000, val_f1: 0.055556
Epoch: 28, loss: 59.2957, train_acc: 0.3347, train_recall: 0.1152, train_f1: 0.0636, val_acc: 0.285714, val_recall: 0.125000, val_f1: 0.055556
Epoch: 29, loss: 88.8643, train_acc: 0.0286, train_recall: 0.1111, train_f1: 0.0062, val_acc: 0.022167, val_recall: 0.125000, val_f1: 0.005422
Epoch: 30, loss: 64.0611, train_acc: 0.0424, train_recall: 0.1157, train_f1: 0.0149, val_acc: 0.034483, val_recall: 0.130388, val_f1: 0.015412
Epoch: 31, loss: 49.5049, train_acc: 0.3347, train_recall: 0.1152, train_f1: 0.0636, val_acc: 0.285714, val_recall: 0.125000, val_f1: 0.055556
Epoch: 32, loss: 58.2770, train_acc: 0.2977, train_recall: 0.1111, train_f1: 0.0510, val_acc: 0.332512, val_recall: 0.125000, val_f1: 0.062384
Epoch: 33, loss: 60.6135, train_acc: 0.2977, train_recall: 0.1111, train_f1: 0.0510, val_acc: 0.332512, val_recall: 0.125000, val_f1: 0.062384
Epoch: 34, loss: 55.1501, train_acc: 0.2987, train_recall: 0.1115, train_f1: 0.0518, val_acc: 0.334975, val_recall: 0.126078, val_f1: 0.064637
Epoch: 35, loss: 56.4658, train_acc: 0.0710, train_recall: 0.1157, train_f1: 0.0208, val_acc: 0.096059, val_recall: 0.131466, val_f1: 0.030950
Epoch: 36, loss: 52.8220, train_acc: 0.3337, train_recall: 0.1111, train_f1: 0.0556, val_acc: 0.285714, val_recall: 0.125000, val_f1: 0.055556
Epoch: 37, loss: 52.5032, train_acc: 0.3337, train_recall: 0.1111, train_f1: 0.0556, val_acc: 0.285714, val_recall: 0.125000, val_f1: 0.055556
Epoch: 38, loss: 45.9789, train_acc: 0.3337, train_recall: 0.1111, train_f1: 0.0556, val_acc: 0.285714, val_recall: 0.125000, val_f1: 0.055556
Epoch: 39, loss: 38.6454, train_acc: 0.2193, train_recall: 0.1111, train_f1: 0.0400, val_acc: 0.226601, val_recall: 0.125000, val_f1: 0.046185
Epoch: 40, loss: 41.1863, train_acc: 0.2193, train_recall: 0.1111, train_f1: 0.0400, val_acc: 0.226601, val_recall: 0.125000, val_f1: 0.046185
Epoch: 41, loss: 47.2614, train_acc: 0.2977, train_recall: 0.1111, train_f1: 0.0510, val_acc: 0.332512, val_recall: 0.125000, val_f1: 0.062384
Epoch: 42, loss: 47.6014, train_acc: 0.2977, train_recall: 0.1111, train_f1: 0.0510, val_acc: 0.332512, val_recall: 0.125000, val_f1: 0.062384
Epoch: 43, loss: 41.0670, train_acc: 0.2987, train_recall: 0.1115, train_f1: 0.0518, val_acc: 0.334975, val_recall: 0.126078, val_f1: 0.064637
Epoch: 44, loss: 35.2783, train_acc: 0.3337, train_recall: 0.1111, train_f1: 0.0569, val_acc: 0.285714, val_recall: 0.125000, val_f1: 0.056311
Epoch: 45, loss: 38.3776, train_acc: 0.3337, train_recall: 0.1111, train_f1: 0.0557, val_acc: 0.285714, val_recall: 0.125000, val_f1: 0.055556
Epoch: 46, loss: 40.8562, train_acc: 0.2320, train_recall: 0.1490, train_f1: 0.0789, val_acc: 0.238916, val_recall: 0.166667, val_f1: 0.092066
Epoch: 47, loss: 48.8063, train_acc: 0.0371, train_recall: 0.1111, train_f1: 0.0080, val_acc: 0.036946, val_recall: 0.125000, val_f1: 0.008907
Epoch: 48, loss: 43.6531, train_acc: 0.0710, train_recall: 0.1157, train_f1: 0.0208, val_acc: 0.096059, val_recall: 0.131466, val_f1: 0.030950
Epoch: 49, loss: 42.6932, train_acc: 0.3337, train_recall: 0.1111, train_f1: 0.0557, val_acc: 0.285714, val_recall: 0.125000, val_f1: 0.055556
Epoch: 50, loss: 40.3463, train_acc: 0.3337, train_recall: 0.1111, train_f1: 0.0556, val_acc: 0.285714, val_recall: 0.125000, val_f1: 0.055556
Epoch: 51, loss: 34.9267, train_acc: 0.2225, train_recall: 0.1105, train_f1: 0.0469, val_acc: 0.229064, val_recall: 0.124953, val_f1: 0.055003
Epoch: 52, loss: 42.2573, train_acc: 0.2987, train_recall: 0.1115, train_f1: 0.0517, val_acc: 0.334975, val_recall: 0.126078, val_f1: 0.064637
Epoch: 53, loss: 44.3817, train_acc: 0.2987, train_recall: 0.1115, train_f1: 0.0517, val_acc: 0.334975, val_recall: 0.126078, val_f1: 0.064637
Epoch: 54, loss: 39.8119, train_acc: 0.3093, train_recall: 0.1149, train_f1: 0.0602, val_acc: 0.342365, val_recall: 0.129462, val_f1: 0.073009
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed0.csv
Epoch: 55, loss: 36.8869, train_acc: 0.3337, train_recall: 0.1111, train_f1: 0.0556, val_acc: 0.285714, val_recall: 0.125000, val_f1: 0.055556
Epoch: 56, loss: 40.9354, train_acc: 0.2203, train_recall: 0.1115, train_f1: 0.0407, val_acc: 0.229064, val_recall: 0.126078, val_f1: 0.048414
Epoch: 57, loss: 39.2742, train_acc: 0.3337, train_recall: 0.1111, train_f1: 0.0570, val_acc: 0.285714, val_recall: 0.125000, val_f1: 0.056311
Epoch: 58, loss: 37.5108, train_acc: 0.3337, train_recall: 0.1111, train_f1: 0.0570, val_acc: 0.285714, val_recall: 0.125000, val_f1: 0.056311
Epoch: 59, loss: 35.4248, train_acc: 0.2203, train_recall: 0.1115, train_f1: 0.0407, val_acc: 0.231527, val_recall: 0.127155, val_f1: 0.050608
Epoch: 60, loss: 32.0410, train_acc: 0.3167, train_recall: 0.1596, train_f1: 0.1011, val_acc: 0.339901, val_recall: 0.152762, val_f1: 0.095421
Epoch: 61, loss: 32.6868, train_acc: 0.3189, train_recall: 0.1602, train_f1: 0.1015, val_acc: 0.339901, val_recall: 0.152762, val_f1: 0.091661
Epoch: 62, loss: 33.7215, train_acc: 0.3411, train_recall: 0.1700, train_f1: 0.0936, val_acc: 0.278325, val_recall: 0.145690, val_f1: 0.065259
Epoch: 63, loss: 36.5747, train_acc: 0.0233, train_recall: 0.1111, train_f1: 0.0051, val_acc: 0.014778, val_recall: 0.126078, val_f1: 0.005165
Epoch: 64, loss: 34.8311, train_acc: 0.0710, train_recall: 0.1157, train_f1: 0.0208, val_acc: 0.096059, val_recall: 0.131466, val_f1: 0.030994
Epoch: 65, loss: 30.3576, train_acc: 0.3347, train_recall: 0.1115, train_f1: 0.0592, val_acc: 0.288177, val_recall: 0.125926, val_f1: 0.059073
Epoch: 66, loss: 30.6979, train_acc: 0.3136, train_recall: 0.1276, train_f1: 0.0797, val_acc: 0.344828, val_recall: 0.145203, val_f1: 0.099415
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed0.csv
Epoch: 67, loss: 30.3026, train_acc: 0.2320, train_recall: 0.1415, train_f1: 0.0830, val_acc: 0.231527, val_recall: 0.139980, val_f1: 0.079346
Epoch: 68, loss: 31.8020, train_acc: 0.2331, train_recall: 0.1421, train_f1: 0.0831, val_acc: 0.233990, val_recall: 0.141620, val_f1: 0.078403
Epoch: 69, loss: 32.6617, train_acc: 0.3422, train_recall: 0.1422, train_f1: 0.0894, val_acc: 0.290640, val_recall: 0.156178, val_f1: 0.091706
Epoch: 70, loss: 31.1851, train_acc: 0.3093, train_recall: 0.1486, train_f1: 0.0833, val_acc: 0.342365, val_recall: 0.180707, val_f1: 0.109906
Epoch: 71, loss: 30.9773, train_acc: 0.3019, train_recall: 0.1544, train_f1: 0.0803, val_acc: 0.337438, val_recall: 0.178855, val_f1: 0.102421
Epoch: 72, loss: 31.8024, train_acc: 0.2172, train_recall: 0.1417, train_f1: 0.0643, val_acc: 0.226601, val_recall: 0.173822, val_f1: 0.086693
Epoch: 73, loss: 32.1665, train_acc: 0.3422, train_recall: 0.1478, train_f1: 0.0889, val_acc: 0.293103, val_recall: 0.179023, val_f1: 0.105777
Epoch: 74, loss: 31.0248, train_acc: 0.2331, train_recall: 0.1481, train_f1: 0.0796, val_acc: 0.236453, val_recall: 0.178695, val_f1: 0.100243
Epoch: 75, loss: 30.5561, train_acc: 0.3167, train_recall: 0.1512, train_f1: 0.0926, val_acc: 0.349754, val_recall: 0.183940, val_f1: 0.124933
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed0.csv
Epoch: 76, loss: 30.2303, train_acc: 0.3432, train_recall: 0.1481, train_f1: 0.0928, val_acc: 0.293103, val_recall: 0.164511, val_f1: 0.098102
Epoch: 77, loss: 29.1581, train_acc: 0.3422, train_recall: 0.1479, train_f1: 0.0949, val_acc: 0.290640, val_recall: 0.163282, val_f1: 0.099427
Epoch: 78, loss: 28.4047, train_acc: 0.2320, train_recall: 0.1490, train_f1: 0.0775, val_acc: 0.236453, val_recall: 0.165308, val_f1: 0.088768
Epoch: 79, loss: 27.2597, train_acc: 0.3114, train_recall: 0.1503, train_f1: 0.1006, val_acc: 0.349754, val_recall: 0.185238, val_f1: 0.132175
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed0.csv
Epoch: 80, loss: 27.0443, train_acc: 0.3496, train_recall: 0.1587, train_f1: 0.0994, val_acc: 0.300493, val_recall: 0.182256, val_f1: 0.111618
Epoch: 81, loss: 28.0326, train_acc: 0.0710, train_recall: 0.1158, train_f1: 0.0214, val_acc: 0.093596, val_recall: 0.130236, val_f1: 0.029170
Epoch: 82, loss: 27.1208, train_acc: 0.3157, train_recall: 0.1509, train_f1: 0.0925, val_acc: 0.349754, val_recall: 0.184221, val_f1: 0.124047
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed0.csv
Epoch: 83, loss: 26.8471, train_acc: 0.2352, train_recall: 0.1576, train_f1: 0.0937, val_acc: 0.246305, val_recall: 0.184130, val_f1: 0.110294
Epoch: 84, loss: 28.6470, train_acc: 0.3464, train_recall: 0.1567, train_f1: 0.1082, val_acc: 0.290640, val_recall: 0.163434, val_f1: 0.100473
Epoch: 85, loss: 26.9979, train_acc: 0.3443, train_recall: 0.1664, train_f1: 0.1127, val_acc: 0.290640, val_recall: 0.200168, val_f1: 0.131258
Epoch: 86, loss: 28.4080, train_acc: 0.3093, train_recall: 0.1722, train_f1: 0.0996, val_acc: 0.337438, val_recall: 0.191667, val_f1: 0.100294
Epoch: 87, loss: 29.8541, train_acc: 0.2235, train_recall: 0.1669, train_f1: 0.0732, val_acc: 0.231527, val_recall: 0.215429, val_f1: 0.091346
Epoch: 88, loss: 30.0387, train_acc: 0.0413, train_recall: 0.1159, train_f1: 0.0150, val_acc: 0.034483, val_recall: 0.129630, val_f1: 0.014204
Epoch: 89, loss: 28.2043, train_acc: 0.3178, train_recall: 0.1744, train_f1: 0.1185, val_acc: 0.349754, val_recall: 0.206854, val_f1: 0.144794
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed0.csv
Epoch: 90, loss: 25.8156, train_acc: 0.2468, train_recall: 0.1681, train_f1: 0.1107, val_acc: 0.236453, val_recall: 0.177852, val_f1: 0.109872
Epoch: 91, loss: 28.5750, train_acc: 0.3485, train_recall: 0.1574, train_f1: 0.1107, val_acc: 0.298030, val_recall: 0.166667, val_f1: 0.103159
Epoch: 92, loss: 26.8671, train_acc: 0.3464, train_recall: 0.1492, train_f1: 0.0948, val_acc: 0.298030, val_recall: 0.166667, val_f1: 0.103159
Epoch: 93, loss: 27.9172, train_acc: 0.2299, train_recall: 0.1429, train_f1: 0.0758, val_acc: 0.231527, val_recall: 0.141667, val_f1: 0.069570
Epoch: 94, loss: 30.4750, train_acc: 0.3072, train_recall: 0.1162, train_f1: 0.0638, val_acc: 0.342365, val_recall: 0.130868, val_f1: 0.076683
Epoch: 95, loss: 30.2552, train_acc: 0.3051, train_recall: 0.1154, train_f1: 0.0639, val_acc: 0.337438, val_recall: 0.129016, val_f1: 0.076715
Epoch: 96, loss: 32.2975, train_acc: 0.0731, train_recall: 0.1166, train_f1: 0.0227, val_acc: 0.096059, val_recall: 0.131747, val_f1: 0.031852
Epoch: 97, loss: 25.8526, train_acc: 0.3538, train_recall: 0.1743, train_f1: 0.1101, val_acc: 0.305419, val_recall: 0.198922, val_f1: 0.123334
Epoch: 98, loss: 26.6099, train_acc: 0.2383, train_recall: 0.1493, train_f1: 0.0883, val_acc: 0.241379, val_recall: 0.166058, val_f1: 0.105323
Epoch: 99, loss: 26.3519, train_acc: 0.2362, train_recall: 0.1535, train_f1: 0.0961, val_acc: 0.241379, val_recall: 0.166339, val_f1: 0.103915
Epoch: 100, loss: 27.3774, train_acc: 0.3199, train_recall: 0.1663, train_f1: 0.1118, val_acc: 0.344828, val_recall: 0.176684, val_f1: 0.131492
Epoch: 101, loss: 27.6629, train_acc: 0.3453, train_recall: 0.1629, train_f1: 0.1074, val_acc: 0.295566, val_recall: 0.175000, val_f1: 0.117058
Epoch: 102, loss: 27.1260, train_acc: 0.3157, train_recall: 0.1645, train_f1: 0.1127, val_acc: 0.334975, val_recall: 0.158317, val_f1: 0.104762
Epoch: 103, loss: 26.5880, train_acc: 0.2436, train_recall: 0.1969, train_f1: 0.1280, val_acc: 0.238916, val_recall: 0.182209, val_f1: 0.115005
Epoch: 104, loss: 27.0940, train_acc: 0.2362, train_recall: 0.1814, train_f1: 0.1022, val_acc: 0.231527, val_recall: 0.151078, val_f1: 0.062718
Epoch: 105, loss: 26.9927, train_acc: 0.3453, train_recall: 0.1761, train_f1: 0.0982, val_acc: 0.280788, val_recall: 0.146767, val_f1: 0.065542
Epoch: 106, loss: 27.5891, train_acc: 0.3136, train_recall: 0.1771, train_f1: 0.0909, val_acc: 0.332512, val_recall: 0.149529, val_f1: 0.078584
Epoch: 107, loss: 26.8742, train_acc: 0.3358, train_recall: 0.1729, train_f1: 0.0863, val_acc: 0.275862, val_recall: 0.144612, val_f1: 0.063545
Epoch: 108, loss: 25.6511, train_acc: 0.3538, train_recall: 0.1968, train_f1: 0.1373, val_acc: 0.290640, val_recall: 0.165589, val_f1: 0.094685
Epoch: 109, loss: 26.5891, train_acc: 0.2331, train_recall: 0.1758, train_f1: 0.0952, val_acc: 0.229064, val_recall: 0.150000, val_f1: 0.062031
Epoch: 110, loss: 25.9764, train_acc: 0.3242, train_recall: 0.1773, train_f1: 0.1320, val_acc: 0.352217, val_recall: 0.181171, val_f1: 0.137079
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed0.csv
Epoch: 111, loss: 24.1339, train_acc: 0.3591, train_recall: 0.1895, train_f1: 0.1440, val_acc: 0.302956, val_recall: 0.219915, val_f1: 0.161056
Epoch: 112, loss: 24.6679, train_acc: 0.3496, train_recall: 0.1738, train_f1: 0.1181, val_acc: 0.295566, val_recall: 0.193367, val_f1: 0.111279
Epoch: 113, loss: 24.9798, train_acc: 0.2352, train_recall: 0.1728, train_f1: 0.0879, val_acc: 0.243842, val_recall: 0.222222, val_f1: 0.115125
Epoch: 114, loss: 26.9549, train_acc: 0.0890, train_recall: 0.1785, train_f1: 0.0641, val_acc: 0.108374, val_recall: 0.226381, val_f1: 0.086465
Epoch: 115, loss: 26.7907, train_acc: 0.3167, train_recall: 0.1736, train_f1: 0.0969, val_acc: 0.347291, val_recall: 0.221751, val_f1: 0.121987
Epoch: 116, loss: 26.1977, train_acc: 0.3443, train_recall: 0.1711, train_f1: 0.0946, val_acc: 0.295566, val_recall: 0.218989, val_f1: 0.108312
Epoch: 117, loss: 26.6456, train_acc: 0.3485, train_recall: 0.1725, train_f1: 0.1026, val_acc: 0.300493, val_recall: 0.221145, val_f1: 0.124238
Epoch: 118, loss: 25.8046, train_acc: 0.3189, train_recall: 0.1884, train_f1: 0.1230, val_acc: 0.349754, val_recall: 0.237492, val_f1: 0.149863
Epoch: 119, loss: 26.9353, train_acc: 0.2352, train_recall: 0.1728, train_f1: 0.0885, val_acc: 0.243842, val_recall: 0.222222, val_f1: 0.115125
Epoch: 120, loss: 26.0338, train_acc: 0.3178, train_recall: 0.1760, train_f1: 0.1102, val_acc: 0.359606, val_recall: 0.228674, val_f1: 0.148436
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed0.csv
Epoch: 121, loss: 26.1880, train_acc: 0.3475, train_recall: 0.1646, train_f1: 0.1024, val_acc: 0.298030, val_recall: 0.194444, val_f1: 0.116387
Epoch: 122, loss: 25.4014, train_acc: 0.3496, train_recall: 0.1657, train_f1: 0.1046, val_acc: 0.300493, val_recall: 0.195803, val_f1: 0.119270
Epoch: 123, loss: 26.0576, train_acc: 0.3157, train_recall: 0.1685, train_f1: 0.1114, val_acc: 0.352217, val_recall: 0.201550, val_f1: 0.144012
Epoch: 124, loss: 25.7286, train_acc: 0.3157, train_recall: 0.1677, train_f1: 0.1080, val_acc: 0.352217, val_recall: 0.199819, val_f1: 0.137740
Epoch: 125, loss: 24.6507, train_acc: 0.3475, train_recall: 0.1800, train_f1: 0.1216, val_acc: 0.290640, val_recall: 0.151640, val_f1: 0.079069
Epoch: 126, loss: 24.9441, train_acc: 0.3485, train_recall: 0.1764, train_f1: 0.1116, val_acc: 0.285714, val_recall: 0.148922, val_f1: 0.068723
Epoch: 127, loss: 24.0899, train_acc: 0.2309, train_recall: 0.1790, train_f1: 0.0861, val_acc: 0.224138, val_recall: 0.147283, val_f1: 0.056089
Epoch: 128, loss: 24.5512, train_acc: 0.3157, train_recall: 0.1834, train_f1: 0.1053, val_acc: 0.339901, val_recall: 0.154167, val_f1: 0.087736
Epoch: 129, loss: 25.4795, train_acc: 0.0911, train_recall: 0.1866, train_f1: 0.0961, val_acc: 0.113300, val_recall: 0.196629, val_f1: 0.097002
Epoch: 130, loss: 23.6728, train_acc: 0.3485, train_recall: 0.1725, train_f1: 0.1044, val_acc: 0.300493, val_recall: 0.221145, val_f1: 0.124465
Epoch: 131, loss: 23.3222, train_acc: 0.2405, train_recall: 0.1746, train_f1: 0.0927, val_acc: 0.251232, val_recall: 0.224871, val_f1: 0.123228
Epoch: 132, loss: 23.9482, train_acc: 0.3189, train_recall: 0.1763, train_f1: 0.1103, val_acc: 0.357143, val_recall: 0.227748, val_f1: 0.148054
Epoch: 133, loss: 23.7327, train_acc: 0.3485, train_recall: 0.1729, train_f1: 0.1059, val_acc: 0.305419, val_recall: 0.223862, val_f1: 0.129962
Epoch: 134, loss: 23.8010, train_acc: 0.3496, train_recall: 0.1732, train_f1: 0.1059, val_acc: 0.302956, val_recall: 0.222503, val_f1: 0.127124
Epoch: 135, loss: 23.7334, train_acc: 0.3220, train_recall: 0.1779, train_f1: 0.1131, val_acc: 0.359606, val_recall: 0.229107, val_f1: 0.150404
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed0.csv
Epoch: 136, loss: 23.2494, train_acc: 0.3199, train_recall: 0.1767, train_f1: 0.1111, val_acc: 0.354680, val_recall: 0.226390, val_f1: 0.145886
Epoch: 137, loss: 23.5722, train_acc: 0.3485, train_recall: 0.1669, train_f1: 0.1073, val_acc: 0.293103, val_recall: 0.195000, val_f1: 0.121838
Epoch: 138, loss: 23.0887, train_acc: 0.3369, train_recall: 0.1590, train_f1: 0.1183, val_acc: 0.290640, val_recall: 0.183928, val_f1: 0.128262
Epoch: 139, loss: 23.2937, train_acc: 0.3242, train_recall: 0.1818, train_f1: 0.1184, val_acc: 0.354680, val_recall: 0.202316, val_f1: 0.135935
Epoch: 140, loss: 23.2937, train_acc: 0.3231, train_recall: 0.1806, train_f1: 0.1341, val_acc: 0.349754, val_recall: 0.194194, val_f1: 0.144800
Epoch: 141, loss: 23.9354, train_acc: 0.3517, train_recall: 0.1851, train_f1: 0.1239, val_acc: 0.290640, val_recall: 0.176700, val_f1: 0.114403
Epoch: 142, loss: 24.2058, train_acc: 0.2352, train_recall: 0.1841, train_f1: 0.1057, val_acc: 0.231527, val_recall: 0.176419, val_f1: 0.104755
Epoch: 143, loss: 23.6697, train_acc: 0.3199, train_recall: 0.1859, train_f1: 0.1179, val_acc: 0.339901, val_recall: 0.178536, val_f1: 0.125662
Epoch: 144, loss: 24.0071, train_acc: 0.3432, train_recall: 0.1819, train_f1: 0.1103, val_acc: 0.290640, val_recall: 0.176700, val_f1: 0.100345
Epoch: 145, loss: 23.5681, train_acc: 0.3453, train_recall: 0.1827, train_f1: 0.1182, val_acc: 0.290640, val_recall: 0.191212, val_f1: 0.127912
Epoch: 146, loss: 23.0167, train_acc: 0.3252, train_recall: 0.1951, train_f1: 0.1443, val_acc: 0.349754, val_recall: 0.208576, val_f1: 0.149097
Epoch: 147, loss: 23.6383, train_acc: 0.2362, train_recall: 0.1787, train_f1: 0.1006, val_acc: 0.231527, val_recall: 0.165308, val_f1: 0.093573
Epoch: 148, loss: 23.1004, train_acc: 0.3453, train_recall: 0.1631, train_f1: 0.1100, val_acc: 0.290640, val_recall: 0.165589, val_f1: 0.105913
Epoch: 149, loss: 23.4033, train_acc: 0.3220, train_recall: 0.1655, train_f1: 0.1115, val_acc: 0.339901, val_recall: 0.152610, val_f1: 0.094552
Epoch: 150, loss: 23.0653, train_acc: 0.3453, train_recall: 0.1706, train_f1: 0.1308, val_acc: 0.288177, val_recall: 0.151406, val_f1: 0.088695
Epoch: 151, loss: 23.5724, train_acc: 0.2341, train_recall: 0.1556, train_f1: 0.1004, val_acc: 0.231527, val_recall: 0.141667, val_f1: 0.070052
Epoch: 152, loss: 23.2844, train_acc: 0.3538, train_recall: 0.1596, train_f1: 0.1257, val_acc: 0.288177, val_recall: 0.143148, val_f1: 0.086966
Epoch: 153, loss: 23.5775, train_acc: 0.3263, train_recall: 0.1621, train_f1: 0.1275, val_acc: 0.347291, val_recall: 0.187880, val_f1: 0.155796
Epoch: 154, loss: 23.1098, train_acc: 0.3538, train_recall: 0.1747, train_f1: 0.1371, val_acc: 0.293103, val_recall: 0.184170, val_f1: 0.127836
Epoch: 155, loss: 23.2115, train_acc: 0.2394, train_recall: 0.1725, train_f1: 0.1104, val_acc: 0.236453, val_recall: 0.181975, val_f1: 0.103524
Epoch: 156, loss: 22.8969, train_acc: 0.3178, train_recall: 0.1684, train_f1: 0.1171, val_acc: 0.349754, val_recall: 0.196608, val_f1: 0.146227
Epoch: 157, loss: 22.9743, train_acc: 0.3570, train_recall: 0.1833, train_f1: 0.1380, val_acc: 0.310345, val_recall: 0.227544, val_f1: 0.170401
Epoch: 158, loss: 22.7287, train_acc: 0.3231, train_recall: 0.1701, train_f1: 0.1289, val_acc: 0.354680, val_recall: 0.213427, val_f1: 0.174757
Epoch: 159, loss: 22.9332, train_acc: 0.2341, train_recall: 0.1831, train_f1: 0.0990, val_acc: 0.226601, val_recall: 0.173702, val_f1: 0.101954
Epoch: 160, loss: 22.7685, train_acc: 0.3443, train_recall: 0.1845, train_f1: 0.1234, val_acc: 0.285714, val_recall: 0.175799, val_f1: 0.124515
Epoch: 161, loss: 22.7555, train_acc: 0.3496, train_recall: 0.1815, train_f1: 0.1439, val_acc: 0.300493, val_recall: 0.204183, val_f1: 0.159959
Epoch: 162, loss: 22.7087, train_acc: 0.3231, train_recall: 0.1703, train_f1: 0.1264, val_acc: 0.354680, val_recall: 0.198612, val_f1: 0.155051
Epoch: 163, loss: 22.8433, train_acc: 0.2447, train_recall: 0.1863, train_f1: 0.1239, val_acc: 0.246305, val_recall: 0.209471, val_f1: 0.139538
Epoch: 164, loss: 22.7473, train_acc: 0.3464, train_recall: 0.1700, train_f1: 0.1289, val_acc: 0.298030, val_recall: 0.179183, val_f1: 0.122105
Epoch: 165, loss: 22.7788, train_acc: 0.3231, train_recall: 0.1776, train_f1: 0.1329, val_acc: 0.349754, val_recall: 0.187501, val_f1: 0.134036
Epoch: 166, loss: 22.6239, train_acc: 0.3220, train_recall: 0.1753, train_f1: 0.1356, val_acc: 0.347291, val_recall: 0.181019, val_f1: 0.135673
Epoch: 167, loss: 22.6750, train_acc: 0.3464, train_recall: 0.1706, train_f1: 0.1363, val_acc: 0.300493, val_recall: 0.195054, val_f1: 0.150189
Epoch: 168, loss: 22.5419, train_acc: 0.3538, train_recall: 0.1727, train_f1: 0.1390, val_acc: 0.302956, val_recall: 0.195698, val_f1: 0.150773
Epoch: 169, loss: 22.6683, train_acc: 0.3242, train_recall: 0.1923, train_f1: 0.1348, val_acc: 0.344828, val_recall: 0.195894, val_f1: 0.152992
Epoch: 170, loss: 22.6063, train_acc: 0.3475, train_recall: 0.1881, train_f1: 0.1244, val_acc: 0.290640, val_recall: 0.177262, val_f1: 0.111273
Epoch: 171, loss: 22.6032, train_acc: 0.3485, train_recall: 0.1885, train_f1: 0.1250, val_acc: 0.290640, val_recall: 0.177262, val_f1: 0.111273
Epoch: 172, loss: 22.5125, train_acc: 0.3294, train_recall: 0.1936, train_f1: 0.1480, val_acc: 0.357143, val_recall: 0.213358, val_f1: 0.164774
Epoch: 173, loss: 22.5635, train_acc: 0.3242, train_recall: 0.1766, train_f1: 0.1069, val_acc: 0.352217, val_recall: 0.223906, val_f1: 0.134783
Epoch: 174, loss: 22.5877, train_acc: 0.3517, train_recall: 0.1740, train_f1: 0.1049, val_acc: 0.300493, val_recall: 0.221145, val_f1: 0.115949
Epoch: 175, loss: 22.5337, train_acc: 0.2479, train_recall: 0.1764, train_f1: 0.1015, val_acc: 0.246305, val_recall: 0.221439, val_f1: 0.128281
Epoch: 176, loss: 22.5798, train_acc: 0.3284, train_recall: 0.1731, train_f1: 0.1255, val_acc: 0.344828, val_recall: 0.167273, val_f1: 0.111468
Epoch: 177, loss: 22.5369, train_acc: 0.3602, train_recall: 0.2002, train_f1: 0.1615, val_acc: 0.300493, val_recall: 0.206178, val_f1: 0.136920
Epoch: 178, loss: 22.5117, train_acc: 0.2542, train_recall: 0.2094, train_f1: 0.1495, val_acc: 0.251232, val_recall: 0.202216, val_f1: 0.132410
Epoch: 179, loss: 22.4559, train_acc: 0.3220, train_recall: 0.1913, train_f1: 0.1290, val_acc: 0.339901, val_recall: 0.178947, val_f1: 0.123106
Epoch: 180, loss: 22.4690, train_acc: 0.3644, train_recall: 0.2114, train_f1: 0.1705, val_acc: 0.300493, val_recall: 0.194915, val_f1: 0.136733
Epoch: 181, loss: 22.4174, train_acc: 0.3570, train_recall: 0.1784, train_f1: 0.1189, val_acc: 0.305419, val_recall: 0.223255, val_f1: 0.129406
Epoch: 182, loss: 22.4673, train_acc: 0.2468, train_recall: 0.1787, train_f1: 0.0979, val_acc: 0.251232, val_recall: 0.224134, val_f1: 0.119612
Epoch: 183, loss: 22.4081, train_acc: 0.3633, train_recall: 0.1974, train_f1: 0.1615, val_acc: 0.305419, val_recall: 0.231930, val_f1: 0.165258
Epoch: 184, loss: 22.4103, train_acc: 0.3697, train_recall: 0.2039, train_f1: 0.1804, val_acc: 0.312808, val_recall: 0.221268, val_f1: 0.188685
Epoch: 185, loss: 22.4310, train_acc: 0.2532, train_recall: 0.2008, train_f1: 0.1463, val_acc: 0.248768, val_recall: 0.193579, val_f1: 0.124969
Epoch: 186, loss: 22.3925, train_acc: 0.3612, train_recall: 0.2024, train_f1: 0.1732, val_acc: 0.305419, val_recall: 0.194621, val_f1: 0.143573
Epoch: 187, loss: 22.3710, train_acc: 0.3633, train_recall: 0.2014, train_f1: 0.1724, val_acc: 0.307882, val_recall: 0.220326, val_f1: 0.178339
Epoch: 188, loss: 22.3438, train_acc: 0.3263, train_recall: 0.1979, train_f1: 0.1563, val_acc: 0.357143, val_recall: 0.236856, val_f1: 0.180107
Epoch: 189, loss: 22.3702, train_acc: 0.3517, train_recall: 0.1765, train_f1: 0.1193, val_acc: 0.310345, val_recall: 0.226535, val_f1: 0.138463
Epoch: 190, loss: 22.3774, train_acc: 0.3549, train_recall: 0.1770, train_f1: 0.1176, val_acc: 0.307882, val_recall: 0.225176, val_f1: 0.138215
Epoch: 191, loss: 22.3535, train_acc: 0.3252, train_recall: 0.2046, train_f1: 0.1679, val_acc: 0.349754, val_recall: 0.205868, val_f1: 0.161545
Epoch: 192, loss: 22.3322, train_acc: 0.3570, train_recall: 0.2007, train_f1: 0.1748, val_acc: 0.305419, val_recall: 0.220524, val_f1: 0.184503
Epoch: 193, loss: 22.3412, train_acc: 0.3581, train_recall: 0.2012, train_f1: 0.1687, val_acc: 0.298030, val_recall: 0.191843, val_f1: 0.137981
Epoch: 194, loss: 22.3169, train_acc: 0.3305, train_recall: 0.2120, train_f1: 0.1833, val_acc: 0.352217, val_recall: 0.204942, val_f1: 0.165643
Epoch: 195, loss: 22.3313, train_acc: 0.3263, train_recall: 0.1930, train_f1: 0.1515, val_acc: 0.359606, val_recall: 0.238388, val_f1: 0.187388
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed0.csv
Epoch: 196, loss: 22.3384, train_acc: 0.3581, train_recall: 0.1921, train_f1: 0.1462, val_acc: 0.307882, val_recall: 0.240143, val_f1: 0.165728
Epoch: 197, loss: 22.2907, train_acc: 0.3273, train_recall: 0.1928, train_f1: 0.1498, val_acc: 0.359606, val_recall: 0.238107, val_f1: 0.186060
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed0.csv
Epoch: 198, loss: 22.2997, train_acc: 0.3294, train_recall: 0.1966, train_f1: 0.1582, val_acc: 0.352217, val_recall: 0.197686, val_f1: 0.154596
Epoch: 199, loss: 22.3049, train_acc: 0.3623, train_recall: 0.2047, train_f1: 0.1746, val_acc: 0.300493, val_recall: 0.202885, val_f1: 0.150907
Epoch: 200, loss: 22.3018, train_acc: 0.3602, train_recall: 0.2049, train_f1: 0.1790, val_acc: 0.302956, val_recall: 0.204373, val_f1: 0.158116
Epoch: 201, loss: 22.3011, train_acc: 0.3305, train_recall: 0.1970, train_f1: 0.1602, val_acc: 0.349754, val_recall: 0.196327, val_f1: 0.152079
Epoch: 202, loss: 22.2805, train_acc: 0.3655, train_recall: 0.1917, train_f1: 0.1550, val_acc: 0.305419, val_recall: 0.195478, val_f1: 0.144955
Epoch: 203, loss: 22.2774, train_acc: 0.3633, train_recall: 0.1919, train_f1: 0.1587, val_acc: 0.307882, val_recall: 0.197117, val_f1: 0.149636
Epoch: 204, loss: 22.2711, train_acc: 0.3273, train_recall: 0.1911, train_f1: 0.1482, val_acc: 0.352217, val_recall: 0.197405, val_f1: 0.154537
Epoch: 205, loss: 22.2625, train_acc: 0.3718, train_recall: 0.2259, train_f1: 0.2088, val_acc: 0.302956, val_recall: 0.186993, val_f1: 0.140185
Epoch: 206, loss: 22.2678, train_acc: 0.3623, train_recall: 0.2056, train_f1: 0.1793, val_acc: 0.305419, val_recall: 0.205451, val_f1: 0.157475
Epoch: 207, loss: 22.2621, train_acc: 0.3633, train_recall: 0.2065, train_f1: 0.1862, val_acc: 0.310345, val_recall: 0.206825, val_f1: 0.169112
Epoch: 208, loss: 22.2589, train_acc: 0.3676, train_recall: 0.1948, train_f1: 0.1699, val_acc: 0.310345, val_recall: 0.197133, val_f1: 0.158755
Epoch: 209, loss: 22.2483, train_acc: 0.3644, train_recall: 0.1923, train_f1: 0.1590, val_acc: 0.305419, val_recall: 0.196040, val_f1: 0.149188
Epoch: 210, loss: 22.2499, train_acc: 0.3612, train_recall: 0.1914, train_f1: 0.1602, val_acc: 0.310345, val_recall: 0.198324, val_f1: 0.153485
Epoch: 211, loss: 22.2299, train_acc: 0.3708, train_recall: 0.2143, train_f1: 0.1935, val_acc: 0.302956, val_recall: 0.195092, val_f1: 0.152154
Epoch: 212, loss: 22.2385, train_acc: 0.3612, train_recall: 0.1978, train_f1: 0.1782, val_acc: 0.305419, val_recall: 0.205732, val_f1: 0.161015
Epoch: 213, loss: 22.2408, train_acc: 0.3570, train_recall: 0.2002, train_f1: 0.1732, val_acc: 0.307882, val_recall: 0.206658, val_f1: 0.162413
Epoch: 214, loss: 22.2232, train_acc: 0.3581, train_recall: 0.2005, train_f1: 0.1734, val_acc: 0.307882, val_recall: 0.206658, val_f1: 0.161347
Epoch: 215, loss: 22.2368, train_acc: 0.3612, train_recall: 0.2053, train_f1: 0.1798, val_acc: 0.307882, val_recall: 0.206658, val_f1: 0.162413
Epoch: 216, loss: 22.2215, train_acc: 0.3581, train_recall: 0.1969, train_f1: 0.1667, val_acc: 0.312808, val_recall: 0.236136, val_f1: 0.178062
Epoch: 217, loss: 22.2220, train_acc: 0.3581, train_recall: 0.1968, train_f1: 0.1662, val_acc: 0.312808, val_recall: 0.236287, val_f1: 0.176719
Epoch: 218, loss: 22.2101, train_acc: 0.3665, train_recall: 0.2024, train_f1: 0.1796, val_acc: 0.307882, val_recall: 0.196966, val_f1: 0.150992
Epoch: 219, loss: 22.2063, train_acc: 0.3602, train_recall: 0.2014, train_f1: 0.1769, val_acc: 0.300493, val_recall: 0.203425, val_f1: 0.161083
Epoch: 220, loss: 22.2028, train_acc: 0.3581, train_recall: 0.2005, train_f1: 0.1741, val_acc: 0.305419, val_recall: 0.205732, val_f1: 0.160609
Epoch: 221, loss: 22.2000, train_acc: 0.3612, train_recall: 0.2053, train_f1: 0.1799, val_acc: 0.302956, val_recall: 0.204373, val_f1: 0.158116
Epoch: 222, loss: 22.2096, train_acc: 0.3305, train_recall: 0.2060, train_f1: 0.1769, val_acc: 0.349754, val_recall: 0.206626, val_f1: 0.171806
Epoch: 223, loss: 22.1890, train_acc: 0.3602, train_recall: 0.2022, train_f1: 0.1762, val_acc: 0.312808, val_recall: 0.247399, val_f1: 0.200018
Epoch: 224, loss: 22.1904, train_acc: 0.3665, train_recall: 0.2137, train_f1: 0.1979, val_acc: 0.302956, val_recall: 0.204373, val_f1: 0.158522
Epoch: 225, loss: 22.1846, train_acc: 0.3379, train_recall: 0.2055, train_f1: 0.1749, val_acc: 0.342365, val_recall: 0.178096, val_f1: 0.139198
Epoch: 226, loss: 22.1860, train_acc: 0.3612, train_recall: 0.2025, train_f1: 0.1736, val_acc: 0.300493, val_recall: 0.177521, val_f1: 0.125565
Epoch: 227, loss: 22.1833, train_acc: 0.3612, train_recall: 0.2025, train_f1: 0.1730, val_acc: 0.298030, val_recall: 0.176595, val_f1: 0.123760
Epoch: 228, loss: 22.1662, train_acc: 0.3623, train_recall: 0.2030, train_f1: 0.1756, val_acc: 0.295566, val_recall: 0.175647, val_f1: 0.126727
Epoch: 229, loss: 22.1648, train_acc: 0.3655, train_recall: 0.2219, train_f1: 0.2033, val_acc: 0.302956, val_recall: 0.204222, val_f1: 0.159459
Epoch: 230, loss: 22.1786, train_acc: 0.3655, train_recall: 0.2209, train_f1: 0.2020, val_acc: 0.302956, val_recall: 0.204373, val_f1: 0.158116
Epoch: 231, loss: 22.1630, train_acc: 0.3591, train_recall: 0.2019, train_f1: 0.1739, val_acc: 0.300493, val_recall: 0.177802, val_f1: 0.127654
Epoch: 232, loss: 22.1636, train_acc: 0.3602, train_recall: 0.2022, train_f1: 0.1741, val_acc: 0.300493, val_recall: 0.177802, val_f1: 0.127654
Epoch: 233, loss: 22.1404, train_acc: 0.3602, train_recall: 0.2022, train_f1: 0.1729, val_acc: 0.300493, val_recall: 0.177954, val_f1: 0.126253
Epoch: 234, loss: 22.1467, train_acc: 0.3581, train_recall: 0.2015, train_f1: 0.1731, val_acc: 0.300493, val_recall: 0.177802, val_f1: 0.127596
Epoch: 235, loss: 22.1231, train_acc: 0.3623, train_recall: 0.2067, train_f1: 0.1902, val_acc: 0.300493, val_recall: 0.177802, val_f1: 0.128703
Epoch: 236, loss: 22.1268, train_acc: 0.3644, train_recall: 0.2102, train_f1: 0.1947, val_acc: 0.300493, val_recall: 0.177954, val_f1: 0.127361
Epoch: 237, loss: 22.1285, train_acc: 0.3623, train_recall: 0.2170, train_f1: 0.2016, val_acc: 0.300493, val_recall: 0.177802, val_f1: 0.127596
Epoch: 238, loss: 22.1403, train_acc: 0.3591, train_recall: 0.2018, train_f1: 0.1733, val_acc: 0.300493, val_recall: 0.177802, val_f1: 0.127596
Epoch: 239, loss: 22.1305, train_acc: 0.3602, train_recall: 0.2022, train_f1: 0.1722, val_acc: 0.302956, val_recall: 0.178880, val_f1: 0.128058
Epoch: 240, loss: 22.1312, train_acc: 0.3581, train_recall: 0.2015, train_f1: 0.1731, val_acc: 0.300493, val_recall: 0.177802, val_f1: 0.127596
Epoch: 241, loss: 22.1170, train_acc: 0.3602, train_recall: 0.1947, train_f1: 0.1689, val_acc: 0.300493, val_recall: 0.177802, val_f1: 0.128703
Epoch: 242, loss: 22.1084, train_acc: 0.3633, train_recall: 0.2098, train_f1: 0.1946, val_acc: 0.300493, val_recall: 0.177802, val_f1: 0.128703
Epoch: 243, loss: 22.1102, train_acc: 0.3633, train_recall: 0.2099, train_f1: 0.1951, val_acc: 0.300493, val_recall: 0.177802, val_f1: 0.128703
Epoch: 244, loss: 22.0917, train_acc: 0.3633, train_recall: 0.2061, train_f1: 0.1890, val_acc: 0.302956, val_recall: 0.178880, val_f1: 0.129165
Epoch: 245, loss: 22.0955, train_acc: 0.3602, train_recall: 0.2022, train_f1: 0.1728, val_acc: 0.300493, val_recall: 0.177802, val_f1: 0.127596
Epoch: 246, loss: 22.0979, train_acc: 0.3591, train_recall: 0.2018, train_f1: 0.1733, val_acc: 0.300493, val_recall: 0.177802, val_f1: 0.127596
Epoch: 247, loss: 22.0903, train_acc: 0.3602, train_recall: 0.2022, train_f1: 0.1729, val_acc: 0.302956, val_recall: 0.178880, val_f1: 0.128058
Epoch: 248, loss: 22.0729, train_acc: 0.3644, train_recall: 0.2112, train_f1: 0.1961, val_acc: 0.300493, val_recall: 0.177802, val_f1: 0.128703
Epoch: 249, loss: 22.0879, train_acc: 0.3623, train_recall: 0.2095, train_f1: 0.1950, val_acc: 0.300493, val_recall: 0.177802, val_f1: 0.128703
Epoch: 250, loss: 22.0613, train_acc: 0.3644, train_recall: 0.2102, train_f1: 0.1947, val_acc: 0.300493, val_recall: 0.177954, val_f1: 0.127361
Epoch: 251, loss: 22.0720, train_acc: 0.3602, train_recall: 0.1947, train_f1: 0.1689, val_acc: 0.300493, val_recall: 0.177802, val_f1: 0.128703
Epoch: 252, loss: 22.0749, train_acc: 0.3581, train_recall: 0.2015, train_f1: 0.1731, val_acc: 0.300493, val_recall: 0.177802, val_f1: 0.127596
Epoch: 253, loss: 22.0502, train_acc: 0.3602, train_recall: 0.2022, train_f1: 0.1729, val_acc: 0.300493, val_recall: 0.177954, val_f1: 0.126253
Epoch: 254, loss: 22.0554, train_acc: 0.3602, train_recall: 0.1947, train_f1: 0.1689, val_acc: 0.300493, val_recall: 0.177802, val_f1: 0.128703
Epoch: 255, loss: 22.0553, train_acc: 0.3612, train_recall: 0.2054, train_f1: 0.1892, val_acc: 0.300493, val_recall: 0.177802, val_f1: 0.128703
Epoch: 256, loss: 22.0418, train_acc: 0.3623, train_recall: 0.2095, train_f1: 0.1944, val_acc: 0.300493, val_recall: 0.177802, val_f1: 0.128703
Epoch: 257, loss: 22.0582, train_acc: 0.3633, train_recall: 0.2108, train_f1: 0.1959, val_acc: 0.300493, val_recall: 0.177802, val_f1: 0.128703
Epoch: 258, loss: 22.0506, train_acc: 0.3581, train_recall: 0.1940, train_f1: 0.1686, val_acc: 0.300493, val_recall: 0.177802, val_f1: 0.128703
Epoch: 259, loss: 22.0058, train_acc: 0.3581, train_recall: 0.2014, train_f1: 0.1725, val_acc: 0.300493, val_recall: 0.177802, val_f1: 0.127596
Epoch: 260, loss: 22.0288, train_acc: 0.3591, train_recall: 0.2018, train_f1: 0.1733, val_acc: 0.300493, val_recall: 0.177802, val_f1: 0.126529
Epoch: 261, loss: 22.0155, train_acc: 0.3633, train_recall: 0.2183, train_f1: 0.2000, val_acc: 0.300493, val_recall: 0.177802, val_f1: 0.126529
Epoch: 262, loss: 22.0157, train_acc: 0.3623, train_recall: 0.2104, train_f1: 0.1952, val_acc: 0.300493, val_recall: 0.177802, val_f1: 0.126933
Epoch: 263, loss: 22.0078, train_acc: 0.3633, train_recall: 0.2108, train_f1: 0.1959, val_acc: 0.300493, val_recall: 0.177802, val_f1: 0.128703
Epoch: 264, loss: 22.0077, train_acc: 0.3591, train_recall: 0.1943, train_f1: 0.1687, val_acc: 0.300493, val_recall: 0.177802, val_f1: 0.126933
Epoch: 265, loss: 22.0060, train_acc: 0.3602, train_recall: 0.2022, train_f1: 0.1735, val_acc: 0.300493, val_recall: 0.177802, val_f1: 0.126529
Epoch: 266, loss: 21.9918, train_acc: 0.3591, train_recall: 0.2018, train_f1: 0.1733, val_acc: 0.300493, val_recall: 0.177802, val_f1: 0.126529
Epoch: 267, loss: 21.9874, train_acc: 0.3623, train_recall: 0.2142, train_f1: 0.1942, val_acc: 0.300493, val_recall: 0.177802, val_f1: 0.126529
Epoch: 268, loss: 21.9875, train_acc: 0.3644, train_recall: 0.2187, train_f1: 0.2002, val_acc: 0.300493, val_recall: 0.177802, val_f1: 0.126529
Epoch: 269, loss: 21.9933, train_acc: 0.3633, train_recall: 0.2183, train_f1: 0.1990, val_acc: 0.300493, val_recall: 0.177802, val_f1: 0.126529
Epoch: 270, loss: 21.9767, train_acc: 0.3591, train_recall: 0.2018, train_f1: 0.1726, val_acc: 0.300493, val_recall: 0.177802, val_f1: 0.126529
Epoch: 271, loss: 21.9646, train_acc: 0.3591, train_recall: 0.2018, train_f1: 0.1726, val_acc: 0.300493, val_recall: 0.177802, val_f1: 0.126529
Epoch: 272, loss: 21.9643, train_acc: 0.3591, train_recall: 0.2018, train_f1: 0.1726, val_acc: 0.300493, val_recall: 0.177802, val_f1: 0.126529
Epoch: 273, loss: 21.9765, train_acc: 0.3633, train_recall: 0.2183, train_f1: 0.1990, val_acc: 0.300493, val_recall: 0.177802, val_f1: 0.126529
Epoch: 274, loss: 21.9785, train_acc: 0.3633, train_recall: 0.2108, train_f1: 0.1941, val_acc: 0.300493, val_recall: 0.177802, val_f1: 0.126933
Epoch: 275, loss: 21.9593, train_acc: 0.3633, train_recall: 0.2183, train_f1: 0.1990, val_acc: 0.300493, val_recall: 0.177802, val_f1: 0.126529
/home/ADS/cyang314/ucr_work/HINI_Baseline/GraphLoRA/model/GraphLoRA.py:218: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.
Consider using tensor.detach() first. (Triggered internally at /pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:836.)
  f.write(f'{pre_dataset} to {downstream_dataset}: seed: %d, epoch: %d, train_loss: %f, train_acc: %f, train_recall: %f, train_f1: %f, val_acc: %f, val_recall: %f, val_f1: %f\n' %
Epoch: 276, loss: 21.9441, train_acc: 0.3633, train_recall: 0.2183, train_f1: 0.1990, val_acc: 0.300493, val_recall: 0.177802, val_f1: 0.126529
Epoch: 277, loss: 21.9423, train_acc: 0.3591, train_recall: 0.2018, train_f1: 0.1721, val_acc: 0.300493, val_recall: 0.177802, val_f1: 0.126529
Epoch: 278, loss: 21.9517, train_acc: 0.3591, train_recall: 0.2018, train_f1: 0.1721, val_acc: 0.300493, val_recall: 0.177802, val_f1: 0.126529
Epoch: 279, loss: 21.9396, train_acc: 0.3633, train_recall: 0.2108, train_f1: 0.1941, val_acc: 0.300493, val_recall: 0.177802, val_f1: 0.126933
Epoch: 280, loss: 21.9225, train_acc: 0.3623, train_recall: 0.2057, train_f1: 0.1877, val_acc: 0.300493, val_recall: 0.177802, val_f1: 0.126933
Epoch: 281, loss: 21.9215, train_acc: 0.3623, train_recall: 0.2057, train_f1: 0.1877, val_acc: 0.300493, val_recall: 0.177802, val_f1: 0.126933
Epoch: 282, loss: 21.9168, train_acc: 0.3633, train_recall: 0.2183, train_f1: 0.1990, val_acc: 0.300493, val_recall: 0.177802, val_f1: 0.126529
Epoch: 283, loss: 21.9314, train_acc: 0.3633, train_recall: 0.2183, train_f1: 0.1990, val_acc: 0.300493, val_recall: 0.177802, val_f1: 0.126529
Epoch: 284, loss: 21.9099, train_acc: 0.3633, train_recall: 0.2183, train_f1: 0.1990, val_acc: 0.300493, val_recall: 0.177802, val_f1: 0.126529
Epoch: 285, loss: 21.9177, train_acc: 0.3633, train_recall: 0.2183, train_f1: 0.1990, val_acc: 0.300493, val_recall: 0.177802, val_f1: 0.126529
Epoch: 286, loss: 21.9082, train_acc: 0.3623, train_recall: 0.2057, train_f1: 0.1877, val_acc: 0.300493, val_recall: 0.177802, val_f1: 0.126933
Epoch: 287, loss: 21.8913, train_acc: 0.3623, train_recall: 0.2057, train_f1: 0.1877, val_acc: 0.300493, val_recall: 0.177802, val_f1: 0.126933
Epoch: 288, loss: 21.8816, train_acc: 0.3623, train_recall: 0.2057, train_f1: 0.1877, val_acc: 0.300493, val_recall: 0.177802, val_f1: 0.126933
Epoch: 289, loss: 21.8824, train_acc: 0.3633, train_recall: 0.2108, train_f1: 0.1941, val_acc: 0.300493, val_recall: 0.177802, val_f1: 0.126933
Epoch: 290, loss: 21.8826, train_acc: 0.3633, train_recall: 0.2183, train_f1: 0.1990, val_acc: 0.300493, val_recall: 0.177802, val_f1: 0.126529
Epoch: 291, loss: 21.8770, train_acc: 0.3623, train_recall: 0.2133, train_f1: 0.1941, val_acc: 0.300493, val_recall: 0.177802, val_f1: 0.126529
Epoch: 292, loss: 21.8545, train_acc: 0.3623, train_recall: 0.2057, train_f1: 0.1877, val_acc: 0.300493, val_recall: 0.177802, val_f1: 0.126933
Epoch: 293, loss: 21.8506, train_acc: 0.3633, train_recall: 0.2099, train_f1: 0.1934, val_acc: 0.302956, val_recall: 0.178728, val_f1: 0.128703
Epoch: 294, loss: 21.8603, train_acc: 0.3633, train_recall: 0.2099, train_f1: 0.1934, val_acc: 0.302956, val_recall: 0.178728, val_f1: 0.128703
Epoch: 295, loss: 21.8622, train_acc: 0.3623, train_recall: 0.2057, train_f1: 0.1877, val_acc: 0.302956, val_recall: 0.178728, val_f1: 0.128703
Epoch: 296, loss: 21.8657, train_acc: 0.3623, train_recall: 0.2057, train_f1: 0.1877, val_acc: 0.302956, val_recall: 0.178728, val_f1: 0.128703
Epoch: 297, loss: 21.8379, train_acc: 0.3623, train_recall: 0.2133, train_f1: 0.1941, val_acc: 0.302956, val_recall: 0.178728, val_f1: 0.128299
Epoch: 298, loss: 21.8132, train_acc: 0.3633, train_recall: 0.2099, train_f1: 0.1934, val_acc: 0.302956, val_recall: 0.178728, val_f1: 0.128703
Epoch: 299, loss: 21.8347, train_acc: 0.3633, train_recall: 0.2099, train_f1: 0.1934, val_acc: 0.302956, val_recall: 0.178728, val_f1: 0.128703
epoch: 198, train_acc: 0.327330, val_acc: 0.359606, val_recall: 0.238107, val_f1: 0.186060
Running: year=2015 â†’ downstream_year=2016, seed=1
Random seed set to 42

==============================
PRE-TRAINING
==============================
create PreTrain instance...
pre-training...
(T) | Epoch=001, loss=7.3009, this epoch 0.0456, total 0.0456
+++model saved ! 2015.pth
(T) | Epoch=002, loss=6.3445, this epoch 0.0332, total 0.0788
+++model saved ! 2015.pth
(T) | Epoch=003, loss=6.3445, this epoch 0.0352, total 0.1139
(T) | Epoch=004, loss=6.3444, this epoch 0.0360, total 0.1499
+++model saved ! 2015.pth
(T) | Epoch=005, loss=6.3451, this epoch 0.0394, total 0.1893
(T) | Epoch=006, loss=6.3445, this epoch 0.0250, total 0.2143
(T) | Epoch=007, loss=6.3472, this epoch 0.0339, total 0.2482
(T) | Epoch=008, loss=6.8267, this epoch 0.0296, total 0.2779
(T) | Epoch=009, loss=6.3447, this epoch 0.0352, total 0.3130
(T) | Epoch=010, loss=6.3446, this epoch 0.0312, total 0.3442
(T) | Epoch=011, loss=6.7118, this epoch 0.0353, total 0.3795
(T) | Epoch=012, loss=6.3444, this epoch 0.0290, total 0.4084
(T) | Epoch=013, loss=6.3447, this epoch 0.0301, total 0.4385
(T) | Epoch=014, loss=6.3439, this epoch 0.0284, total 0.4670
+++model saved ! 2015.pth
(T) | Epoch=015, loss=6.3438, this epoch 0.0368, total 0.5038
+++model saved ! 2015.pth
(T) | Epoch=016, loss=6.3444, this epoch 0.0245, total 0.5283
(T) | Epoch=017, loss=6.3436, this epoch 0.0340, total 0.5623
+++model saved ! 2015.pth
(T) | Epoch=018, loss=6.3435, this epoch 0.0373, total 0.5996
+++model saved ! 2015.pth
(T) | Epoch=019, loss=6.5342, this epoch 0.0246, total 0.6242
(T) | Epoch=020, loss=6.3434, this epoch 0.0269, total 0.6511
+++model saved ! 2015.pth
(T) | Epoch=021, loss=6.3474, this epoch 0.0321, total 0.6833
(T) | Epoch=022, loss=6.3466, this epoch 0.0227, total 0.7060
(T) | Epoch=023, loss=6.4797, this epoch 0.0215, total 0.7275
(T) | Epoch=024, loss=6.3427, this epoch 0.0284, total 0.7559
+++model saved ! 2015.pth
(T) | Epoch=025, loss=6.3428, this epoch 0.0294, total 0.7852
(T) | Epoch=026, loss=6.3444, this epoch 0.0285, total 0.8137
(T) | Epoch=027, loss=6.3420, this epoch 0.0211, total 0.8348
+++model saved ! 2015.pth
(T) | Epoch=028, loss=6.3464, this epoch 0.0346, total 0.8695
(T) | Epoch=029, loss=6.3437, this epoch 0.0304, total 0.8999
(T) | Epoch=030, loss=6.4340, this epoch 0.0224, total 0.9223
(T) | Epoch=031, loss=6.3460, this epoch 0.0283, total 0.9506
(T) | Epoch=032, loss=6.3407, this epoch 0.0278, total 0.9783
+++model saved ! 2015.pth
(T) | Epoch=033, loss=6.3406, this epoch 0.0237, total 1.0020
+++model saved ! 2015.pth
(T) | Epoch=034, loss=6.3411, this epoch 0.0460, total 1.0480
(T) | Epoch=035, loss=6.3453, this epoch 0.0354, total 1.0833
(T) | Epoch=036, loss=6.3412, this epoch 0.0354, total 1.1187
(T) | Epoch=037, loss=6.3867, this epoch 0.0311, total 1.1498
(T) | Epoch=038, loss=6.4135, this epoch 0.0281, total 1.1780
(T) | Epoch=039, loss=6.3379, this epoch 0.0365, total 1.2144
+++model saved ! 2015.pth
(T) | Epoch=040, loss=6.3458, this epoch 0.0253, total 1.2397
(T) | Epoch=041, loss=6.3442, this epoch 0.0219, total 1.2616
(T) | Epoch=042, loss=6.3398, this epoch 0.0214, total 1.2830
(T) | Epoch=043, loss=6.3361, this epoch 0.0213, total 1.3043
+++model saved ! 2015.pth
(T) | Epoch=044, loss=6.3448, this epoch 0.0367, total 1.3410
(T) | Epoch=045, loss=6.3339, this epoch 0.0359, total 1.3769
+++model saved ! 2015.pth
(T) | Epoch=046, loss=6.3378, this epoch 0.0401, total 1.4170
(T) | Epoch=047, loss=6.3318, this epoch 0.0304, total 1.4474
+++model saved ! 2015.pth
(T) | Epoch=048, loss=6.3725, this epoch 0.0367, total 1.4841
(T) | Epoch=049, loss=6.3294, this epoch 0.0339, total 1.5180
+++model saved ! 2015.pth
(T) | Epoch=050, loss=6.3329, this epoch 0.0321, total 1.5501
(T) | Epoch=051, loss=6.3295, this epoch 0.0285, total 1.5786
(T) | Epoch=052, loss=6.3292, this epoch 0.0268, total 1.6053
+++model saved ! 2015.pth
(T) | Epoch=053, loss=6.3262, this epoch 0.0251, total 1.6304
+++model saved ! 2015.pth
(T) | Epoch=054, loss=6.3243, this epoch 0.0353, total 1.6657
+++model saved ! 2015.pth
(T) | Epoch=055, loss=6.3177, this epoch 0.0391, total 1.7048
+++model saved ! 2015.pth
(T) | Epoch=056, loss=6.3145, this epoch 0.0320, total 1.7368
+++model saved ! 2015.pth
(T) | Epoch=057, loss=6.3199, this epoch 0.0302, total 1.7670
(T) | Epoch=058, loss=6.3309, this epoch 0.0302, total 1.7971
(T) | Epoch=059, loss=6.3194, this epoch 0.0228, total 1.8200
(T) | Epoch=060, loss=6.2954, this epoch 0.0221, total 1.8421
+++model saved ! 2015.pth
(T) | Epoch=061, loss=6.2872, this epoch 0.0230, total 1.8651
+++model saved ! 2015.pth
(T) | Epoch=062, loss=6.2811, this epoch 0.0427, total 1.9078
+++model saved ! 2015.pth
(T) | Epoch=063, loss=6.2604, this epoch 0.0248, total 1.9326
+++model saved ! 2015.pth
(T) | Epoch=064, loss=6.2564, this epoch 0.0350, total 1.9676
+++model saved ! 2015.pth
(T) | Epoch=065, loss=6.2311, this epoch 0.0308, total 1.9984
+++model saved ! 2015.pth
(T) | Epoch=066, loss=6.2235, this epoch 0.0323, total 2.0307
+++model saved ! 2015.pth
(T) | Epoch=067, loss=6.2264, this epoch 0.0358, total 2.0665
(T) | Epoch=068, loss=6.1711, this epoch 0.0304, total 2.0969
+++model saved ! 2015.pth
(T) | Epoch=069, loss=6.2524, this epoch 0.0309, total 2.1278
(T) | Epoch=070, loss=6.0517, this epoch 0.0235, total 2.1513
+++model saved ! 2015.pth
(T) | Epoch=071, loss=5.9781, this epoch 0.0431, total 2.1944
+++model saved ! 2015.pth
(T) | Epoch=072, loss=5.9220, this epoch 0.0317, total 2.2261
+++model saved ! 2015.pth
(T) | Epoch=073, loss=5.8787, this epoch 0.0417, total 2.2678
+++model saved ! 2015.pth
(T) | Epoch=074, loss=5.8249, this epoch 0.0418, total 2.3096
+++model saved ! 2015.pth
(T) | Epoch=075, loss=5.7711, this epoch 0.0294, total 2.3390
+++model saved ! 2015.pth
(T) | Epoch=076, loss=6.6189, this epoch 0.0342, total 2.3732
(T) | Epoch=077, loss=5.7260, this epoch 0.0285, total 2.4017
+++model saved ! 2015.pth
(T) | Epoch=078, loss=5.6917, this epoch 0.0282, total 2.4299
+++model saved ! 2015.pth
(T) | Epoch=079, loss=5.7709, this epoch 0.0329, total 2.4628
(T) | Epoch=080, loss=5.6984, this epoch 0.0334, total 2.4962
(T) | Epoch=081, loss=5.6744, this epoch 0.0297, total 2.5259
+++model saved ! 2015.pth
(T) | Epoch=082, loss=5.7215, this epoch 0.0294, total 2.5553
(T) | Epoch=083, loss=5.6127, this epoch 0.0295, total 2.5848
+++model saved ! 2015.pth
(T) | Epoch=084, loss=5.9439, this epoch 0.0419, total 2.6267
(T) | Epoch=085, loss=5.6125, this epoch 0.0289, total 2.6556
+++model saved ! 2015.pth
(T) | Epoch=086, loss=5.5824, this epoch 0.0235, total 2.6791
+++model saved ! 2015.pth
(T) | Epoch=087, loss=5.5724, this epoch 0.0234, total 2.7025
+++model saved ! 2015.pth
(T) | Epoch=088, loss=5.9507, this epoch 0.0362, total 2.7387
(T) | Epoch=089, loss=6.1384, this epoch 0.0304, total 2.7691
(T) | Epoch=090, loss=5.6252, this epoch 0.0294, total 2.7985
(T) | Epoch=091, loss=6.0038, this epoch 0.0289, total 2.8274
(T) | Epoch=092, loss=5.6440, this epoch 0.0337, total 2.8611
(T) | Epoch=093, loss=5.6387, this epoch 0.0349, total 2.8960
(T) | Epoch=094, loss=5.8832, this epoch 0.0303, total 2.9263
(T) | Epoch=095, loss=5.7851, this epoch 0.0291, total 2.9554
(T) | Epoch=096, loss=5.5782, this epoch 0.0289, total 2.9843
(T) | Epoch=097, loss=5.6243, this epoch 0.0372, total 3.0215
(T) | Epoch=098, loss=5.7081, this epoch 0.0342, total 3.0557
(T) | Epoch=099, loss=5.6286, this epoch 0.0380, total 3.0937
(T) | Epoch=100, loss=5.6361, this epoch 0.0369, total 3.1306
(T) | Epoch=101, loss=5.9126, this epoch 0.0259, total 3.1565
(T) | Epoch=102, loss=5.6857, this epoch 0.0315, total 3.1880
(T) | Epoch=103, loss=5.6386, this epoch 0.0363, total 3.2243
(T) | Epoch=104, loss=5.6401, this epoch 0.0279, total 3.2521
(T) | Epoch=105, loss=5.6428, this epoch 0.0307, total 3.2828
(T) | Epoch=106, loss=5.7198, this epoch 0.0249, total 3.3078
(T) | Epoch=107, loss=6.5084, this epoch 0.0306, total 3.3383
(T) | Epoch=108, loss=5.6058, this epoch 0.0330, total 3.3714
(T) | Epoch=109, loss=6.0109, this epoch 0.0348, total 3.4061
(T) | Epoch=110, loss=5.7566, this epoch 0.0345, total 3.4407
(T) | Epoch=111, loss=5.7794, this epoch 0.0366, total 3.4773
(T) | Epoch=112, loss=5.8127, this epoch 0.0278, total 3.5051
(T) | Epoch=113, loss=5.8358, this epoch 0.0340, total 3.5392
(T) | Epoch=114, loss=6.3422, this epoch 0.0268, total 3.5659
(T) | Epoch=115, loss=5.9372, this epoch 0.0367, total 3.6027
(T) | Epoch=116, loss=5.8214, this epoch 0.0319, total 3.6345
(T) | Epoch=117, loss=6.2301, this epoch 0.0370, total 3.6716
(T) | Epoch=118, loss=5.8718, this epoch 0.0344, total 3.7060
(T) | Epoch=119, loss=5.7465, this epoch 0.0371, total 3.7430
(T) | Epoch=120, loss=5.9975, this epoch 0.0377, total 3.7807
(T) | Epoch=121, loss=5.7267, this epoch 0.0313, total 3.8121
(T) | Epoch=122, loss=5.7174, this epoch 0.0365, total 3.8486
(T) | Epoch=123, loss=5.6807, this epoch 0.0372, total 3.8858
(T) | Epoch=124, loss=5.6803, this epoch 0.0376, total 3.9233
(T) | Epoch=125, loss=5.7367, this epoch 0.0232, total 3.9465
(T) | Epoch=126, loss=6.0703, this epoch 0.0357, total 3.9821
(T) | Epoch=127, loss=5.8209, this epoch 0.0314, total 4.0135
(T) | Epoch=128, loss=5.6466, this epoch 0.0285, total 4.0420
(T) | Epoch=129, loss=5.6444, this epoch 0.0352, total 4.0771
(T) | Epoch=130, loss=5.6630, this epoch 0.0352, total 4.1124
(T) | Epoch=131, loss=5.6858, this epoch 0.0314, total 4.1438
(T) | Epoch=132, loss=5.6466, this epoch 0.0337, total 4.1774
(T) | Epoch=133, loss=5.7401, this epoch 0.0311, total 4.2086
(T) | Epoch=134, loss=6.0010, this epoch 0.0412, total 4.2498
(T) | Epoch=135, loss=5.8345, this epoch 0.0378, total 4.2876
(T) | Epoch=136, loss=5.6735, this epoch 0.0286, total 4.3162
(T) | Epoch=137, loss=5.5890, this epoch 0.0391, total 4.3553
(T) | Epoch=138, loss=5.5860, this epoch 0.0353, total 4.3906
(T) | Epoch=139, loss=5.6898, this epoch 0.0329, total 4.4236
(T) | Epoch=140, loss=5.6042, this epoch 0.0318, total 4.4554
(T) | Epoch=141, loss=6.1395, this epoch 0.0332, total 4.4885
(T) | Epoch=142, loss=5.6056, this epoch 0.0296, total 4.5181
(T) | Epoch=143, loss=5.6725, this epoch 0.0267, total 4.5448
(T) | Epoch=144, loss=5.6118, this epoch 0.0239, total 4.5687
(T) | Epoch=145, loss=5.6213, this epoch 0.0281, total 4.5967
(T) | Epoch=146, loss=5.7486, this epoch 0.0350, total 4.6318
(T) | Epoch=147, loss=5.6208, this epoch 0.0328, total 4.6646
(T) | Epoch=148, loss=5.6559, this epoch 0.0359, total 4.7006
(T) | Epoch=149, loss=5.6870, this epoch 0.0297, total 4.7303
(T) | Epoch=150, loss=5.6693, this epoch 0.0285, total 4.7588
(T) | Epoch=151, loss=5.5899, this epoch 0.0278, total 4.7866
(T) | Epoch=152, loss=5.7057, this epoch 0.0331, total 4.8197
(T) | Epoch=153, loss=5.5816, this epoch 0.0335, total 4.8532
(T) | Epoch=154, loss=5.5361, this epoch 0.0339, total 4.8871
+++model saved ! 2015.pth
(T) | Epoch=155, loss=5.5700, this epoch 0.0362, total 4.9233
(T) | Epoch=156, loss=5.9975, this epoch 0.0253, total 4.9486
(T) | Epoch=157, loss=5.5145, this epoch 0.0313, total 4.9799
+++model saved ! 2015.pth
(T) | Epoch=158, loss=5.6933, this epoch 0.0324, total 5.0123
(T) | Epoch=159, loss=5.6869, this epoch 0.0331, total 5.0454
(T) | Epoch=160, loss=5.5327, this epoch 0.0351, total 5.0804
(T) | Epoch=161, loss=5.6936, this epoch 0.0310, total 5.1114
(T) | Epoch=162, loss=5.5443, this epoch 0.0299, total 5.1413
(T) | Epoch=163, loss=5.5563, this epoch 0.0317, total 5.1730
(T) | Epoch=164, loss=5.6716, this epoch 0.0353, total 5.2082
(T) | Epoch=165, loss=5.5633, this epoch 0.0316, total 5.2398
(T) | Epoch=166, loss=5.6526, this epoch 0.0350, total 5.2748
(T) | Epoch=167, loss=5.5458, this epoch 0.0274, total 5.3022
(T) | Epoch=168, loss=5.6109, this epoch 0.0297, total 5.3319
(T) | Epoch=169, loss=6.0887, this epoch 0.0339, total 5.3658
(T) | Epoch=170, loss=5.6300, this epoch 0.0309, total 5.3967
(T) | Epoch=171, loss=5.5377, this epoch 0.0346, total 5.4313
(T) | Epoch=172, loss=5.6407, this epoch 0.0319, total 5.4632
(T) | Epoch=173, loss=5.6512, this epoch 0.0276, total 5.4908
(T) | Epoch=174, loss=5.6599, this epoch 0.0360, total 5.5268
(T) | Epoch=175, loss=5.5950, this epoch 0.0301, total 5.5569
(T) | Epoch=176, loss=5.6222, this epoch 0.0279, total 5.5848
(T) | Epoch=177, loss=5.5519, this epoch 0.0294, total 5.6142
(T) | Epoch=178, loss=5.6009, this epoch 0.0297, total 5.6439
(T) | Epoch=179, loss=5.8427, this epoch 0.0350, total 5.6789
(T) | Epoch=180, loss=5.5160, this epoch 0.0297, total 5.7086
(T) | Epoch=181, loss=5.5264, this epoch 0.0214, total 5.7299
(T) | Epoch=182, loss=5.5991, this epoch 0.0331, total 5.7630
(T) | Epoch=183, loss=5.5318, this epoch 0.0283, total 5.7913
(T) | Epoch=184, loss=5.6236, this epoch 0.0379, total 5.8292
(T) | Epoch=185, loss=5.5202, this epoch 0.0299, total 5.8591
(T) | Epoch=186, loss=5.5019, this epoch 0.0293, total 5.8885
+++model saved ! 2015.pth
(T) | Epoch=187, loss=5.5239, this epoch 0.0396, total 5.9281
(T) | Epoch=188, loss=5.4774, this epoch 0.0426, total 5.9706
+++model saved ! 2015.pth
(T) | Epoch=189, loss=5.6433, this epoch 0.0370, total 6.0077
(T) | Epoch=190, loss=5.5375, this epoch 0.0332, total 6.0409
(T) | Epoch=191, loss=5.4963, this epoch 0.0306, total 6.0715
(T) | Epoch=192, loss=5.6377, this epoch 0.0390, total 6.1105
(T) | Epoch=193, loss=5.7552, this epoch 0.0407, total 6.1512
(T) | Epoch=194, loss=5.4859, this epoch 0.0314, total 6.1826
(T) | Epoch=195, loss=5.8543, this epoch 0.0304, total 6.2130
(T) | Epoch=196, loss=5.8948, this epoch 0.0351, total 6.2481
(T) | Epoch=197, loss=5.4898, this epoch 0.0319, total 6.2800
(T) | Epoch=198, loss=5.5865, this epoch 0.0310, total 6.3109
(T) | Epoch=199, loss=5.5759, this epoch 0.0355, total 6.3464
(T) | Epoch=200, loss=5.6207, this epoch 0.0370, total 6.3834
(T) | Epoch=201, loss=5.6858, this epoch 0.0387, total 6.4221
(T) | Epoch=202, loss=5.5701, this epoch 0.0353, total 6.4574
(T) | Epoch=203, loss=5.7228, this epoch 0.0317, total 6.4891
(T) | Epoch=204, loss=5.6139, this epoch 0.0287, total 6.5179
(T) | Epoch=205, loss=5.5377, this epoch 0.0326, total 6.5505
(T) | Epoch=206, loss=5.7066, this epoch 0.0350, total 6.5855
(T) | Epoch=207, loss=5.6557, this epoch 0.0285, total 6.6140
(T) | Epoch=208, loss=5.7168, this epoch 0.0338, total 6.6478
(T) | Epoch=209, loss=5.5084, this epoch 0.0363, total 6.6841
(T) | Epoch=210, loss=5.4506, this epoch 0.0315, total 6.7155
+++model saved ! 2015.pth
(T) | Epoch=211, loss=5.5602, this epoch 0.0351, total 6.7506
(T) | Epoch=212, loss=5.5185, this epoch 0.0254, total 6.7760
(T) | Epoch=213, loss=5.5186, this epoch 0.0284, total 6.8044
(T) | Epoch=214, loss=5.5161, this epoch 0.0331, total 6.8374
(T) | Epoch=215, loss=5.5392, this epoch 0.0290, total 6.8665
(T) | Epoch=216, loss=5.6644, this epoch 0.0298, total 6.8962
(T) | Epoch=217, loss=5.5933, this epoch 0.0271, total 6.9233
(T) | Epoch=218, loss=5.4884, this epoch 0.0355, total 6.9588
(T) | Epoch=219, loss=5.4725, this epoch 0.0385, total 6.9973
(T) | Epoch=220, loss=5.5151, this epoch 0.0324, total 7.0296
(T) | Epoch=221, loss=5.5940, this epoch 0.0216, total 7.0512
(T) | Epoch=222, loss=5.7623, this epoch 0.0371, total 7.0883
(T) | Epoch=223, loss=5.5258, this epoch 0.0363, total 7.1246
(T) | Epoch=224, loss=5.5260, this epoch 0.0305, total 7.1551
(T) | Epoch=225, loss=5.5467, this epoch 0.0289, total 7.1841
(T) | Epoch=226, loss=5.5767, this epoch 0.0301, total 7.2141
(T) | Epoch=227, loss=5.5687, this epoch 0.0356, total 7.2497
(T) | Epoch=228, loss=5.5500, this epoch 0.0348, total 7.2846
(T) | Epoch=229, loss=5.4830, this epoch 0.0236, total 7.3082
(T) | Epoch=230, loss=5.4849, this epoch 0.0361, total 7.3443
(T) | Epoch=231, loss=5.6051, this epoch 0.0366, total 7.3809
(T) | Epoch=232, loss=5.5024, this epoch 0.0363, total 7.4172
(T) | Epoch=233, loss=5.4967, this epoch 0.0306, total 7.4478
(T) | Epoch=234, loss=5.5081, this epoch 0.0237, total 7.4715
(T) | Epoch=235, loss=5.4733, this epoch 0.0220, total 7.4935
(T) | Epoch=236, loss=5.4702, this epoch 0.0451, total 7.5386
(T) | Epoch=237, loss=5.5384, this epoch 0.0278, total 7.5664
(T) | Epoch=238, loss=5.4866, this epoch 0.0290, total 7.5955
(T) | Epoch=239, loss=5.4858, this epoch 0.0324, total 7.6279
(T) | Epoch=240, loss=5.4585, this epoch 0.0295, total 7.6574
(T) | Epoch=241, loss=5.4662, this epoch 0.0366, total 7.6940
(T) | Epoch=242, loss=5.4382, this epoch 0.0262, total 7.7202
+++model saved ! 2015.pth
(T) | Epoch=243, loss=5.5176, this epoch 0.0294, total 7.7496
(T) | Epoch=244, loss=5.4797, this epoch 0.0220, total 7.7717
(T) | Epoch=245, loss=5.4400, this epoch 0.0270, total 7.7987
(T) | Epoch=246, loss=5.7701, this epoch 0.0226, total 7.8213
(T) | Epoch=247, loss=5.5373, this epoch 0.0217, total 7.8430
(T) | Epoch=248, loss=5.5813, this epoch 0.0225, total 7.8655
(T) | Epoch=249, loss=5.5397, this epoch 0.0224, total 7.8879
(T) | Epoch=250, loss=5.6056, this epoch 0.0284, total 7.9162
(T) | Epoch=251, loss=5.5702, this epoch 0.0218, total 7.9381
(T) | Epoch=252, loss=5.7220, this epoch 0.0216, total 7.9596
(T) | Epoch=253, loss=5.5406, this epoch 0.0217, total 7.9814
(T) | Epoch=254, loss=5.5306, this epoch 0.0222, total 8.0035
(T) | Epoch=255, loss=5.4585, this epoch 0.0263, total 8.0298
(T) | Epoch=256, loss=5.4566, this epoch 0.0222, total 8.0521
(T) | Epoch=257, loss=5.8813, this epoch 0.0270, total 8.0790
(T) | Epoch=258, loss=5.5537, this epoch 0.0232, total 8.1022
(T) | Epoch=259, loss=5.4958, this epoch 0.0218, total 8.1240
(T) | Epoch=260, loss=5.4418, this epoch 0.0279, total 8.1520
(T) | Epoch=261, loss=5.4479, this epoch 0.0218, total 8.1738
(T) | Epoch=262, loss=5.4576, this epoch 0.0216, total 8.1954
(T) | Epoch=263, loss=5.5335, this epoch 0.0217, total 8.2171
(T) | Epoch=264, loss=5.4604, this epoch 0.0280, total 8.2451
(T) | Epoch=265, loss=5.5186, this epoch 0.0268, total 8.2718
(T) | Epoch=266, loss=5.4479, this epoch 0.0294, total 8.3013
(T) | Epoch=267, loss=5.4676, this epoch 0.0237, total 8.3249
(T) | Epoch=268, loss=5.8167, this epoch 0.0329, total 8.3578
(T) | Epoch=269, loss=5.6987, this epoch 0.0358, total 8.3936
(T) | Epoch=270, loss=5.4679, this epoch 0.0236, total 8.4172
(T) | Epoch=271, loss=6.1941, this epoch 0.0218, total 8.4391
(T) | Epoch=272, loss=5.4472, this epoch 0.0285, total 8.4676
(T) | Epoch=273, loss=5.6609, this epoch 0.0270, total 8.4946
(T) | Epoch=274, loss=6.1314, this epoch 0.0305, total 8.5251
(T) | Epoch=275, loss=6.0612, this epoch 0.0286, total 8.5537
(T) | Epoch=276, loss=6.0916, this epoch 0.0220, total 8.5757
(T) | Epoch=277, loss=6.2599, this epoch 0.0222, total 8.5979
(T) | Epoch=278, loss=6.5025, this epoch 0.0218, total 8.6197
(T) | Epoch=279, loss=6.5513, this epoch 0.0267, total 8.6464
(T) | Epoch=280, loss=5.4721, this epoch 0.0233, total 8.6696
(T) | Epoch=281, loss=6.2545, this epoch 0.0220, total 8.6917
(T) | Epoch=282, loss=6.2623, this epoch 0.0219, total 8.7135
(T) | Epoch=283, loss=6.6019, this epoch 0.0214, total 8.7349
(T) | Epoch=284, loss=6.2044, this epoch 0.0220, total 8.7569
(T) | Epoch=285, loss=6.2058, this epoch 0.0221, total 8.7790
(T) | Epoch=286, loss=6.2458, this epoch 0.0220, total 8.8009
(T) | Epoch=287, loss=6.6400, this epoch 0.0216, total 8.8225
(T) | Epoch=288, loss=6.5835, this epoch 0.0218, total 8.8444
(T) | Epoch=289, loss=6.2874, this epoch 0.0217, total 8.8661
(T) | Epoch=290, loss=6.2720, this epoch 0.0285, total 8.8946
(T) | Epoch=291, loss=6.5008, this epoch 0.0219, total 8.9164
(T) | Epoch=292, loss=6.2123, this epoch 0.0267, total 8.9431
(T) | Epoch=293, loss=6.2381, this epoch 0.0233, total 8.9663
(T) | Epoch=294, loss=6.2386, this epoch 0.0219, total 8.9882
(T) | Epoch=295, loss=6.2049, this epoch 0.0218, total 9.0101
(T) | Epoch=296, loss=6.2041, this epoch 0.0219, total 9.0319
(T) | Epoch=297, loss=6.2023, this epoch 0.0217, total 9.0536
(T) | Epoch=298, loss=6.1916, this epoch 0.0221, total 9.0757
(T) | Epoch=299, loss=6.1928, this epoch 0.0216, total 9.0973
(T) | Epoch=300, loss=6.2349, this epoch 0.0309, total 9.1283
(T) | Epoch=301, loss=6.1909, this epoch 0.0218, total 9.1500
(T) | Epoch=302, loss=6.2020, this epoch 0.0216, total 9.1716
(T) | Epoch=303, loss=6.6337, this epoch 0.0223, total 9.1939
(T) | Epoch=304, loss=6.2454, this epoch 0.0215, total 9.2154
(T) | Epoch=305, loss=6.1594, this epoch 0.0220, total 9.2374
(T) | Epoch=306, loss=6.2184, this epoch 0.0216, total 9.2590
(T) | Epoch=307, loss=6.2316, this epoch 0.0217, total 9.2807
(T) | Epoch=308, loss=6.1389, this epoch 0.0218, total 9.3025
(T) | Epoch=309, loss=6.1474, this epoch 0.0222, total 9.3247
(T) | Epoch=310, loss=6.1200, this epoch 0.0285, total 9.3531
(T) | Epoch=311, loss=6.2501, this epoch 0.0284, total 9.3815
(T) | Epoch=312, loss=6.3235, this epoch 0.0216, total 9.4031
(T) | Epoch=313, loss=6.0877, this epoch 0.0211, total 9.4243
(T) | Epoch=314, loss=6.1825, this epoch 0.0214, total 9.4457
(T) | Epoch=315, loss=6.1606, this epoch 0.0222, total 9.4679
(T) | Epoch=316, loss=6.1346, this epoch 0.0217, total 9.4896
(T) | Epoch=317, loss=6.0499, this epoch 0.0219, total 9.5115
(T) | Epoch=318, loss=6.1340, this epoch 0.0214, total 9.5329
(T) | Epoch=319, loss=6.1094, this epoch 0.0311, total 9.5640
(T) | Epoch=320, loss=5.9998, this epoch 0.0292, total 9.5931
(T) | Epoch=321, loss=6.0074, this epoch 0.0217, total 9.6149
(T) | Epoch=322, loss=6.1328, this epoch 0.0289, total 9.6438
(T) | Epoch=323, loss=6.2367, this epoch 0.0218, total 9.6656
(T) | Epoch=324, loss=6.0144, this epoch 0.0216, total 9.6872
(T) | Epoch=325, loss=5.9449, this epoch 0.0212, total 9.7084
(T) | Epoch=326, loss=6.0434, this epoch 0.0213, total 9.7297
(T) | Epoch=327, loss=5.9020, this epoch 0.0218, total 9.7515
(T) | Epoch=328, loss=6.1169, this epoch 0.0288, total 9.7803
(T) | Epoch=329, loss=5.8791, this epoch 0.0357, total 9.8160
(T) | Epoch=330, loss=5.8667, this epoch 0.0310, total 9.8470
(T) | Epoch=331, loss=5.8375, this epoch 0.0217, total 9.8687
(T) | Epoch=332, loss=5.8661, this epoch 0.0342, total 9.9029
(T) | Epoch=333, loss=6.2030, this epoch 0.0226, total 9.9255
(T) | Epoch=334, loss=5.8829, this epoch 0.0223, total 9.9478
(T) | Epoch=335, loss=5.8186, this epoch 0.0219, total 9.9698
(T) | Epoch=336, loss=5.8234, this epoch 0.0217, total 9.9914
(T) | Epoch=337, loss=5.8930, this epoch 0.0214, total 10.0129
(T) | Epoch=338, loss=5.7735, this epoch 0.0216, total 10.0345
(T) | Epoch=339, loss=6.1419, this epoch 0.0288, total 10.0633
(T) | Epoch=340, loss=5.7431, this epoch 0.0219, total 10.0852
(T) | Epoch=341, loss=5.7633, this epoch 0.0224, total 10.1076
(T) | Epoch=342, loss=5.8036, this epoch 0.0219, total 10.1295
(T) | Epoch=343, loss=5.7080, this epoch 0.0339, total 10.1635
(T) | Epoch=344, loss=5.7409, this epoch 0.0240, total 10.1875
(T) | Epoch=345, loss=5.7964, this epoch 0.0367, total 10.2242
(T) | Epoch=346, loss=5.7684, this epoch 0.0311, total 10.2553
(T) | Epoch=347, loss=5.7332, this epoch 0.0224, total 10.2777
(T) | Epoch=348, loss=5.8051, this epoch 0.0218, total 10.2995
(T) | Epoch=349, loss=5.6643, this epoch 0.0218, total 10.3213
(T) | Epoch=350, loss=5.6727, this epoch 0.0343, total 10.3556
(T) | Epoch=351, loss=5.5700, this epoch 0.0234, total 10.3790
(T) | Epoch=352, loss=5.6498, this epoch 0.0219, total 10.4009
(T) | Epoch=353, loss=5.8596, this epoch 0.0270, total 10.4279
(T) | Epoch=354, loss=5.5419, this epoch 0.0226, total 10.4505
(T) | Epoch=355, loss=5.5567, this epoch 0.0356, total 10.4861
(T) | Epoch=356, loss=5.6211, this epoch 0.0344, total 10.5205
(T) | Epoch=357, loss=5.6299, this epoch 0.0423, total 10.5629
(T) | Epoch=358, loss=5.6012, this epoch 0.0388, total 10.6016
(T) | Epoch=359, loss=5.5336, this epoch 0.0370, total 10.6386
(T) | Epoch=360, loss=5.6693, this epoch 0.0227, total 10.6613
(T) | Epoch=361, loss=5.4747, this epoch 0.0219, total 10.6832
(T) | Epoch=362, loss=5.4624, this epoch 0.0222, total 10.7054
(T) | Epoch=363, loss=5.4755, this epoch 0.0215, total 10.7269
(T) | Epoch=364, loss=5.9041, this epoch 0.0275, total 10.7544
(T) | Epoch=365, loss=5.8374, this epoch 0.0234, total 10.7778
(T) | Epoch=366, loss=5.6171, this epoch 0.0316, total 10.8094
(T) | Epoch=367, loss=5.9630, this epoch 0.0283, total 10.8377
(T) | Epoch=368, loss=5.5305, this epoch 0.0335, total 10.8712
(T) | Epoch=369, loss=5.5830, this epoch 0.0235, total 10.8947
(T) | Epoch=370, loss=5.7993, this epoch 0.0214, total 10.9161
(T) | Epoch=371, loss=5.7036, this epoch 0.0252, total 10.9413
(T) | Epoch=372, loss=5.8526, this epoch 0.0274, total 10.9687
(T) | Epoch=373, loss=5.6596, this epoch 0.0223, total 10.9910
(T) | Epoch=374, loss=5.8250, this epoch 0.0229, total 11.0139
(T) | Epoch=375, loss=6.0241, this epoch 0.0303, total 11.0442
(T) | Epoch=376, loss=5.6424, this epoch 0.0230, total 11.0673
(T) | Epoch=377, loss=5.6452, this epoch 0.0214, total 11.0887
(T) | Epoch=378, loss=5.7309, this epoch 0.0218, total 11.1105
(T) | Epoch=379, loss=5.7704, this epoch 0.0256, total 11.1361
(T) | Epoch=380, loss=5.6148, this epoch 0.0234, total 11.1595
(T) | Epoch=381, loss=5.6558, this epoch 0.0265, total 11.1860
(T) | Epoch=382, loss=5.6497, this epoch 0.0212, total 11.2072
(T) | Epoch=383, loss=5.6110, this epoch 0.0217, total 11.2288
(T) | Epoch=384, loss=5.7810, this epoch 0.0214, total 11.2502
(T) | Epoch=385, loss=5.9786, this epoch 0.0219, total 11.2721
(T) | Epoch=386, loss=5.6451, this epoch 0.0216, total 11.2937
(T) | Epoch=387, loss=5.5955, this epoch 0.0218, total 11.3155
(T) | Epoch=388, loss=5.6465, this epoch 0.0215, total 11.3370
(T) | Epoch=389, loss=5.4910, this epoch 0.0219, total 11.3589
(T) | Epoch=390, loss=5.6189, this epoch 0.0215, total 11.3804
(T) | Epoch=391, loss=5.4685, this epoch 0.0219, total 11.4023
(T) | Epoch=392, loss=5.4958, this epoch 0.0266, total 11.4289
(T) | Epoch=393, loss=5.5641, this epoch 0.0255, total 11.4544
(T) | Epoch=394, loss=5.4502, this epoch 0.0301, total 11.4846
(T) | Epoch=395, loss=5.4532, this epoch 0.0278, total 11.5124
(T) | Epoch=396, loss=5.4759, this epoch 0.0232, total 11.5356
(T) | Epoch=397, loss=5.4575, this epoch 0.0218, total 11.5574
(T) | Epoch=398, loss=5.6009, this epoch 0.0221, total 11.5795
(T) | Epoch=399, loss=5.5847, this epoch 0.0262, total 11.6057
(T) | Epoch=400, loss=5.4685, this epoch 0.0296, total 11.6353
(T) | Epoch=401, loss=5.5455, this epoch 0.0226, total 11.6579
(T) | Epoch=402, loss=5.6247, this epoch 0.0218, total 11.6797
(T) | Epoch=403, loss=5.4525, this epoch 0.0227, total 11.7025
(T) | Epoch=404, loss=5.4362, this epoch 0.0218, total 11.7242
+++model saved ! 2015.pth
(T) | Epoch=405, loss=5.4400, this epoch 0.0426, total 11.7669
(T) | Epoch=406, loss=5.4798, this epoch 0.0313, total 11.7982
(T) | Epoch=407, loss=5.5529, this epoch 0.0361, total 11.8343
(T) | Epoch=408, loss=5.4490, this epoch 0.0369, total 11.8712
(T) | Epoch=409, loss=5.4630, this epoch 0.0392, total 11.9104
(T) | Epoch=410, loss=5.5495, this epoch 0.0321, total 11.9424
(T) | Epoch=411, loss=5.5500, this epoch 0.0289, total 11.9713
(T) | Epoch=412, loss=5.6063, this epoch 0.0312, total 12.0025
(T) | Epoch=413, loss=5.4396, this epoch 0.0369, total 12.0393
(T) | Epoch=414, loss=5.4364, this epoch 0.0380, total 12.0773
(T) | Epoch=415, loss=5.6077, this epoch 0.0336, total 12.1110
(T) | Epoch=416, loss=5.5016, this epoch 0.0260, total 12.1369
(T) | Epoch=417, loss=5.4417, this epoch 0.0292, total 12.1661
(T) | Epoch=418, loss=5.4503, this epoch 0.0360, total 12.2022
(T) | Epoch=419, loss=5.4448, this epoch 0.0307, total 12.2329
(T) | Epoch=420, loss=5.6126, this epoch 0.0356, total 12.2685
(T) | Epoch=421, loss=5.4671, this epoch 0.0300, total 12.2985
(T) | Epoch=422, loss=5.4307, this epoch 0.0290, total 12.3275
+++model saved ! 2015.pth
(T) | Epoch=423, loss=5.5285, this epoch 0.0309, total 12.3584
(T) | Epoch=424, loss=5.5160, this epoch 0.0326, total 12.3910
(T) | Epoch=425, loss=5.5378, this epoch 0.0399, total 12.4309
(T) | Epoch=426, loss=5.4382, this epoch 0.0326, total 12.4636
(T) | Epoch=427, loss=5.4949, this epoch 0.0300, total 12.4936
(T) | Epoch=428, loss=5.4308, this epoch 0.0310, total 12.5246
(T) | Epoch=429, loss=5.4630, this epoch 0.0368, total 12.5614
(T) | Epoch=430, loss=5.4905, this epoch 0.0297, total 12.5912
(T) | Epoch=431, loss=5.4332, this epoch 0.0376, total 12.6287
(T) | Epoch=432, loss=5.4322, this epoch 0.0302, total 12.6590
(T) | Epoch=433, loss=5.4577, this epoch 0.0286, total 12.6876
(T) | Epoch=434, loss=5.4955, this epoch 0.0304, total 12.7180
(T) | Epoch=435, loss=5.4330, this epoch 0.0308, total 12.7488
(T) | Epoch=436, loss=5.4945, this epoch 0.0356, total 12.7844
(T) | Epoch=437, loss=5.4604, this epoch 0.0405, total 12.8249
(T) | Epoch=438, loss=5.4203, this epoch 0.0320, total 12.8570
+++model saved ! 2015.pth
(T) | Epoch=439, loss=5.5291, this epoch 0.0312, total 12.8881
(T) | Epoch=440, loss=5.4317, this epoch 0.0236, total 12.9117
(T) | Epoch=441, loss=5.5912, this epoch 0.0217, total 12.9334
(T) | Epoch=442, loss=5.4522, this epoch 0.0293, total 12.9627
(T) | Epoch=443, loss=5.4724, this epoch 0.0385, total 13.0013
(T) | Epoch=444, loss=5.5039, this epoch 0.0233, total 13.0246
(T) | Epoch=445, loss=5.4231, this epoch 0.0219, total 13.0465
(T) | Epoch=446, loss=5.7531, this epoch 0.0218, total 13.0682
(T) | Epoch=447, loss=5.7189, this epoch 0.0215, total 13.0898
(T) | Epoch=448, loss=5.4252, this epoch 0.0217, total 13.1115
(T) | Epoch=449, loss=5.5057, this epoch 0.0301, total 13.1415
(T) | Epoch=450, loss=5.5007, this epoch 0.0307, total 13.1722
(T) | Epoch=451, loss=5.4943, this epoch 0.0270, total 13.1992
(T) | Epoch=452, loss=5.4707, this epoch 0.0300, total 13.2293
(T) | Epoch=453, loss=5.4843, this epoch 0.0306, total 13.2598
(T) | Epoch=454, loss=5.4385, this epoch 0.0326, total 13.2924
(T) | Epoch=455, loss=5.4589, this epoch 0.0287, total 13.3211
(T) | Epoch=456, loss=5.5895, this epoch 0.0218, total 13.3429
(T) | Epoch=457, loss=5.4486, this epoch 0.0225, total 13.3654
(T) | Epoch=458, loss=5.5444, this epoch 0.0220, total 13.3874
(T) | Epoch=459, loss=5.4526, this epoch 0.0224, total 13.4098
(T) | Epoch=460, loss=5.4215, this epoch 0.0285, total 13.4383
(T) | Epoch=461, loss=5.4392, this epoch 0.0280, total 13.4662
(T) | Epoch=462, loss=5.5083, this epoch 0.0227, total 13.4889
(T) | Epoch=463, loss=5.4156, this epoch 0.0220, total 13.5110
+++model saved ! 2015.pth
(T) | Epoch=464, loss=5.4269, this epoch 0.0318, total 13.5428
(T) | Epoch=465, loss=5.4246, this epoch 0.0244, total 13.5672
(T) | Epoch=466, loss=5.4477, this epoch 0.0225, total 13.5898
(T) | Epoch=467, loss=5.4322, this epoch 0.0342, total 13.6240
(T) | Epoch=468, loss=5.6179, this epoch 0.0276, total 13.6515
(T) | Epoch=469, loss=5.4339, this epoch 0.0214, total 13.6730
(T) | Epoch=470, loss=5.5125, this epoch 0.0215, total 13.6945
(T) | Epoch=471, loss=5.4552, this epoch 0.0217, total 13.7162
(T) | Epoch=472, loss=5.4483, this epoch 0.0218, total 13.7380
(T) | Epoch=473, loss=5.4204, this epoch 0.0277, total 13.7657
(T) | Epoch=474, loss=5.4519, this epoch 0.0340, total 13.7998
(T) | Epoch=475, loss=5.4931, this epoch 0.0249, total 13.8246
(T) | Epoch=476, loss=5.5540, this epoch 0.0213, total 13.8460
(T) | Epoch=477, loss=5.4611, this epoch 0.0278, total 13.8738
(T) | Epoch=478, loss=5.4274, this epoch 0.0290, total 13.9028
(T) | Epoch=479, loss=5.4271, this epoch 0.0285, total 13.9313
(T) | Epoch=480, loss=5.4793, this epoch 0.0218, total 13.9531
(T) | Epoch=481, loss=5.5025, this epoch 0.0286, total 13.9817
(T) | Epoch=482, loss=5.4387, this epoch 0.0284, total 14.0101
(T) | Epoch=483, loss=5.5007, this epoch 0.0223, total 14.0324
(T) | Epoch=484, loss=5.4266, this epoch 0.0289, total 14.0613
(T) | Epoch=485, loss=5.4199, this epoch 0.0212, total 14.0825
(T) | Epoch=486, loss=5.5012, this epoch 0.0336, total 14.1161
(T) | Epoch=487, loss=5.4642, this epoch 0.0245, total 14.1406
(T) | Epoch=488, loss=5.4254, this epoch 0.0276, total 14.1682
(T) | Epoch=489, loss=5.4601, this epoch 0.0282, total 14.1964
(T) | Epoch=490, loss=5.4232, this epoch 0.0218, total 14.2182
(T) | Epoch=491, loss=5.4730, this epoch 0.0215, total 14.2397
(T) | Epoch=492, loss=5.5828, this epoch 0.0342, total 14.2739
(T) | Epoch=493, loss=5.6463, this epoch 0.0245, total 14.2984
(T) | Epoch=494, loss=5.4714, this epoch 0.0277, total 14.3261
(T) | Epoch=495, loss=5.4351, this epoch 0.0286, total 14.3548
(T) | Epoch=496, loss=5.5540, this epoch 0.0367, total 14.3914
(T) | Epoch=497, loss=5.4227, this epoch 0.0310, total 14.4224
(T) | Epoch=498, loss=5.4147, this epoch 0.0345, total 14.4569
+++model saved ! 2015.pth
(T) | Epoch=499, loss=5.4123, this epoch 0.0433, total 14.5002
+++model saved ! 2015.pth
(T) | Epoch=500, loss=5.4194, this epoch 0.0256, total 14.5259
(T) | Epoch=501, loss=5.5119, this epoch 0.0218, total 14.5476
(T) | Epoch=502, loss=5.5781, this epoch 0.0217, total 14.5693
(T) | Epoch=503, loss=5.4564, this epoch 0.0221, total 14.5914
(T) | Epoch=504, loss=5.4778, this epoch 0.0222, total 14.6136
(T) | Epoch=505, loss=5.5049, this epoch 0.0220, total 14.6356
(T) | Epoch=506, loss=5.4150, this epoch 0.0261, total 14.6617
(T) | Epoch=507, loss=5.4572, this epoch 0.0232, total 14.6850
(T) | Epoch=508, loss=5.4238, this epoch 0.0267, total 14.7117
(T) | Epoch=509, loss=5.4941, this epoch 0.0258, total 14.7375
(T) | Epoch=510, loss=5.5819, this epoch 0.0223, total 14.7599
(T) | Epoch=511, loss=5.4522, this epoch 0.0258, total 14.7857
(T) | Epoch=512, loss=5.4787, this epoch 0.0274, total 14.8131
(T) | Epoch=513, loss=5.4105, this epoch 0.0213, total 14.8344
+++model saved ! 2015.pth
(T) | Epoch=514, loss=5.4909, this epoch 0.0296, total 14.8640
(T) | Epoch=515, loss=5.4507, this epoch 0.0225, total 14.8865
(T) | Epoch=516, loss=5.4115, this epoch 0.0220, total 14.9085
(T) | Epoch=517, loss=5.4083, this epoch 0.0350, total 14.9435
+++model saved ! 2015.pth
(T) | Epoch=518, loss=5.4304, this epoch 0.0393, total 14.9828
(T) | Epoch=519, loss=5.4396, this epoch 0.0254, total 15.0082
(T) | Epoch=520, loss=5.4213, this epoch 0.0242, total 15.0325
(T) | Epoch=521, loss=5.6394, this epoch 0.0214, total 15.0539
(T) | Epoch=522, loss=5.4966, this epoch 0.0266, total 15.0804
(T) | Epoch=523, loss=5.4166, this epoch 0.0357, total 15.1161
(T) | Epoch=524, loss=5.5204, this epoch 0.0303, total 15.1464
(T) | Epoch=525, loss=5.4660, this epoch 0.0287, total 15.1752
(T) | Epoch=526, loss=5.4446, this epoch 0.0285, total 15.2036
(T) | Epoch=527, loss=5.5216, this epoch 0.0224, total 15.2260
(T) | Epoch=528, loss=5.4499, this epoch 0.0344, total 15.2604
(T) | Epoch=529, loss=5.4461, this epoch 0.0284, total 15.2888
(T) | Epoch=530, loss=5.7697, this epoch 0.0285, total 15.3173
(T) | Epoch=531, loss=5.4530, this epoch 0.0293, total 15.3466
(T) | Epoch=532, loss=5.4320, this epoch 0.0362, total 15.3829
(T) | Epoch=533, loss=5.4671, this epoch 0.0344, total 15.4173
(T) | Epoch=534, loss=5.4261, this epoch 0.0305, total 15.4478
(T) | Epoch=535, loss=5.4369, this epoch 0.0218, total 15.4696
(T) | Epoch=536, loss=5.4156, this epoch 0.0269, total 15.4965
(T) | Epoch=537, loss=5.4810, this epoch 0.0363, total 15.5327
(T) | Epoch=538, loss=5.4287, this epoch 0.0313, total 15.5640
(T) | Epoch=539, loss=5.4810, this epoch 0.0289, total 15.5929
(T) | Epoch=540, loss=5.4084, this epoch 0.0288, total 15.6217
(T) | Epoch=541, loss=5.4099, this epoch 0.0300, total 15.6517
(T) | Epoch=542, loss=5.4558, this epoch 0.0293, total 15.6810
(T) | Epoch=543, loss=5.4880, this epoch 0.0280, total 15.7090
(T) | Epoch=544, loss=5.3996, this epoch 0.0339, total 15.7429
+++model saved ! 2015.pth
(T) | Epoch=545, loss=5.4638, this epoch 0.0310, total 15.7739
(T) | Epoch=546, loss=5.4003, this epoch 0.0308, total 15.8048
(T) | Epoch=547, loss=5.4178, this epoch 0.0340, total 15.8388
(T) | Epoch=548, loss=5.4627, this epoch 0.0363, total 15.8751
(T) | Epoch=549, loss=5.4088, this epoch 0.0291, total 15.9042
(T) | Epoch=550, loss=5.4664, this epoch 0.0305, total 15.9347
(T) | Epoch=551, loss=5.4519, this epoch 0.0394, total 15.9741
(T) | Epoch=552, loss=5.4807, this epoch 0.0370, total 16.0111
(T) | Epoch=553, loss=5.4028, this epoch 0.0396, total 16.0507
(T) | Epoch=554, loss=5.4194, this epoch 0.0290, total 16.0797
(T) | Epoch=555, loss=5.4337, this epoch 0.0340, total 16.1137
(T) | Epoch=556, loss=5.4022, this epoch 0.0307, total 16.1444
(T) | Epoch=557, loss=5.4235, this epoch 0.0291, total 16.1735
(T) | Epoch=558, loss=5.3940, this epoch 0.0367, total 16.2102
+++model saved ! 2015.pth
(T) | Epoch=559, loss=5.4017, this epoch 0.0256, total 16.2358
(T) | Epoch=560, loss=5.4002, this epoch 0.0272, total 16.2630
(T) | Epoch=561, loss=5.4339, this epoch 0.0275, total 16.2905
(T) | Epoch=562, loss=5.4761, this epoch 0.0278, total 16.3183
(T) | Epoch=563, loss=5.4846, this epoch 0.0226, total 16.3409
(T) | Epoch=564, loss=5.4196, this epoch 0.0217, total 16.3626
(T) | Epoch=565, loss=5.3929, this epoch 0.0348, total 16.3974
+++model saved ! 2015.pth
(T) | Epoch=566, loss=5.4045, this epoch 0.0292, total 16.4266
(T) | Epoch=567, loss=5.3953, this epoch 0.0295, total 16.4561
(T) | Epoch=568, loss=5.4318, this epoch 0.0223, total 16.4784
(T) | Epoch=569, loss=5.4803, this epoch 0.0357, total 16.5141
(T) | Epoch=570, loss=5.4449, this epoch 0.0289, total 16.5430
(T) | Epoch=571, loss=5.3929, this epoch 0.0281, total 16.5711
(T) | Epoch=572, loss=5.4055, this epoch 0.0272, total 16.5983
(T) | Epoch=573, loss=5.5337, this epoch 0.0245, total 16.6228
(T) | Epoch=574, loss=5.4835, this epoch 0.0214, total 16.6441
(T) | Epoch=575, loss=5.3933, this epoch 0.0280, total 16.6721
(T) | Epoch=576, loss=5.4332, this epoch 0.0355, total 16.7077
(T) | Epoch=577, loss=5.4184, this epoch 0.0235, total 16.7311
(T) | Epoch=578, loss=5.4450, this epoch 0.0302, total 16.7613
(T) | Epoch=579, loss=5.4045, this epoch 0.0346, total 16.7959
(T) | Epoch=580, loss=5.4775, this epoch 0.0319, total 16.8278
(T) | Epoch=581, loss=5.5230, this epoch 0.0315, total 16.8592
(T) | Epoch=582, loss=5.3867, this epoch 0.0298, total 16.8890
+++model saved ! 2015.pth
(T) | Epoch=583, loss=5.4258, this epoch 0.0309, total 16.9200
(T) | Epoch=584, loss=5.4008, this epoch 0.0228, total 16.9428
(T) | Epoch=585, loss=5.4499, this epoch 0.0218, total 16.9646
(T) | Epoch=586, loss=5.3967, this epoch 0.0363, total 17.0009
(T) | Epoch=587, loss=5.3927, this epoch 0.0274, total 17.0283
(T) | Epoch=588, loss=5.3894, this epoch 0.0447, total 17.0731
(T) | Epoch=589, loss=5.3882, this epoch 0.0309, total 17.1040
(T) | Epoch=590, loss=5.4413, this epoch 0.0288, total 17.1328
(T) | Epoch=591, loss=5.4813, this epoch 0.0388, total 17.1715
(T) | Epoch=592, loss=5.4879, this epoch 0.0442, total 17.2158
(T) | Epoch=593, loss=5.4199, this epoch 0.0354, total 17.2511
(T) | Epoch=594, loss=5.4542, this epoch 0.0318, total 17.2830
(T) | Epoch=595, loss=5.4107, this epoch 0.0339, total 17.3169
(T) | Epoch=596, loss=5.4156, this epoch 0.0317, total 17.3486
(T) | Epoch=597, loss=5.4049, this epoch 0.0371, total 17.3857
(T) | Epoch=598, loss=5.4162, this epoch 0.0362, total 17.4219
(T) | Epoch=599, loss=5.4080, this epoch 0.0384, total 17.4602
(T) | Epoch=600, loss=5.3987, this epoch 0.0253, total 17.4855
(T) | Epoch=601, loss=5.4383, this epoch 0.0330, total 17.5185
(T) | Epoch=602, loss=5.4394, this epoch 0.0243, total 17.5428
(T) | Epoch=603, loss=5.4434, this epoch 0.0266, total 17.5694
(T) | Epoch=604, loss=5.3968, this epoch 0.0365, total 17.6059
(T) | Epoch=605, loss=5.4324, this epoch 0.0305, total 17.6365
(T) | Epoch=606, loss=5.3914, this epoch 0.0387, total 17.6752
(T) | Epoch=607, loss=5.4746, this epoch 0.0363, total 17.7114
(T) | Epoch=608, loss=5.4322, this epoch 0.0234, total 17.7348
(T) | Epoch=609, loss=5.4214, this epoch 0.0219, total 17.7567
(T) | Epoch=610, loss=5.3942, this epoch 0.0281, total 17.7847
(T) | Epoch=611, loss=5.3984, this epoch 0.0370, total 17.8217
(T) | Epoch=612, loss=5.4614, this epoch 0.0335, total 17.8552
(T) | Epoch=613, loss=5.3986, this epoch 0.0291, total 17.8843
(T) | Epoch=614, loss=5.3817, this epoch 0.0281, total 17.9124
+++model saved ! 2015.pth
(T) | Epoch=615, loss=5.4214, this epoch 0.0297, total 17.9421
(T) | Epoch=616, loss=5.3858, this epoch 0.0327, total 17.9748
(T) | Epoch=617, loss=5.4838, this epoch 0.0299, total 18.0048
(T) | Epoch=618, loss=5.4272, this epoch 0.0341, total 18.0389
(T) | Epoch=619, loss=5.4520, this epoch 0.0373, total 18.0761
(T) | Epoch=620, loss=5.3841, this epoch 0.0299, total 18.1061
(T) | Epoch=621, loss=5.4481, this epoch 0.0289, total 18.1350
(T) | Epoch=622, loss=5.3893, this epoch 0.0218, total 18.1568
(T) | Epoch=623, loss=5.4128, this epoch 0.0332, total 18.1900
(T) | Epoch=624, loss=5.4245, this epoch 0.0304, total 18.2204
(T) | Epoch=625, loss=5.4436, this epoch 0.0279, total 18.2483
(T) | Epoch=626, loss=5.4665, this epoch 0.0350, total 18.2833
(T) | Epoch=627, loss=5.4290, this epoch 0.0277, total 18.3110
(T) | Epoch=628, loss=5.4125, this epoch 0.0300, total 18.3409
(T) | Epoch=629, loss=5.5287, this epoch 0.0335, total 18.3744
(T) | Epoch=630, loss=5.4578, this epoch 0.0372, total 18.4116
(T) | Epoch=631, loss=5.4757, this epoch 0.0279, total 18.4395
(T) | Epoch=632, loss=5.4147, this epoch 0.0364, total 18.4760
(T) | Epoch=633, loss=5.4140, this epoch 0.0373, total 18.5133
(T) | Epoch=634, loss=5.3873, this epoch 0.0254, total 18.5387
(T) | Epoch=635, loss=5.3738, this epoch 0.0351, total 18.5738
+++model saved ! 2015.pth
(T) | Epoch=636, loss=5.4164, this epoch 0.0390, total 18.6127
(T) | Epoch=637, loss=5.4186, this epoch 0.0247, total 18.6375
(T) | Epoch=638, loss=5.3951, this epoch 0.0270, total 18.6644
(T) | Epoch=639, loss=5.3783, this epoch 0.0350, total 18.6994
(T) | Epoch=640, loss=5.4784, this epoch 0.0315, total 18.7309
(T) | Epoch=641, loss=5.3808, this epoch 0.0298, total 18.7607
(T) | Epoch=642, loss=5.4367, this epoch 0.0329, total 18.7936
(T) | Epoch=643, loss=5.3911, this epoch 0.0286, total 18.8221
(T) | Epoch=644, loss=5.4279, this epoch 0.0290, total 18.8511
(T) | Epoch=645, loss=5.4048, this epoch 0.0223, total 18.8734
(T) | Epoch=646, loss=5.3812, this epoch 0.0304, total 18.9038
(T) | Epoch=647, loss=5.3814, this epoch 0.0285, total 18.9324
(T) | Epoch=648, loss=5.3960, this epoch 0.0222, total 18.9545
(T) | Epoch=649, loss=5.3807, this epoch 0.0321, total 18.9867
(T) | Epoch=650, loss=5.3812, this epoch 0.0363, total 19.0229
(T) | Epoch=651, loss=5.3959, this epoch 0.0353, total 19.0582
(T) | Epoch=652, loss=5.3792, this epoch 0.0356, total 19.0938
(T) | Epoch=653, loss=5.4079, this epoch 0.0285, total 19.1223
(T) | Epoch=654, loss=5.4052, this epoch 0.0290, total 19.1513
(T) | Epoch=655, loss=5.4203, this epoch 0.0352, total 19.1866
(T) | Epoch=656, loss=5.4435, this epoch 0.0395, total 19.2261
(T) | Epoch=657, loss=5.4626, this epoch 0.0335, total 19.2596
(T) | Epoch=658, loss=5.3795, this epoch 0.0324, total 19.2920
(T) | Epoch=659, loss=5.4804, this epoch 0.0292, total 19.3212
(T) | Epoch=660, loss=5.4538, this epoch 0.0292, total 19.3504
(T) | Epoch=661, loss=5.4180, this epoch 0.0288, total 19.3792
(T) | Epoch=662, loss=5.4859, this epoch 0.0299, total 19.4091
(T) | Epoch=663, loss=5.3740, this epoch 0.0222, total 19.4314
(T) | Epoch=664, loss=5.4250, this epoch 0.0364, total 19.4678
(T) | Epoch=665, loss=5.4059, this epoch 0.0373, total 19.5051
(T) | Epoch=666, loss=5.3698, this epoch 0.0353, total 19.5404
+++model saved ! 2015.pth
(T) | Epoch=667, loss=5.4287, this epoch 0.0366, total 19.5770
(T) | Epoch=668, loss=5.4953, this epoch 0.0387, total 19.6157
(T) | Epoch=669, loss=5.3876, this epoch 0.0322, total 19.6479
(T) | Epoch=670, loss=5.3914, this epoch 0.0239, total 19.6718
(T) | Epoch=671, loss=5.3856, this epoch 0.0386, total 19.7105
(T) | Epoch=672, loss=5.3890, this epoch 0.0390, total 19.7494
(T) | Epoch=673, loss=5.3692, this epoch 0.0413, total 19.7907
+++model saved ! 2015.pth
(T) | Epoch=674, loss=5.4544, this epoch 0.0433, total 19.8340
(T) | Epoch=675, loss=5.3751, this epoch 0.0336, total 19.8676
(T) | Epoch=676, loss=5.4191, this epoch 0.0298, total 19.8974
(T) | Epoch=677, loss=5.3874, this epoch 0.0309, total 19.9283
(T) | Epoch=678, loss=5.3973, this epoch 0.0385, total 19.9667
(T) | Epoch=679, loss=5.3596, this epoch 0.0243, total 19.9911
+++model saved ! 2015.pth
(T) | Epoch=680, loss=5.4302, this epoch 0.0233, total 20.0143
(T) | Epoch=681, loss=5.4128, this epoch 0.0361, total 20.0505
(T) | Epoch=682, loss=5.3741, this epoch 0.0296, total 20.0800
(T) | Epoch=683, loss=5.3642, this epoch 0.0375, total 20.1175
(T) | Epoch=684, loss=5.3765, this epoch 0.0227, total 20.1402
(T) | Epoch=685, loss=5.4445, this epoch 0.0219, total 20.1621
(T) | Epoch=686, loss=5.3769, this epoch 0.0216, total 20.1837
(T) | Epoch=687, loss=5.3681, this epoch 0.0305, total 20.2142
(T) | Epoch=688, loss=5.3696, this epoch 0.0360, total 20.2502
(T) | Epoch=689, loss=5.4498, this epoch 0.0225, total 20.2727
(T) | Epoch=690, loss=5.4072, this epoch 0.0374, total 20.3100
(T) | Epoch=691, loss=5.4061, this epoch 0.0372, total 20.3472
(T) | Epoch=692, loss=5.3829, this epoch 0.0313, total 20.3786
(T) | Epoch=693, loss=5.3655, this epoch 0.0288, total 20.4074
(T) | Epoch=694, loss=5.3944, this epoch 0.0223, total 20.4297
(T) | Epoch=695, loss=5.5199, this epoch 0.0220, total 20.4517
(T) | Epoch=696, loss=5.4869, this epoch 0.0376, total 20.4893
(T) | Epoch=697, loss=5.3692, this epoch 0.0294, total 20.5187
(T) | Epoch=698, loss=5.3732, this epoch 0.0289, total 20.5476
(T) | Epoch=699, loss=5.4472, this epoch 0.0277, total 20.5753
(T) | Epoch=700, loss=5.4123, this epoch 0.0261, total 20.6014
(T) | Epoch=701, loss=5.4575, this epoch 0.0332, total 20.6346
(T) | Epoch=702, loss=5.5159, this epoch 0.0269, total 20.6615
(T) | Epoch=703, loss=5.4134, this epoch 0.0373, total 20.6987
(T) | Epoch=704, loss=5.4620, this epoch 0.0390, total 20.7377
(T) | Epoch=705, loss=5.4137, this epoch 0.0290, total 20.7668
(T) | Epoch=706, loss=5.4683, this epoch 0.0287, total 20.7955
(T) | Epoch=707, loss=5.5749, this epoch 0.0375, total 20.8330
(T) | Epoch=708, loss=5.5047, this epoch 0.0302, total 20.8632
(T) | Epoch=709, loss=5.3673, this epoch 0.0313, total 20.8944
(T) | Epoch=710, loss=5.3699, this epoch 0.0355, total 20.9300
(T) | Epoch=711, loss=5.4399, this epoch 0.0301, total 20.9601
(T) | Epoch=712, loss=5.3803, this epoch 0.0223, total 20.9824
(T) | Epoch=713, loss=5.3843, this epoch 0.0363, total 21.0187
(T) | Epoch=714, loss=5.3793, this epoch 0.0234, total 21.0420
(T) | Epoch=715, loss=5.3783, this epoch 0.0268, total 21.0688
(T) | Epoch=716, loss=5.4011, this epoch 0.0292, total 21.0980
(T) | Epoch=717, loss=5.3734, this epoch 0.0307, total 21.1287
(T) | Epoch=718, loss=5.4123, this epoch 0.0314, total 21.1601
(T) | Epoch=719, loss=5.4271, this epoch 0.0376, total 21.1977
(T) | Epoch=720, loss=5.4221, this epoch 0.0391, total 21.2368
(T) | Epoch=721, loss=5.3811, this epoch 0.0236, total 21.2604
(T) | Epoch=722, loss=5.4150, this epoch 0.0287, total 21.2890
(T) | Epoch=723, loss=5.4235, this epoch 0.0281, total 21.3171
(T) | Epoch=724, loss=5.4279, this epoch 0.0388, total 21.3559
(T) | Epoch=725, loss=5.3678, this epoch 0.0293, total 21.3852
(T) | Epoch=726, loss=5.4044, this epoch 0.0307, total 21.4160
(T) | Epoch=727, loss=5.3635, this epoch 0.0369, total 21.4529
(T) | Epoch=728, loss=5.5054, this epoch 0.0299, total 21.4828
(T) | Epoch=729, loss=5.3890, this epoch 0.0224, total 21.5052
(T) | Epoch=730, loss=5.4012, this epoch 0.0367, total 21.5419
(T) | Epoch=731, loss=5.4674, this epoch 0.0288, total 21.5708
(T) | Epoch=732, loss=5.3972, this epoch 0.0304, total 21.6012
(T) | Epoch=733, loss=5.4267, this epoch 0.0224, total 21.6235
(T) | Epoch=734, loss=5.4719, this epoch 0.0272, total 21.6507
(T) | Epoch=735, loss=5.4134, this epoch 0.0369, total 21.6877
(T) | Epoch=736, loss=5.3880, this epoch 0.0232, total 21.7109
(T) | Epoch=737, loss=5.3600, this epoch 0.0282, total 21.7391
(T) | Epoch=738, loss=5.3930, this epoch 0.0274, total 21.7665
(T) | Epoch=739, loss=5.3802, this epoch 0.0384, total 21.8049
(T) | Epoch=740, loss=5.3979, this epoch 0.0294, total 21.8343
(T) | Epoch=741, loss=5.4039, this epoch 0.0294, total 21.8637
(T) | Epoch=742, loss=5.3895, this epoch 0.0367, total 21.9004
(T) | Epoch=743, loss=5.4181, this epoch 0.0297, total 21.9301
(T) | Epoch=744, loss=5.3665, this epoch 0.0297, total 21.9599
(T) | Epoch=745, loss=5.4168, this epoch 0.0280, total 21.9879
(T) | Epoch=746, loss=5.3958, this epoch 0.0382, total 22.0261
(T) | Epoch=747, loss=5.4048, this epoch 0.0235, total 22.0496
(T) | Epoch=748, loss=5.4325, this epoch 0.0370, total 22.0866
(T) | Epoch=749, loss=5.3737, this epoch 0.0291, total 22.1157
(T) | Epoch=750, loss=5.4135, this epoch 0.0363, total 22.1520
(T) | Epoch=751, loss=5.3698, this epoch 0.0386, total 22.1906
(T) | Epoch=752, loss=5.4782, this epoch 0.0388, total 22.2294
(T) | Epoch=753, loss=5.4658, this epoch 0.0366, total 22.2660
(T) | Epoch=754, loss=5.3614, this epoch 0.0238, total 22.2898
(T) | Epoch=755, loss=5.4456, this epoch 0.0273, total 22.3171
(T) | Epoch=756, loss=5.4411, this epoch 0.0366, total 22.3538
(T) | Epoch=757, loss=5.3949, this epoch 0.0321, total 22.3858
(T) | Epoch=758, loss=5.4387, this epoch 0.0301, total 22.4159
(T) | Epoch=759, loss=5.4691, this epoch 0.0304, total 22.4464
(T) | Epoch=760, loss=5.4357, this epoch 0.0358, total 22.4822
(T) | Epoch=761, loss=5.3539, this epoch 0.0293, total 22.5115
+++model saved ! 2015.pth
(T) | Epoch=762, loss=5.4288, this epoch 0.0397, total 22.5512
(T) | Epoch=763, loss=5.3986, this epoch 0.0301, total 22.5813
(T) | Epoch=764, loss=5.3650, this epoch 0.0380, total 22.6193
(T) | Epoch=765, loss=5.3671, this epoch 0.0376, total 22.6569
(T) | Epoch=766, loss=5.4010, this epoch 0.0221, total 22.6790
(T) | Epoch=767, loss=5.3996, this epoch 0.0345, total 22.7135
(T) | Epoch=768, loss=5.4140, this epoch 0.0299, total 22.7434
(T) | Epoch=769, loss=5.3702, this epoch 0.0360, total 22.7794
(T) | Epoch=770, loss=5.4556, this epoch 0.0224, total 22.8017
(T) | Epoch=771, loss=5.3688, this epoch 0.0357, total 22.8374
(T) | Epoch=772, loss=5.3623, this epoch 0.0223, total 22.8597
(T) | Epoch=773, loss=5.4249, this epoch 0.0300, total 22.8896
(T) | Epoch=774, loss=5.3988, this epoch 0.0291, total 22.9187
(T) | Epoch=775, loss=5.3867, this epoch 0.0212, total 22.9399
(T) | Epoch=776, loss=5.3730, this epoch 0.0366, total 22.9765
(T) | Epoch=777, loss=5.4105, this epoch 0.0245, total 23.0010
(T) | Epoch=778, loss=5.3803, this epoch 0.0375, total 23.0386
(T) | Epoch=779, loss=5.3936, this epoch 0.0367, total 23.0752
(T) | Epoch=780, loss=5.3704, this epoch 0.0379, total 23.1131
(T) | Epoch=781, loss=5.4121, this epoch 0.0389, total 23.1520
(T) | Epoch=782, loss=5.3946, this epoch 0.0314, total 23.1834
(T) | Epoch=783, loss=5.3889, this epoch 0.0347, total 23.2180
(T) | Epoch=784, loss=5.3704, this epoch 0.0395, total 23.2575
(T) | Epoch=785, loss=5.4164, this epoch 0.0315, total 23.2890
(T) | Epoch=786, loss=5.4374, this epoch 0.0287, total 23.3177
(T) | Epoch=787, loss=5.3925, this epoch 0.0292, total 23.3469
(T) | Epoch=788, loss=5.3876, this epoch 0.0379, total 23.3848
(T) | Epoch=789, loss=5.3820, this epoch 0.0331, total 23.4179
(T) | Epoch=790, loss=5.3742, this epoch 0.0337, total 23.4516
(T) | Epoch=791, loss=5.3743, this epoch 0.0297, total 23.4813
(T) | Epoch=792, loss=5.4089, this epoch 0.0320, total 23.5133
(T) | Epoch=793, loss=5.3674, this epoch 0.0290, total 23.5424
(T) | Epoch=794, loss=5.3959, this epoch 0.0374, total 23.5797
(T) | Epoch=795, loss=5.4421, this epoch 0.0405, total 23.6202
(T) | Epoch=796, loss=5.4073, this epoch 0.0329, total 23.6531
(T) | Epoch=797, loss=5.3637, this epoch 0.0378, total 23.6908
(T) | Epoch=798, loss=5.3967, this epoch 0.0417, total 23.7325
(T) | Epoch=799, loss=5.4872, this epoch 0.0337, total 23.7662
(T) | Epoch=800, loss=5.3723, this epoch 0.0438, total 23.8100
(T) | Epoch=801, loss=5.3682, this epoch 0.0402, total 23.8502
(T) | Epoch=802, loss=5.4193, this epoch 0.0299, total 23.8801
(T) | Epoch=803, loss=5.4744, this epoch 0.0295, total 23.9095
(T) | Epoch=804, loss=5.3613, this epoch 0.0379, total 23.9475
(T) | Epoch=805, loss=5.4679, this epoch 0.0365, total 23.9840
(T) | Epoch=806, loss=5.3684, this epoch 0.0369, total 24.0209
(T) | Epoch=807, loss=5.3621, this epoch 0.0311, total 24.0520
(T) | Epoch=808, loss=5.3791, this epoch 0.0361, total 24.0881
(T) | Epoch=809, loss=5.4098, this epoch 0.0355, total 24.1236
(T) | Epoch=810, loss=5.4331, this epoch 0.0335, total 24.1571
(T) | Epoch=811, loss=5.4196, this epoch 0.0288, total 24.1859
(T) | Epoch=812, loss=5.3887, this epoch 0.0427, total 24.2286
(T) | Epoch=813, loss=5.4669, this epoch 0.0404, total 24.2690
(T) | Epoch=814, loss=5.3837, this epoch 0.0406, total 24.3097
(T) | Epoch=815, loss=5.4138, this epoch 0.0340, total 24.3436
(T) | Epoch=816, loss=5.3783, this epoch 0.0387, total 24.3823
(T) | Epoch=817, loss=5.4120, this epoch 0.0307, total 24.4130
(T) | Epoch=818, loss=5.3779, this epoch 0.0406, total 24.4536
(T) | Epoch=819, loss=5.4079, this epoch 0.0384, total 24.4920
(T) | Epoch=820, loss=5.3661, this epoch 0.0338, total 24.5258
(T) | Epoch=821, loss=5.3759, this epoch 0.0368, total 24.5626
(T) | Epoch=822, loss=5.3918, this epoch 0.0383, total 24.6009
(T) | Epoch=823, loss=5.3698, this epoch 0.0331, total 24.6339
(T) | Epoch=824, loss=5.3619, this epoch 0.0368, total 24.6707
(T) | Epoch=825, loss=5.4502, this epoch 0.0303, total 24.7010
(T) | Epoch=826, loss=5.3947, this epoch 0.0363, total 24.7373
(T) | Epoch=827, loss=5.3738, this epoch 0.0382, total 24.7755
(T) | Epoch=828, loss=5.3934, this epoch 0.0363, total 24.8118
(T) | Epoch=829, loss=5.3747, this epoch 0.0307, total 24.8425
(T) | Epoch=830, loss=5.4158, this epoch 0.0294, total 24.8720
(T) | Epoch=831, loss=5.3569, this epoch 0.0316, total 24.9036
(T) | Epoch=832, loss=5.3737, this epoch 0.0285, total 24.9321
(T) | Epoch=833, loss=5.4222, this epoch 0.0377, total 24.9697
(T) | Epoch=834, loss=5.3644, this epoch 0.0325, total 25.0022
(T) | Epoch=835, loss=5.6100, this epoch 0.0367, total 25.0389
(T) | Epoch=836, loss=5.3763, this epoch 0.0364, total 25.0753
(T) | Epoch=837, loss=5.4399, this epoch 0.0303, total 25.1056
(T) | Epoch=838, loss=5.4819, this epoch 0.0377, total 25.1434
(T) | Epoch=839, loss=5.4625, this epoch 0.0302, total 25.1736
(T) | Epoch=840, loss=5.4507, this epoch 0.0382, total 25.2118
(T) | Epoch=841, loss=5.4639, this epoch 0.0315, total 25.2433
(T) | Epoch=842, loss=5.5462, this epoch 0.0308, total 25.2740
(T) | Epoch=843, loss=5.4335, this epoch 0.0370, total 25.3110
(T) | Epoch=844, loss=5.4010, this epoch 0.0392, total 25.3502
(T) | Epoch=845, loss=5.4892, this epoch 0.0321, total 25.3823
(T) | Epoch=846, loss=5.3663, this epoch 0.0366, total 25.4189
(T) | Epoch=847, loss=5.4921, this epoch 0.0315, total 25.4504
(T) | Epoch=848, loss=5.3733, this epoch 0.0328, total 25.4832
(T) | Epoch=849, loss=5.3962, this epoch 0.0373, total 25.5205
(T) | Epoch=850, loss=5.3674, this epoch 0.0302, total 25.5507
(T) | Epoch=851, loss=5.4591, this epoch 0.0364, total 25.5871
(T) | Epoch=852, loss=5.4907, this epoch 0.0405, total 25.6276
(T) | Epoch=853, loss=5.4064, this epoch 0.0306, total 25.6582
(T) | Epoch=854, loss=5.4593, this epoch 0.0316, total 25.6899
(T) | Epoch=855, loss=5.3664, this epoch 0.0375, total 25.7274
(T) | Epoch=856, loss=5.4139, this epoch 0.0303, total 25.7577
(T) | Epoch=857, loss=5.4293, this epoch 0.0359, total 25.7936
(T) | Epoch=858, loss=5.3614, this epoch 0.0366, total 25.8303
(T) | Epoch=859, loss=5.3615, this epoch 0.0310, total 25.8613
(T) | Epoch=860, loss=5.4114, this epoch 0.0219, total 25.8832
(T) | Epoch=861, loss=5.3956, this epoch 0.0269, total 25.9101
(T) | Epoch=862, loss=5.4147, this epoch 0.0392, total 25.9493
(T) | Epoch=863, loss=5.4119, this epoch 0.0373, total 25.9866
(T) | Epoch=864, loss=5.4430, this epoch 0.0359, total 26.0225
(T) | Epoch=865, loss=5.4192, this epoch 0.0316, total 26.0541
(T) | Epoch=866, loss=5.4617, this epoch 0.0322, total 26.0864
(T) | Epoch=867, loss=5.3594, this epoch 0.0332, total 26.1196
(T) | Epoch=868, loss=5.3947, this epoch 0.0311, total 26.1507
(T) | Epoch=869, loss=5.3578, this epoch 0.0372, total 26.1879
(T) | Epoch=870, loss=5.3803, this epoch 0.0341, total 26.2220
(T) | Epoch=871, loss=5.4395, this epoch 0.0377, total 26.2597
(T) | Epoch=872, loss=5.5254, this epoch 0.0346, total 26.2942
(T) | Epoch=873, loss=5.4054, this epoch 0.0287, total 26.3229
(T) | Epoch=874, loss=5.3704, this epoch 0.0366, total 26.3596
(T) | Epoch=875, loss=5.3595, this epoch 0.0380, total 26.3976
(T) | Epoch=876, loss=5.3830, this epoch 0.0311, total 26.4287
(T) | Epoch=877, loss=5.3606, this epoch 0.0329, total 26.4615
(T) | Epoch=878, loss=5.3745, this epoch 0.0363, total 26.4978
(T) | Epoch=879, loss=5.4077, this epoch 0.0305, total 26.5284
(T) | Epoch=880, loss=5.4641, this epoch 0.0316, total 26.5600
(T) | Epoch=881, loss=5.3625, this epoch 0.0367, total 26.5967
(T) | Epoch=882, loss=5.5027, this epoch 0.0381, total 26.6349
(T) | Epoch=883, loss=5.4419, this epoch 0.0394, total 26.6743
(T) | Epoch=884, loss=5.4442, this epoch 0.0351, total 26.7094
(T) | Epoch=885, loss=5.4889, this epoch 0.0315, total 26.7409
(T) | Epoch=886, loss=5.3904, this epoch 0.0408, total 26.7817
(T) | Epoch=887, loss=5.3811, this epoch 0.0383, total 26.8200
(T) | Epoch=888, loss=5.3536, this epoch 0.0312, total 26.8512
+++model saved ! 2015.pth
(T) | Epoch=889, loss=5.3893, this epoch 0.0401, total 26.8913
(T) | Epoch=890, loss=5.3615, this epoch 0.0305, total 26.9218
(T) | Epoch=891, loss=5.3730, this epoch 0.0285, total 26.9503
(T) | Epoch=892, loss=5.4291, this epoch 0.0327, total 26.9829
(T) | Epoch=893, loss=5.4850, this epoch 0.0339, total 27.0168
(T) | Epoch=894, loss=5.3885, this epoch 0.0243, total 27.0411
(T) | Epoch=895, loss=5.4378, this epoch 0.0357, total 27.0768
(T) | Epoch=896, loss=5.4141, this epoch 0.0307, total 27.1075
(T) | Epoch=897, loss=5.4474, this epoch 0.0343, total 27.1418
(T) | Epoch=898, loss=5.3651, this epoch 0.0292, total 27.1710
(T) | Epoch=899, loss=5.3834, this epoch 0.0307, total 27.2017
(T) | Epoch=900, loss=5.3587, this epoch 0.0294, total 27.2311
(T) | Epoch=901, loss=5.4534, this epoch 0.0285, total 27.2596
(T) | Epoch=902, loss=5.3763, this epoch 0.0340, total 27.2936
(T) | Epoch=903, loss=5.3644, this epoch 0.0301, total 27.3237
(T) | Epoch=904, loss=5.4079, this epoch 0.0268, total 27.3505
(T) | Epoch=905, loss=5.4267, this epoch 0.0302, total 27.3808
(T) | Epoch=906, loss=5.3807, this epoch 0.0292, total 27.4100
(T) | Epoch=907, loss=5.4625, this epoch 0.0284, total 27.4384
(T) | Epoch=908, loss=5.3605, this epoch 0.0288, total 27.4672
(T) | Epoch=909, loss=5.3567, this epoch 0.0284, total 27.4956
(T) | Epoch=910, loss=5.4293, this epoch 0.0226, total 27.5182
(T) | Epoch=911, loss=5.3916, this epoch 0.0274, total 27.5456
(T) | Epoch=912, loss=5.3558, this epoch 0.0337, total 27.5793
(T) | Epoch=913, loss=5.3560, this epoch 0.0308, total 27.6102
(T) | Epoch=914, loss=5.3572, this epoch 0.0289, total 27.6390
(T) | Epoch=915, loss=5.3912, this epoch 0.0285, total 27.6676
(T) | Epoch=916, loss=5.3535, this epoch 0.0287, total 27.6963
+++model saved ! 2015.pth
(T) | Epoch=917, loss=5.3664, this epoch 0.0304, total 27.7267
(T) | Epoch=918, loss=5.3662, this epoch 0.0289, total 27.7557
(T) | Epoch=919, loss=5.4028, this epoch 0.0289, total 27.7846
(T) | Epoch=920, loss=5.4187, this epoch 0.0292, total 27.8137
(T) | Epoch=921, loss=5.3978, this epoch 0.0289, total 27.8426
(T) | Epoch=922, loss=5.3634, this epoch 0.0287, total 27.8713
(T) | Epoch=923, loss=5.4691, this epoch 0.0288, total 27.9001
(T) | Epoch=924, loss=5.3935, this epoch 0.0292, total 27.9293
(T) | Epoch=925, loss=5.3615, this epoch 0.0301, total 27.9594
(T) | Epoch=926, loss=5.4377, this epoch 0.0338, total 27.9931
(T) | Epoch=927, loss=5.3582, this epoch 0.0304, total 28.0236
(T) | Epoch=928, loss=5.4260, this epoch 0.0338, total 28.0574
(T) | Epoch=929, loss=5.3660, this epoch 0.0308, total 28.0882
(T) | Epoch=930, loss=5.3632, this epoch 0.0365, total 28.1247
(T) | Epoch=931, loss=5.3678, this epoch 0.0289, total 28.1536
(T) | Epoch=932, loss=5.4873, this epoch 0.0222, total 28.1758
(T) | Epoch=933, loss=5.3579, this epoch 0.0283, total 28.2041
(T) | Epoch=934, loss=5.4084, this epoch 0.0298, total 28.2339
(T) | Epoch=935, loss=5.3909, this epoch 0.0357, total 28.2695
(T) | Epoch=936, loss=5.5196, this epoch 0.0227, total 28.2922
(T) | Epoch=937, loss=5.3984, this epoch 0.0332, total 28.3254
(T) | Epoch=938, loss=5.3748, this epoch 0.0380, total 28.3634
(T) | Epoch=939, loss=5.3695, this epoch 0.0317, total 28.3950
(T) | Epoch=940, loss=5.4638, this epoch 0.0257, total 28.4207
(T) | Epoch=941, loss=5.3916, this epoch 0.0307, total 28.4514
(T) | Epoch=942, loss=5.4531, this epoch 0.0362, total 28.4876
(T) | Epoch=943, loss=5.3814, this epoch 0.0381, total 28.5258
(T) | Epoch=944, loss=5.4165, this epoch 0.0290, total 28.5547
(T) | Epoch=945, loss=5.4232, this epoch 0.0383, total 28.5931
(T) | Epoch=946, loss=5.3983, this epoch 0.0365, total 28.6296
(T) | Epoch=947, loss=5.3631, this epoch 0.0370, total 28.6666
(T) | Epoch=948, loss=5.3577, this epoch 0.0378, total 28.7044
(T) | Epoch=949, loss=5.4128, this epoch 0.0305, total 28.7349
(T) | Epoch=950, loss=5.4152, this epoch 0.0310, total 28.7659
(T) | Epoch=951, loss=5.5175, this epoch 0.0290, total 28.7949
(T) | Epoch=952, loss=5.4276, this epoch 0.0286, total 28.8235
(T) | Epoch=953, loss=5.3794, this epoch 0.0298, total 28.8532
(T) | Epoch=954, loss=5.3561, this epoch 0.0290, total 28.8822
(T) | Epoch=955, loss=5.5146, this epoch 0.0359, total 28.9181
(T) | Epoch=956, loss=5.4424, this epoch 0.0359, total 28.9540
(T) | Epoch=957, loss=5.4077, this epoch 0.0334, total 28.9874
(T) | Epoch=958, loss=5.3551, this epoch 0.0352, total 29.0226
(T) | Epoch=959, loss=5.3544, this epoch 0.0357, total 29.0583
(T) | Epoch=960, loss=5.3548, this epoch 0.0307, total 29.0890
(T) | Epoch=961, loss=5.3850, this epoch 0.0328, total 29.1218
(T) | Epoch=962, loss=5.3611, this epoch 0.0287, total 29.1506
(T) | Epoch=963, loss=5.3906, this epoch 0.0339, total 29.1844
(T) | Epoch=964, loss=5.4360, this epoch 0.0313, total 29.2157
(T) | Epoch=965, loss=5.3804, this epoch 0.0285, total 29.2442
(T) | Epoch=966, loss=5.3631, this epoch 0.0304, total 29.2747
(T) | Epoch=967, loss=5.3901, this epoch 0.0343, total 29.3090
(T) | Epoch=968, loss=5.3902, this epoch 0.0302, total 29.3392
(T) | Epoch=969, loss=5.7143, this epoch 0.0365, total 29.3757
(T) | Epoch=970, loss=5.3718, this epoch 0.0381, total 29.4138
(T) | Epoch=971, loss=5.3679, this epoch 0.0377, total 29.4515
(T) | Epoch=972, loss=5.3615, this epoch 0.0383, total 29.4898
(T) | Epoch=973, loss=5.3878, this epoch 0.0303, total 29.5201
(T) | Epoch=974, loss=5.4083, this epoch 0.0360, total 29.5561
(T) | Epoch=975, loss=5.3535, this epoch 0.0382, total 29.5943
(T) | Epoch=976, loss=5.4206, this epoch 0.0379, total 29.6322
(T) | Epoch=977, loss=5.4463, this epoch 0.0304, total 29.6626
(T) | Epoch=978, loss=5.3644, this epoch 0.0361, total 29.6987
(T) | Epoch=979, loss=5.4362, this epoch 0.0307, total 29.7294
(T) | Epoch=980, loss=5.3540, this epoch 0.0344, total 29.7638
(T) | Epoch=981, loss=5.4379, this epoch 0.0379, total 29.8017
(T) | Epoch=982, loss=5.3980, this epoch 0.0312, total 29.8329
(T) | Epoch=983, loss=5.3550, this epoch 0.0408, total 29.8737
(T) | Epoch=984, loss=5.3559, this epoch 0.0305, total 29.9042
(T) | Epoch=985, loss=5.3511, this epoch 0.0317, total 29.9358
+++model saved ! 2015.pth
(T) | Epoch=986, loss=5.3598, this epoch 0.0300, total 29.9658
(T) | Epoch=987, loss=5.4850, this epoch 0.0297, total 29.9955
(T) | Epoch=988, loss=5.4163, this epoch 0.0220, total 30.0175
(T) | Epoch=989, loss=5.3497, this epoch 0.0218, total 30.0393
+++model saved ! 2015.pth
(T) | Epoch=990, loss=5.3871, this epoch 0.0303, total 30.0696
(T) | Epoch=991, loss=5.4118, this epoch 0.0339, total 30.1035
(T) | Epoch=992, loss=5.4212, this epoch 0.0237, total 30.1272
(T) | Epoch=993, loss=5.4191, this epoch 0.0213, total 30.1484
(T) | Epoch=994, loss=5.4329, this epoch 0.0213, total 30.1697
(T) | Epoch=995, loss=5.3584, this epoch 0.0338, total 30.2035
(T) | Epoch=996, loss=5.3596, this epoch 0.0238, total 30.2273
(T) | Epoch=997, loss=5.4289, this epoch 0.0227, total 30.2500
(T) | Epoch=998, loss=5.3952, this epoch 0.0280, total 30.2780
(T) | Epoch=999, loss=5.4049, this epoch 0.0210, total 30.2990
(T) | Epoch=1000, loss=5.4135, this epoch 0.0215, total 30.3206
=== Final ===

==============================
LoRA FINE-TUNING
==============================
Random seed set to 1
Epoch: 0, loss: 26.6674, train_acc: 0.0032, train_recall: 0.1119, train_f1: 0.0016, val_acc: 0.009852, val_recall: 0.113580, val_f1: 0.004658
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed1.csv
Epoch: 1, loss: 29.7909, train_acc: 0.3136, train_recall: 0.1111, train_f1: 0.0530, val_acc: 0.332512, val_recall: 0.125000, val_f1: 0.062384
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed1.csv
Epoch: 2, loss: 62.5714, train_acc: 0.3008, train_recall: 0.1070, train_f1: 0.0521, val_acc: 0.288177, val_recall: 0.120868, val_f1: 0.056576
Epoch: 3, loss: 99.6033, train_acc: 0.0212, train_recall: 0.1173, train_f1: 0.0161, val_acc: 0.019704, val_recall: 0.125000, val_f1: 0.004831
Epoch: 4, loss: 131.3661, train_acc: 0.2362, train_recall: 0.1111, train_f1: 0.0425, val_acc: 0.187192, val_recall: 0.125000, val_f1: 0.039419
Epoch: 5, loss: 112.4085, train_acc: 0.2362, train_recall: 0.1111, train_f1: 0.0425, val_acc: 0.187192, val_recall: 0.125000, val_f1: 0.039419
Epoch: 6, loss: 131.6388, train_acc: 0.3125, train_recall: 0.1111, train_f1: 0.0529, val_acc: 0.298030, val_recall: 0.125000, val_f1: 0.057400
Epoch: 7, loss: 130.2204, train_acc: 0.3125, train_recall: 0.1111, train_f1: 0.0529, val_acc: 0.298030, val_recall: 0.125000, val_f1: 0.057400
Epoch: 8, loss: 113.4056, train_acc: 0.3125, train_recall: 0.1111, train_f1: 0.0529, val_acc: 0.298030, val_recall: 0.125000, val_f1: 0.057400
Epoch: 9, loss: 86.9909, train_acc: 0.3125, train_recall: 0.1111, train_f1: 0.0529, val_acc: 0.298030, val_recall: 0.125000, val_f1: 0.057400
Epoch: 10, loss: 54.2215, train_acc: 0.3125, train_recall: 0.1111, train_f1: 0.0529, val_acc: 0.298030, val_recall: 0.125000, val_f1: 0.057400
Epoch: 11, loss: 46.6356, train_acc: 0.3146, train_recall: 0.1135, train_f1: 0.0656, val_acc: 0.325123, val_recall: 0.122222, val_f1: 0.061338
Epoch: 12, loss: 59.7516, train_acc: 0.3114, train_recall: 0.1104, train_f1: 0.0528, val_acc: 0.332512, val_recall: 0.125000, val_f1: 0.062384
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed1.csv
Epoch: 13, loss: 62.4177, train_acc: 0.2362, train_recall: 0.1111, train_f1: 0.0425, val_acc: 0.187192, val_recall: 0.125000, val_f1: 0.039419
Epoch: 14, loss: 78.2209, train_acc: 0.0307, train_recall: 0.1111, train_f1: 0.0066, val_acc: 0.051724, val_recall: 0.125000, val_f1: 0.012295
Epoch: 15, loss: 69.8340, train_acc: 0.3114, train_recall: 0.1104, train_f1: 0.0528, val_acc: 0.332512, val_recall: 0.125000, val_f1: 0.062384
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed1.csv
Epoch: 16, loss: 66.5337, train_acc: 0.2362, train_recall: 0.1111, train_f1: 0.0425, val_acc: 0.187192, val_recall: 0.125000, val_f1: 0.039419
Epoch: 17, loss: 64.1770, train_acc: 0.2362, train_recall: 0.1111, train_f1: 0.0425, val_acc: 0.187192, val_recall: 0.125000, val_f1: 0.039419
Epoch: 18, loss: 78.6871, train_acc: 0.0667, train_recall: 0.1119, train_f1: 0.0151, val_acc: 0.071429, val_recall: 0.127778, val_f1: 0.020586
Epoch: 19, loss: 64.5808, train_acc: 0.0837, train_recall: 0.1179, train_f1: 0.0254, val_acc: 0.078818, val_recall: 0.130556, val_f1: 0.025327
Epoch: 20, loss: 64.5509, train_acc: 0.3136, train_recall: 0.1111, train_f1: 0.0530, val_acc: 0.332512, val_recall: 0.125000, val_f1: 0.062384
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed1.csv
Epoch: 21, loss: 57.9861, train_acc: 0.3136, train_recall: 0.1111, train_f1: 0.0530, val_acc: 0.332512, val_recall: 0.125000, val_f1: 0.062384
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed1.csv
Epoch: 22, loss: 45.2027, train_acc: 0.3125, train_recall: 0.1121, train_f1: 0.0618, val_acc: 0.325123, val_recall: 0.122222, val_f1: 0.061338
Epoch: 23, loss: 45.4116, train_acc: 0.2362, train_recall: 0.1111, train_f1: 0.0425, val_acc: 0.187192, val_recall: 0.125000, val_f1: 0.039419
Epoch: 24, loss: 42.6350, train_acc: 0.3189, train_recall: 0.1156, train_f1: 0.0674, val_acc: 0.290640, val_recall: 0.121901, val_f1: 0.058765
Epoch: 25, loss: 52.5312, train_acc: 0.3263, train_recall: 0.1180, train_f1: 0.0677, val_acc: 0.298030, val_recall: 0.125000, val_f1: 0.057729
Epoch: 26, loss: 55.1350, train_acc: 0.3263, train_recall: 0.1180, train_f1: 0.0677, val_acc: 0.298030, val_recall: 0.125000, val_f1: 0.057729
Epoch: 27, loss: 51.7298, train_acc: 0.3263, train_recall: 0.1180, train_f1: 0.0677, val_acc: 0.298030, val_recall: 0.125000, val_f1: 0.057729
Epoch: 28, loss: 48.0943, train_acc: 0.2362, train_recall: 0.1111, train_f1: 0.0425, val_acc: 0.187192, val_recall: 0.125000, val_f1: 0.039419
Epoch: 29, loss: 44.9665, train_acc: 0.2362, train_recall: 0.1111, train_f1: 0.0425, val_acc: 0.187192, val_recall: 0.125000, val_f1: 0.039419
Epoch: 30, loss: 35.8180, train_acc: 0.3189, train_recall: 0.1156, train_f1: 0.0674, val_acc: 0.288177, val_recall: 0.121479, val_f1: 0.060520
Epoch: 31, loss: 33.3664, train_acc: 0.3146, train_recall: 0.1173, train_f1: 0.0648, val_acc: 0.332512, val_recall: 0.125000, val_f1: 0.062384
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed1.csv
Epoch: 32, loss: 36.8006, train_acc: 0.3157, train_recall: 0.1235, train_f1: 0.0743, val_acc: 0.332512, val_recall: 0.125000, val_f1: 0.062384
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed1.csv
Epoch: 33, loss: 43.8433, train_acc: 0.0244, train_recall: 0.1130, train_f1: 0.0078, val_acc: 0.044335, val_recall: 0.125000, val_f1: 0.010613
Epoch: 34, loss: 33.7718, train_acc: 0.3189, train_recall: 0.1249, train_f1: 0.0876, val_acc: 0.300493, val_recall: 0.125390, val_f1: 0.067441
Epoch: 35, loss: 33.6309, train_acc: 0.3220, train_recall: 0.1145, train_f1: 0.0647, val_acc: 0.305419, val_recall: 0.127564, val_f1: 0.066497
Epoch: 36, loss: 32.8017, train_acc: 0.3136, train_recall: 0.1111, train_f1: 0.0530, val_acc: 0.332512, val_recall: 0.125000, val_f1: 0.062384
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed1.csv
Epoch: 37, loss: 28.6974, train_acc: 0.3146, train_recall: 0.1115, train_f1: 0.0539, val_acc: 0.332512, val_recall: 0.125000, val_f1: 0.062384
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed1.csv
Epoch: 38, loss: 31.4446, train_acc: 0.2362, train_recall: 0.1111, train_f1: 0.0425, val_acc: 0.187192, val_recall: 0.125000, val_f1: 0.039419
Epoch: 39, loss: 30.9998, train_acc: 0.2373, train_recall: 0.1115, train_f1: 0.0433, val_acc: 0.187192, val_recall: 0.125000, val_f1: 0.039419
Epoch: 40, loss: 32.6547, train_acc: 0.3263, train_recall: 0.1180, train_f1: 0.0677, val_acc: 0.298030, val_recall: 0.125000, val_f1: 0.057729
Epoch: 41, loss: 30.2688, train_acc: 0.3347, train_recall: 0.1702, train_f1: 0.1131, val_acc: 0.298030, val_recall: 0.139592, val_f1: 0.069172
Epoch: 42, loss: 32.3509, train_acc: 0.0360, train_recall: 0.1197, train_f1: 0.0196, val_acc: 0.019704, val_recall: 0.125000, val_f1: 0.004950
Epoch: 43, loss: 33.1565, train_acc: 0.0858, train_recall: 0.1428, train_f1: 0.0519, val_acc: 0.088670, val_recall: 0.169444, val_f1: 0.068471
Epoch: 44, loss: 33.7088, train_acc: 0.3231, train_recall: 0.1525, train_f1: 0.0919, val_acc: 0.359606, val_recall: 0.190476, val_f1: 0.121912
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed1.csv
Epoch: 45, loss: 34.8901, train_acc: 0.3220, train_recall: 0.1521, train_f1: 0.0897, val_acc: 0.354680, val_recall: 0.188624, val_f1: 0.118990
Epoch: 46, loss: 32.8730, train_acc: 0.2458, train_recall: 0.1450, train_f1: 0.0818, val_acc: 0.214286, val_recall: 0.169652, val_f1: 0.095768
Epoch: 47, loss: 34.2971, train_acc: 0.2362, train_recall: 0.1111, train_f1: 0.0425, val_acc: 0.187192, val_recall: 0.125000, val_f1: 0.039419
Epoch: 48, loss: 29.4150, train_acc: 0.2436, train_recall: 0.1365, train_f1: 0.0792, val_acc: 0.214286, val_recall: 0.169652, val_f1: 0.096923
Epoch: 49, loss: 34.2610, train_acc: 0.3231, train_recall: 0.1149, train_f1: 0.0627, val_acc: 0.305419, val_recall: 0.127778, val_f1: 0.063164
Epoch: 50, loss: 36.3393, train_acc: 0.3231, train_recall: 0.1149, train_f1: 0.0627, val_acc: 0.305419, val_recall: 0.127778, val_f1: 0.063164
Epoch: 51, loss: 33.9384, train_acc: 0.3178, train_recall: 0.1130, train_f1: 0.0639, val_acc: 0.302956, val_recall: 0.126423, val_f1: 0.067000
Epoch: 52, loss: 36.5503, train_acc: 0.3136, train_recall: 0.1111, train_f1: 0.0531, val_acc: 0.332512, val_recall: 0.125000, val_f1: 0.062384
Epoch: 53, loss: 34.2249, train_acc: 0.3167, train_recall: 0.1364, train_f1: 0.0792, val_acc: 0.342365, val_recall: 0.163889, val_f1: 0.106023
Epoch: 54, loss: 32.6856, train_acc: 0.0318, train_recall: 0.1116, train_f1: 0.0076, val_acc: 0.051724, val_recall: 0.125000, val_f1: 0.012295
Epoch: 55, loss: 32.1285, train_acc: 0.3210, train_recall: 0.1169, train_f1: 0.0707, val_acc: 0.293103, val_recall: 0.125381, val_f1: 0.067157
Epoch: 56, loss: 34.1132, train_acc: 0.2373, train_recall: 0.1115, train_f1: 0.0433, val_acc: 0.187192, val_recall: 0.125000, val_f1: 0.039419
Epoch: 57, loss: 34.7743, train_acc: 0.3252, train_recall: 0.1176, train_f1: 0.0675, val_acc: 0.298030, val_recall: 0.125000, val_f1: 0.057729
Epoch: 58, loss: 32.3728, train_acc: 0.3252, train_recall: 0.1177, train_f1: 0.0681, val_acc: 0.298030, val_recall: 0.125000, val_f1: 0.057729
Epoch: 59, loss: 30.2028, train_acc: 0.2362, train_recall: 0.1111, train_f1: 0.0425, val_acc: 0.187192, val_recall: 0.125000, val_f1: 0.039419
Epoch: 60, loss: 35.0711, train_acc: 0.3136, train_recall: 0.1111, train_f1: 0.0530, val_acc: 0.332512, val_recall: 0.125000, val_f1: 0.062384
Epoch: 61, loss: 35.7640, train_acc: 0.3136, train_recall: 0.1111, train_f1: 0.0530, val_acc: 0.332512, val_recall: 0.125000, val_f1: 0.062384
Epoch: 62, loss: 31.5998, train_acc: 0.3136, train_recall: 0.1111, train_f1: 0.0530, val_acc: 0.332512, val_recall: 0.125000, val_f1: 0.062384
Epoch: 63, loss: 34.1186, train_acc: 0.0816, train_recall: 0.1191, train_f1: 0.0278, val_acc: 0.064039, val_recall: 0.125000, val_f1: 0.015152
Epoch: 64, loss: 32.6186, train_acc: 0.3263, train_recall: 0.1181, train_f1: 0.0683, val_acc: 0.298030, val_recall: 0.125000, val_f1: 0.057729
Epoch: 65, loss: 33.7319, train_acc: 0.2415, train_recall: 0.1130, train_f1: 0.0471, val_acc: 0.189655, val_recall: 0.126033, val_f1: 0.043130
Epoch: 66, loss: 34.2332, train_acc: 0.2383, train_recall: 0.1119, train_f1: 0.0450, val_acc: 0.189655, val_recall: 0.126033, val_f1: 0.043053
Epoch: 67, loss: 33.7445, train_acc: 0.3263, train_recall: 0.1181, train_f1: 0.0684, val_acc: 0.298030, val_recall: 0.125000, val_f1: 0.057729
Epoch: 68, loss: 30.2939, train_acc: 0.3242, train_recall: 0.1172, train_f1: 0.0675, val_acc: 0.298030, val_recall: 0.125000, val_f1: 0.057729
Epoch: 69, loss: 28.2284, train_acc: 0.2447, train_recall: 0.1631, train_f1: 0.0873, val_acc: 0.189655, val_recall: 0.140625, val_f1: 0.050564
Epoch: 70, loss: 32.6554, train_acc: 0.3199, train_recall: 0.1626, train_f1: 0.0981, val_acc: 0.327586, val_recall: 0.137847, val_f1: 0.072713
Epoch: 71, loss: 35.0620, train_acc: 0.3189, train_recall: 0.1622, train_f1: 0.0937, val_acc: 0.327586, val_recall: 0.137847, val_f1: 0.072713
Epoch: 72, loss: 33.0299, train_acc: 0.3189, train_recall: 0.1622, train_f1: 0.0937, val_acc: 0.327586, val_recall: 0.137847, val_f1: 0.072268
Epoch: 73, loss: 28.2709, train_acc: 0.0784, train_recall: 0.1707, train_f1: 0.0429, val_acc: 0.076355, val_recall: 0.173727, val_f1: 0.041265
Epoch: 74, loss: 28.3261, train_acc: 0.3125, train_recall: 0.1658, train_f1: 0.0793, val_acc: 0.300493, val_recall: 0.169809, val_f1: 0.080453
Epoch: 75, loss: 28.2912, train_acc: 0.3125, train_recall: 0.1661, train_f1: 0.0805, val_acc: 0.300493, val_recall: 0.169809, val_f1: 0.080453
Epoch: 76, loss: 29.7031, train_acc: 0.2426, train_recall: 0.1676, train_f1: 0.0771, val_acc: 0.194581, val_recall: 0.171875, val_f1: 0.063187
Epoch: 77, loss: 28.7339, train_acc: 0.2436, train_recall: 0.1738, train_f1: 0.0903, val_acc: 0.194581, val_recall: 0.171875, val_f1: 0.063187
Epoch: 78, loss: 24.5615, train_acc: 0.3210, train_recall: 0.1805, train_f1: 0.1076, val_acc: 0.298030, val_recall: 0.186403, val_f1: 0.107787
Epoch: 79, loss: 26.5098, train_acc: 0.3189, train_recall: 0.1710, train_f1: 0.0870, val_acc: 0.352217, val_recall: 0.204630, val_f1: 0.123292
Epoch: 80, loss: 26.7116, train_acc: 0.3199, train_recall: 0.1717, train_f1: 0.0947, val_acc: 0.320197, val_recall: 0.205020, val_f1: 0.121700
Epoch: 81, loss: 27.1465, train_acc: 0.3273, train_recall: 0.1627, train_f1: 0.0955, val_acc: 0.317734, val_recall: 0.186467, val_f1: 0.116011
Epoch: 82, loss: 26.1570, train_acc: 0.3220, train_recall: 0.1626, train_f1: 0.1152, val_acc: 0.344828, val_recall: 0.170833, val_f1: 0.117506
Epoch: 83, loss: 24.7014, train_acc: 0.3210, train_recall: 0.1541, train_f1: 0.0967, val_acc: 0.342365, val_recall: 0.173942, val_f1: 0.111104
Epoch: 84, loss: 25.0938, train_acc: 0.0858, train_recall: 0.1818, train_f1: 0.0620, val_acc: 0.093596, val_recall: 0.196429, val_f1: 0.065011
Epoch: 85, loss: 27.0194, train_acc: 0.2436, train_recall: 0.1536, train_f1: 0.0863, val_acc: 0.209360, val_recall: 0.178571, val_f1: 0.088633
Epoch: 86, loss: 25.4159, train_acc: 0.2458, train_recall: 0.1544, train_f1: 0.0879, val_acc: 0.209360, val_recall: 0.178571, val_f1: 0.088633
Epoch: 87, loss: 26.2191, train_acc: 0.3347, train_recall: 0.1593, train_f1: 0.1127, val_acc: 0.317734, val_recall: 0.177538, val_f1: 0.108674
Epoch: 88, loss: 27.2075, train_acc: 0.3220, train_recall: 0.1441, train_f1: 0.0962, val_acc: 0.349754, val_recall: 0.166667, val_f1: 0.110461
Epoch: 89, loss: 27.1558, train_acc: 0.3220, train_recall: 0.1441, train_f1: 0.0962, val_acc: 0.349754, val_recall: 0.166667, val_f1: 0.110461
Epoch: 90, loss: 27.3368, train_acc: 0.3337, train_recall: 0.1486, train_f1: 0.1072, val_acc: 0.322660, val_recall: 0.175356, val_f1: 0.125717
Epoch: 91, loss: 25.6387, train_acc: 0.3347, train_recall: 0.1641, train_f1: 0.1183, val_acc: 0.334975, val_recall: 0.217974, val_f1: 0.166332
Epoch: 92, loss: 25.1518, train_acc: 0.2468, train_recall: 0.1720, train_f1: 0.0765, val_acc: 0.219212, val_recall: 0.207822, val_f1: 0.103705
Epoch: 93, loss: 26.4015, train_acc: 0.2479, train_recall: 0.1486, train_f1: 0.0897, val_acc: 0.216749, val_recall: 0.181349, val_f1: 0.099336
Epoch: 94, loss: 27.6097, train_acc: 0.3220, train_recall: 0.1441, train_f1: 0.0962, val_acc: 0.349754, val_recall: 0.166667, val_f1: 0.110461
Epoch: 95, loss: 27.2112, train_acc: 0.0922, train_recall: 0.1520, train_f1: 0.0632, val_acc: 0.096059, val_recall: 0.172222, val_f1: 0.072386
Epoch: 96, loss: 24.5874, train_acc: 0.3231, train_recall: 0.1490, train_f1: 0.0918, val_acc: 0.354680, val_recall: 0.178571, val_f1: 0.117354
Epoch: 97, loss: 24.5055, train_acc: 0.3326, train_recall: 0.1549, train_f1: 0.1066, val_acc: 0.310345, val_recall: 0.174439, val_f1: 0.109871
Epoch: 98, loss: 26.3851, train_acc: 0.3369, train_recall: 0.1564, train_f1: 0.1080, val_acc: 0.317734, val_recall: 0.177538, val_f1: 0.112087
Epoch: 99, loss: 25.9830, train_acc: 0.2511, train_recall: 0.1509, train_f1: 0.0838, val_acc: 0.209360, val_recall: 0.178571, val_f1: 0.092531
Epoch: 100, loss: 25.4906, train_acc: 0.0424, train_recall: 0.1140, train_f1: 0.0289, val_acc: 0.022167, val_recall: 0.116972, val_f1: 0.018569
Epoch: 101, loss: 24.3892, train_acc: 0.3252, train_recall: 0.1534, train_f1: 0.0937, val_acc: 0.359606, val_recall: 0.190476, val_f1: 0.120918
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed1.csv
Epoch: 102, loss: 25.1753, train_acc: 0.3231, train_recall: 0.1490, train_f1: 0.0918, val_acc: 0.354680, val_recall: 0.178571, val_f1: 0.117354
Epoch: 103, loss: 25.0160, train_acc: 0.3369, train_recall: 0.1495, train_f1: 0.1029, val_acc: 0.317734, val_recall: 0.177538, val_f1: 0.112087
Epoch: 104, loss: 25.1785, train_acc: 0.2489, train_recall: 0.1502, train_f1: 0.0834, val_acc: 0.209360, val_recall: 0.178571, val_f1: 0.093630
Epoch: 105, loss: 24.2173, train_acc: 0.3294, train_recall: 0.1534, train_f1: 0.1139, val_acc: 0.337438, val_recall: 0.172304, val_f1: 0.119101
Epoch: 106, loss: 24.5772, train_acc: 0.3210, train_recall: 0.1414, train_f1: 0.0858, val_acc: 0.354680, val_recall: 0.178571, val_f1: 0.117354
Epoch: 107, loss: 24.7197, train_acc: 0.3337, train_recall: 0.1470, train_f1: 0.1016, val_acc: 0.325123, val_recall: 0.180316, val_f1: 0.118612
Epoch: 108, loss: 24.4627, train_acc: 0.3326, train_recall: 0.1494, train_f1: 0.1050, val_acc: 0.315271, val_recall: 0.176505, val_f1: 0.112793
Epoch: 109, loss: 24.2718, train_acc: 0.2511, train_recall: 0.1509, train_f1: 0.0850, val_acc: 0.209360, val_recall: 0.178571, val_f1: 0.093630
Epoch: 110, loss: 24.5263, train_acc: 0.3273, train_recall: 0.1512, train_f1: 0.0974, val_acc: 0.354680, val_recall: 0.178571, val_f1: 0.117354
Epoch: 111, loss: 24.2387, train_acc: 0.3369, train_recall: 0.1495, train_f1: 0.1029, val_acc: 0.317734, val_recall: 0.177538, val_f1: 0.112087
Epoch: 112, loss: 24.2352, train_acc: 0.3369, train_recall: 0.1598, train_f1: 0.1103, val_acc: 0.322660, val_recall: 0.189443, val_f1: 0.116606
Epoch: 113, loss: 24.0018, train_acc: 0.3294, train_recall: 0.1572, train_f1: 0.1136, val_acc: 0.344828, val_recall: 0.184921, val_f1: 0.119243
Epoch: 114, loss: 23.9023, train_acc: 0.3294, train_recall: 0.1668, train_f1: 0.1226, val_acc: 0.359606, val_recall: 0.190476, val_f1: 0.120918
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed1.csv
Epoch: 115, loss: 23.5430, train_acc: 0.3358, train_recall: 0.1655, train_f1: 0.1151, val_acc: 0.320197, val_recall: 0.191427, val_f1: 0.136135
Epoch: 116, loss: 23.4108, train_acc: 0.3369, train_recall: 0.1777, train_f1: 0.1213, val_acc: 0.317734, val_recall: 0.196306, val_f1: 0.134606
Epoch: 117, loss: 23.7350, train_acc: 0.0975, train_recall: 0.1815, train_f1: 0.0734, val_acc: 0.105911, val_recall: 0.212963, val_f1: 0.089050
Epoch: 118, loss: 23.5816, train_acc: 0.3242, train_recall: 0.1731, train_f1: 0.0926, val_acc: 0.357143, val_recall: 0.206481, val_f1: 0.125324
Epoch: 119, loss: 23.3517, train_acc: 0.3358, train_recall: 0.1789, train_f1: 0.1134, val_acc: 0.330049, val_recall: 0.209045, val_f1: 0.129366
Epoch: 120, loss: 23.4776, train_acc: 0.3347, train_recall: 0.1789, train_f1: 0.1141, val_acc: 0.322660, val_recall: 0.206267, val_f1: 0.124956
Epoch: 121, loss: 23.2683, train_acc: 0.3326, train_recall: 0.1661, train_f1: 0.1160, val_acc: 0.359606, val_recall: 0.203439, val_f1: 0.153004
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed1.csv
Epoch: 122, loss: 23.2684, train_acc: 0.3326, train_recall: 0.1780, train_f1: 0.1343, val_acc: 0.359606, val_recall: 0.197421, val_f1: 0.146800
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed1.csv
Epoch: 123, loss: 23.1207, train_acc: 0.3358, train_recall: 0.1829, train_f1: 0.1396, val_acc: 0.305419, val_recall: 0.162368, val_f1: 0.099180
Epoch: 124, loss: 23.0171, train_acc: 0.3369, train_recall: 0.1859, train_f1: 0.1361, val_acc: 0.302956, val_recall: 0.180681, val_f1: 0.102994
Epoch: 125, loss: 22.8836, train_acc: 0.3326, train_recall: 0.1825, train_f1: 0.1271, val_acc: 0.342365, val_recall: 0.182854, val_f1: 0.108270
Epoch: 126, loss: 23.3146, train_acc: 0.0985, train_recall: 0.1827, train_f1: 0.0833, val_acc: 0.083744, val_recall: 0.176505, val_f1: 0.048355
Epoch: 127, loss: 22.9873, train_acc: 0.3316, train_recall: 0.1800, train_f1: 0.1141, val_acc: 0.302956, val_recall: 0.185434, val_f1: 0.084613
Epoch: 128, loss: 22.9947, train_acc: 0.3358, train_recall: 0.1819, train_f1: 0.1258, val_acc: 0.300493, val_recall: 0.169809, val_f1: 0.083221
Epoch: 129, loss: 23.0001, train_acc: 0.3337, train_recall: 0.1809, train_f1: 0.1302, val_acc: 0.325123, val_recall: 0.166319, val_f1: 0.087553
Epoch: 130, loss: 22.9427, train_acc: 0.3337, train_recall: 0.1838, train_f1: 0.1358, val_acc: 0.344828, val_recall: 0.183887, val_f1: 0.111109
Epoch: 131, loss: 22.8653, train_acc: 0.3453, train_recall: 0.2142, train_f1: 0.1811, val_acc: 0.310345, val_recall: 0.175265, val_f1: 0.118963
Epoch: 132, loss: 22.7005, train_acc: 0.3432, train_recall: 0.1978, train_f1: 0.1622, val_acc: 0.325123, val_recall: 0.208251, val_f1: 0.153325
Epoch: 133, loss: 22.6868, train_acc: 0.3326, train_recall: 0.1779, train_f1: 0.1170, val_acc: 0.352217, val_recall: 0.204630, val_f1: 0.128298
Epoch: 134, loss: 22.8849, train_acc: 0.1165, train_recall: 0.1876, train_f1: 0.0894, val_acc: 0.100985, val_recall: 0.203348, val_f1: 0.087632
Epoch: 135, loss: 22.7404, train_acc: 0.3369, train_recall: 0.1796, train_f1: 0.1151, val_acc: 0.330049, val_recall: 0.209045, val_f1: 0.129366
Epoch: 136, loss: 22.7472, train_acc: 0.3379, train_recall: 0.1802, train_f1: 0.1159, val_acc: 0.322660, val_recall: 0.206267, val_f1: 0.124052
Epoch: 137, loss: 22.7324, train_acc: 0.3316, train_recall: 0.1773, train_f1: 0.1189, val_acc: 0.359606, val_recall: 0.207515, val_f1: 0.132446
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed1.csv
Epoch: 138, loss: 22.6062, train_acc: 0.3337, train_recall: 0.1818, train_f1: 0.1482, val_acc: 0.347291, val_recall: 0.171915, val_f1: 0.129039
Epoch: 139, loss: 22.6017, train_acc: 0.3464, train_recall: 0.2178, train_f1: 0.1844, val_acc: 0.317734, val_recall: 0.177051, val_f1: 0.117562
Epoch: 140, loss: 22.6470, train_acc: 0.3453, train_recall: 0.2174, train_f1: 0.1855, val_acc: 0.315271, val_recall: 0.176018, val_f1: 0.116314
Epoch: 141, loss: 22.5621, train_acc: 0.3411, train_recall: 0.1905, train_f1: 0.1522, val_acc: 0.337438, val_recall: 0.161657, val_f1: 0.105136
Epoch: 142, loss: 22.5541, train_acc: 0.3337, train_recall: 0.1862, train_f1: 0.1439, val_acc: 0.347291, val_recall: 0.193351, val_f1: 0.132380
Epoch: 143, loss: 22.5887, train_acc: 0.3411, train_recall: 0.1897, train_f1: 0.1398, val_acc: 0.310345, val_recall: 0.191675, val_f1: 0.116734
Epoch: 144, loss: 22.5240, train_acc: 0.3379, train_recall: 0.1880, train_f1: 0.1432, val_acc: 0.307882, val_recall: 0.189999, val_f1: 0.123556
Epoch: 145, loss: 22.4942, train_acc: 0.3337, train_recall: 0.1923, train_f1: 0.1479, val_acc: 0.334975, val_recall: 0.146399, val_f1: 0.093891
Epoch: 146, loss: 22.4978, train_acc: 0.3358, train_recall: 0.1720, train_f1: 0.1338, val_acc: 0.298030, val_recall: 0.132129, val_f1: 0.079205
Epoch: 147, loss: 22.4656, train_acc: 0.3443, train_recall: 0.1965, train_f1: 0.1771, val_acc: 0.307882, val_recall: 0.160862, val_f1: 0.110062
Epoch: 148, loss: 22.4391, train_acc: 0.3517, train_recall: 0.2201, train_f1: 0.1946, val_acc: 0.302956, val_recall: 0.151869, val_f1: 0.103802
Epoch: 149, loss: 22.4715, train_acc: 0.3400, train_recall: 0.1949, train_f1: 0.1672, val_acc: 0.337438, val_recall: 0.166897, val_f1: 0.114861
Epoch: 150, loss: 22.4638, train_acc: 0.3379, train_recall: 0.1945, train_f1: 0.1601, val_acc: 0.305419, val_recall: 0.167288, val_f1: 0.106422
Epoch: 151, loss: 22.4400, train_acc: 0.3422, train_recall: 0.1960, train_f1: 0.1613, val_acc: 0.307882, val_recall: 0.168321, val_f1: 0.106807
Epoch: 152, loss: 22.4347, train_acc: 0.3379, train_recall: 0.1965, train_f1: 0.1725, val_acc: 0.342365, val_recall: 0.177073, val_f1: 0.141206
Epoch: 153, loss: 22.4095, train_acc: 0.3369, train_recall: 0.1789, train_f1: 0.1584, val_acc: 0.354680, val_recall: 0.197163, val_f1: 0.155979
Epoch: 154, loss: 22.3851, train_acc: 0.3379, train_recall: 0.1707, train_f1: 0.1343, val_acc: 0.322660, val_recall: 0.198331, val_f1: 0.146303
Epoch: 155, loss: 22.4093, train_acc: 0.3379, train_recall: 0.1871, train_f1: 0.1265, val_acc: 0.320197, val_recall: 0.205234, val_f1: 0.121306
Epoch: 156, loss: 22.4020, train_acc: 0.3379, train_recall: 0.1867, train_f1: 0.1349, val_acc: 0.352217, val_recall: 0.205165, val_f1: 0.134508
Epoch: 157, loss: 22.3726, train_acc: 0.3432, train_recall: 0.2028, train_f1: 0.1790, val_acc: 0.342365, val_recall: 0.197946, val_f1: 0.145611
Epoch: 158, loss: 22.3863, train_acc: 0.3432, train_recall: 0.2066, train_f1: 0.1726, val_acc: 0.310345, val_recall: 0.196595, val_f1: 0.128204
Epoch: 159, loss: 22.3728, train_acc: 0.3411, train_recall: 0.1753, train_f1: 0.1420, val_acc: 0.315271, val_recall: 0.194239, val_f1: 0.141450
Epoch: 160, loss: 22.3628, train_acc: 0.3400, train_recall: 0.1746, train_f1: 0.1489, val_acc: 0.354680, val_recall: 0.197163, val_f1: 0.156108
Epoch: 161, loss: 22.3660, train_acc: 0.3400, train_recall: 0.1691, train_f1: 0.1358, val_acc: 0.320197, val_recall: 0.174208, val_f1: 0.131535
Epoch: 162, loss: 22.3535, train_acc: 0.3358, train_recall: 0.1584, train_f1: 0.1179, val_acc: 0.310345, val_recall: 0.150684, val_f1: 0.095517
Epoch: 163, loss: 22.3377, train_acc: 0.3284, train_recall: 0.1649, train_f1: 0.1595, val_acc: 0.325123, val_recall: 0.161254, val_f1: 0.155615
Epoch: 164, loss: 22.3459, train_acc: 0.3411, train_recall: 0.1763, train_f1: 0.1393, val_acc: 0.344828, val_recall: 0.158882, val_f1: 0.114784
Epoch: 165, loss: 22.3464, train_acc: 0.3369, train_recall: 0.1809, train_f1: 0.1303, val_acc: 0.307882, val_recall: 0.156821, val_f1: 0.097398
Epoch: 166, loss: 22.3579, train_acc: 0.3379, train_recall: 0.1707, train_f1: 0.1358, val_acc: 0.317734, val_recall: 0.196265, val_f1: 0.145532
Epoch: 167, loss: 22.3340, train_acc: 0.3326, train_recall: 0.1686, train_f1: 0.1647, val_acc: 0.330049, val_recall: 0.192324, val_f1: 0.179158
Epoch: 168, loss: 22.3374, train_acc: 0.3400, train_recall: 0.1749, train_f1: 0.1447, val_acc: 0.322660, val_recall: 0.202822, val_f1: 0.158181
Epoch: 169, loss: 22.3408, train_acc: 0.3390, train_recall: 0.1745, train_f1: 0.1416, val_acc: 0.320197, val_recall: 0.202217, val_f1: 0.151777
Epoch: 170, loss: 22.3440, train_acc: 0.3411, train_recall: 0.1718, train_f1: 0.1411, val_acc: 0.320197, val_recall: 0.196976, val_f1: 0.150482
Epoch: 171, loss: 22.3033, train_acc: 0.3337, train_recall: 0.1975, train_f1: 0.1988, val_acc: 0.320197, val_recall: 0.173267, val_f1: 0.161751
Epoch: 172, loss: 22.2996, train_acc: 0.3432, train_recall: 0.2031, train_f1: 0.1683, val_acc: 0.305419, val_recall: 0.195413, val_f1: 0.127222
Epoch: 173, loss: 22.2984, train_acc: 0.3411, train_recall: 0.2003, train_f1: 0.1711, val_acc: 0.307882, val_recall: 0.177101, val_f1: 0.129066
Epoch: 174, loss: 22.3216, train_acc: 0.3411, train_recall: 0.1852, train_f1: 0.1540, val_acc: 0.317734, val_recall: 0.195984, val_f1: 0.141554
Epoch: 175, loss: 22.3064, train_acc: 0.3379, train_recall: 0.1780, train_f1: 0.1499, val_acc: 0.315271, val_recall: 0.189147, val_f1: 0.137972
Epoch: 176, loss: 22.2962, train_acc: 0.3422, train_recall: 0.2007, train_f1: 0.1712, val_acc: 0.310345, val_recall: 0.178134, val_f1: 0.129466
Epoch: 177, loss: 22.2982, train_acc: 0.3422, train_recall: 0.2007, train_f1: 0.1741, val_acc: 0.307882, val_recall: 0.176994, val_f1: 0.130655
Epoch: 178, loss: 22.2990, train_acc: 0.3432, train_recall: 0.2011, train_f1: 0.1754, val_acc: 0.310345, val_recall: 0.177920, val_f1: 0.132598
Epoch: 179, loss: 22.2975, train_acc: 0.3411, train_recall: 0.2003, train_f1: 0.1716, val_acc: 0.307882, val_recall: 0.177101, val_f1: 0.129066
Epoch: 180, loss: 22.2958, train_acc: 0.3400, train_recall: 0.2000, train_f1: 0.1709, val_acc: 0.307882, val_recall: 0.177101, val_f1: 0.129066
Epoch: 181, loss: 22.2922, train_acc: 0.3422, train_recall: 0.2007, train_f1: 0.1742, val_acc: 0.310345, val_recall: 0.177920, val_f1: 0.132598
Epoch: 182, loss: 22.2914, train_acc: 0.3422, train_recall: 0.2007, train_f1: 0.1730, val_acc: 0.310345, val_recall: 0.178027, val_f1: 0.130973
Epoch: 183, loss: 22.2802, train_acc: 0.3432, train_recall: 0.2092, train_f1: 0.1788, val_acc: 0.307882, val_recall: 0.173174, val_f1: 0.111027
Epoch: 184, loss: 22.2682, train_acc: 0.3443, train_recall: 0.2096, train_f1: 0.1823, val_acc: 0.310345, val_recall: 0.173993, val_f1: 0.114500
Epoch: 185, loss: 22.2838, train_acc: 0.3422, train_recall: 0.2007, train_f1: 0.1747, val_acc: 0.312808, val_recall: 0.178953, val_f1: 0.133026
Epoch: 186, loss: 22.2460, train_acc: 0.3400, train_recall: 0.2000, train_f1: 0.1709, val_acc: 0.307882, val_recall: 0.177101, val_f1: 0.129066
Epoch: 187, loss: 22.2587, train_acc: 0.3411, train_recall: 0.2038, train_f1: 0.1777, val_acc: 0.310345, val_recall: 0.183054, val_f1: 0.137869
Epoch: 188, loss: 22.2602, train_acc: 0.3453, train_recall: 0.2053, train_f1: 0.1825, val_acc: 0.315271, val_recall: 0.184905, val_f1: 0.141831
Epoch: 189, loss: 22.2422, train_acc: 0.3422, train_recall: 0.2007, train_f1: 0.1736, val_acc: 0.310345, val_recall: 0.178027, val_f1: 0.130577
Epoch: 190, loss: 22.2666, train_acc: 0.3422, train_recall: 0.2007, train_f1: 0.1717, val_acc: 0.310345, val_recall: 0.178134, val_f1: 0.128973
Epoch: 191, loss: 22.2492, train_acc: 0.3432, train_recall: 0.2011, train_f1: 0.1749, val_acc: 0.312808, val_recall: 0.178953, val_f1: 0.132447
Epoch: 192, loss: 22.2545, train_acc: 0.3443, train_recall: 0.2015, train_f1: 0.1745, val_acc: 0.312808, val_recall: 0.178953, val_f1: 0.132533
Epoch: 193, loss: 22.2320, train_acc: 0.3411, train_recall: 0.2003, train_f1: 0.1699, val_acc: 0.307882, val_recall: 0.177101, val_f1: 0.128572
Epoch: 194, loss: 22.2315, train_acc: 0.3411, train_recall: 0.2003, train_f1: 0.1699, val_acc: 0.307882, val_recall: 0.177101, val_f1: 0.128572
Epoch: 195, loss: 22.2225, train_acc: 0.3453, train_recall: 0.2053, train_f1: 0.1812, val_acc: 0.315271, val_recall: 0.184905, val_f1: 0.141831
Epoch: 196, loss: 22.2315, train_acc: 0.3411, train_recall: 0.2003, train_f1: 0.1718, val_acc: 0.310345, val_recall: 0.178027, val_f1: 0.130577
Epoch: 197, loss: 22.2342, train_acc: 0.3411, train_recall: 0.2002, train_f1: 0.1677, val_acc: 0.307882, val_recall: 0.177101, val_f1: 0.124593
Epoch: 198, loss: 22.2295, train_acc: 0.3411, train_recall: 0.2002, train_f1: 0.1687, val_acc: 0.310345, val_recall: 0.178027, val_f1: 0.126598
Epoch: 199, loss: 22.2184, train_acc: 0.3432, train_recall: 0.2010, train_f1: 0.1707, val_acc: 0.312808, val_recall: 0.178953, val_f1: 0.128555
Epoch: 200, loss: 22.2325, train_acc: 0.3400, train_recall: 0.1998, train_f1: 0.1661, val_acc: 0.307882, val_recall: 0.177101, val_f1: 0.124593
Epoch: 201, loss: 22.2176, train_acc: 0.3390, train_recall: 0.1995, train_f1: 0.1660, val_acc: 0.307882, val_recall: 0.177101, val_f1: 0.124593
Epoch: 202, loss: 22.2203, train_acc: 0.3422, train_recall: 0.2006, train_f1: 0.1705, val_acc: 0.312808, val_recall: 0.178953, val_f1: 0.128555
Epoch: 203, loss: 22.1997, train_acc: 0.3390, train_recall: 0.1995, train_f1: 0.1667, val_acc: 0.310345, val_recall: 0.178027, val_f1: 0.126598
Epoch: 204, loss: 22.1994, train_acc: 0.3400, train_recall: 0.1998, train_f1: 0.1666, val_acc: 0.307882, val_recall: 0.177101, val_f1: 0.124593
Epoch: 205, loss: 22.1888, train_acc: 0.3400, train_recall: 0.1998, train_f1: 0.1684, val_acc: 0.310345, val_recall: 0.178027, val_f1: 0.126598
Epoch: 206, loss: 22.1991, train_acc: 0.3411, train_recall: 0.2002, train_f1: 0.1697, val_acc: 0.312808, val_recall: 0.178953, val_f1: 0.128469
Epoch: 207, loss: 22.1889, train_acc: 0.3400, train_recall: 0.2033, train_f1: 0.1724, val_acc: 0.310345, val_recall: 0.183054, val_f1: 0.133583
Epoch: 208, loss: 22.1919, train_acc: 0.3390, train_recall: 0.2029, train_f1: 0.1723, val_acc: 0.310345, val_recall: 0.183054, val_f1: 0.133583
Epoch: 209, loss: 22.1681, train_acc: 0.3411, train_recall: 0.2037, train_f1: 0.1750, val_acc: 0.312808, val_recall: 0.183980, val_f1: 0.135588
Epoch: 210, loss: 22.1769, train_acc: 0.3411, train_recall: 0.2037, train_f1: 0.1744, val_acc: 0.312808, val_recall: 0.183980, val_f1: 0.135588
Epoch: 211, loss: 22.1882, train_acc: 0.3411, train_recall: 0.2037, train_f1: 0.1748, val_acc: 0.310345, val_recall: 0.183054, val_f1: 0.133583
Epoch: 212, loss: 22.1800, train_acc: 0.3411, train_recall: 0.2037, train_f1: 0.1760, val_acc: 0.312808, val_recall: 0.183980, val_f1: 0.135588
Epoch: 213, loss: 22.1771, train_acc: 0.3422, train_recall: 0.2040, train_f1: 0.1767, val_acc: 0.312808, val_recall: 0.183980, val_f1: 0.135588
Epoch: 214, loss: 22.1672, train_acc: 0.3411, train_recall: 0.2037, train_f1: 0.1748, val_acc: 0.310345, val_recall: 0.183054, val_f1: 0.133583
Epoch: 215, loss: 22.1521, train_acc: 0.3422, train_recall: 0.2040, train_f1: 0.1761, val_acc: 0.312808, val_recall: 0.183980, val_f1: 0.135491
Epoch: 216, loss: 22.1383, train_acc: 0.3411, train_recall: 0.2037, train_f1: 0.1766, val_acc: 0.312808, val_recall: 0.183980, val_f1: 0.135588
Epoch: 217, loss: 22.1646, train_acc: 0.3411, train_recall: 0.2037, train_f1: 0.1760, val_acc: 0.312808, val_recall: 0.183980, val_f1: 0.135956
Epoch: 218, loss: 22.1510, train_acc: 0.3422, train_recall: 0.2040, train_f1: 0.1772, val_acc: 0.312808, val_recall: 0.183980, val_f1: 0.135859
Epoch: 219, loss: 22.1417, train_acc: 0.3411, train_recall: 0.2037, train_f1: 0.1771, val_acc: 0.312808, val_recall: 0.183980, val_f1: 0.135588
Epoch: 220, loss: 22.1442, train_acc: 0.3411, train_recall: 0.2037, train_f1: 0.1771, val_acc: 0.312808, val_recall: 0.183980, val_f1: 0.135588
Epoch: 221, loss: 22.1356, train_acc: 0.3411, train_recall: 0.2037, train_f1: 0.1754, val_acc: 0.312808, val_recall: 0.183980, val_f1: 0.135491
Epoch: 222, loss: 22.1393, train_acc: 0.3400, train_recall: 0.2033, train_f1: 0.1743, val_acc: 0.312808, val_recall: 0.183980, val_f1: 0.135956
Epoch: 223, loss: 22.1290, train_acc: 0.3390, train_recall: 0.2029, train_f1: 0.1741, val_acc: 0.312808, val_recall: 0.183980, val_f1: 0.135956
Epoch: 224, loss: 22.1249, train_acc: 0.3400, train_recall: 0.2033, train_f1: 0.1743, val_acc: 0.312808, val_recall: 0.183980, val_f1: 0.135956
Epoch: 225, loss: 22.1204, train_acc: 0.3400, train_recall: 0.2033, train_f1: 0.1743, val_acc: 0.312808, val_recall: 0.183980, val_f1: 0.135956
Epoch: 226, loss: 22.1081, train_acc: 0.3390, train_recall: 0.2029, train_f1: 0.1741, val_acc: 0.312808, val_recall: 0.183980, val_f1: 0.135956
Epoch: 227, loss: 22.1259, train_acc: 0.3400, train_recall: 0.2033, train_f1: 0.1743, val_acc: 0.312808, val_recall: 0.183980, val_f1: 0.135956
Epoch: 228, loss: 22.1178, train_acc: 0.3400, train_recall: 0.2033, train_f1: 0.1743, val_acc: 0.312808, val_recall: 0.183980, val_f1: 0.135956
Epoch: 229, loss: 22.1110, train_acc: 0.3400, train_recall: 0.2033, train_f1: 0.1743, val_acc: 0.312808, val_recall: 0.183980, val_f1: 0.135956
Epoch: 230, loss: 22.1113, train_acc: 0.3400, train_recall: 0.2033, train_f1: 0.1753, val_acc: 0.312808, val_recall: 0.183980, val_f1: 0.135956
Epoch: 231, loss: 22.0966, train_acc: 0.3400, train_recall: 0.2033, train_f1: 0.1753, val_acc: 0.312808, val_recall: 0.183980, val_f1: 0.135956
Epoch: 232, loss: 22.0884, train_acc: 0.3400, train_recall: 0.2033, train_f1: 0.1753, val_acc: 0.312808, val_recall: 0.183980, val_f1: 0.135956
Epoch: 233, loss: 22.0840, train_acc: 0.3400, train_recall: 0.2033, train_f1: 0.1753, val_acc: 0.312808, val_recall: 0.183980, val_f1: 0.135956
Epoch: 234, loss: 22.1073, train_acc: 0.3400, train_recall: 0.2033, train_f1: 0.1753, val_acc: 0.312808, val_recall: 0.183980, val_f1: 0.135956
Epoch: 235, loss: 22.0848, train_acc: 0.3400, train_recall: 0.2033, train_f1: 0.1753, val_acc: 0.312808, val_recall: 0.183980, val_f1: 0.135956
Epoch: 236, loss: 22.0755, train_acc: 0.3400, train_recall: 0.2033, train_f1: 0.1753, val_acc: 0.312808, val_recall: 0.183980, val_f1: 0.135956
Epoch: 237, loss: 22.0810, train_acc: 0.3400, train_recall: 0.2033, train_f1: 0.1753, val_acc: 0.312808, val_recall: 0.183980, val_f1: 0.135956
Epoch: 238, loss: 22.0856, train_acc: 0.3400, train_recall: 0.2033, train_f1: 0.1753, val_acc: 0.312808, val_recall: 0.183980, val_f1: 0.135956
Epoch: 239, loss: 22.0645, train_acc: 0.3411, train_recall: 0.2037, train_f1: 0.1762, val_acc: 0.312808, val_recall: 0.183980, val_f1: 0.136054
Epoch: 240, loss: 22.0667, train_acc: 0.3411, train_recall: 0.2037, train_f1: 0.1762, val_acc: 0.312808, val_recall: 0.183980, val_f1: 0.136054
Epoch: 241, loss: 22.0491, train_acc: 0.3411, train_recall: 0.2037, train_f1: 0.1762, val_acc: 0.312808, val_recall: 0.183980, val_f1: 0.136054
Epoch: 242, loss: 22.0560, train_acc: 0.3411, train_recall: 0.2037, train_f1: 0.1762, val_acc: 0.315271, val_recall: 0.184905, val_f1: 0.137913
Epoch: 243, loss: 22.0536, train_acc: 0.3411, train_recall: 0.2037, train_f1: 0.1762, val_acc: 0.315271, val_recall: 0.184905, val_f1: 0.137913
Epoch: 244, loss: 22.0568, train_acc: 0.3411, train_recall: 0.2037, train_f1: 0.1762, val_acc: 0.315271, val_recall: 0.184905, val_f1: 0.137913
Epoch: 245, loss: 22.0488, train_acc: 0.3411, train_recall: 0.2037, train_f1: 0.1762, val_acc: 0.315271, val_recall: 0.184905, val_f1: 0.137913
Epoch: 246, loss: 22.0344, train_acc: 0.3411, train_recall: 0.2037, train_f1: 0.1762, val_acc: 0.315271, val_recall: 0.184905, val_f1: 0.137913
Epoch: 247, loss: 22.0354, train_acc: 0.3411, train_recall: 0.2037, train_f1: 0.1762, val_acc: 0.315271, val_recall: 0.184905, val_f1: 0.137913
Epoch: 248, loss: 22.0400, train_acc: 0.3411, train_recall: 0.2037, train_f1: 0.1762, val_acc: 0.315271, val_recall: 0.184905, val_f1: 0.137913
Epoch: 249, loss: 22.0409, train_acc: 0.3400, train_recall: 0.2033, train_f1: 0.1761, val_acc: 0.315271, val_recall: 0.184905, val_f1: 0.137913
Epoch: 250, loss: 22.0252, train_acc: 0.3400, train_recall: 0.2033, train_f1: 0.1761, val_acc: 0.315271, val_recall: 0.184905, val_f1: 0.137913
Epoch: 251, loss: 22.0119, train_acc: 0.3411, train_recall: 0.2037, train_f1: 0.1768, val_acc: 0.315271, val_recall: 0.184905, val_f1: 0.137913
Epoch: 252, loss: 22.0233, train_acc: 0.3411, train_recall: 0.2037, train_f1: 0.1768, val_acc: 0.315271, val_recall: 0.184905, val_f1: 0.137913
Epoch: 253, loss: 22.0360, train_acc: 0.3411, train_recall: 0.2037, train_f1: 0.1768, val_acc: 0.315271, val_recall: 0.184905, val_f1: 0.137913
Epoch: 254, loss: 22.0064, train_acc: 0.3411, train_recall: 0.2037, train_f1: 0.1768, val_acc: 0.315271, val_recall: 0.184905, val_f1: 0.137913
Epoch: 255, loss: 22.0163, train_acc: 0.3411, train_recall: 0.2037, train_f1: 0.1768, val_acc: 0.315271, val_recall: 0.184905, val_f1: 0.137913
Epoch: 256, loss: 21.9851, train_acc: 0.3411, train_recall: 0.2037, train_f1: 0.1768, val_acc: 0.315271, val_recall: 0.184905, val_f1: 0.137913
Epoch: 257, loss: 22.0015, train_acc: 0.3422, train_recall: 0.2040, train_f1: 0.1786, val_acc: 0.315271, val_recall: 0.184905, val_f1: 0.137913
Epoch: 258, loss: 22.0099, train_acc: 0.3422, train_recall: 0.2040, train_f1: 0.1786, val_acc: 0.315271, val_recall: 0.184905, val_f1: 0.137913
Epoch: 259, loss: 21.9756, train_acc: 0.3422, train_recall: 0.2040, train_f1: 0.1786, val_acc: 0.315271, val_recall: 0.184905, val_f1: 0.137913
Epoch: 260, loss: 21.9726, train_acc: 0.3422, train_recall: 0.2040, train_f1: 0.1786, val_acc: 0.315271, val_recall: 0.184905, val_f1: 0.137913
Epoch: 261, loss: 21.9694, train_acc: 0.3422, train_recall: 0.2040, train_f1: 0.1786, val_acc: 0.315271, val_recall: 0.184905, val_f1: 0.137913
Epoch: 262, loss: 21.9774, train_acc: 0.3422, train_recall: 0.2040, train_f1: 0.1786, val_acc: 0.315271, val_recall: 0.184905, val_f1: 0.137913
Epoch: 263, loss: 21.9821, train_acc: 0.3400, train_recall: 0.1975, train_f1: 0.1699, val_acc: 0.315271, val_recall: 0.184905, val_f1: 0.137913
Epoch: 264, loss: 21.9691, train_acc: 0.3400, train_recall: 0.1975, train_f1: 0.1699, val_acc: 0.312808, val_recall: 0.177961, val_f1: 0.129976
Epoch: 265, loss: 21.9700, train_acc: 0.3400, train_recall: 0.1975, train_f1: 0.1699, val_acc: 0.312808, val_recall: 0.177961, val_f1: 0.129976
Epoch: 266, loss: 21.9757, train_acc: 0.3400, train_recall: 0.1975, train_f1: 0.1699, val_acc: 0.312808, val_recall: 0.177961, val_f1: 0.129976
Epoch: 267, loss: 21.9585, train_acc: 0.3400, train_recall: 0.1975, train_f1: 0.1700, val_acc: 0.312808, val_recall: 0.177961, val_f1: 0.129976
Epoch: 268, loss: 21.9433, train_acc: 0.3411, train_recall: 0.2037, train_f1: 0.1784, val_acc: 0.312808, val_recall: 0.177961, val_f1: 0.129976
Epoch: 269, loss: 21.9463, train_acc: 0.3411, train_recall: 0.2037, train_f1: 0.1784, val_acc: 0.312808, val_recall: 0.177961, val_f1: 0.129976
Epoch: 270, loss: 21.9523, train_acc: 0.3411, train_recall: 0.2037, train_f1: 0.1784, val_acc: 0.312808, val_recall: 0.177961, val_f1: 0.129976
Epoch: 271, loss: 21.9559, train_acc: 0.3411, train_recall: 0.2037, train_f1: 0.1784, val_acc: 0.315271, val_recall: 0.184905, val_f1: 0.137913
Epoch: 272, loss: 21.9352, train_acc: 0.3411, train_recall: 0.2037, train_f1: 0.1784, val_acc: 0.315271, val_recall: 0.184905, val_f1: 0.137913
Epoch: 273, loss: 21.9331, train_acc: 0.3422, train_recall: 0.2055, train_f1: 0.1820, val_acc: 0.315271, val_recall: 0.184905, val_f1: 0.137913
Epoch: 274, loss: 21.9313, train_acc: 0.3422, train_recall: 0.2055, train_f1: 0.1820, val_acc: 0.315271, val_recall: 0.184905, val_f1: 0.137913
Epoch: 275, loss: 21.9276, train_acc: 0.3400, train_recall: 0.1978, train_f1: 0.1679, val_acc: 0.315271, val_recall: 0.184905, val_f1: 0.137913
/home/ADS/cyang314/ucr_work/HINI_Baseline/GraphLoRA/model/GraphLoRA.py:218: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.
Consider using tensor.detach() first. (Triggered internally at /pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:836.)
  f.write(f'{pre_dataset} to {downstream_dataset}: seed: %d, epoch: %d, train_loss: %f, train_acc: %f, train_recall: %f, train_f1: %f, val_acc: %f, val_recall: %f, val_f1: %f\n' %
Epoch: 276, loss: 21.9368, train_acc: 0.3400, train_recall: 0.1978, train_f1: 0.1679, val_acc: 0.315271, val_recall: 0.184905, val_f1: 0.137913
Epoch: 277, loss: 21.9181, train_acc: 0.3400, train_recall: 0.1978, train_f1: 0.1679, val_acc: 0.315271, val_recall: 0.184905, val_f1: 0.137913
Epoch: 278, loss: 21.9209, train_acc: 0.3400, train_recall: 0.1978, train_f1: 0.1679, val_acc: 0.315271, val_recall: 0.184905, val_f1: 0.137913
Epoch: 279, loss: 21.9186, train_acc: 0.3400, train_recall: 0.1978, train_f1: 0.1679, val_acc: 0.315271, val_recall: 0.184905, val_f1: 0.137913
Epoch: 280, loss: 21.9134, train_acc: 0.3400, train_recall: 0.1978, train_f1: 0.1679, val_acc: 0.312808, val_recall: 0.177961, val_f1: 0.129976
Epoch: 281, loss: 21.8984, train_acc: 0.3400, train_recall: 0.1978, train_f1: 0.1679, val_acc: 0.312808, val_recall: 0.177961, val_f1: 0.129976
Epoch: 282, loss: 21.9072, train_acc: 0.3294, train_recall: 0.1939, train_f1: 0.1911, val_acc: 0.325123, val_recall: 0.174449, val_f1: 0.160996
Epoch: 283, loss: 21.9102, train_acc: 0.3411, train_recall: 0.1982, train_f1: 0.1680, val_acc: 0.312808, val_recall: 0.177961, val_f1: 0.129976
Epoch: 284, loss: 21.8977, train_acc: 0.3411, train_recall: 0.1982, train_f1: 0.1680, val_acc: 0.315271, val_recall: 0.184905, val_f1: 0.137913
Epoch: 285, loss: 21.8996, train_acc: 0.3305, train_recall: 0.1942, train_f1: 0.1914, val_acc: 0.327586, val_recall: 0.181393, val_f1: 0.168933
Epoch: 286, loss: 21.8744, train_acc: 0.3549, train_recall: 0.2030, train_f1: 0.1889, val_acc: 0.325123, val_recall: 0.186895, val_f1: 0.160218
Epoch: 287, loss: 21.8819, train_acc: 0.3305, train_recall: 0.1942, train_f1: 0.1914, val_acc: 0.327586, val_recall: 0.181393, val_f1: 0.168933
Epoch: 288, loss: 21.8814, train_acc: 0.3305, train_recall: 0.1942, train_f1: 0.1914, val_acc: 0.327586, val_recall: 0.181393, val_f1: 0.168933
Epoch: 289, loss: 21.8649, train_acc: 0.3305, train_recall: 0.1942, train_f1: 0.1914, val_acc: 0.327586, val_recall: 0.181393, val_f1: 0.168933
Epoch: 290, loss: 21.8798, train_acc: 0.3305, train_recall: 0.1942, train_f1: 0.1914, val_acc: 0.327586, val_recall: 0.181393, val_f1: 0.168933
Epoch: 291, loss: 21.8656, train_acc: 0.3316, train_recall: 0.1947, train_f1: 0.1947, val_acc: 0.327586, val_recall: 0.181393, val_f1: 0.173219
Epoch: 292, loss: 21.8697, train_acc: 0.3316, train_recall: 0.1947, train_f1: 0.1947, val_acc: 0.327586, val_recall: 0.181393, val_f1: 0.173219
Epoch: 293, loss: 21.8595, train_acc: 0.3305, train_recall: 0.1943, train_f1: 0.1943, val_acc: 0.330049, val_recall: 0.182319, val_f1: 0.173894
Epoch: 294, loss: 21.8557, train_acc: 0.3273, train_recall: 0.1932, train_f1: 0.1931, val_acc: 0.322660, val_recall: 0.179113, val_f1: 0.170613
Epoch: 295, loss: 21.8698, train_acc: 0.3273, train_recall: 0.1932, train_f1: 0.1931, val_acc: 0.322660, val_recall: 0.179113, val_f1: 0.170619
Epoch: 296, loss: 21.8437, train_acc: 0.3263, train_recall: 0.1928, train_f1: 0.1916, val_acc: 0.322660, val_recall: 0.179113, val_f1: 0.170619
Epoch: 297, loss: 21.8327, train_acc: 0.3263, train_recall: 0.1928, train_f1: 0.1916, val_acc: 0.322660, val_recall: 0.179113, val_f1: 0.170619
Epoch: 298, loss: 21.8353, train_acc: 0.3284, train_recall: 0.1936, train_f1: 0.1924, val_acc: 0.327586, val_recall: 0.181286, val_f1: 0.172920
Epoch: 299, loss: 21.8320, train_acc: 0.3263, train_recall: 0.1928, train_f1: 0.1916, val_acc: 0.322660, val_recall: 0.179113, val_f1: 0.170619
epoch: 138, train_acc: 0.331568, val_acc: 0.359606, val_recall: 0.207515, val_f1: 0.132446
Running: year=2015 â†’ downstream_year=2016, seed=2
Random seed set to 42

==============================
PRE-TRAINING
==============================
create PreTrain instance...
pre-training...
(T) | Epoch=001, loss=7.3009, this epoch 0.0415, total 0.0415
+++model saved ! 2015.pth
(T) | Epoch=002, loss=6.3445, this epoch 0.0335, total 0.0749
+++model saved ! 2015.pth
(T) | Epoch=003, loss=6.3445, this epoch 0.0374, total 0.1123
(T) | Epoch=004, loss=6.3444, this epoch 0.0393, total 0.1516
+++model saved ! 2015.pth
(T) | Epoch=005, loss=6.3451, this epoch 0.0320, total 0.1836
(T) | Epoch=006, loss=6.3445, this epoch 0.0219, total 0.2055
(T) | Epoch=007, loss=6.3472, this epoch 0.0276, total 0.2331
(T) | Epoch=008, loss=6.8267, this epoch 0.0362, total 0.2693
(T) | Epoch=009, loss=6.3447, this epoch 0.0356, total 0.3049
(T) | Epoch=010, loss=6.3446, this epoch 0.0326, total 0.3375
(T) | Epoch=011, loss=6.7118, this epoch 0.0369, total 0.3744
(T) | Epoch=012, loss=6.3444, this epoch 0.0292, total 0.4037
(T) | Epoch=013, loss=6.3447, this epoch 0.0324, total 0.4361
(T) | Epoch=014, loss=6.3439, this epoch 0.0309, total 0.4670
+++model saved ! 2015.pth
(T) | Epoch=015, loss=6.3438, this epoch 0.0403, total 0.5073
+++model saved ! 2015.pth
(T) | Epoch=016, loss=6.3444, this epoch 0.0302, total 0.5375
(T) | Epoch=017, loss=6.3436, this epoch 0.0361, total 0.5735
+++model saved ! 2015.pth
(T) | Epoch=018, loss=6.3435, this epoch 0.0374, total 0.6110
+++model saved ! 2015.pth
(T) | Epoch=019, loss=6.5342, this epoch 0.0317, total 0.6426
(T) | Epoch=020, loss=6.3434, this epoch 0.0293, total 0.6719
+++model saved ! 2015.pth
(T) | Epoch=021, loss=6.3474, this epoch 0.0313, total 0.7032
(T) | Epoch=022, loss=6.3466, this epoch 0.0235, total 0.7268
(T) | Epoch=023, loss=6.4797, this epoch 0.0261, total 0.7528
(T) | Epoch=024, loss=6.3427, this epoch 0.0231, total 0.7759
+++model saved ! 2015.pth
(T) | Epoch=025, loss=6.3428, this epoch 0.0300, total 0.8060
(T) | Epoch=026, loss=6.3444, this epoch 0.0218, total 0.8278
(T) | Epoch=027, loss=6.3420, this epoch 0.0257, total 0.8535
+++model saved ! 2015.pth
(T) | Epoch=028, loss=6.3464, this epoch 0.0374, total 0.8910
(T) | Epoch=029, loss=6.3437, this epoch 0.0240, total 0.9150
(T) | Epoch=030, loss=6.4340, this epoch 0.0280, total 0.9429
(T) | Epoch=031, loss=6.3460, this epoch 0.0288, total 0.9718
(T) | Epoch=032, loss=6.3407, this epoch 0.0227, total 0.9944
+++model saved ! 2015.pth
(T) | Epoch=033, loss=6.3406, this epoch 0.0232, total 1.0176
+++model saved ! 2015.pth
(T) | Epoch=034, loss=6.3411, this epoch 0.0484, total 1.0660
(T) | Epoch=035, loss=6.3453, this epoch 0.0244, total 1.0904
(T) | Epoch=036, loss=6.3412, this epoch 0.0361, total 1.1265
(T) | Epoch=037, loss=6.3867, this epoch 0.0254, total 1.1519
(T) | Epoch=038, loss=6.4135, this epoch 0.0436, total 1.1955
(T) | Epoch=039, loss=6.3379, this epoch 0.0317, total 1.2272
+++model saved ! 2015.pth
(T) | Epoch=040, loss=6.3458, this epoch 0.0230, total 1.2502
(T) | Epoch=041, loss=6.3442, this epoch 0.0292, total 1.2794
(T) | Epoch=042, loss=6.3398, this epoch 0.0214, total 1.3008
(T) | Epoch=043, loss=6.3361, this epoch 0.0216, total 1.3224
+++model saved ! 2015.pth
(T) | Epoch=044, loss=6.3448, this epoch 0.0439, total 1.3663
(T) | Epoch=045, loss=6.3339, this epoch 0.0367, total 1.4030
+++model saved ! 2015.pth
(T) | Epoch=046, loss=6.3378, this epoch 0.0404, total 1.4433
(T) | Epoch=047, loss=6.3318, this epoch 0.0313, total 1.4747
+++model saved ! 2015.pth
(T) | Epoch=048, loss=6.3725, this epoch 0.0409, total 1.5156
(T) | Epoch=049, loss=6.3294, this epoch 0.0307, total 1.5463
+++model saved ! 2015.pth
(T) | Epoch=050, loss=6.3329, this epoch 0.0322, total 1.5785
(T) | Epoch=051, loss=6.3295, this epoch 0.0289, total 1.6074
(T) | Epoch=052, loss=6.3292, this epoch 0.0350, total 1.6424
+++model saved ! 2015.pth
(T) | Epoch=053, loss=6.3262, this epoch 0.0326, total 1.6750
+++model saved ! 2015.pth
(T) | Epoch=054, loss=6.3243, this epoch 0.0391, total 1.7141
+++model saved ! 2015.pth
(T) | Epoch=055, loss=6.3177, this epoch 0.0324, total 1.7464
+++model saved ! 2015.pth
(T) | Epoch=056, loss=6.3145, this epoch 0.0421, total 1.7885
+++model saved ! 2015.pth
(T) | Epoch=057, loss=6.3199, this epoch 0.0309, total 1.8194
(T) | Epoch=058, loss=6.3309, this epoch 0.0229, total 1.8423
(T) | Epoch=059, loss=6.3194, this epoch 0.0210, total 1.8633
(T) | Epoch=060, loss=6.2954, this epoch 0.0211, total 1.8844
+++model saved ! 2015.pth
(T) | Epoch=061, loss=6.2872, this epoch 0.0472, total 1.9316
+++model saved ! 2015.pth
(T) | Epoch=062, loss=6.2811, this epoch 0.0242, total 1.9558
+++model saved ! 2015.pth
(T) | Epoch=063, loss=6.2604, this epoch 0.0227, total 1.9785
+++model saved ! 2015.pth
(T) | Epoch=064, loss=6.2564, this epoch 0.0367, total 2.0152
+++model saved ! 2015.pth
(T) | Epoch=065, loss=6.2311, this epoch 0.0321, total 2.0473
+++model saved ! 2015.pth
(T) | Epoch=066, loss=6.2235, this epoch 0.0307, total 2.0780
+++model saved ! 2015.pth
(T) | Epoch=067, loss=6.2264, this epoch 0.0389, total 2.1169
(T) | Epoch=068, loss=6.1711, this epoch 0.0326, total 2.1495
+++model saved ! 2015.pth
(T) | Epoch=069, loss=6.2524, this epoch 0.0296, total 2.1791
(T) | Epoch=070, loss=6.0517, this epoch 0.0235, total 2.2026
+++model saved ! 2015.pth
(T) | Epoch=071, loss=5.9781, this epoch 0.0225, total 2.2251
+++model saved ! 2015.pth
(T) | Epoch=072, loss=5.9220, this epoch 0.0388, total 2.2639
+++model saved ! 2015.pth
(T) | Epoch=073, loss=5.8787, this epoch 0.0363, total 2.3002
+++model saved ! 2015.pth
(T) | Epoch=074, loss=5.8249, this epoch 0.0330, total 2.3332
+++model saved ! 2015.pth
(T) | Epoch=075, loss=5.7711, this epoch 0.0380, total 2.3712
+++model saved ! 2015.pth
(T) | Epoch=076, loss=6.6189, this epoch 0.0413, total 2.4126
(T) | Epoch=077, loss=5.7260, this epoch 0.0320, total 2.4445
+++model saved ! 2015.pth
(T) | Epoch=078, loss=5.6917, this epoch 0.0310, total 2.4756
+++model saved ! 2015.pth
(T) | Epoch=079, loss=5.7709, this epoch 0.0444, total 2.5200
(T) | Epoch=080, loss=5.6984, this epoch 0.0354, total 2.5554
(T) | Epoch=081, loss=5.6744, this epoch 0.0383, total 2.5937
+++model saved ! 2015.pth
(T) | Epoch=082, loss=5.7215, this epoch 0.0251, total 2.6188
(T) | Epoch=083, loss=5.6127, this epoch 0.0292, total 2.6480
+++model saved ! 2015.pth
(T) | Epoch=084, loss=5.9439, this epoch 0.0362, total 2.6842
(T) | Epoch=085, loss=5.6125, this epoch 0.0293, total 2.7134
+++model saved ! 2015.pth
(T) | Epoch=086, loss=5.5824, this epoch 0.0237, total 2.7371
+++model saved ! 2015.pth
(T) | Epoch=087, loss=5.5724, this epoch 0.0374, total 2.7745
+++model saved ! 2015.pth
(T) | Epoch=088, loss=5.9507, this epoch 0.0409, total 2.8154
(T) | Epoch=089, loss=6.1384, this epoch 0.0326, total 2.8479
(T) | Epoch=090, loss=5.6252, this epoch 0.0317, total 2.8797
(T) | Epoch=091, loss=6.0038, this epoch 0.0307, total 2.9104
(T) | Epoch=092, loss=5.6440, this epoch 0.0335, total 2.9439
(T) | Epoch=093, loss=5.6387, this epoch 0.0382, total 2.9821
(T) | Epoch=094, loss=5.8832, this epoch 0.0345, total 3.0166
(T) | Epoch=095, loss=5.7851, this epoch 0.0305, total 3.0471
(T) | Epoch=096, loss=5.5782, this epoch 0.0321, total 3.0792
(T) | Epoch=097, loss=5.6243, this epoch 0.0366, total 3.1157
(T) | Epoch=098, loss=5.7081, this epoch 0.0292, total 3.1450
(T) | Epoch=099, loss=5.6286, this epoch 0.0384, total 3.1833
(T) | Epoch=100, loss=5.6361, this epoch 0.0412, total 3.2245
(T) | Epoch=101, loss=5.9126, this epoch 0.0330, total 3.2575
(T) | Epoch=102, loss=5.6857, this epoch 0.0390, total 3.2965
(T) | Epoch=103, loss=5.6386, this epoch 0.0381, total 3.3346
(T) | Epoch=104, loss=5.6401, this epoch 0.0334, total 3.3680
(T) | Epoch=105, loss=5.6428, this epoch 0.0326, total 3.4006
(T) | Epoch=106, loss=5.7198, this epoch 0.0267, total 3.4273
(T) | Epoch=107, loss=6.5084, this epoch 0.0345, total 3.4618
(T) | Epoch=108, loss=5.6058, this epoch 0.0319, total 3.4937
(T) | Epoch=109, loss=6.0109, this epoch 0.0379, total 3.5316
(T) | Epoch=110, loss=5.7566, this epoch 0.0391, total 3.5707
(T) | Epoch=111, loss=5.7794, this epoch 0.0381, total 3.6089
(T) | Epoch=112, loss=5.8127, this epoch 0.0333, total 3.6422
(T) | Epoch=113, loss=5.8358, this epoch 0.0403, total 3.6825
(T) | Epoch=114, loss=6.3422, this epoch 0.0291, total 3.7116
(T) | Epoch=115, loss=5.9372, this epoch 0.0323, total 3.7439
(T) | Epoch=116, loss=5.8214, this epoch 0.0300, total 3.7739
(T) | Epoch=117, loss=6.2301, this epoch 0.0370, total 3.8109
(T) | Epoch=118, loss=5.8718, this epoch 0.0415, total 3.8524
(T) | Epoch=119, loss=5.7465, this epoch 0.0384, total 3.8908
(T) | Epoch=120, loss=5.9975, this epoch 0.0381, total 3.9289
(T) | Epoch=121, loss=5.7267, this epoch 0.0344, total 3.9633
(T) | Epoch=122, loss=5.7174, this epoch 0.0389, total 4.0022
(T) | Epoch=123, loss=5.6807, this epoch 0.0391, total 4.0414
(T) | Epoch=124, loss=5.6803, this epoch 0.0371, total 4.0785
(T) | Epoch=125, loss=5.7367, this epoch 0.0225, total 4.1010
(T) | Epoch=126, loss=6.0703, this epoch 0.0366, total 4.1376
(T) | Epoch=127, loss=5.8209, this epoch 0.0398, total 4.1774
(T) | Epoch=128, loss=5.6466, this epoch 0.0381, total 4.2155
(T) | Epoch=129, loss=5.6444, this epoch 0.0333, total 4.2488
(T) | Epoch=130, loss=5.6630, this epoch 0.0373, total 4.2861
(T) | Epoch=131, loss=5.6858, this epoch 0.0397, total 4.3258
(T) | Epoch=132, loss=5.6466, this epoch 0.0383, total 4.3640
(T) | Epoch=133, loss=5.7401, this epoch 0.0397, total 4.4038
(T) | Epoch=134, loss=6.0010, this epoch 0.0410, total 4.4448
(T) | Epoch=135, loss=5.8345, this epoch 0.0393, total 4.4841
(T) | Epoch=136, loss=5.6735, this epoch 0.0291, total 4.5132
(T) | Epoch=137, loss=5.5890, this epoch 0.0397, total 4.5529
(T) | Epoch=138, loss=5.5860, this epoch 0.0300, total 4.5829
(T) | Epoch=139, loss=5.6898, this epoch 0.0383, total 4.6213
(T) | Epoch=140, loss=5.6042, this epoch 0.0382, total 4.6595
(T) | Epoch=141, loss=6.1395, this epoch 0.0385, total 4.6980
(T) | Epoch=142, loss=5.6056, this epoch 0.0404, total 4.7383
(T) | Epoch=143, loss=5.6725, this epoch 0.0328, total 4.7711
(T) | Epoch=144, loss=5.6118, this epoch 0.0343, total 4.8054
(T) | Epoch=145, loss=5.6213, this epoch 0.0300, total 4.8354
(T) | Epoch=146, loss=5.7486, this epoch 0.0331, total 4.8685
(T) | Epoch=147, loss=5.6208, this epoch 0.0279, total 4.8964
(T) | Epoch=148, loss=5.6559, this epoch 0.0347, total 4.9311
(T) | Epoch=149, loss=5.6870, this epoch 0.0308, total 4.9619
(T) | Epoch=150, loss=5.6693, this epoch 0.0354, total 4.9973
(T) | Epoch=151, loss=5.5899, this epoch 0.0348, total 5.0321
(T) | Epoch=152, loss=5.7057, this epoch 0.0346, total 5.0667
(T) | Epoch=153, loss=5.5816, this epoch 0.0348, total 5.1015
(T) | Epoch=154, loss=5.5361, this epoch 0.0306, total 5.1321
+++model saved ! 2015.pth
(T) | Epoch=155, loss=5.5700, this epoch 0.0299, total 5.1620
(T) | Epoch=156, loss=5.9975, this epoch 0.0284, total 5.1904
(T) | Epoch=157, loss=5.5145, this epoch 0.0296, total 5.2200
+++model saved ! 2015.pth
(T) | Epoch=158, loss=5.6933, this epoch 0.0234, total 5.2434
(T) | Epoch=159, loss=5.6869, this epoch 0.0339, total 5.2774
(T) | Epoch=160, loss=5.5327, this epoch 0.0273, total 5.3047
(T) | Epoch=161, loss=5.6936, this epoch 0.0267, total 5.3313
(T) | Epoch=162, loss=5.5443, this epoch 0.0291, total 5.3604
(T) | Epoch=163, loss=5.5563, this epoch 0.0235, total 5.3840
(T) | Epoch=164, loss=5.6716, this epoch 0.0325, total 5.4165
(T) | Epoch=165, loss=5.5633, this epoch 0.0312, total 5.4478
(T) | Epoch=166, loss=5.6526, this epoch 0.0289, total 5.4767
(T) | Epoch=167, loss=5.5458, this epoch 0.0218, total 5.4985
(T) | Epoch=168, loss=5.6109, this epoch 0.0300, total 5.5285
(T) | Epoch=169, loss=6.0887, this epoch 0.0334, total 5.5620
(T) | Epoch=170, loss=5.6300, this epoch 0.0305, total 5.5925
(T) | Epoch=171, loss=5.5377, this epoch 0.0342, total 5.6267
(T) | Epoch=172, loss=5.6407, this epoch 0.0304, total 5.6571
(T) | Epoch=173, loss=5.6512, this epoch 0.0306, total 5.6877
(T) | Epoch=174, loss=5.6599, this epoch 0.0296, total 5.7173
(T) | Epoch=175, loss=5.5950, this epoch 0.0346, total 5.7520
(T) | Epoch=176, loss=5.6222, this epoch 0.0271, total 5.7791
(T) | Epoch=177, loss=5.5519, this epoch 0.0300, total 5.8090
(T) | Epoch=178, loss=5.6009, this epoch 0.0301, total 5.8391
(T) | Epoch=179, loss=5.8427, this epoch 0.0299, total 5.8690
(T) | Epoch=180, loss=5.5160, this epoch 0.0299, total 5.8989
(T) | Epoch=181, loss=5.5264, this epoch 0.0213, total 5.9202
(T) | Epoch=182, loss=5.5991, this epoch 0.0276, total 5.9478
(T) | Epoch=183, loss=5.5318, this epoch 0.0279, total 5.9757
(T) | Epoch=184, loss=5.6236, this epoch 0.0366, total 6.0123
(T) | Epoch=185, loss=5.5202, this epoch 0.0317, total 6.0440
(T) | Epoch=186, loss=5.5019, this epoch 0.0302, total 6.0741
+++model saved ! 2015.pth
(T) | Epoch=187, loss=5.5239, this epoch 0.0445, total 6.1186
(T) | Epoch=188, loss=5.4774, this epoch 0.0353, total 6.1539
+++model saved ! 2015.pth
(T) | Epoch=189, loss=5.6433, this epoch 0.0314, total 6.1853
(T) | Epoch=190, loss=5.5375, this epoch 0.0353, total 6.2206
(T) | Epoch=191, loss=5.4963, this epoch 0.0378, total 6.2584
(T) | Epoch=192, loss=5.6377, this epoch 0.0286, total 6.2870
(T) | Epoch=193, loss=5.7552, this epoch 0.0408, total 6.3278
(T) | Epoch=194, loss=5.4859, this epoch 0.0346, total 6.3624
(T) | Epoch=195, loss=5.8543, this epoch 0.0382, total 6.4006
(T) | Epoch=196, loss=5.8948, this epoch 0.0365, total 6.4371
(T) | Epoch=197, loss=5.4898, this epoch 0.0321, total 6.4692
(T) | Epoch=198, loss=5.5865, this epoch 0.0373, total 6.5065
(T) | Epoch=199, loss=5.5759, this epoch 0.0364, total 6.5429
(T) | Epoch=200, loss=5.6207, this epoch 0.0367, total 6.5796
(T) | Epoch=201, loss=5.6858, this epoch 0.0416, total 6.6212
(T) | Epoch=202, loss=5.5701, this epoch 0.0380, total 6.6592
(T) | Epoch=203, loss=5.7228, this epoch 0.0369, total 6.6961
(T) | Epoch=204, loss=5.6139, this epoch 0.0309, total 6.7270
(T) | Epoch=205, loss=5.5377, this epoch 0.0367, total 6.7636
(T) | Epoch=206, loss=5.7066, this epoch 0.0317, total 6.7953
(T) | Epoch=207, loss=5.6557, this epoch 0.0334, total 6.8288
(T) | Epoch=208, loss=5.7168, this epoch 0.0378, total 6.8666
(T) | Epoch=209, loss=5.5084, this epoch 0.0402, total 6.9068
(T) | Epoch=210, loss=5.4506, this epoch 0.0395, total 6.9463
+++model saved ! 2015.pth
(T) | Epoch=211, loss=5.5602, this epoch 0.0403, total 6.9866
(T) | Epoch=212, loss=5.5185, this epoch 0.0381, total 7.0247
(T) | Epoch=213, loss=5.5186, this epoch 0.0292, total 7.0539
(T) | Epoch=214, loss=5.5161, this epoch 0.0365, total 7.0904
(T) | Epoch=215, loss=5.5392, this epoch 0.0319, total 7.1223
(T) | Epoch=216, loss=5.6644, this epoch 0.0320, total 7.1543
(T) | Epoch=217, loss=5.5933, this epoch 0.0388, total 7.1931
(T) | Epoch=218, loss=5.4884, this epoch 0.0313, total 7.2244
(T) | Epoch=219, loss=5.4725, this epoch 0.0393, total 7.2637
(T) | Epoch=220, loss=5.5151, this epoch 0.0327, total 7.2964
(T) | Epoch=221, loss=5.5940, this epoch 0.0434, total 7.3398
(T) | Epoch=222, loss=5.7623, this epoch 0.0308, total 7.3707
(T) | Epoch=223, loss=5.5258, this epoch 0.0355, total 7.4062
(T) | Epoch=224, loss=5.5260, this epoch 0.0327, total 7.4389
(T) | Epoch=225, loss=5.5467, this epoch 0.0314, total 7.4702
(T) | Epoch=226, loss=5.5767, this epoch 0.0301, total 7.5003
(T) | Epoch=227, loss=5.5687, this epoch 0.0362, total 7.5366
(T) | Epoch=228, loss=5.5500, this epoch 0.0346, total 7.5712
(T) | Epoch=229, loss=5.4830, this epoch 0.0307, total 7.6019
(T) | Epoch=230, loss=5.4849, this epoch 0.0303, total 7.6322
(T) | Epoch=231, loss=5.6051, this epoch 0.0389, total 7.6711
(T) | Epoch=232, loss=5.5024, this epoch 0.0366, total 7.7077
(T) | Epoch=233, loss=5.4967, this epoch 0.0308, total 7.7385
(T) | Epoch=234, loss=5.5081, this epoch 0.0309, total 7.7695
(T) | Epoch=235, loss=5.4733, this epoch 0.0436, total 7.8131
(T) | Epoch=236, loss=5.4702, this epoch 0.0316, total 7.8447
(T) | Epoch=237, loss=5.5384, this epoch 0.0259, total 7.8706
(T) | Epoch=238, loss=5.4866, this epoch 0.0312, total 7.9018
(T) | Epoch=239, loss=5.4858, this epoch 0.0375, total 7.9393
(T) | Epoch=240, loss=5.4585, this epoch 0.0310, total 7.9703
(T) | Epoch=241, loss=5.4662, this epoch 0.0375, total 8.0077
(T) | Epoch=242, loss=5.4382, this epoch 0.0375, total 8.0452
+++model saved ! 2015.pth
(T) | Epoch=243, loss=5.5176, this epoch 0.0325, total 8.0778
(T) | Epoch=244, loss=5.4797, this epoch 0.0295, total 8.1072
(T) | Epoch=245, loss=5.4400, this epoch 0.0319, total 8.1392
(T) | Epoch=246, loss=5.7701, this epoch 0.0292, total 8.1683
(T) | Epoch=247, loss=5.5373, this epoch 0.0294, total 8.1977
(T) | Epoch=248, loss=5.5813, this epoch 0.0351, total 8.2327
(T) | Epoch=249, loss=5.5397, this epoch 0.0314, total 8.2641
(T) | Epoch=250, loss=5.6056, this epoch 0.0302, total 8.2943
(T) | Epoch=251, loss=5.5702, this epoch 0.0267, total 8.3209
(T) | Epoch=252, loss=5.7220, this epoch 0.0234, total 8.3444
(T) | Epoch=253, loss=5.5406, this epoch 0.0355, total 8.3798
(T) | Epoch=254, loss=5.5306, this epoch 0.0250, total 8.4049
(T) | Epoch=255, loss=5.4585, this epoch 0.0333, total 8.4382
(T) | Epoch=256, loss=5.4566, this epoch 0.0215, total 8.4597
(T) | Epoch=257, loss=5.8813, this epoch 0.0298, total 8.4895
(T) | Epoch=258, loss=5.5537, this epoch 0.0377, total 8.5272
(T) | Epoch=259, loss=5.4958, this epoch 0.0296, total 8.5568
(T) | Epoch=260, loss=5.4418, this epoch 0.0398, total 8.5966
(T) | Epoch=261, loss=5.4479, this epoch 0.0233, total 8.6199
(T) | Epoch=262, loss=5.4576, this epoch 0.0353, total 8.6552
(T) | Epoch=263, loss=5.5335, this epoch 0.0231, total 8.6783
(T) | Epoch=264, loss=5.4604, this epoch 0.0376, total 8.7159
(T) | Epoch=265, loss=5.5186, this epoch 0.0312, total 8.7471
(T) | Epoch=266, loss=5.4479, this epoch 0.0290, total 8.7761
(T) | Epoch=267, loss=5.4676, this epoch 0.0387, total 8.8148
(T) | Epoch=268, loss=5.8167, this epoch 0.0313, total 8.8461
(T) | Epoch=269, loss=5.6987, this epoch 0.0366, total 8.8827
(T) | Epoch=270, loss=5.4679, this epoch 0.0303, total 8.9130
(T) | Epoch=271, loss=6.1941, this epoch 0.0224, total 8.9354
(T) | Epoch=272, loss=5.4472, this epoch 0.0349, total 8.9703
(T) | Epoch=273, loss=5.6609, this epoch 0.0314, total 9.0017
(T) | Epoch=274, loss=6.1314, this epoch 0.0327, total 9.0344
(T) | Epoch=275, loss=6.0612, this epoch 0.0370, total 9.0713
(T) | Epoch=276, loss=6.0916, this epoch 0.0331, total 9.1044
(T) | Epoch=277, loss=6.2599, this epoch 0.0316, total 9.1360
(T) | Epoch=278, loss=6.5025, this epoch 0.0299, total 9.1659
(T) | Epoch=279, loss=6.5513, this epoch 0.0280, total 9.1939
(T) | Epoch=280, loss=5.4721, this epoch 0.0329, total 9.2267
(T) | Epoch=281, loss=6.2545, this epoch 0.0334, total 9.2601
(T) | Epoch=282, loss=6.2623, this epoch 0.0232, total 9.2833
(T) | Epoch=283, loss=6.6019, this epoch 0.0314, total 9.3147
(T) | Epoch=284, loss=6.2044, this epoch 0.0232, total 9.3379
(T) | Epoch=285, loss=6.2058, this epoch 0.0273, total 9.3652
(T) | Epoch=286, loss=6.2458, this epoch 0.0217, total 9.3869
(T) | Epoch=287, loss=6.6400, this epoch 0.0308, total 9.4177
(T) | Epoch=288, loss=6.5835, this epoch 0.0328, total 9.4505
(T) | Epoch=289, loss=6.2874, this epoch 0.0278, total 9.4784
(T) | Epoch=290, loss=6.2720, this epoch 0.0361, total 9.5145
(T) | Epoch=291, loss=6.5008, this epoch 0.0334, total 9.5479
(T) | Epoch=292, loss=6.2123, this epoch 0.0295, total 9.5774
(T) | Epoch=293, loss=6.2381, this epoch 0.0272, total 9.6046
(T) | Epoch=294, loss=6.2386, this epoch 0.0337, total 9.6382
(T) | Epoch=295, loss=6.2049, this epoch 0.0233, total 9.6615
(T) | Epoch=296, loss=6.2041, this epoch 0.0275, total 9.6890
(T) | Epoch=297, loss=6.2023, this epoch 0.0204, total 9.7094
(T) | Epoch=298, loss=6.1916, this epoch 0.0212, total 9.7306
(T) | Epoch=299, loss=6.1928, this epoch 0.0313, total 9.7619
(T) | Epoch=300, loss=6.2349, this epoch 0.0306, total 9.7926
(T) | Epoch=301, loss=6.1909, this epoch 0.0325, total 9.8251
(T) | Epoch=302, loss=6.2020, this epoch 0.0254, total 9.8505
(T) | Epoch=303, loss=6.6337, this epoch 0.0292, total 9.8797
(T) | Epoch=304, loss=6.2454, this epoch 0.0214, total 9.9011
(T) | Epoch=305, loss=6.1594, this epoch 0.0267, total 9.9278
(T) | Epoch=306, loss=6.2184, this epoch 0.0283, total 9.9561
(T) | Epoch=307, loss=6.2316, this epoch 0.0299, total 9.9860
(T) | Epoch=308, loss=6.1389, this epoch 0.0262, total 10.0122
(T) | Epoch=309, loss=6.1474, this epoch 0.0293, total 10.0415
(T) | Epoch=310, loss=6.1200, this epoch 0.0336, total 10.0751
(T) | Epoch=311, loss=6.2501, this epoch 0.0303, total 10.1055
(T) | Epoch=312, loss=6.3235, this epoch 0.0218, total 10.1273
(T) | Epoch=313, loss=6.0877, this epoch 0.0351, total 10.1624
(T) | Epoch=314, loss=6.1825, this epoch 0.0234, total 10.1858
(T) | Epoch=315, loss=6.1606, this epoch 0.0281, total 10.2139
(T) | Epoch=316, loss=6.1346, this epoch 0.0270, total 10.2409
(T) | Epoch=317, loss=6.0499, this epoch 0.0352, total 10.2761
(T) | Epoch=318, loss=6.1340, this epoch 0.0304, total 10.3065
(T) | Epoch=319, loss=6.1094, this epoch 0.0345, total 10.3410
(T) | Epoch=320, loss=5.9998, this epoch 0.0370, total 10.3780
(T) | Epoch=321, loss=6.0074, this epoch 0.0287, total 10.4066
(T) | Epoch=322, loss=6.1328, this epoch 0.0295, total 10.4362
(T) | Epoch=323, loss=6.2367, this epoch 0.0209, total 10.4571
(T) | Epoch=324, loss=6.0144, this epoch 0.0266, total 10.4837
(T) | Epoch=325, loss=5.9449, this epoch 0.0285, total 10.5121
(T) | Epoch=326, loss=6.0434, this epoch 0.0314, total 10.5435
(T) | Epoch=327, loss=5.9020, this epoch 0.0288, total 10.5723
(T) | Epoch=328, loss=6.1169, this epoch 0.0285, total 10.6008
(T) | Epoch=329, loss=5.8791, this epoch 0.0376, total 10.6384
(T) | Epoch=330, loss=5.8667, this epoch 0.0340, total 10.6724
(T) | Epoch=331, loss=5.8375, this epoch 0.0231, total 10.6954
(T) | Epoch=332, loss=5.8661, this epoch 0.0266, total 10.7221
(T) | Epoch=333, loss=6.2030, this epoch 0.0319, total 10.7540
(T) | Epoch=334, loss=5.8829, this epoch 0.0276, total 10.7815
(T) | Epoch=335, loss=5.8186, this epoch 0.0235, total 10.8050
(T) | Epoch=336, loss=5.8234, this epoch 0.0215, total 10.8266
(T) | Epoch=337, loss=5.8930, this epoch 0.0213, total 10.8478
(T) | Epoch=338, loss=5.7735, this epoch 0.0217, total 10.8696
(T) | Epoch=339, loss=6.1419, this epoch 0.0349, total 10.9045
(T) | Epoch=340, loss=5.7431, this epoch 0.0302, total 10.9347
(T) | Epoch=341, loss=5.7633, this epoch 0.0274, total 10.9621
(T) | Epoch=342, loss=5.8036, this epoch 0.0225, total 10.9846
(T) | Epoch=343, loss=5.7080, this epoch 0.0317, total 11.0163
(T) | Epoch=344, loss=5.7409, this epoch 0.0295, total 11.0458
(T) | Epoch=345, loss=5.7964, this epoch 0.0350, total 11.0808
(T) | Epoch=346, loss=5.7684, this epoch 0.0297, total 11.1105
(T) | Epoch=347, loss=5.7332, this epoch 0.0266, total 11.1371
(T) | Epoch=348, loss=5.8051, this epoch 0.0343, total 11.1714
(T) | Epoch=349, loss=5.6643, this epoch 0.0279, total 11.1993
(T) | Epoch=350, loss=5.6727, this epoch 0.0362, total 11.2356
(T) | Epoch=351, loss=5.5700, this epoch 0.0339, total 11.2695
(T) | Epoch=352, loss=5.6498, this epoch 0.0280, total 11.2975
(T) | Epoch=353, loss=5.8596, this epoch 0.0298, total 11.3273
(T) | Epoch=354, loss=5.5419, this epoch 0.0353, total 11.3626
(T) | Epoch=355, loss=5.5567, this epoch 0.0329, total 11.3955
(T) | Epoch=356, loss=5.6211, this epoch 0.0279, total 11.4234
(T) | Epoch=357, loss=5.6299, this epoch 0.0305, total 11.4539
(T) | Epoch=358, loss=5.6012, this epoch 0.0401, total 11.4941
(T) | Epoch=359, loss=5.5336, this epoch 0.0376, total 11.5316
(T) | Epoch=360, loss=5.6693, this epoch 0.0229, total 11.5546
(T) | Epoch=361, loss=5.4747, this epoch 0.0260, total 11.5806
(T) | Epoch=362, loss=5.4624, this epoch 0.0299, total 11.6104
(T) | Epoch=363, loss=5.4755, this epoch 0.0217, total 11.6321
(T) | Epoch=364, loss=5.9041, this epoch 0.0303, total 11.6624
(T) | Epoch=365, loss=5.8374, this epoch 0.0328, total 11.6952
(T) | Epoch=366, loss=5.6171, this epoch 0.0315, total 11.7267
(T) | Epoch=367, loss=5.9630, this epoch 0.0336, total 11.7603
(T) | Epoch=368, loss=5.5305, this epoch 0.0377, total 11.7980
(T) | Epoch=369, loss=5.5830, this epoch 0.0298, total 11.8277
(T) | Epoch=370, loss=5.7993, this epoch 0.0218, total 11.8495
(T) | Epoch=371, loss=5.7036, this epoch 0.0260, total 11.8756
(T) | Epoch=372, loss=5.8526, this epoch 0.0298, total 11.9054
(T) | Epoch=373, loss=5.6596, this epoch 0.0211, total 11.9265
(T) | Epoch=374, loss=5.8250, this epoch 0.0281, total 11.9545
(T) | Epoch=375, loss=6.0241, this epoch 0.0365, total 11.9911
(T) | Epoch=376, loss=5.6424, this epoch 0.0273, total 12.0184
(T) | Epoch=377, loss=5.6452, this epoch 0.0230, total 12.0414
(T) | Epoch=378, loss=5.7309, this epoch 0.0286, total 12.0701
(T) | Epoch=379, loss=5.7704, this epoch 0.0305, total 12.1006
(T) | Epoch=380, loss=5.6148, this epoch 0.0281, total 12.1287
(T) | Epoch=381, loss=5.6558, this epoch 0.0314, total 12.1601
(T) | Epoch=382, loss=5.6497, this epoch 0.0267, total 12.1868
(T) | Epoch=383, loss=5.6110, this epoch 0.0279, total 12.2148
(T) | Epoch=384, loss=5.7810, this epoch 0.0223, total 12.2370
(T) | Epoch=385, loss=5.9786, this epoch 0.0217, total 12.2587
(T) | Epoch=386, loss=5.6451, this epoch 0.0293, total 12.2880
(T) | Epoch=387, loss=5.5955, this epoch 0.0236, total 12.3116
(T) | Epoch=388, loss=5.6465, this epoch 0.0257, total 12.3373
(T) | Epoch=389, loss=5.4910, this epoch 0.0284, total 12.3657
(T) | Epoch=390, loss=5.6189, this epoch 0.0261, total 12.3919
(T) | Epoch=391, loss=5.4685, this epoch 0.0234, total 12.4152
(T) | Epoch=392, loss=5.4958, this epoch 0.0340, total 12.4493
(T) | Epoch=393, loss=5.5641, this epoch 0.0294, total 12.4786
(T) | Epoch=394, loss=5.4502, this epoch 0.0351, total 12.5137
(T) | Epoch=395, loss=5.4532, this epoch 0.0290, total 12.5427
(T) | Epoch=396, loss=5.4759, this epoch 0.0217, total 12.5644
(T) | Epoch=397, loss=5.4575, this epoch 0.0213, total 12.5856
(T) | Epoch=398, loss=5.6009, this epoch 0.0267, total 12.6123
(T) | Epoch=399, loss=5.5847, this epoch 0.0341, total 12.6465
(T) | Epoch=400, loss=5.4685, this epoch 0.0310, total 12.6775
(T) | Epoch=401, loss=5.5455, this epoch 0.0239, total 12.7013
(T) | Epoch=402, loss=5.6247, this epoch 0.0223, total 12.7237
(T) | Epoch=403, loss=5.4525, this epoch 0.0228, total 12.7465
(T) | Epoch=404, loss=5.4362, this epoch 0.0209, total 12.7675
+++model saved ! 2015.pth
(T) | Epoch=405, loss=5.4400, this epoch 0.0391, total 12.8066
(T) | Epoch=406, loss=5.4798, this epoch 0.0314, total 12.8380
(T) | Epoch=407, loss=5.5529, this epoch 0.0290, total 12.8670
(T) | Epoch=408, loss=5.4490, this epoch 0.0279, total 12.8949
(T) | Epoch=409, loss=5.4630, this epoch 0.0295, total 12.9244
(T) | Epoch=410, loss=5.5495, this epoch 0.0316, total 12.9559
(T) | Epoch=411, loss=5.5500, this epoch 0.0286, total 12.9845
(T) | Epoch=412, loss=5.6063, this epoch 0.0293, total 13.0139
(T) | Epoch=413, loss=5.4396, this epoch 0.0328, total 13.0467
(T) | Epoch=414, loss=5.4364, this epoch 0.0360, total 13.0827
(T) | Epoch=415, loss=5.6077, this epoch 0.0284, total 13.1111
(T) | Epoch=416, loss=5.5016, this epoch 0.0308, total 13.1419
(T) | Epoch=417, loss=5.4417, this epoch 0.0304, total 13.1724
(T) | Epoch=418, loss=5.4503, this epoch 0.0300, total 13.2023
(T) | Epoch=419, loss=5.4448, this epoch 0.0306, total 13.2330
(T) | Epoch=420, loss=5.6126, this epoch 0.0336, total 13.2666
(T) | Epoch=421, loss=5.4671, this epoch 0.0290, total 13.2955
(T) | Epoch=422, loss=5.4307, this epoch 0.0288, total 13.3243
+++model saved ! 2015.pth
(T) | Epoch=423, loss=5.5285, this epoch 0.0375, total 13.3618
(T) | Epoch=424, loss=5.5160, this epoch 0.0348, total 13.3967
(T) | Epoch=425, loss=5.5378, this epoch 0.0283, total 13.4250
(T) | Epoch=426, loss=5.4382, this epoch 0.0278, total 13.4528
(T) | Epoch=427, loss=5.4949, this epoch 0.0340, total 13.4868
(T) | Epoch=428, loss=5.4308, this epoch 0.0303, total 13.5171
(T) | Epoch=429, loss=5.4630, this epoch 0.0381, total 13.5552
(T) | Epoch=430, loss=5.4905, this epoch 0.0303, total 13.5855
(T) | Epoch=431, loss=5.4332, this epoch 0.0353, total 13.6207
(T) | Epoch=432, loss=5.4322, this epoch 0.0298, total 13.6505
(T) | Epoch=433, loss=5.4577, this epoch 0.0333, total 13.6838
(T) | Epoch=434, loss=5.4955, this epoch 0.0355, total 13.7193
(T) | Epoch=435, loss=5.4330, this epoch 0.0353, total 13.7546
(T) | Epoch=436, loss=5.4945, this epoch 0.0301, total 13.7848
(T) | Epoch=437, loss=5.4604, this epoch 0.0382, total 13.8230
(T) | Epoch=438, loss=5.4203, this epoch 0.0303, total 13.8533
+++model saved ! 2015.pth
(T) | Epoch=439, loss=5.5291, this epoch 0.0292, total 13.8825
(T) | Epoch=440, loss=5.4317, this epoch 0.0266, total 13.9091
(T) | Epoch=441, loss=5.5912, this epoch 0.0222, total 13.9313
(T) | Epoch=442, loss=5.4522, this epoch 0.0286, total 13.9599
(T) | Epoch=443, loss=5.4724, this epoch 0.0346, total 13.9945
(T) | Epoch=444, loss=5.5039, this epoch 0.0292, total 14.0237
(T) | Epoch=445, loss=5.4231, this epoch 0.0221, total 14.0458
(T) | Epoch=446, loss=5.7531, this epoch 0.0224, total 14.0682
(T) | Epoch=447, loss=5.7189, this epoch 0.0214, total 14.0896
(T) | Epoch=448, loss=5.4252, this epoch 0.0215, total 14.1111
(T) | Epoch=449, loss=5.5057, this epoch 0.0276, total 14.1387
(T) | Epoch=450, loss=5.5007, this epoch 0.0306, total 14.1693
(T) | Epoch=451, loss=5.4943, this epoch 0.0275, total 14.1967
(T) | Epoch=452, loss=5.4707, this epoch 0.0287, total 14.2255
(T) | Epoch=453, loss=5.4843, this epoch 0.0298, total 14.2553
(T) | Epoch=454, loss=5.4385, this epoch 0.0301, total 14.2854
(T) | Epoch=455, loss=5.4589, this epoch 0.0278, total 14.3132
(T) | Epoch=456, loss=5.5895, this epoch 0.0210, total 14.3342
(T) | Epoch=457, loss=5.4486, this epoch 0.0282, total 14.3624
(T) | Epoch=458, loss=5.5444, this epoch 0.0229, total 14.3853
(T) | Epoch=459, loss=5.4526, this epoch 0.0289, total 14.4142
(T) | Epoch=460, loss=5.4215, this epoch 0.0362, total 14.4504
(T) | Epoch=461, loss=5.4392, this epoch 0.0291, total 14.4795
(T) | Epoch=462, loss=5.5083, this epoch 0.0246, total 14.5041
(T) | Epoch=463, loss=5.4156, this epoch 0.0229, total 14.5270
+++model saved ! 2015.pth
(T) | Epoch=464, loss=5.4269, this epoch 0.0419, total 14.5689
(T) | Epoch=465, loss=5.4246, this epoch 0.0324, total 14.6013
(T) | Epoch=466, loss=5.4477, this epoch 0.0350, total 14.6363
(T) | Epoch=467, loss=5.4322, this epoch 0.0286, total 14.6649
(T) | Epoch=468, loss=5.6179, this epoch 0.0290, total 14.6939
(T) | Epoch=469, loss=5.4339, this epoch 0.0215, total 14.7154
(T) | Epoch=470, loss=5.5125, this epoch 0.0354, total 14.7509
(T) | Epoch=471, loss=5.4552, this epoch 0.0295, total 14.7804
(T) | Epoch=472, loss=5.4483, this epoch 0.0279, total 14.8083
(T) | Epoch=473, loss=5.4204, this epoch 0.0272, total 14.8355
(T) | Epoch=474, loss=5.4519, this epoch 0.0324, total 14.8679
(T) | Epoch=475, loss=5.4931, this epoch 0.0289, total 14.8968
(T) | Epoch=476, loss=5.5540, this epoch 0.0268, total 14.9236
(T) | Epoch=477, loss=5.4611, this epoch 0.0247, total 14.9483
(T) | Epoch=478, loss=5.4274, this epoch 0.0294, total 14.9778
(T) | Epoch=479, loss=5.4271, this epoch 0.0273, total 15.0051
(T) | Epoch=480, loss=5.4793, this epoch 0.0269, total 15.0320
(T) | Epoch=481, loss=5.5025, this epoch 0.0277, total 15.0597
(T) | Epoch=482, loss=5.4387, this epoch 0.0226, total 15.0824
(T) | Epoch=483, loss=5.5007, this epoch 0.0218, total 15.1042
(T) | Epoch=484, loss=5.4266, this epoch 0.0215, total 15.1257
(T) | Epoch=485, loss=5.4199, this epoch 0.0312, total 15.1569
(T) | Epoch=486, loss=5.5012, this epoch 0.0233, total 15.1802
(T) | Epoch=487, loss=5.4642, this epoch 0.0338, total 15.2140
(T) | Epoch=488, loss=5.4254, this epoch 0.0316, total 15.2456
(T) | Epoch=489, loss=5.4601, this epoch 0.0375, total 15.2831
(T) | Epoch=490, loss=5.4232, this epoch 0.0303, total 15.3134
(T) | Epoch=491, loss=5.4730, this epoch 0.0339, total 15.3473
(T) | Epoch=492, loss=5.5828, this epoch 0.0383, total 15.3856
(T) | Epoch=493, loss=5.6463, this epoch 0.0315, total 15.4171
(T) | Epoch=494, loss=5.4714, this epoch 0.0294, total 15.4465
(T) | Epoch=495, loss=5.4351, this epoch 0.0324, total 15.4789
(T) | Epoch=496, loss=5.5540, this epoch 0.0356, total 15.5144
(T) | Epoch=497, loss=5.4227, this epoch 0.0374, total 15.5518
(T) | Epoch=498, loss=5.4147, this epoch 0.0378, total 15.5897
+++model saved ! 2015.pth
(T) | Epoch=499, loss=5.4123, this epoch 0.0401, total 15.6297
+++model saved ! 2015.pth
(T) | Epoch=500, loss=5.4194, this epoch 0.0400, total 15.6697
(T) | Epoch=501, loss=5.5119, this epoch 0.0304, total 15.7001
(T) | Epoch=502, loss=5.5781, this epoch 0.0248, total 15.7249
(T) | Epoch=503, loss=5.4564, this epoch 0.0320, total 15.7569
(T) | Epoch=504, loss=5.4778, this epoch 0.0234, total 15.7803
(T) | Epoch=505, loss=5.5049, this epoch 0.0276, total 15.8080
(T) | Epoch=506, loss=5.4150, this epoch 0.0211, total 15.8291
(T) | Epoch=507, loss=5.4572, this epoch 0.0273, total 15.8565
(T) | Epoch=508, loss=5.4238, this epoch 0.0260, total 15.8824
(T) | Epoch=509, loss=5.4941, this epoch 0.0267, total 15.9091
(T) | Epoch=510, loss=5.5819, this epoch 0.0222, total 15.9313
(T) | Epoch=511, loss=5.4522, this epoch 0.0273, total 15.9586
(T) | Epoch=512, loss=5.4787, this epoch 0.0292, total 15.9878
(T) | Epoch=513, loss=5.4105, this epoch 0.0271, total 16.0149
+++model saved ! 2015.pth
(T) | Epoch=514, loss=5.4909, this epoch 0.0230, total 16.0378
(T) | Epoch=515, loss=5.4507, this epoch 0.0209, total 16.0587
(T) | Epoch=516, loss=5.4115, this epoch 0.0215, total 16.0803
(T) | Epoch=517, loss=5.4083, this epoch 0.0280, total 16.1083
+++model saved ! 2015.pth
(T) | Epoch=518, loss=5.4304, this epoch 0.0353, total 16.1436
(T) | Epoch=519, loss=5.4396, this epoch 0.0220, total 16.1657
(T) | Epoch=520, loss=5.4213, this epoch 0.0209, total 16.1866
(T) | Epoch=521, loss=5.6394, this epoch 0.0310, total 16.2176
(T) | Epoch=522, loss=5.4966, this epoch 0.0266, total 16.2442
(T) | Epoch=523, loss=5.4166, this epoch 0.0335, total 16.2776
(T) | Epoch=524, loss=5.5204, this epoch 0.0318, total 16.3094
(T) | Epoch=525, loss=5.4660, this epoch 0.0283, total 16.3378
(T) | Epoch=526, loss=5.4446, this epoch 0.0286, total 16.3664
(T) | Epoch=527, loss=5.5216, this epoch 0.0274, total 16.3937
(T) | Epoch=528, loss=5.4499, this epoch 0.0362, total 16.4299
(T) | Epoch=529, loss=5.4461, this epoch 0.0293, total 16.4593
(T) | Epoch=530, loss=5.7697, this epoch 0.0273, total 16.4866
(T) | Epoch=531, loss=5.4530, this epoch 0.0265, total 16.5131
(T) | Epoch=532, loss=5.4320, this epoch 0.0295, total 16.5426
(T) | Epoch=533, loss=5.4671, this epoch 0.0323, total 16.5750
(T) | Epoch=534, loss=5.4261, this epoch 0.0296, total 16.6046
(T) | Epoch=535, loss=5.4369, this epoch 0.0277, total 16.6322
(T) | Epoch=536, loss=5.4156, this epoch 0.0296, total 16.6619
(T) | Epoch=537, loss=5.4810, this epoch 0.0327, total 16.6946
(T) | Epoch=538, loss=5.4287, this epoch 0.0295, total 16.7241
(T) | Epoch=539, loss=5.4810, this epoch 0.0278, total 16.7519
(T) | Epoch=540, loss=5.4084, this epoch 0.0328, total 16.7846
(T) | Epoch=541, loss=5.4099, this epoch 0.0305, total 16.8151
(T) | Epoch=542, loss=5.4558, this epoch 0.0339, total 16.8490
(T) | Epoch=543, loss=5.4880, this epoch 0.0263, total 16.8753
(T) | Epoch=544, loss=5.3996, this epoch 0.0349, total 16.9102
+++model saved ! 2015.pth
(T) | Epoch=545, loss=5.4638, this epoch 0.0313, total 16.9416
(T) | Epoch=546, loss=5.4003, this epoch 0.0286, total 16.9702
(T) | Epoch=547, loss=5.4178, this epoch 0.0284, total 16.9985
(T) | Epoch=548, loss=5.4627, this epoch 0.0346, total 17.0331
(T) | Epoch=549, loss=5.4088, this epoch 0.0275, total 17.0606
(T) | Epoch=550, loss=5.4664, this epoch 0.0301, total 17.0907
(T) | Epoch=551, loss=5.4519, this epoch 0.0352, total 17.1258
(T) | Epoch=552, loss=5.4807, this epoch 0.0354, total 17.1613
(T) | Epoch=553, loss=5.4028, this epoch 0.0385, total 17.1997
(T) | Epoch=554, loss=5.4194, this epoch 0.0296, total 17.2293
(T) | Epoch=555, loss=5.4337, this epoch 0.0332, total 17.2625
(T) | Epoch=556, loss=5.4022, this epoch 0.0298, total 17.2923
(T) | Epoch=557, loss=5.4235, this epoch 0.0277, total 17.3200
(T) | Epoch=558, loss=5.3940, this epoch 0.0347, total 17.3547
+++model saved ! 2015.pth
(T) | Epoch=559, loss=5.4017, this epoch 0.0245, total 17.3792
(T) | Epoch=560, loss=5.4002, this epoch 0.0332, total 17.4124
(T) | Epoch=561, loss=5.4339, this epoch 0.0347, total 17.4471
(T) | Epoch=562, loss=5.4761, this epoch 0.0352, total 17.4823
(T) | Epoch=563, loss=5.4846, this epoch 0.0234, total 17.5057
(T) | Epoch=564, loss=5.4196, this epoch 0.0215, total 17.5272
(T) | Epoch=565, loss=5.3929, this epoch 0.0357, total 17.5629
+++model saved ! 2015.pth
(T) | Epoch=566, loss=5.4045, this epoch 0.0361, total 17.5991
(T) | Epoch=567, loss=5.3953, this epoch 0.0241, total 17.6231
(T) | Epoch=568, loss=5.4318, this epoch 0.0308, total 17.6539
(T) | Epoch=569, loss=5.4803, this epoch 0.0347, total 17.6886
(T) | Epoch=570, loss=5.4449, this epoch 0.0304, total 17.7190
(T) | Epoch=571, loss=5.3929, this epoch 0.0340, total 17.7530
(T) | Epoch=572, loss=5.4055, this epoch 0.0297, total 17.7827
(T) | Epoch=573, loss=5.5337, this epoch 0.0280, total 17.8106
(T) | Epoch=574, loss=5.4835, this epoch 0.0267, total 17.8373
(T) | Epoch=575, loss=5.3933, this epoch 0.0302, total 17.8675
(T) | Epoch=576, loss=5.4332, this epoch 0.0358, total 17.9032
(T) | Epoch=577, loss=5.4184, this epoch 0.0297, total 17.9329
(T) | Epoch=578, loss=5.4450, this epoch 0.0271, total 17.9600
(T) | Epoch=579, loss=5.4045, this epoch 0.0301, total 17.9901
(T) | Epoch=580, loss=5.4775, this epoch 0.0332, total 18.0232
(T) | Epoch=581, loss=5.5230, this epoch 0.0349, total 18.0582
(T) | Epoch=582, loss=5.3867, this epoch 0.0287, total 18.0869
+++model saved ! 2015.pth
(T) | Epoch=583, loss=5.4258, this epoch 0.0322, total 18.1191
(T) | Epoch=584, loss=5.4008, this epoch 0.0239, total 18.1430
(T) | Epoch=585, loss=5.4499, this epoch 0.0219, total 18.1649
(T) | Epoch=586, loss=5.3967, this epoch 0.0222, total 18.1871
(T) | Epoch=587, loss=5.3927, this epoch 0.0221, total 18.2093
(T) | Epoch=588, loss=5.3894, this epoch 0.0220, total 18.2313
(T) | Epoch=589, loss=5.3882, this epoch 0.0299, total 18.2612
(T) | Epoch=590, loss=5.4413, this epoch 0.0424, total 18.3036
(T) | Epoch=591, loss=5.4813, this epoch 0.0402, total 18.3438
(T) | Epoch=592, loss=5.4879, this epoch 0.0323, total 18.3761
(T) | Epoch=593, loss=5.4199, this epoch 0.0304, total 18.4065
(T) | Epoch=594, loss=5.4542, this epoch 0.0358, total 18.4422
(T) | Epoch=595, loss=5.4107, this epoch 0.0324, total 18.4746
(T) | Epoch=596, loss=5.4156, this epoch 0.0302, total 18.5048
(T) | Epoch=597, loss=5.4049, this epoch 0.0359, total 18.5408
(T) | Epoch=598, loss=5.4162, this epoch 0.0315, total 18.5722
(T) | Epoch=599, loss=5.4080, this epoch 0.0286, total 18.6008
(T) | Epoch=600, loss=5.3987, this epoch 0.0352, total 18.6360
(T) | Epoch=601, loss=5.4383, this epoch 0.0299, total 18.6659
(T) | Epoch=602, loss=5.4394, this epoch 0.0367, total 18.7025
(T) | Epoch=603, loss=5.4434, this epoch 0.0333, total 18.7359
(T) | Epoch=604, loss=5.3968, this epoch 0.0367, total 18.7726
(T) | Epoch=605, loss=5.4324, this epoch 0.0391, total 18.8117
(T) | Epoch=606, loss=5.3914, this epoch 0.0416, total 18.8533
(T) | Epoch=607, loss=5.4746, this epoch 0.0389, total 18.8922
(T) | Epoch=608, loss=5.4322, this epoch 0.0392, total 18.9313
(T) | Epoch=609, loss=5.4214, this epoch 0.0337, total 18.9650
(T) | Epoch=610, loss=5.3942, this epoch 0.0292, total 18.9942
(T) | Epoch=611, loss=5.3984, this epoch 0.0321, total 19.0263
(T) | Epoch=612, loss=5.4614, this epoch 0.0298, total 19.0561
(T) | Epoch=613, loss=5.3986, this epoch 0.0288, total 19.0849
(T) | Epoch=614, loss=5.3817, this epoch 0.0319, total 19.1168
+++model saved ! 2015.pth
(T) | Epoch=615, loss=5.4214, this epoch 0.0361, total 19.1529
(T) | Epoch=616, loss=5.3858, this epoch 0.0301, total 19.1830
(T) | Epoch=617, loss=5.4838, this epoch 0.0410, total 19.2240
(T) | Epoch=618, loss=5.4272, this epoch 0.0379, total 19.2619
(T) | Epoch=619, loss=5.4520, this epoch 0.0371, total 19.2990
(T) | Epoch=620, loss=5.3841, this epoch 0.0303, total 19.3293
(T) | Epoch=621, loss=5.4481, this epoch 0.0400, total 19.3694
(T) | Epoch=622, loss=5.3893, this epoch 0.0381, total 19.4074
(T) | Epoch=623, loss=5.4128, this epoch 0.0330, total 19.4405
(T) | Epoch=624, loss=5.4245, this epoch 0.0309, total 19.4714
(T) | Epoch=625, loss=5.4436, this epoch 0.0302, total 19.5016
(T) | Epoch=626, loss=5.4665, this epoch 0.0315, total 19.5331
(T) | Epoch=627, loss=5.4290, this epoch 0.0391, total 19.5722
(T) | Epoch=628, loss=5.4125, this epoch 0.0367, total 19.6089
(T) | Epoch=629, loss=5.5287, this epoch 0.0396, total 19.6485
(T) | Epoch=630, loss=5.4578, this epoch 0.0378, total 19.6863
(T) | Epoch=631, loss=5.4757, this epoch 0.0302, total 19.7165
(T) | Epoch=632, loss=5.4147, this epoch 0.0370, total 19.7535
(T) | Epoch=633, loss=5.4140, this epoch 0.0304, total 19.7839
(T) | Epoch=634, loss=5.3873, this epoch 0.0320, total 19.8159
(T) | Epoch=635, loss=5.3738, this epoch 0.0394, total 19.8553
+++model saved ! 2015.pth
(T) | Epoch=636, loss=5.4164, this epoch 0.0387, total 19.8941
(T) | Epoch=637, loss=5.4186, this epoch 0.0245, total 19.9186
(T) | Epoch=638, loss=5.3951, this epoch 0.0350, total 19.9536
(T) | Epoch=639, loss=5.3783, this epoch 0.0362, total 19.9897
(T) | Epoch=640, loss=5.4784, this epoch 0.0310, total 20.0207
(T) | Epoch=641, loss=5.3808, this epoch 0.0306, total 20.0514
(T) | Epoch=642, loss=5.4367, this epoch 0.0394, total 20.0908
(T) | Epoch=643, loss=5.3911, this epoch 0.0313, total 20.1220
(T) | Epoch=644, loss=5.4279, this epoch 0.0291, total 20.1511
(T) | Epoch=645, loss=5.4048, this epoch 0.0301, total 20.1812
(T) | Epoch=646, loss=5.3812, this epoch 0.0309, total 20.2121
(T) | Epoch=647, loss=5.3814, this epoch 0.0292, total 20.2414
(T) | Epoch=648, loss=5.3960, this epoch 0.0378, total 20.2792
(T) | Epoch=649, loss=5.3807, this epoch 0.0302, total 20.3094
(T) | Epoch=650, loss=5.3812, this epoch 0.0304, total 20.3398
(T) | Epoch=651, loss=5.3959, this epoch 0.0365, total 20.3762
(T) | Epoch=652, loss=5.3792, this epoch 0.0328, total 20.4091
(T) | Epoch=653, loss=5.4079, this epoch 0.0348, total 20.4439
(T) | Epoch=654, loss=5.4052, this epoch 0.0337, total 20.4776
(T) | Epoch=655, loss=5.4203, this epoch 0.0350, total 20.5126
(T) | Epoch=656, loss=5.4435, this epoch 0.0242, total 20.5367
(T) | Epoch=657, loss=5.4626, this epoch 0.0410, total 20.5778
(T) | Epoch=658, loss=5.3795, this epoch 0.0339, total 20.6117
(T) | Epoch=659, loss=5.4804, this epoch 0.0296, total 20.6413
(T) | Epoch=660, loss=5.4538, this epoch 0.0296, total 20.6709
(T) | Epoch=661, loss=5.4180, this epoch 0.0381, total 20.7090
(T) | Epoch=662, loss=5.4859, this epoch 0.0399, total 20.7489
(T) | Epoch=663, loss=5.3740, this epoch 0.0240, total 20.7729
(T) | Epoch=664, loss=5.4250, this epoch 0.0374, total 20.8104
(T) | Epoch=665, loss=5.4059, this epoch 0.0340, total 20.8444
(T) | Epoch=666, loss=5.3698, this epoch 0.0291, total 20.8734
+++model saved ! 2015.pth
(T) | Epoch=667, loss=5.4287, this epoch 0.0320, total 20.9054
(T) | Epoch=668, loss=5.4953, this epoch 0.0304, total 20.9358
(T) | Epoch=669, loss=5.3876, this epoch 0.0300, total 20.9658
(T) | Epoch=670, loss=5.3914, this epoch 0.0302, total 20.9961
(T) | Epoch=671, loss=5.3856, this epoch 0.0394, total 21.0355
(T) | Epoch=672, loss=5.3890, this epoch 0.0343, total 21.0698
(T) | Epoch=673, loss=5.3692, this epoch 0.0400, total 21.1098
+++model saved ! 2015.pth
(T) | Epoch=674, loss=5.4544, this epoch 0.0414, total 21.1512
(T) | Epoch=675, loss=5.3751, this epoch 0.0323, total 21.1834
(T) | Epoch=676, loss=5.4191, this epoch 0.0369, total 21.2203
(T) | Epoch=677, loss=5.3874, this epoch 0.0302, total 21.2505
(T) | Epoch=678, loss=5.3973, this epoch 0.0326, total 21.2832
(T) | Epoch=679, loss=5.3596, this epoch 0.0223, total 21.3054
+++model saved ! 2015.pth
(T) | Epoch=680, loss=5.4302, this epoch 0.0231, total 21.3285
(T) | Epoch=681, loss=5.4128, this epoch 0.0365, total 21.3650
(T) | Epoch=682, loss=5.3741, this epoch 0.0303, total 21.3953
(T) | Epoch=683, loss=5.3642, this epoch 0.0374, total 21.4327
(T) | Epoch=684, loss=5.3765, this epoch 0.0306, total 21.4633
(T) | Epoch=685, loss=5.4445, this epoch 0.0292, total 21.4925
(T) | Epoch=686, loss=5.3769, this epoch 0.0294, total 21.5219
(T) | Epoch=687, loss=5.3681, this epoch 0.0240, total 21.5459
(T) | Epoch=688, loss=5.3696, this epoch 0.0361, total 21.5820
(T) | Epoch=689, loss=5.4498, this epoch 0.0220, total 21.6040
(T) | Epoch=690, loss=5.4072, this epoch 0.0380, total 21.6420
(T) | Epoch=691, loss=5.4061, this epoch 0.0375, total 21.6795
(T) | Epoch=692, loss=5.3829, this epoch 0.0338, total 21.7133
(T) | Epoch=693, loss=5.3655, this epoch 0.0290, total 21.7424
(T) | Epoch=694, loss=5.3944, this epoch 0.0222, total 21.7646
(T) | Epoch=695, loss=5.5199, this epoch 0.0219, total 21.7865
(T) | Epoch=696, loss=5.4869, this epoch 0.0381, total 21.8246
(T) | Epoch=697, loss=5.3692, this epoch 0.0294, total 21.8540
(T) | Epoch=698, loss=5.3732, this epoch 0.0304, total 21.8844
(T) | Epoch=699, loss=5.4472, this epoch 0.0295, total 21.9139
(T) | Epoch=700, loss=5.4123, this epoch 0.0308, total 21.9447
(T) | Epoch=701, loss=5.4575, this epoch 0.0374, total 21.9821
(T) | Epoch=702, loss=5.5159, this epoch 0.0297, total 22.0118
(T) | Epoch=703, loss=5.4134, this epoch 0.0401, total 22.0519
(T) | Epoch=704, loss=5.4620, this epoch 0.0404, total 22.0923
(T) | Epoch=705, loss=5.4137, this epoch 0.0294, total 22.1217
(T) | Epoch=706, loss=5.4683, this epoch 0.0294, total 22.1511
(T) | Epoch=707, loss=5.5749, this epoch 0.0402, total 22.1912
(T) | Epoch=708, loss=5.5047, this epoch 0.0306, total 22.2219
(T) | Epoch=709, loss=5.3673, this epoch 0.0294, total 22.2513
(T) | Epoch=710, loss=5.3699, this epoch 0.0284, total 22.2797
(T) | Epoch=711, loss=5.4399, this epoch 0.0306, total 22.3103
(T) | Epoch=712, loss=5.3803, this epoch 0.0222, total 22.3325
(T) | Epoch=713, loss=5.3843, this epoch 0.0339, total 22.3663
(T) | Epoch=714, loss=5.3793, this epoch 0.0236, total 22.3899
(T) | Epoch=715, loss=5.3783, this epoch 0.0253, total 22.4152
(T) | Epoch=716, loss=5.4011, this epoch 0.0272, total 22.4424
(T) | Epoch=717, loss=5.3734, this epoch 0.0282, total 22.4706
(T) | Epoch=718, loss=5.4123, this epoch 0.0292, total 22.4997
(T) | Epoch=719, loss=5.4271, this epoch 0.0375, total 22.5373
(T) | Epoch=720, loss=5.4221, this epoch 0.0387, total 22.5760
(T) | Epoch=721, loss=5.3811, this epoch 0.0232, total 22.5992
(T) | Epoch=722, loss=5.4150, this epoch 0.0279, total 22.6271
(T) | Epoch=723, loss=5.4235, this epoch 0.0275, total 22.6546
(T) | Epoch=724, loss=5.4279, this epoch 0.0355, total 22.6901
(T) | Epoch=725, loss=5.3678, this epoch 0.0292, total 22.7193
(T) | Epoch=726, loss=5.4044, this epoch 0.0328, total 22.7521
(T) | Epoch=727, loss=5.3635, this epoch 0.0358, total 22.7879
(T) | Epoch=728, loss=5.5054, this epoch 0.0295, total 22.8174
(T) | Epoch=729, loss=5.3890, this epoch 0.0222, total 22.8396
(T) | Epoch=730, loss=5.4012, this epoch 0.0361, total 22.8757
(T) | Epoch=731, loss=5.4674, this epoch 0.0287, total 22.9044
(T) | Epoch=732, loss=5.3972, this epoch 0.0296, total 22.9340
(T) | Epoch=733, loss=5.4267, this epoch 0.0284, total 22.9625
(T) | Epoch=734, loss=5.4719, this epoch 0.0351, total 22.9976
(T) | Epoch=735, loss=5.4134, this epoch 0.0359, total 23.0335
(T) | Epoch=736, loss=5.3880, this epoch 0.0231, total 23.0565
(T) | Epoch=737, loss=5.3600, this epoch 0.0287, total 23.0852
(T) | Epoch=738, loss=5.3930, this epoch 0.0266, total 23.1118
(T) | Epoch=739, loss=5.3802, this epoch 0.0382, total 23.1500
(T) | Epoch=740, loss=5.3979, this epoch 0.0290, total 23.1790
(T) | Epoch=741, loss=5.4039, this epoch 0.0290, total 23.2080
(T) | Epoch=742, loss=5.3895, this epoch 0.0360, total 23.2440
(T) | Epoch=743, loss=5.4181, this epoch 0.0294, total 23.2735
(T) | Epoch=744, loss=5.3665, this epoch 0.0295, total 23.3030
(T) | Epoch=745, loss=5.4168, this epoch 0.0282, total 23.3311
(T) | Epoch=746, loss=5.3958, this epoch 0.0385, total 23.3696
(T) | Epoch=747, loss=5.4048, this epoch 0.0231, total 23.3927
(T) | Epoch=748, loss=5.4325, this epoch 0.0377, total 23.4304
(T) | Epoch=749, loss=5.3737, this epoch 0.0294, total 23.4598
(T) | Epoch=750, loss=5.4135, this epoch 0.0288, total 23.4886
(T) | Epoch=751, loss=5.3698, this epoch 0.0370, total 23.5256
(T) | Epoch=752, loss=5.4782, this epoch 0.0392, total 23.5648
(T) | Epoch=753, loss=5.4658, this epoch 0.0309, total 23.5957
(T) | Epoch=754, loss=5.3614, this epoch 0.0216, total 23.6174
(T) | Epoch=755, loss=5.4456, this epoch 0.0271, total 23.6445
(T) | Epoch=756, loss=5.4411, this epoch 0.0374, total 23.6819
(T) | Epoch=757, loss=5.3949, this epoch 0.0303, total 23.7122
(T) | Epoch=758, loss=5.4387, this epoch 0.0302, total 23.7424
(T) | Epoch=759, loss=5.4691, this epoch 0.0302, total 23.7727
(T) | Epoch=760, loss=5.4357, this epoch 0.0356, total 23.8083
(T) | Epoch=761, loss=5.3539, this epoch 0.0293, total 23.8375
+++model saved ! 2015.pth
(T) | Epoch=762, loss=5.4288, this epoch 0.0392, total 23.8768
(T) | Epoch=763, loss=5.3986, this epoch 0.0315, total 23.9083
(T) | Epoch=764, loss=5.3650, this epoch 0.0355, total 23.9438
(T) | Epoch=765, loss=5.3671, this epoch 0.0379, total 23.9817
(T) | Epoch=766, loss=5.4010, this epoch 0.0285, total 24.0101
(T) | Epoch=767, loss=5.3996, this epoch 0.0380, total 24.0481
(T) | Epoch=768, loss=5.4140, this epoch 0.0303, total 24.0784
(T) | Epoch=769, loss=5.3702, this epoch 0.0365, total 24.1149
(T) | Epoch=770, loss=5.4556, this epoch 0.0230, total 24.1379
(T) | Epoch=771, loss=5.3688, this epoch 0.0296, total 24.1675
(T) | Epoch=772, loss=5.3623, this epoch 0.0271, total 24.1947
(T) | Epoch=773, loss=5.4249, this epoch 0.0310, total 24.2257
(T) | Epoch=774, loss=5.3988, this epoch 0.0274, total 24.2531
(T) | Epoch=775, loss=5.3867, this epoch 0.0225, total 24.2756
(T) | Epoch=776, loss=5.3730, this epoch 0.0391, total 24.3148
(T) | Epoch=777, loss=5.4105, this epoch 0.0310, total 24.3458
(T) | Epoch=778, loss=5.3803, this epoch 0.0293, total 24.3751
(T) | Epoch=779, loss=5.3936, this epoch 0.0369, total 24.4120
(T) | Epoch=780, loss=5.3704, this epoch 0.0302, total 24.4422
(T) | Epoch=781, loss=5.4121, this epoch 0.0313, total 24.4735
(T) | Epoch=782, loss=5.3946, this epoch 0.0355, total 24.5090
(T) | Epoch=783, loss=5.3889, this epoch 0.0353, total 24.5443
(T) | Epoch=784, loss=5.3704, this epoch 0.0310, total 24.5754
(T) | Epoch=785, loss=5.4164, this epoch 0.0226, total 24.5980
(T) | Epoch=786, loss=5.4374, this epoch 0.0286, total 24.6266
(T) | Epoch=787, loss=5.3925, this epoch 0.0293, total 24.6559
(T) | Epoch=788, loss=5.3876, this epoch 0.0379, total 24.6938
(T) | Epoch=789, loss=5.3820, this epoch 0.0302, total 24.7239
(T) | Epoch=790, loss=5.3742, this epoch 0.0327, total 24.7566
(T) | Epoch=791, loss=5.3743, this epoch 0.0292, total 24.7859
(T) | Epoch=792, loss=5.4089, this epoch 0.0382, total 24.8240
(T) | Epoch=793, loss=5.3674, this epoch 0.0319, total 24.8559
(T) | Epoch=794, loss=5.3959, this epoch 0.0282, total 24.8842
(T) | Epoch=795, loss=5.4421, this epoch 0.0366, total 24.9208
(T) | Epoch=796, loss=5.4073, this epoch 0.0303, total 24.9511
(T) | Epoch=797, loss=5.3637, this epoch 0.0379, total 24.9890
(T) | Epoch=798, loss=5.3967, this epoch 0.0422, total 25.0311
(T) | Epoch=799, loss=5.4872, this epoch 0.0312, total 25.0623
(T) | Epoch=800, loss=5.3723, this epoch 0.0366, total 25.0989
(T) | Epoch=801, loss=5.3682, this epoch 0.0324, total 25.1313
(T) | Epoch=802, loss=5.4193, this epoch 0.0278, total 25.1591
(T) | Epoch=803, loss=5.4744, this epoch 0.0328, total 25.1919
(T) | Epoch=804, loss=5.3613, this epoch 0.0362, total 25.2281
(T) | Epoch=805, loss=5.4679, this epoch 0.0367, total 25.2648
(T) | Epoch=806, loss=5.3684, this epoch 0.0364, total 25.3012
(T) | Epoch=807, loss=5.3621, this epoch 0.0301, total 25.3312
(T) | Epoch=808, loss=5.3791, this epoch 0.0353, total 25.3665
(T) | Epoch=809, loss=5.4098, this epoch 0.0358, total 25.4023
(T) | Epoch=810, loss=5.4331, this epoch 0.0316, total 25.4339
(T) | Epoch=811, loss=5.4196, this epoch 0.0262, total 25.4601
(T) | Epoch=812, loss=5.3887, this epoch 0.0316, total 25.4917
(T) | Epoch=813, loss=5.4669, this epoch 0.0335, total 25.5252
(T) | Epoch=814, loss=5.3837, this epoch 0.0291, total 25.5542
(T) | Epoch=815, loss=5.4138, this epoch 0.0277, total 25.5820
(T) | Epoch=816, loss=5.3783, this epoch 0.0347, total 25.6167
(T) | Epoch=817, loss=5.4120, this epoch 0.0324, total 25.6491
(T) | Epoch=818, loss=5.3779, this epoch 0.0326, total 25.6817
(T) | Epoch=819, loss=5.4079, this epoch 0.0262, total 25.7079
(T) | Epoch=820, loss=5.3661, this epoch 0.0314, total 25.7392
(T) | Epoch=821, loss=5.3759, this epoch 0.0354, total 25.7746
(T) | Epoch=822, loss=5.3918, this epoch 0.0322, total 25.8068
(T) | Epoch=823, loss=5.3698, this epoch 0.0279, total 25.8347
(T) | Epoch=824, loss=5.3619, this epoch 0.0324, total 25.8671
(T) | Epoch=825, loss=5.4502, this epoch 0.0293, total 25.8964
(T) | Epoch=826, loss=5.3947, this epoch 0.0339, total 25.9303
(T) | Epoch=827, loss=5.3738, this epoch 0.0378, total 25.9681
(T) | Epoch=828, loss=5.3934, this epoch 0.0285, total 25.9966
(T) | Epoch=829, loss=5.3747, this epoch 0.0300, total 26.0266
(T) | Epoch=830, loss=5.4158, this epoch 0.0284, total 26.0550
(T) | Epoch=831, loss=5.3569, this epoch 0.0338, total 26.0888
(T) | Epoch=832, loss=5.3737, this epoch 0.0278, total 26.1166
(T) | Epoch=833, loss=5.4222, this epoch 0.0329, total 26.1495
(T) | Epoch=834, loss=5.3644, this epoch 0.0308, total 26.1804
(T) | Epoch=835, loss=5.6100, this epoch 0.0364, total 26.2168
(T) | Epoch=836, loss=5.3763, this epoch 0.0282, total 26.2450
(T) | Epoch=837, loss=5.4399, this epoch 0.0307, total 26.2757
(T) | Epoch=838, loss=5.4819, this epoch 0.0356, total 26.3113
(T) | Epoch=839, loss=5.4625, this epoch 0.0357, total 26.3471
(T) | Epoch=840, loss=5.4507, this epoch 0.0329, total 26.3800
(T) | Epoch=841, loss=5.4639, this epoch 0.0343, total 26.4143
(T) | Epoch=842, loss=5.5462, this epoch 0.0384, total 26.4526
(T) | Epoch=843, loss=5.4335, this epoch 0.0383, total 26.4910
(T) | Epoch=844, loss=5.4010, this epoch 0.0380, total 26.5290
(T) | Epoch=845, loss=5.4892, this epoch 0.0305, total 26.5595
(T) | Epoch=846, loss=5.3663, this epoch 0.0365, total 26.5960
(T) | Epoch=847, loss=5.4921, this epoch 0.0382, total 26.6342
(T) | Epoch=848, loss=5.3733, this epoch 0.0329, total 26.6671
(T) | Epoch=849, loss=5.3962, this epoch 0.0301, total 26.6972
(T) | Epoch=850, loss=5.3674, this epoch 0.0278, total 26.7250
(T) | Epoch=851, loss=5.4591, this epoch 0.0352, total 26.7601
(T) | Epoch=852, loss=5.4907, this epoch 0.0379, total 26.7980
(T) | Epoch=853, loss=5.4064, this epoch 0.0363, total 26.8343
(T) | Epoch=854, loss=5.4593, this epoch 0.0330, total 26.8673
(T) | Epoch=855, loss=5.3664, this epoch 0.0302, total 26.8975
(T) | Epoch=856, loss=5.4139, this epoch 0.0261, total 26.9236
(T) | Epoch=857, loss=5.4293, this epoch 0.0306, total 26.9542
(T) | Epoch=858, loss=5.3614, this epoch 0.0346, total 26.9888
(T) | Epoch=859, loss=5.3615, this epoch 0.0336, total 27.0223
(T) | Epoch=860, loss=5.4114, this epoch 0.0288, total 27.0511
(T) | Epoch=861, loss=5.3956, this epoch 0.0347, total 27.0858
(T) | Epoch=862, loss=5.4147, this epoch 0.0317, total 27.1175
(T) | Epoch=863, loss=5.4119, this epoch 0.0358, total 27.1533
(T) | Epoch=864, loss=5.4430, this epoch 0.0365, total 27.1898
(T) | Epoch=865, loss=5.4192, this epoch 0.0362, total 27.2261
(T) | Epoch=866, loss=5.4617, this epoch 0.0313, total 27.2574
(T) | Epoch=867, loss=5.3594, this epoch 0.0390, total 27.2964
(T) | Epoch=868, loss=5.3947, this epoch 0.0331, total 27.3295
(T) | Epoch=869, loss=5.3578, this epoch 0.0303, total 27.3597
(T) | Epoch=870, loss=5.3803, this epoch 0.0342, total 27.3939
(T) | Epoch=871, loss=5.4395, this epoch 0.0386, total 27.4325
(T) | Epoch=872, loss=5.5254, this epoch 0.0335, total 27.4660
(T) | Epoch=873, loss=5.4054, this epoch 0.0292, total 27.4951
(T) | Epoch=874, loss=5.3704, this epoch 0.0223, total 27.5174
(T) | Epoch=875, loss=5.3595, this epoch 0.0388, total 27.5563
(T) | Epoch=876, loss=5.3830, this epoch 0.0291, total 27.5854
(T) | Epoch=877, loss=5.3606, this epoch 0.0311, total 27.6165
(T) | Epoch=878, loss=5.3745, this epoch 0.0370, total 27.6535
(T) | Epoch=879, loss=5.4077, this epoch 0.0367, total 27.6902
(T) | Epoch=880, loss=5.4641, this epoch 0.0335, total 27.7236
(T) | Epoch=881, loss=5.3625, this epoch 0.0373, total 27.7610
(T) | Epoch=882, loss=5.5027, this epoch 0.0390, total 27.8000
(T) | Epoch=883, loss=5.4419, this epoch 0.0392, total 27.8392
(T) | Epoch=884, loss=5.4442, this epoch 0.0369, total 27.8762
(T) | Epoch=885, loss=5.4889, this epoch 0.0296, total 27.9057
(T) | Epoch=886, loss=5.3904, this epoch 0.0398, total 27.9455
(T) | Epoch=887, loss=5.3811, this epoch 0.0392, total 27.9847
(T) | Epoch=888, loss=5.3536, this epoch 0.0388, total 28.0235
+++model saved ! 2015.pth
(T) | Epoch=889, loss=5.3893, this epoch 0.0411, total 28.0646
(T) | Epoch=890, loss=5.3615, this epoch 0.0320, total 28.0966
(T) | Epoch=891, loss=5.3730, this epoch 0.0363, total 28.1329
(T) | Epoch=892, loss=5.4291, this epoch 0.0380, total 28.1709
(T) | Epoch=893, loss=5.4850, this epoch 0.0392, total 28.2101
(T) | Epoch=894, loss=5.3885, this epoch 0.0379, total 28.2480
(T) | Epoch=895, loss=5.4378, this epoch 0.0406, total 28.2886
(T) | Epoch=896, loss=5.4141, this epoch 0.0319, total 28.3205
(T) | Epoch=897, loss=5.4474, this epoch 0.0368, total 28.3573
(T) | Epoch=898, loss=5.3651, this epoch 0.0376, total 28.3949
(T) | Epoch=899, loss=5.3834, this epoch 0.0385, total 28.4335
(T) | Epoch=900, loss=5.3587, this epoch 0.0386, total 28.4720
(T) | Epoch=901, loss=5.4534, this epoch 0.0394, total 28.5114
(T) | Epoch=902, loss=5.3763, this epoch 0.0338, total 28.5452
(T) | Epoch=903, loss=5.3644, this epoch 0.0357, total 28.5809
(T) | Epoch=904, loss=5.4079, this epoch 0.0330, total 28.6139
(T) | Epoch=905, loss=5.4267, this epoch 0.0351, total 28.6490
(T) | Epoch=906, loss=5.3807, this epoch 0.0306, total 28.6796
(T) | Epoch=907, loss=5.4625, this epoch 0.0330, total 28.7126
(T) | Epoch=908, loss=5.3605, this epoch 0.0270, total 28.7396
(T) | Epoch=909, loss=5.3567, this epoch 0.0299, total 28.7695
(T) | Epoch=910, loss=5.4293, this epoch 0.0274, total 28.7969
(T) | Epoch=911, loss=5.3916, this epoch 0.0327, total 28.8296
(T) | Epoch=912, loss=5.3558, this epoch 0.0358, total 28.8654
(T) | Epoch=913, loss=5.3560, this epoch 0.0267, total 28.8921
(T) | Epoch=914, loss=5.3572, this epoch 0.0294, total 28.9215
(T) | Epoch=915, loss=5.3912, this epoch 0.0287, total 28.9502
(T) | Epoch=916, loss=5.3535, this epoch 0.0305, total 28.9808
+++model saved ! 2015.pth
(T) | Epoch=917, loss=5.3664, this epoch 0.0232, total 29.0039
(T) | Epoch=918, loss=5.3662, this epoch 0.0266, total 29.0305
(T) | Epoch=919, loss=5.4028, this epoch 0.0286, total 29.0592
(T) | Epoch=920, loss=5.4187, this epoch 0.0272, total 29.0864
(T) | Epoch=921, loss=5.3978, this epoch 0.0281, total 29.1145
(T) | Epoch=922, loss=5.3634, this epoch 0.0224, total 29.1369
(T) | Epoch=923, loss=5.4691, this epoch 0.0217, total 29.1586
(T) | Epoch=924, loss=5.3935, this epoch 0.0214, total 29.1800
(T) | Epoch=925, loss=5.3615, this epoch 0.0277, total 29.2076
(T) | Epoch=926, loss=5.4377, this epoch 0.0255, total 29.2331
(T) | Epoch=927, loss=5.3582, this epoch 0.0295, total 29.2626
(T) | Epoch=928, loss=5.4260, this epoch 0.0212, total 29.2838
(T) | Epoch=929, loss=5.3660, this epoch 0.0224, total 29.3062
(T) | Epoch=930, loss=5.3632, this epoch 0.0254, total 29.3315
(T) | Epoch=931, loss=5.3678, this epoch 0.0229, total 29.3544
(T) | Epoch=932, loss=5.4873, this epoch 0.0214, total 29.3758
(T) | Epoch=933, loss=5.3579, this epoch 0.0279, total 29.4037
(T) | Epoch=934, loss=5.4084, this epoch 0.0296, total 29.4333
(T) | Epoch=935, loss=5.3909, this epoch 0.0218, total 29.4551
(T) | Epoch=936, loss=5.5196, this epoch 0.0284, total 29.4835
(T) | Epoch=937, loss=5.3984, this epoch 0.0268, total 29.5103
(T) | Epoch=938, loss=5.3748, this epoch 0.0279, total 29.5382
(T) | Epoch=939, loss=5.3695, this epoch 0.0288, total 29.5670
(T) | Epoch=940, loss=5.4638, this epoch 0.0216, total 29.5886
(T) | Epoch=941, loss=5.3916, this epoch 0.0217, total 29.6103
(T) | Epoch=942, loss=5.4531, this epoch 0.0278, total 29.6380
(T) | Epoch=943, loss=5.3814, this epoch 0.0278, total 29.6659
(T) | Epoch=944, loss=5.4165, this epoch 0.0282, total 29.6941
(T) | Epoch=945, loss=5.4232, this epoch 0.0280, total 29.7221
(T) | Epoch=946, loss=5.3983, this epoch 0.0323, total 29.7544
(T) | Epoch=947, loss=5.3631, this epoch 0.0290, total 29.7834
(T) | Epoch=948, loss=5.3577, this epoch 0.0226, total 29.8060
(T) | Epoch=949, loss=5.4128, this epoch 0.0220, total 29.8280
(T) | Epoch=950, loss=5.4152, this epoch 0.0274, total 29.8554
(T) | Epoch=951, loss=5.5175, this epoch 0.0226, total 29.8780
(T) | Epoch=952, loss=5.4276, this epoch 0.0219, total 29.8999
(T) | Epoch=953, loss=5.3794, this epoch 0.0283, total 29.9282
(T) | Epoch=954, loss=5.3561, this epoch 0.0293, total 29.9575
(T) | Epoch=955, loss=5.5146, this epoch 0.0282, total 29.9857
(T) | Epoch=956, loss=5.4424, this epoch 0.0215, total 30.0072
(T) | Epoch=957, loss=5.4077, this epoch 0.0213, total 30.0285
(T) | Epoch=958, loss=5.3551, this epoch 0.0315, total 30.0599
(T) | Epoch=959, loss=5.3544, this epoch 0.0231, total 30.0831
(T) | Epoch=960, loss=5.3548, this epoch 0.0318, total 30.1149
(T) | Epoch=961, loss=5.3850, this epoch 0.0286, total 30.1435
(T) | Epoch=962, loss=5.3611, this epoch 0.0293, total 30.1729
(T) | Epoch=963, loss=5.3906, this epoch 0.0250, total 30.1979
(T) | Epoch=964, loss=5.4360, this epoch 0.0309, total 30.2288
(T) | Epoch=965, loss=5.3804, this epoch 0.0300, total 30.2588
(T) | Epoch=966, loss=5.3631, this epoch 0.0225, total 30.2813
(T) | Epoch=967, loss=5.3901, this epoch 0.0300, total 30.3113
(T) | Epoch=968, loss=5.3902, this epoch 0.0289, total 30.3402
(T) | Epoch=969, loss=5.7143, this epoch 0.0327, total 30.3729
(T) | Epoch=970, loss=5.3718, this epoch 0.0311, total 30.4040
(T) | Epoch=971, loss=5.3679, this epoch 0.0351, total 30.4391
(T) | Epoch=972, loss=5.3615, this epoch 0.0306, total 30.4697
(T) | Epoch=973, loss=5.3878, this epoch 0.0304, total 30.5002
(T) | Epoch=974, loss=5.4083, this epoch 0.0310, total 30.5311
(T) | Epoch=975, loss=5.3535, this epoch 0.0298, total 30.5609
(T) | Epoch=976, loss=5.4206, this epoch 0.0221, total 30.5829
(T) | Epoch=977, loss=5.4463, this epoch 0.0225, total 30.6054
(T) | Epoch=978, loss=5.3644, this epoch 0.0269, total 30.6323
(T) | Epoch=979, loss=5.4362, this epoch 0.0251, total 30.6574
(T) | Epoch=980, loss=5.3540, this epoch 0.0268, total 30.6842
(T) | Epoch=981, loss=5.4379, this epoch 0.0213, total 30.7055
(T) | Epoch=982, loss=5.3980, this epoch 0.0271, total 30.7326
(T) | Epoch=983, loss=5.3550, this epoch 0.0223, total 30.7549
(T) | Epoch=984, loss=5.3559, this epoch 0.0211, total 30.7760
(T) | Epoch=985, loss=5.3511, this epoch 0.0207, total 30.7967
+++model saved ! 2015.pth
(T) | Epoch=986, loss=5.3598, this epoch 0.0334, total 30.8300
(T) | Epoch=987, loss=5.4850, this epoch 0.0264, total 30.8564
(T) | Epoch=988, loss=5.4163, this epoch 0.0359, total 30.8923
(T) | Epoch=989, loss=5.3497, this epoch 0.0325, total 30.9248
+++model saved ! 2015.pth
(T) | Epoch=990, loss=5.3871, this epoch 0.0377, total 30.9626
(T) | Epoch=991, loss=5.4118, this epoch 0.0342, total 30.9968
(T) | Epoch=992, loss=5.4212, this epoch 0.0286, total 31.0254
(T) | Epoch=993, loss=5.4191, this epoch 0.0349, total 31.0604
(T) | Epoch=994, loss=5.4329, this epoch 0.0342, total 31.0946
(T) | Epoch=995, loss=5.3584, this epoch 0.0344, total 31.1290
(T) | Epoch=996, loss=5.3596, this epoch 0.0228, total 31.1518
(T) | Epoch=997, loss=5.4289, this epoch 0.0283, total 31.1801
(T) | Epoch=998, loss=5.3952, this epoch 0.0308, total 31.2109
(T) | Epoch=999, loss=5.4049, this epoch 0.0348, total 31.2457
(T) | Epoch=1000, loss=5.4135, this epoch 0.0289, total 31.2746
=== Final ===

==============================
LoRA FINE-TUNING
==============================
Random seed set to 2
Epoch: 0, loss: 27.6969, train_acc: 0.2309, train_recall: 0.1111, train_f1: 0.0417, val_acc: 0.199507, val_recall: 0.125000, val_f1: 0.041581
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed2.csv
Epoch: 1, loss: 51.8655, train_acc: 0.2913, train_recall: 0.1000, train_f1: 0.0452, val_acc: 0.347291, val_recall: 0.125000, val_f1: 0.064442
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed2.csv
Epoch: 2, loss: 91.5196, train_acc: 0.0233, train_recall: 0.1178, train_f1: 0.0206, val_acc: 0.027094, val_recall: 0.125000, val_f1: 0.034830
Epoch: 3, loss: 128.0650, train_acc: 0.3210, train_recall: 0.1097, train_f1: 0.0540, val_acc: 0.302956, val_recall: 0.123992, val_f1: 0.058129
Epoch: 4, loss: 156.9780, train_acc: 0.0328, train_recall: 0.1111, train_f1: 0.0071, val_acc: 0.046798, val_recall: 0.125000, val_f1: 0.011176
Epoch: 5, loss: 135.8726, train_acc: 0.0731, train_recall: 0.1000, train_f1: 0.0139, val_acc: 0.044335, val_recall: 0.111111, val_f1: 0.009569
Epoch: 6, loss: 134.5233, train_acc: 0.3104, train_recall: 0.0954, train_f1: 0.0479, val_acc: 0.295566, val_recall: 0.107527, val_f1: 0.051184
Epoch: 7, loss: 123.6042, train_acc: 0.3252, train_recall: 0.1111, train_f1: 0.0545, val_acc: 0.305419, val_recall: 0.125000, val_f1: 0.058491
Epoch: 8, loss: 101.2660, train_acc: 0.3252, train_recall: 0.1111, train_f1: 0.0545, val_acc: 0.305419, val_recall: 0.125000, val_f1: 0.058491
Epoch: 9, loss: 71.2673, train_acc: 0.3252, train_recall: 0.1111, train_f1: 0.0545, val_acc: 0.305419, val_recall: 0.125000, val_f1: 0.058491
Epoch: 10, loss: 94.8255, train_acc: 0.2913, train_recall: 0.1111, train_f1: 0.0501, val_acc: 0.347291, val_recall: 0.125000, val_f1: 0.064442
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed2.csv
Epoch: 11, loss: 103.2518, train_acc: 0.2913, train_recall: 0.1111, train_f1: 0.0501, val_acc: 0.347291, val_recall: 0.125000, val_f1: 0.064442
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed2.csv
Epoch: 12, loss: 98.5903, train_acc: 0.2913, train_recall: 0.1111, train_f1: 0.0501, val_acc: 0.347291, val_recall: 0.125000, val_f1: 0.064442
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed2.csv
Epoch: 13, loss: 84.3269, train_acc: 0.2913, train_recall: 0.1111, train_f1: 0.0501, val_acc: 0.347291, val_recall: 0.125000, val_f1: 0.064442
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed2.csv
Epoch: 14, loss: 62.8577, train_acc: 0.2913, train_recall: 0.1111, train_f1: 0.0501, val_acc: 0.347291, val_recall: 0.125000, val_f1: 0.064442
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed2.csv
Epoch: 15, loss: 87.5345, train_acc: 0.2309, train_recall: 0.1111, train_f1: 0.0417, val_acc: 0.199507, val_recall: 0.125000, val_f1: 0.041581
Epoch: 16, loss: 98.4513, train_acc: 0.2309, train_recall: 0.1111, train_f1: 0.0417, val_acc: 0.199507, val_recall: 0.125000, val_f1: 0.041581
Epoch: 17, loss: 95.6926, train_acc: 0.2309, train_recall: 0.1111, train_f1: 0.0417, val_acc: 0.199507, val_recall: 0.125000, val_f1: 0.041581
Epoch: 18, loss: 82.9772, train_acc: 0.2309, train_recall: 0.1111, train_f1: 0.0417, val_acc: 0.199507, val_recall: 0.125000, val_f1: 0.041581
Epoch: 19, loss: 62.7016, train_acc: 0.2309, train_recall: 0.1111, train_f1: 0.0417, val_acc: 0.199507, val_recall: 0.125000, val_f1: 0.041581
Epoch: 20, loss: 49.0228, train_acc: 0.3252, train_recall: 0.1111, train_f1: 0.0545, val_acc: 0.305419, val_recall: 0.125000, val_f1: 0.058491
Epoch: 21, loss: 56.2328, train_acc: 0.3252, train_recall: 0.1111, train_f1: 0.0545, val_acc: 0.305419, val_recall: 0.125000, val_f1: 0.058491
Epoch: 22, loss: 62.0707, train_acc: 0.0879, train_recall: 0.1162, train_f1: 0.0249, val_acc: 0.054187, val_recall: 0.129032, val_f1: 0.018332
Epoch: 23, loss: 57.6714, train_acc: 0.3252, train_recall: 0.1111, train_f1: 0.0545, val_acc: 0.305419, val_recall: 0.125000, val_f1: 0.058491
Epoch: 24, loss: 53.0230, train_acc: 0.3263, train_recall: 0.1156, train_f1: 0.0631, val_acc: 0.305419, val_recall: 0.125000, val_f1: 0.058491
Epoch: 25, loss: 62.3881, train_acc: 0.0265, train_recall: 0.1111, train_f1: 0.0057, val_acc: 0.027094, val_recall: 0.125000, val_f1: 0.006595
Epoch: 26, loss: 44.9753, train_acc: 0.2934, train_recall: 0.1159, train_f1: 0.0592, val_acc: 0.349754, val_recall: 0.126008, val_f1: 0.066560
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed2.csv
Epoch: 27, loss: 49.3222, train_acc: 0.2913, train_recall: 0.1111, train_f1: 0.0501, val_acc: 0.347291, val_recall: 0.125000, val_f1: 0.064442
Epoch: 28, loss: 46.6597, train_acc: 0.2913, train_recall: 0.1111, train_f1: 0.0501, val_acc: 0.347291, val_recall: 0.125000, val_f1: 0.064442
Epoch: 29, loss: 38.0853, train_acc: 0.3040, train_recall: 0.1154, train_f1: 0.0602, val_acc: 0.354680, val_recall: 0.128146, val_f1: 0.072575
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed2.csv
Epoch: 30, loss: 41.4834, train_acc: 0.3252, train_recall: 0.1111, train_f1: 0.0545, val_acc: 0.305419, val_recall: 0.125000, val_f1: 0.058491
Epoch: 31, loss: 41.2384, train_acc: 0.3252, train_recall: 0.1111, train_f1: 0.0545, val_acc: 0.305419, val_recall: 0.125000, val_f1: 0.058491
Epoch: 32, loss: 42.7297, train_acc: 0.0879, train_recall: 0.1162, train_f1: 0.0249, val_acc: 0.054187, val_recall: 0.129032, val_f1: 0.018362
Epoch: 33, loss: 33.1050, train_acc: 0.3252, train_recall: 0.1111, train_f1: 0.0546, val_acc: 0.305419, val_recall: 0.125000, val_f1: 0.058491
Epoch: 34, loss: 42.1287, train_acc: 0.2309, train_recall: 0.1111, train_f1: 0.0417, val_acc: 0.199507, val_recall: 0.125000, val_f1: 0.041581
Epoch: 35, loss: 45.5522, train_acc: 0.2309, train_recall: 0.1111, train_f1: 0.0417, val_acc: 0.199507, val_recall: 0.125000, val_f1: 0.041581
Epoch: 36, loss: 41.2989, train_acc: 0.2309, train_recall: 0.1111, train_f1: 0.0417, val_acc: 0.199507, val_recall: 0.125000, val_f1: 0.041581
Epoch: 37, loss: 32.0111, train_acc: 0.0339, train_recall: 0.1116, train_f1: 0.0082, val_acc: 0.046798, val_recall: 0.125000, val_f1: 0.011310
Epoch: 38, loss: 34.8143, train_acc: 0.0011, train_recall: 0.1111, train_f1: 0.0002, val_acc: 0.002463, val_recall: 0.125000, val_f1: 0.000614
Epoch: 39, loss: 33.3587, train_acc: 0.3051, train_recall: 0.1156, train_f1: 0.0631, val_acc: 0.349754, val_recall: 0.126494, val_f1: 0.074440
Epoch: 40, loss: 36.5080, train_acc: 0.2998, train_recall: 0.1133, train_f1: 0.0645, val_acc: 0.344828, val_recall: 0.124964, val_f1: 0.076734
Epoch: 41, loss: 38.2016, train_acc: 0.2998, train_recall: 0.1135, train_f1: 0.0623, val_acc: 0.344828, val_recall: 0.124721, val_f1: 0.074106
Epoch: 42, loss: 38.5588, train_acc: 0.3210, train_recall: 0.1171, train_f1: 0.0877, val_acc: 0.327586, val_recall: 0.123256, val_f1: 0.096288
Epoch: 43, loss: 37.7391, train_acc: 0.3051, train_recall: 0.1156, train_f1: 0.0631, val_acc: 0.352217, val_recall: 0.127381, val_f1: 0.074848
Epoch: 44, loss: 37.1211, train_acc: 0.0339, train_recall: 0.1103, train_f1: 0.0145, val_acc: 0.034483, val_recall: 0.107313, val_f1: 0.014442
Epoch: 45, loss: 35.1458, train_acc: 0.3252, train_recall: 0.1111, train_f1: 0.0545, val_acc: 0.305419, val_recall: 0.125000, val_f1: 0.058491
Epoch: 46, loss: 33.7039, train_acc: 0.0297, train_recall: 0.1210, train_f1: 0.0190, val_acc: 0.014778, val_recall: 0.005160, val_f1: 0.009686
Epoch: 47, loss: 32.4764, train_acc: 0.3019, train_recall: 0.1157, train_f1: 0.0610, val_acc: 0.357143, val_recall: 0.130224, val_f1: 0.076677
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed2.csv
Epoch: 48, loss: 35.9686, train_acc: 0.2309, train_recall: 0.1111, train_f1: 0.0417, val_acc: 0.199507, val_recall: 0.125000, val_f1: 0.041581
Epoch: 49, loss: 36.6847, train_acc: 0.2309, train_recall: 0.1111, train_f1: 0.0417, val_acc: 0.199507, val_recall: 0.125000, val_f1: 0.041581
Epoch: 50, loss: 30.5430, train_acc: 0.2299, train_recall: 0.1102, train_f1: 0.0437, val_acc: 0.201970, val_recall: 0.124938, val_f1: 0.047239
Epoch: 51, loss: 34.3385, train_acc: 0.3252, train_recall: 0.1111, train_f1: 0.0545, val_acc: 0.305419, val_recall: 0.125000, val_f1: 0.058491
Epoch: 52, loss: 34.0185, train_acc: 0.3252, train_recall: 0.1111, train_f1: 0.0545, val_acc: 0.305419, val_recall: 0.125000, val_f1: 0.058491
Epoch: 53, loss: 36.6074, train_acc: 0.3030, train_recall: 0.1150, train_f1: 0.0595, val_acc: 0.354680, val_recall: 0.128146, val_f1: 0.072511
Epoch: 54, loss: 37.2315, train_acc: 0.2966, train_recall: 0.1129, train_f1: 0.0540, val_acc: 0.352217, val_recall: 0.127016, val_f1: 0.068647
Epoch: 55, loss: 40.6180, train_acc: 0.0879, train_recall: 0.1162, train_f1: 0.0249, val_acc: 0.054187, val_recall: 0.129032, val_f1: 0.018393
Epoch: 56, loss: 34.8076, train_acc: 0.0890, train_recall: 0.1165, train_f1: 0.0255, val_acc: 0.054187, val_recall: 0.129032, val_f1: 0.018332
Epoch: 57, loss: 34.6603, train_acc: 0.3252, train_recall: 0.1111, train_f1: 0.0545, val_acc: 0.305419, val_recall: 0.125000, val_f1: 0.058491
Epoch: 58, loss: 33.1924, train_acc: 0.3252, train_recall: 0.1111, train_f1: 0.0545, val_acc: 0.305419, val_recall: 0.125000, val_f1: 0.058491
Epoch: 59, loss: 32.5218, train_acc: 0.3040, train_recall: 0.1154, train_f1: 0.0602, val_acc: 0.354680, val_recall: 0.128146, val_f1: 0.072575
Epoch: 60, loss: 30.8323, train_acc: 0.2998, train_recall: 0.1193, train_f1: 0.0658, val_acc: 0.354680, val_recall: 0.140107, val_f1: 0.090464
Epoch: 61, loss: 36.2963, train_acc: 0.2299, train_recall: 0.1106, train_f1: 0.0416, val_acc: 0.199507, val_recall: 0.125000, val_f1: 0.041667
Epoch: 62, loss: 37.7345, train_acc: 0.2288, train_recall: 0.1140, train_f1: 0.0477, val_acc: 0.199507, val_recall: 0.134820, val_f1: 0.057119
Epoch: 63, loss: 33.5296, train_acc: 0.2352, train_recall: 0.1761, train_f1: 0.0780, val_acc: 0.201970, val_recall: 0.185466, val_f1: 0.072929
Epoch: 64, loss: 36.4779, train_acc: 0.3210, train_recall: 0.1505, train_f1: 0.0832, val_acc: 0.307882, val_recall: 0.177786, val_f1: 0.094128
Epoch: 65, loss: 36.3456, train_acc: 0.3210, train_recall: 0.1505, train_f1: 0.0832, val_acc: 0.307882, val_recall: 0.177786, val_f1: 0.094128
Epoch: 66, loss: 32.6209, train_acc: 0.3231, train_recall: 0.1757, train_f1: 0.0896, val_acc: 0.307882, val_recall: 0.188141, val_f1: 0.090126
Epoch: 67, loss: 34.2679, train_acc: 0.2966, train_recall: 0.1681, train_f1: 0.0899, val_acc: 0.364532, val_recall: 0.205207, val_f1: 0.120110
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed2.csv
Epoch: 68, loss: 36.9251, train_acc: 0.2956, train_recall: 0.1350, train_f1: 0.0705, val_acc: 0.362069, val_recall: 0.170166, val_f1: 0.106823
Epoch: 69, loss: 35.2290, train_acc: 0.2956, train_recall: 0.1350, train_f1: 0.0705, val_acc: 0.362069, val_recall: 0.170166, val_f1: 0.107747
Epoch: 70, loss: 29.8130, train_acc: 0.2956, train_recall: 0.1360, train_f1: 0.0835, val_acc: 0.362069, val_recall: 0.172793, val_f1: 0.122850
Epoch: 71, loss: 31.7471, train_acc: 0.2383, train_recall: 0.1325, train_f1: 0.0706, val_acc: 0.214286, val_recall: 0.163939, val_f1: 0.089902
Epoch: 72, loss: 34.1734, train_acc: 0.3252, train_recall: 0.1111, train_f1: 0.0545, val_acc: 0.305419, val_recall: 0.125000, val_f1: 0.058491
Epoch: 73, loss: 35.1517, train_acc: 0.3252, train_recall: 0.1111, train_f1: 0.0545, val_acc: 0.305419, val_recall: 0.125000, val_f1: 0.058491
Epoch: 74, loss: 31.8319, train_acc: 0.3252, train_recall: 0.1111, train_f1: 0.0545, val_acc: 0.305419, val_recall: 0.125000, val_f1: 0.058491
Epoch: 75, loss: 31.3497, train_acc: 0.2394, train_recall: 0.1332, train_f1: 0.0702, val_acc: 0.216749, val_recall: 0.165482, val_f1: 0.089111
Epoch: 76, loss: 28.6182, train_acc: 0.2415, train_recall: 0.1500, train_f1: 0.0776, val_acc: 0.226601, val_recall: 0.197368, val_f1: 0.107785
Epoch: 77, loss: 30.4385, train_acc: 0.3051, train_recall: 0.1584, train_f1: 0.0924, val_acc: 0.362069, val_recall: 0.178163, val_f1: 0.117313
Epoch: 78, loss: 31.3227, train_acc: 0.2966, train_recall: 0.1523, train_f1: 0.0744, val_acc: 0.357143, val_recall: 0.170576, val_f1: 0.099321
Epoch: 79, loss: 28.0906, train_acc: 0.2977, train_recall: 0.1687, train_f1: 0.0922, val_acc: 0.357143, val_recall: 0.191530, val_f1: 0.104235
Epoch: 80, loss: 28.7554, train_acc: 0.0943, train_recall: 0.1710, train_f1: 0.0502, val_acc: 0.064039, val_recall: 0.195198, val_f1: 0.050389
Epoch: 81, loss: 30.6438, train_acc: 0.3316, train_recall: 0.1528, train_f1: 0.0889, val_acc: 0.315271, val_recall: 0.170455, val_f1: 0.097348
Epoch: 82, loss: 30.9860, train_acc: 0.3326, train_recall: 0.1572, train_f1: 0.1001, val_acc: 0.315271, val_recall: 0.170455, val_f1: 0.098656
Epoch: 83, loss: 27.4690, train_acc: 0.3326, train_recall: 0.1572, train_f1: 0.0992, val_acc: 0.315271, val_recall: 0.170455, val_f1: 0.098656
Epoch: 84, loss: 27.6598, train_acc: 0.2426, train_recall: 0.1581, train_f1: 0.0906, val_acc: 0.211823, val_recall: 0.170392, val_f1: 0.085233
Epoch: 85, loss: 27.4025, train_acc: 0.2436, train_recall: 0.1786, train_f1: 0.1177, val_acc: 0.221675, val_recall: 0.208134, val_f1: 0.141513
Epoch: 86, loss: 27.5128, train_acc: 0.3199, train_recall: 0.1906, train_f1: 0.1251, val_acc: 0.364532, val_recall: 0.202631, val_f1: 0.132135
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed2.csv
Epoch: 87, loss: 30.3453, train_acc: 0.3083, train_recall: 0.1577, train_f1: 0.1035, val_acc: 0.364532, val_recall: 0.183834, val_f1: 0.118649
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed2.csv
Epoch: 88, loss: 29.3618, train_acc: 0.3146, train_recall: 0.1598, train_f1: 0.1084, val_acc: 0.366995, val_recall: 0.184964, val_f1: 0.122447
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed2.csv
Epoch: 89, loss: 27.3677, train_acc: 0.0996, train_recall: 0.1642, train_f1: 0.0795, val_acc: 0.068966, val_recall: 0.192429, val_f1: 0.079374
Epoch: 90, loss: 24.3778, train_acc: 0.3400, train_recall: 0.1733, train_f1: 0.1126, val_acc: 0.322660, val_recall: 0.190191, val_f1: 0.128801
Epoch: 91, loss: 25.4012, train_acc: 0.3358, train_recall: 0.1556, train_f1: 0.1028, val_acc: 0.315271, val_recall: 0.170455, val_f1: 0.100194
Epoch: 92, loss: 26.2958, train_acc: 0.2405, train_recall: 0.1562, train_f1: 0.0950, val_acc: 0.211823, val_recall: 0.179678, val_f1: 0.109963
Epoch: 93, loss: 25.1963, train_acc: 0.2405, train_recall: 0.1562, train_f1: 0.0950, val_acc: 0.211823, val_recall: 0.179678, val_f1: 0.109963
Epoch: 94, loss: 26.4780, train_acc: 0.3125, train_recall: 0.1780, train_f1: 0.1298, val_acc: 0.366995, val_recall: 0.184964, val_f1: 0.135419
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed2.csv
Epoch: 95, loss: 26.6409, train_acc: 0.3369, train_recall: 0.1750, train_f1: 0.1246, val_acc: 0.317734, val_recall: 0.181818, val_f1: 0.121028
Epoch: 96, loss: 27.3378, train_acc: 0.3432, train_recall: 0.1967, train_f1: 0.1504, val_acc: 0.317734, val_recall: 0.181818, val_f1: 0.118512
Epoch: 97, loss: 27.0434, train_acc: 0.3083, train_recall: 0.1878, train_f1: 0.1251, val_acc: 0.352217, val_recall: 0.201628, val_f1: 0.129745
Epoch: 98, loss: 26.4645, train_acc: 0.3146, train_recall: 0.1886, train_f1: 0.1371, val_acc: 0.369458, val_recall: 0.202020, val_f1: 0.133024
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed2.csv
Epoch: 99, loss: 24.6334, train_acc: 0.3369, train_recall: 0.1816, train_f1: 0.1265, val_acc: 0.322660, val_recall: 0.204545, val_f1: 0.128732
Epoch: 100, loss: 25.9715, train_acc: 0.2447, train_recall: 0.1563, train_f1: 0.0932, val_acc: 0.214286, val_recall: 0.182826, val_f1: 0.093071
Epoch: 101, loss: 24.9653, train_acc: 0.2489, train_recall: 0.1576, train_f1: 0.0968, val_acc: 0.209360, val_recall: 0.179205, val_f1: 0.093953
Epoch: 102, loss: 25.0304, train_acc: 0.3358, train_recall: 0.1556, train_f1: 0.1028, val_acc: 0.315271, val_recall: 0.170455, val_f1: 0.100194
Epoch: 103, loss: 25.0904, train_acc: 0.3146, train_recall: 0.1630, train_f1: 0.1153, val_acc: 0.364532, val_recall: 0.179171, val_f1: 0.124636
Epoch: 104, loss: 24.1583, train_acc: 0.3199, train_recall: 0.1819, train_f1: 0.1404, val_acc: 0.374384, val_recall: 0.191133, val_f1: 0.144692
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed2.csv
Epoch: 105, loss: 23.5930, train_acc: 0.3379, train_recall: 0.1610, train_f1: 0.1264, val_acc: 0.330049, val_recall: 0.202215, val_f1: 0.148366
Epoch: 106, loss: 24.9804, train_acc: 0.0996, train_recall: 0.1637, train_f1: 0.0876, val_acc: 0.081281, val_recall: 0.208326, val_f1: 0.105570
Epoch: 107, loss: 25.3061, train_acc: 0.2436, train_recall: 0.1584, train_f1: 0.1021, val_acc: 0.226601, val_recall: 0.202153, val_f1: 0.126882
Epoch: 108, loss: 25.3954, train_acc: 0.3411, train_recall: 0.1782, train_f1: 0.1265, val_acc: 0.325123, val_recall: 0.197841, val_f1: 0.143487
Epoch: 109, loss: 23.9154, train_acc: 0.3411, train_recall: 0.1899, train_f1: 0.1325, val_acc: 0.322660, val_recall: 0.196582, val_f1: 0.127952
Epoch: 110, loss: 23.9921, train_acc: 0.3167, train_recall: 0.1885, train_f1: 0.1374, val_acc: 0.366995, val_recall: 0.203274, val_f1: 0.157792
Epoch: 111, loss: 24.8301, train_acc: 0.3146, train_recall: 0.1905, train_f1: 0.1414, val_acc: 0.366995, val_recall: 0.203274, val_f1: 0.136889
Epoch: 112, loss: 24.3523, train_acc: 0.2468, train_recall: 0.1863, train_f1: 0.1231, val_acc: 0.216749, val_recall: 0.199761, val_f1: 0.102422
Epoch: 113, loss: 23.0079, train_acc: 0.3400, train_recall: 0.1878, train_f1: 0.1452, val_acc: 0.325123, val_recall: 0.213265, val_f1: 0.152476
Epoch: 114, loss: 23.8002, train_acc: 0.3422, train_recall: 0.1854, train_f1: 0.1242, val_acc: 0.317734, val_recall: 0.187389, val_f1: 0.112878
Epoch: 115, loss: 23.3328, train_acc: 0.3189, train_recall: 0.1859, train_f1: 0.1457, val_acc: 0.376847, val_recall: 0.199560, val_f1: 0.154614
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed2.csv
Epoch: 116, loss: 23.6357, train_acc: 0.3157, train_recall: 0.1684, train_f1: 0.1301, val_acc: 0.374384, val_recall: 0.210472, val_f1: 0.164881
Epoch: 117, loss: 22.8911, train_acc: 0.2617, train_recall: 0.1684, train_f1: 0.1166, val_acc: 0.238916, val_recall: 0.210965, val_f1: 0.138549
Epoch: 118, loss: 23.0862, train_acc: 0.2617, train_recall: 0.1900, train_f1: 0.1204, val_acc: 0.214286, val_recall: 0.187327, val_f1: 0.098479
Epoch: 119, loss: 23.4372, train_acc: 0.3464, train_recall: 0.1875, train_f1: 0.1283, val_acc: 0.320197, val_recall: 0.188932, val_f1: 0.116010
Epoch: 120, loss: 23.1077, train_acc: 0.3199, train_recall: 0.1907, train_f1: 0.1329, val_acc: 0.366995, val_recall: 0.192383, val_f1: 0.130548
Epoch: 121, loss: 23.1932, train_acc: 0.3157, train_recall: 0.1909, train_f1: 0.1435, val_acc: 0.366995, val_recall: 0.213751, val_f1: 0.158759
Epoch: 122, loss: 22.8539, train_acc: 0.2564, train_recall: 0.1892, train_f1: 0.1258, val_acc: 0.214286, val_recall: 0.197147, val_f1: 0.103129
Epoch: 123, loss: 23.0263, train_acc: 0.3432, train_recall: 0.1786, train_f1: 0.1356, val_acc: 0.325123, val_recall: 0.187987, val_f1: 0.130450
Epoch: 124, loss: 22.8996, train_acc: 0.1102, train_recall: 0.1694, train_f1: 0.0936, val_acc: 0.096059, val_recall: 0.217854, val_f1: 0.113818
Epoch: 125, loss: 22.9370, train_acc: 0.3231, train_recall: 0.1892, train_f1: 0.1429, val_acc: 0.364532, val_recall: 0.171450, val_f1: 0.125728
Epoch: 126, loss: 22.7797, train_acc: 0.3199, train_recall: 0.1905, train_f1: 0.1333, val_acc: 0.362069, val_recall: 0.190610, val_f1: 0.129682
Epoch: 127, loss: 22.9513, train_acc: 0.3443, train_recall: 0.1865, train_f1: 0.1263, val_acc: 0.317734, val_recall: 0.187389, val_f1: 0.112998
Epoch: 128, loss: 22.8226, train_acc: 0.3475, train_recall: 0.1878, train_f1: 0.1288, val_acc: 0.320197, val_recall: 0.188932, val_f1: 0.115042
Epoch: 129, loss: 22.8203, train_acc: 0.3220, train_recall: 0.1921, train_f1: 0.1516, val_acc: 0.366995, val_recall: 0.183722, val_f1: 0.142534
Epoch: 130, loss: 22.6654, train_acc: 0.3157, train_recall: 0.1667, train_f1: 0.1277, val_acc: 0.379310, val_recall: 0.211832, val_f1: 0.165633
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed2.csv
Epoch: 131, loss: 22.7318, train_acc: 0.3411, train_recall: 0.1834, train_f1: 0.1407, val_acc: 0.325123, val_recall: 0.202341, val_f1: 0.144116
Epoch: 132, loss: 22.7315, train_acc: 0.3400, train_recall: 0.1897, train_f1: 0.1428, val_acc: 0.325123, val_recall: 0.206875, val_f1: 0.133712
Epoch: 133, loss: 22.6048, train_acc: 0.3189, train_recall: 0.1968, train_f1: 0.1568, val_acc: 0.362069, val_recall: 0.206780, val_f1: 0.168642
Epoch: 134, loss: 22.7197, train_acc: 0.3178, train_recall: 0.1900, train_f1: 0.1300, val_acc: 0.362069, val_recall: 0.190489, val_f1: 0.128049
Epoch: 135, loss: 22.6238, train_acc: 0.3496, train_recall: 0.1898, train_f1: 0.1384, val_acc: 0.327586, val_recall: 0.192784, val_f1: 0.124430
Epoch: 136, loss: 22.6469, train_acc: 0.3453, train_recall: 0.1881, train_f1: 0.1341, val_acc: 0.322660, val_recall: 0.191011, val_f1: 0.120490
Epoch: 137, loss: 22.5844, train_acc: 0.3189, train_recall: 0.1869, train_f1: 0.1481, val_acc: 0.362069, val_recall: 0.194655, val_f1: 0.150840
Epoch: 138, loss: 22.6564, train_acc: 0.3125, train_recall: 0.1662, train_f1: 0.1259, val_acc: 0.376847, val_recall: 0.211359, val_f1: 0.164074
Epoch: 139, loss: 22.5463, train_acc: 0.3432, train_recall: 0.1856, train_f1: 0.1533, val_acc: 0.339901, val_recall: 0.219864, val_f1: 0.182610
Epoch: 140, loss: 22.6069, train_acc: 0.3400, train_recall: 0.1875, train_f1: 0.1442, val_acc: 0.325123, val_recall: 0.202374, val_f1: 0.128425
Epoch: 141, loss: 22.5613, train_acc: 0.3114, train_recall: 0.1943, train_f1: 0.1563, val_acc: 0.344828, val_recall: 0.203614, val_f1: 0.150340
Epoch: 142, loss: 22.5688, train_acc: 0.3146, train_recall: 0.1953, train_f1: 0.1529, val_acc: 0.366995, val_recall: 0.208966, val_f1: 0.168488
Epoch: 143, loss: 22.5264, train_acc: 0.3506, train_recall: 0.1935, train_f1: 0.1461, val_acc: 0.332512, val_recall: 0.200128, val_f1: 0.138796
Epoch: 144, loss: 22.5605, train_acc: 0.3464, train_recall: 0.1908, train_f1: 0.1348, val_acc: 0.325123, val_recall: 0.197054, val_f1: 0.129172
Epoch: 145, loss: 22.4981, train_acc: 0.3157, train_recall: 0.1862, train_f1: 0.1462, val_acc: 0.369458, val_recall: 0.195936, val_f1: 0.163008
Epoch: 146, loss: 22.5194, train_acc: 0.3136, train_recall: 0.1861, train_f1: 0.1509, val_acc: 0.371921, val_recall: 0.216926, val_f1: 0.184909
Epoch: 147, loss: 22.4693, train_acc: 0.3485, train_recall: 0.1842, train_f1: 0.1513, val_acc: 0.339901, val_recall: 0.214050, val_f1: 0.177770
Epoch: 148, loss: 22.5076, train_acc: 0.3464, train_recall: 0.1824, train_f1: 0.1411, val_acc: 0.332512, val_recall: 0.210442, val_f1: 0.165795
Epoch: 149, loss: 22.4659, train_acc: 0.3199, train_recall: 0.1890, train_f1: 0.1498, val_acc: 0.366995, val_recall: 0.215153, val_f1: 0.183613
Epoch: 150, loss: 22.4968, train_acc: 0.3167, train_recall: 0.1927, train_f1: 0.1345, val_acc: 0.357143, val_recall: 0.194287, val_f1: 0.134443
Epoch: 151, loss: 22.4476, train_acc: 0.3506, train_recall: 0.1927, train_f1: 0.1413, val_acc: 0.332512, val_recall: 0.199592, val_f1: 0.136287
Epoch: 152, loss: 22.4737, train_acc: 0.3517, train_recall: 0.1929, train_f1: 0.1392, val_acc: 0.334975, val_recall: 0.200601, val_f1: 0.136471
Epoch: 153, loss: 22.4421, train_acc: 0.3210, train_recall: 0.1942, train_f1: 0.1400, val_acc: 0.366995, val_recall: 0.198197, val_f1: 0.142862
Epoch: 154, loss: 22.4461, train_acc: 0.3178, train_recall: 0.1855, train_f1: 0.1328, val_acc: 0.359606, val_recall: 0.201773, val_f1: 0.155867
Epoch: 155, loss: 22.4319, train_acc: 0.3485, train_recall: 0.1985, train_f1: 0.1708, val_acc: 0.337438, val_recall: 0.198145, val_f1: 0.161242
Epoch: 156, loss: 22.4224, train_acc: 0.3475, train_recall: 0.1912, train_f1: 0.1645, val_acc: 0.339901, val_recall: 0.200726, val_f1: 0.154907
Epoch: 157, loss: 22.4116, train_acc: 0.3136, train_recall: 0.1835, train_f1: 0.1291, val_acc: 0.366995, val_recall: 0.204554, val_f1: 0.157594
Epoch: 158, loss: 22.4043, train_acc: 0.3189, train_recall: 0.1941, train_f1: 0.1377, val_acc: 0.354680, val_recall: 0.193400, val_f1: 0.134259
Epoch: 159, loss: 22.4165, train_acc: 0.3485, train_recall: 0.1927, train_f1: 0.1441, val_acc: 0.332512, val_recall: 0.200128, val_f1: 0.137611
Epoch: 160, loss: 22.3995, train_acc: 0.3475, train_recall: 0.1923, train_f1: 0.1437, val_acc: 0.330049, val_recall: 0.199120, val_f1: 0.136321
Epoch: 161, loss: 22.3825, train_acc: 0.3146, train_recall: 0.1920, train_f1: 0.1326, val_acc: 0.359606, val_recall: 0.195173, val_f1: 0.134233
Epoch: 162, loss: 22.3747, train_acc: 0.3157, train_recall: 0.1841, train_f1: 0.1322, val_acc: 0.366995, val_recall: 0.204919, val_f1: 0.163738
Epoch: 163, loss: 22.3727, train_acc: 0.3485, train_recall: 0.1844, train_f1: 0.1391, val_acc: 0.339901, val_recall: 0.209509, val_f1: 0.161807
Epoch: 164, loss: 22.3713, train_acc: 0.3485, train_recall: 0.1813, train_f1: 0.1354, val_acc: 0.337438, val_recall: 0.202808, val_f1: 0.154078
Epoch: 165, loss: 22.3428, train_acc: 0.3210, train_recall: 0.1861, train_f1: 0.1356, val_acc: 0.369458, val_recall: 0.205562, val_f1: 0.162151
Epoch: 166, loss: 22.3433, train_acc: 0.3443, train_recall: 0.1986, train_f1: 0.1721, val_acc: 0.359606, val_recall: 0.200521, val_f1: 0.173487
Epoch: 167, loss: 22.3589, train_acc: 0.3506, train_recall: 0.1933, train_f1: 0.1448, val_acc: 0.334975, val_recall: 0.201136, val_f1: 0.138954
Epoch: 168, loss: 22.3176, train_acc: 0.3517, train_recall: 0.1940, train_f1: 0.1485, val_acc: 0.334975, val_recall: 0.200771, val_f1: 0.142997
Epoch: 169, loss: 22.3325, train_acc: 0.3220, train_recall: 0.1946, train_f1: 0.1394, val_acc: 0.364532, val_recall: 0.197189, val_f1: 0.139981
Epoch: 170, loss: 22.3132, train_acc: 0.3538, train_recall: 0.1946, train_f1: 0.1490, val_acc: 0.332512, val_recall: 0.200006, val_f1: 0.140084
Epoch: 171, loss: 22.3181, train_acc: 0.3485, train_recall: 0.1893, train_f1: 0.1384, val_acc: 0.332512, val_recall: 0.194557, val_f1: 0.128007
Epoch: 172, loss: 22.3155, train_acc: 0.3220, train_recall: 0.1941, train_f1: 0.1465, val_acc: 0.364532, val_recall: 0.198162, val_f1: 0.152363
Epoch: 173, loss: 22.2946, train_acc: 0.3210, train_recall: 0.1860, train_f1: 0.1360, val_acc: 0.366995, val_recall: 0.204797, val_f1: 0.163407
Epoch: 174, loss: 22.2933, train_acc: 0.3496, train_recall: 0.1849, train_f1: 0.1411, val_acc: 0.342365, val_recall: 0.210395, val_f1: 0.162248
Epoch: 175, loss: 22.2919, train_acc: 0.3496, train_recall: 0.1849, train_f1: 0.1411, val_acc: 0.339901, val_recall: 0.209509, val_f1: 0.160428
Epoch: 176, loss: 22.2815, train_acc: 0.3231, train_recall: 0.1950, train_f1: 0.1421, val_acc: 0.364532, val_recall: 0.197554, val_f1: 0.145614
Epoch: 177, loss: 22.2743, train_acc: 0.3210, train_recall: 0.1937, train_f1: 0.1450, val_acc: 0.362069, val_recall: 0.197153, val_f1: 0.150688
Epoch: 178, loss: 22.2680, train_acc: 0.3517, train_recall: 0.1938, train_f1: 0.1468, val_acc: 0.334975, val_recall: 0.201136, val_f1: 0.139114
Epoch: 179, loss: 22.2600, train_acc: 0.3538, train_recall: 0.1947, train_f1: 0.1489, val_acc: 0.330049, val_recall: 0.198998, val_f1: 0.138699
Epoch: 180, loss: 22.2798, train_acc: 0.3242, train_recall: 0.1870, train_f1: 0.1389, val_acc: 0.371921, val_recall: 0.206935, val_f1: 0.167336
Epoch: 181, loss: 22.2421, train_acc: 0.3496, train_recall: 0.1850, train_f1: 0.1432, val_acc: 0.332512, val_recall: 0.206242, val_f1: 0.160228
Epoch: 182, loss: 22.2408, train_acc: 0.3485, train_recall: 0.1813, train_f1: 0.1354, val_acc: 0.337438, val_recall: 0.202808, val_f1: 0.154078
Epoch: 183, loss: 22.2350, train_acc: 0.3496, train_recall: 0.1852, train_f1: 0.1452, val_acc: 0.334975, val_recall: 0.207128, val_f1: 0.161982
Epoch: 184, loss: 22.2358, train_acc: 0.3199, train_recall: 0.1929, train_f1: 0.1468, val_acc: 0.357143, val_recall: 0.195745, val_f1: 0.152390
Epoch: 185, loss: 22.2170, train_acc: 0.3517, train_recall: 0.1938, train_f1: 0.1462, val_acc: 0.332512, val_recall: 0.200006, val_f1: 0.139121
Epoch: 186, loss: 22.2170, train_acc: 0.3517, train_recall: 0.1938, train_f1: 0.1467, val_acc: 0.332512, val_recall: 0.199885, val_f1: 0.140373
Epoch: 187, loss: 22.2111, train_acc: 0.3432, train_recall: 0.1981, train_f1: 0.1699, val_acc: 0.357143, val_recall: 0.199513, val_f1: 0.171586
Epoch: 188, loss: 22.1987, train_acc: 0.3506, train_recall: 0.1854, train_f1: 0.1442, val_acc: 0.332512, val_recall: 0.206242, val_f1: 0.160260
Epoch: 189, loss: 22.1918, train_acc: 0.3506, train_recall: 0.1853, train_f1: 0.1425, val_acc: 0.334975, val_recall: 0.207371, val_f1: 0.159400
Epoch: 190, loss: 22.1986, train_acc: 0.3496, train_recall: 0.1851, train_f1: 0.1441, val_acc: 0.334975, val_recall: 0.207128, val_f1: 0.161927
Epoch: 191, loss: 22.1946, train_acc: 0.3485, train_recall: 0.1849, train_f1: 0.1452, val_acc: 0.332512, val_recall: 0.206120, val_f1: 0.161496
Epoch: 192, loss: 22.1704, train_acc: 0.3517, train_recall: 0.1938, train_f1: 0.1472, val_acc: 0.332512, val_recall: 0.199885, val_f1: 0.140342
Epoch: 193, loss: 22.1808, train_acc: 0.3538, train_recall: 0.1947, train_f1: 0.1496, val_acc: 0.330049, val_recall: 0.198876, val_f1: 0.139889
Epoch: 194, loss: 22.1496, train_acc: 0.3506, train_recall: 0.1938, train_f1: 0.1515, val_acc: 0.327586, val_recall: 0.197747, val_f1: 0.140846
Epoch: 195, loss: 22.1496, train_acc: 0.3538, train_recall: 0.1947, train_f1: 0.1502, val_acc: 0.327586, val_recall: 0.197868, val_f1: 0.139497
Epoch: 196, loss: 22.1365, train_acc: 0.3506, train_recall: 0.1853, train_f1: 0.1425, val_acc: 0.337438, val_recall: 0.208258, val_f1: 0.161106
Epoch: 197, loss: 22.1205, train_acc: 0.3506, train_recall: 0.1855, train_f1: 0.1448, val_acc: 0.334975, val_recall: 0.207128, val_f1: 0.161927
Epoch: 198, loss: 22.1380, train_acc: 0.3506, train_recall: 0.1855, train_f1: 0.1448, val_acc: 0.334975, val_recall: 0.207128, val_f1: 0.161927
Epoch: 199, loss: 22.1012, train_acc: 0.3528, train_recall: 0.1942, train_f1: 0.1483, val_acc: 0.332512, val_recall: 0.199885, val_f1: 0.140406
Epoch: 200, loss: 22.1054, train_acc: 0.3528, train_recall: 0.1943, train_f1: 0.1500, val_acc: 0.327586, val_recall: 0.197868, val_f1: 0.139497
Epoch: 201, loss: 22.0817, train_acc: 0.3528, train_recall: 0.1944, train_f1: 0.1511, val_acc: 0.330049, val_recall: 0.198755, val_f1: 0.141334
Epoch: 202, loss: 22.0823, train_acc: 0.3528, train_recall: 0.1943, train_f1: 0.1494, val_acc: 0.327586, val_recall: 0.197868, val_f1: 0.139497
Epoch: 203, loss: 22.0791, train_acc: 0.3549, train_recall: 0.1950, train_f1: 0.1498, val_acc: 0.332512, val_recall: 0.199885, val_f1: 0.140406
Epoch: 204, loss: 22.0974, train_acc: 0.3528, train_recall: 0.1944, train_f1: 0.1505, val_acc: 0.330049, val_recall: 0.198755, val_f1: 0.141164
Epoch: 205, loss: 22.0780, train_acc: 0.3496, train_recall: 0.1851, train_f1: 0.1441, val_acc: 0.334975, val_recall: 0.207128, val_f1: 0.161927
Epoch: 206, loss: 22.0467, train_acc: 0.3528, train_recall: 0.1861, train_f1: 0.1440, val_acc: 0.337438, val_recall: 0.208258, val_f1: 0.161169
Epoch: 207, loss: 22.0436, train_acc: 0.3475, train_recall: 0.1845, train_f1: 0.1444, val_acc: 0.330049, val_recall: 0.205234, val_f1: 0.159775
Epoch: 208, loss: 22.0467, train_acc: 0.3496, train_recall: 0.1934, train_f1: 0.1502, val_acc: 0.327586, val_recall: 0.197747, val_f1: 0.140678
Epoch: 209, loss: 22.0358, train_acc: 0.3538, train_recall: 0.1947, train_f1: 0.1496, val_acc: 0.330049, val_recall: 0.198876, val_f1: 0.139952
Epoch: 210, loss: 22.0212, train_acc: 0.3506, train_recall: 0.1937, train_f1: 0.1498, val_acc: 0.325123, val_recall: 0.196860, val_f1: 0.139012
Epoch: 211, loss: 22.0100, train_acc: 0.3496, train_recall: 0.1934, train_f1: 0.1502, val_acc: 0.327586, val_recall: 0.197747, val_f1: 0.140678
Epoch: 212, loss: 22.0245, train_acc: 0.3517, train_recall: 0.1858, train_f1: 0.1444, val_acc: 0.332512, val_recall: 0.206242, val_f1: 0.160260
Epoch: 213, loss: 21.9896, train_acc: 0.3506, train_recall: 0.1854, train_f1: 0.1437, val_acc: 0.332512, val_recall: 0.206242, val_f1: 0.160260
Epoch: 214, loss: 21.9906, train_acc: 0.3496, train_recall: 0.1934, train_f1: 0.1502, val_acc: 0.327586, val_recall: 0.197747, val_f1: 0.140678
Epoch: 215, loss: 21.9676, train_acc: 0.3517, train_recall: 0.1940, train_f1: 0.1498, val_acc: 0.327586, val_recall: 0.197868, val_f1: 0.139497
Epoch: 216, loss: 21.9711, train_acc: 0.3528, train_recall: 0.1943, train_f1: 0.1494, val_acc: 0.327586, val_recall: 0.197868, val_f1: 0.139497
Epoch: 217, loss: 21.9708, train_acc: 0.3485, train_recall: 0.1930, train_f1: 0.1490, val_acc: 0.325123, val_recall: 0.196860, val_f1: 0.139012
Epoch: 218, loss: 21.9519, train_acc: 0.3485, train_recall: 0.1930, train_f1: 0.1490, val_acc: 0.325123, val_recall: 0.196860, val_f1: 0.139012
Epoch: 219, loss: 21.9430, train_acc: 0.3506, train_recall: 0.1854, train_f1: 0.1437, val_acc: 0.332512, val_recall: 0.206242, val_f1: 0.160260
Epoch: 220, loss: 21.9631, train_acc: 0.3496, train_recall: 0.1934, train_f1: 0.1502, val_acc: 0.325123, val_recall: 0.196860, val_f1: 0.139012
Epoch: 221, loss: 21.9411, train_acc: 0.3485, train_recall: 0.1930, train_f1: 0.1490, val_acc: 0.325123, val_recall: 0.196860, val_f1: 0.139012
Epoch: 222, loss: 21.9623, train_acc: 0.3496, train_recall: 0.1934, train_f1: 0.1486, val_acc: 0.325123, val_recall: 0.196860, val_f1: 0.139012
Epoch: 223, loss: 21.9152, train_acc: 0.3485, train_recall: 0.1930, train_f1: 0.1490, val_acc: 0.325123, val_recall: 0.196860, val_f1: 0.139012
Epoch: 224, loss: 21.9221, train_acc: 0.3485, train_recall: 0.1930, train_f1: 0.1490, val_acc: 0.325123, val_recall: 0.196860, val_f1: 0.139012
Epoch: 225, loss: 21.8992, train_acc: 0.3496, train_recall: 0.1934, train_f1: 0.1486, val_acc: 0.325123, val_recall: 0.196860, val_f1: 0.139012
Epoch: 226, loss: 21.8927, train_acc: 0.3485, train_recall: 0.1930, train_f1: 0.1490, val_acc: 0.325123, val_recall: 0.196860, val_f1: 0.139012
Epoch: 227, loss: 21.8970, train_acc: 0.3464, train_recall: 0.1841, train_f1: 0.1433, val_acc: 0.330049, val_recall: 0.205234, val_f1: 0.159775
Epoch: 228, loss: 21.8930, train_acc: 0.3485, train_recall: 0.1849, train_f1: 0.1437, val_acc: 0.330049, val_recall: 0.205234, val_f1: 0.159775
Epoch: 229, loss: 21.8518, train_acc: 0.3485, train_recall: 0.1930, train_f1: 0.1490, val_acc: 0.325123, val_recall: 0.196860, val_f1: 0.139012
Epoch: 230, loss: 21.8555, train_acc: 0.3485, train_recall: 0.1930, train_f1: 0.1490, val_acc: 0.325123, val_recall: 0.196860, val_f1: 0.139012
Epoch: 231, loss: 21.8533, train_acc: 0.3506, train_recall: 0.1938, train_f1: 0.1494, val_acc: 0.325123, val_recall: 0.196860, val_f1: 0.139012
Epoch: 232, loss: 21.8521, train_acc: 0.3496, train_recall: 0.1934, train_f1: 0.1492, val_acc: 0.325123, val_recall: 0.196860, val_f1: 0.139012
Epoch: 233, loss: 21.8232, train_acc: 0.3485, train_recall: 0.1930, train_f1: 0.1490, val_acc: 0.325123, val_recall: 0.196860, val_f1: 0.139012
Epoch: 234, loss: 21.8064, train_acc: 0.3485, train_recall: 0.1849, train_f1: 0.1437, val_acc: 0.330049, val_recall: 0.205234, val_f1: 0.159775
Epoch: 235, loss: 21.8122, train_acc: 0.3475, train_recall: 0.1845, train_f1: 0.1435, val_acc: 0.330049, val_recall: 0.205234, val_f1: 0.159775
Epoch: 236, loss: 21.8159, train_acc: 0.3485, train_recall: 0.1930, train_f1: 0.1490, val_acc: 0.325123, val_recall: 0.196860, val_f1: 0.139012
Epoch: 237, loss: 21.7862, train_acc: 0.3496, train_recall: 0.1934, train_f1: 0.1492, val_acc: 0.325123, val_recall: 0.196860, val_f1: 0.139012
Epoch: 238, loss: 21.7947, train_acc: 0.3496, train_recall: 0.1934, train_f1: 0.1492, val_acc: 0.325123, val_recall: 0.196860, val_f1: 0.139012
Epoch: 239, loss: 21.7958, train_acc: 0.3475, train_recall: 0.1925, train_f1: 0.1481, val_acc: 0.322660, val_recall: 0.164886, val_f1: 0.118474
Epoch: 240, loss: 21.7969, train_acc: 0.3485, train_recall: 0.1930, train_f1: 0.1487, val_acc: 0.325123, val_recall: 0.196860, val_f1: 0.139012
Epoch: 241, loss: 21.7714, train_acc: 0.3475, train_recall: 0.1927, train_f1: 0.1485, val_acc: 0.325123, val_recall: 0.196860, val_f1: 0.139012
Epoch: 242, loss: 21.7911, train_acc: 0.3475, train_recall: 0.1927, train_f1: 0.1485, val_acc: 0.325123, val_recall: 0.196860, val_f1: 0.139012
Epoch: 243, loss: 21.7654, train_acc: 0.3475, train_recall: 0.1845, train_f1: 0.1442, val_acc: 0.330049, val_recall: 0.205234, val_f1: 0.159775
Epoch: 244, loss: 21.7535, train_acc: 0.3464, train_recall: 0.1841, train_f1: 0.1440, val_acc: 0.330049, val_recall: 0.205234, val_f1: 0.159775
Epoch: 245, loss: 21.7398, train_acc: 0.3453, train_recall: 0.1838, train_f1: 0.1428, val_acc: 0.330049, val_recall: 0.205234, val_f1: 0.159775
Epoch: 246, loss: 21.7327, train_acc: 0.3485, train_recall: 0.1930, train_f1: 0.1487, val_acc: 0.325123, val_recall: 0.196860, val_f1: 0.139012
Epoch: 247, loss: 21.7259, train_acc: 0.3475, train_recall: 0.1927, train_f1: 0.1485, val_acc: 0.325123, val_recall: 0.196860, val_f1: 0.139012
Epoch: 248, loss: 21.7522, train_acc: 0.3485, train_recall: 0.1930, train_f1: 0.1487, val_acc: 0.325123, val_recall: 0.196860, val_f1: 0.139012
Epoch: 249, loss: 21.7373, train_acc: 0.3496, train_recall: 0.1934, train_f1: 0.1492, val_acc: 0.325123, val_recall: 0.196860, val_f1: 0.139012
Epoch: 250, loss: 21.7234, train_acc: 0.3464, train_recall: 0.1841, train_f1: 0.1433, val_acc: 0.330049, val_recall: 0.205234, val_f1: 0.159775
Epoch: 251, loss: 21.7163, train_acc: 0.3475, train_recall: 0.1845, train_f1: 0.1435, val_acc: 0.330049, val_recall: 0.205234, val_f1: 0.159775
Epoch: 252, loss: 21.7143, train_acc: 0.3464, train_recall: 0.1841, train_f1: 0.1433, val_acc: 0.330049, val_recall: 0.205234, val_f1: 0.159775
Epoch: 253, loss: 21.7051, train_acc: 0.3485, train_recall: 0.1930, train_f1: 0.1490, val_acc: 0.325123, val_recall: 0.196860, val_f1: 0.139012
Epoch: 254, loss: 21.6944, train_acc: 0.3496, train_recall: 0.1934, train_f1: 0.1492, val_acc: 0.325123, val_recall: 0.196860, val_f1: 0.139012
Epoch: 255, loss: 21.6850, train_acc: 0.3485, train_recall: 0.1930, train_f1: 0.1490, val_acc: 0.325123, val_recall: 0.196860, val_f1: 0.139012
Epoch: 256, loss: 21.6850, train_acc: 0.3475, train_recall: 0.1845, train_f1: 0.1435, val_acc: 0.330049, val_recall: 0.205234, val_f1: 0.159775
Epoch: 257, loss: 21.6828, train_acc: 0.3475, train_recall: 0.1845, train_f1: 0.1435, val_acc: 0.330049, val_recall: 0.205234, val_f1: 0.159775
Epoch: 258, loss: 21.6571, train_acc: 0.3464, train_recall: 0.1841, train_f1: 0.1436, val_acc: 0.330049, val_recall: 0.205234, val_f1: 0.161225
Epoch: 259, loss: 21.6524, train_acc: 0.3475, train_recall: 0.1845, train_f1: 0.1435, val_acc: 0.330049, val_recall: 0.205234, val_f1: 0.159775
Epoch: 260, loss: 21.6642, train_acc: 0.3496, train_recall: 0.1934, train_f1: 0.1496, val_acc: 0.325123, val_recall: 0.196860, val_f1: 0.140000
Epoch: 261, loss: 21.6497, train_acc: 0.3496, train_recall: 0.1934, train_f1: 0.1496, val_acc: 0.325123, val_recall: 0.196860, val_f1: 0.140000
Epoch: 262, loss: 21.6392, train_acc: 0.3496, train_recall: 0.1934, train_f1: 0.1496, val_acc: 0.325123, val_recall: 0.196860, val_f1: 0.140000
Epoch: 263, loss: 21.6419, train_acc: 0.3464, train_recall: 0.1841, train_f1: 0.1436, val_acc: 0.330049, val_recall: 0.205234, val_f1: 0.161315
Epoch: 264, loss: 21.6433, train_acc: 0.3475, train_recall: 0.1845, train_f1: 0.1438, val_acc: 0.330049, val_recall: 0.205234, val_f1: 0.161315
Epoch: 265, loss: 21.6188, train_acc: 0.3475, train_recall: 0.1845, train_f1: 0.1438, val_acc: 0.330049, val_recall: 0.205234, val_f1: 0.161315
Epoch: 266, loss: 21.6170, train_acc: 0.3475, train_recall: 0.1845, train_f1: 0.1438, val_acc: 0.330049, val_recall: 0.205234, val_f1: 0.161315
Epoch: 267, loss: 21.6215, train_acc: 0.3496, train_recall: 0.1934, train_f1: 0.1496, val_acc: 0.325123, val_recall: 0.196860, val_f1: 0.140000
Epoch: 268, loss: 21.6137, train_acc: 0.3464, train_recall: 0.1841, train_f1: 0.1436, val_acc: 0.330049, val_recall: 0.205234, val_f1: 0.161315
Epoch: 269, loss: 21.5801, train_acc: 0.3475, train_recall: 0.1845, train_f1: 0.1438, val_acc: 0.330049, val_recall: 0.205234, val_f1: 0.161315
Epoch: 270, loss: 21.5687, train_acc: 0.3475, train_recall: 0.1845, train_f1: 0.1438, val_acc: 0.330049, val_recall: 0.205234, val_f1: 0.161315
Epoch: 271, loss: 21.5745, train_acc: 0.3475, train_recall: 0.1845, train_f1: 0.1438, val_acc: 0.330049, val_recall: 0.205234, val_f1: 0.161315
Epoch: 272, loss: 21.5808, train_acc: 0.3475, train_recall: 0.1845, train_f1: 0.1438, val_acc: 0.330049, val_recall: 0.205234, val_f1: 0.161315
Epoch: 273, loss: 21.5638, train_acc: 0.3464, train_recall: 0.1841, train_f1: 0.1436, val_acc: 0.330049, val_recall: 0.205234, val_f1: 0.161315
/home/ADS/cyang314/ucr_work/HINI_Baseline/GraphLoRA/model/GraphLoRA.py:218: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.
Consider using tensor.detach() first. (Triggered internally at /pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:836.)
  f.write(f'{pre_dataset} to {downstream_dataset}: seed: %d, epoch: %d, train_loss: %f, train_acc: %f, train_recall: %f, train_f1: %f, val_acc: %f, val_recall: %f, val_f1: %f\n' %
Epoch: 274, loss: 21.5630, train_acc: 0.3475, train_recall: 0.1845, train_f1: 0.1438, val_acc: 0.330049, val_recall: 0.205234, val_f1: 0.161315
Epoch: 275, loss: 21.5139, train_acc: 0.3475, train_recall: 0.1845, train_f1: 0.1438, val_acc: 0.330049, val_recall: 0.205234, val_f1: 0.161315
Epoch: 276, loss: 21.5403, train_acc: 0.3475, train_recall: 0.1845, train_f1: 0.1438, val_acc: 0.330049, val_recall: 0.205234, val_f1: 0.161315
Epoch: 277, loss: 21.5497, train_acc: 0.3475, train_recall: 0.1845, train_f1: 0.1438, val_acc: 0.330049, val_recall: 0.205234, val_f1: 0.161315
Epoch: 278, loss: 21.5267, train_acc: 0.3475, train_recall: 0.1845, train_f1: 0.1438, val_acc: 0.330049, val_recall: 0.205234, val_f1: 0.161315
Epoch: 279, loss: 21.5303, train_acc: 0.3475, train_recall: 0.1845, train_f1: 0.1438, val_acc: 0.330049, val_recall: 0.205234, val_f1: 0.161315
Epoch: 280, loss: 21.5184, train_acc: 0.3475, train_recall: 0.1845, train_f1: 0.1438, val_acc: 0.330049, val_recall: 0.205234, val_f1: 0.161315
Epoch: 281, loss: 21.5157, train_acc: 0.3475, train_recall: 0.1845, train_f1: 0.1438, val_acc: 0.330049, val_recall: 0.205234, val_f1: 0.161315
Epoch: 282, loss: 21.5044, train_acc: 0.3475, train_recall: 0.1845, train_f1: 0.1438, val_acc: 0.330049, val_recall: 0.205234, val_f1: 0.161315
Epoch: 283, loss: 21.4780, train_acc: 0.3475, train_recall: 0.1845, train_f1: 0.1438, val_acc: 0.330049, val_recall: 0.205234, val_f1: 0.161315
Epoch: 284, loss: 21.4769, train_acc: 0.3496, train_recall: 0.1858, train_f1: 0.1470, val_acc: 0.327586, val_recall: 0.204225, val_f1: 0.160830
Epoch: 285, loss: 21.4763, train_acc: 0.3475, train_recall: 0.1845, train_f1: 0.1438, val_acc: 0.330049, val_recall: 0.205234, val_f1: 0.161315
Epoch: 286, loss: 21.4584, train_acc: 0.3496, train_recall: 0.1858, train_f1: 0.1470, val_acc: 0.327586, val_recall: 0.204225, val_f1: 0.160830
Epoch: 287, loss: 21.4482, train_acc: 0.3506, train_recall: 0.1862, train_f1: 0.1472, val_acc: 0.327586, val_recall: 0.204225, val_f1: 0.160830
Epoch: 288, loss: 21.4569, train_acc: 0.3496, train_recall: 0.1858, train_f1: 0.1470, val_acc: 0.327586, val_recall: 0.204225, val_f1: 0.160957
Epoch: 289, loss: 21.4518, train_acc: 0.3528, train_recall: 0.1868, train_f1: 0.1469, val_acc: 0.330049, val_recall: 0.205234, val_f1: 0.161281
Epoch: 290, loss: 21.4435, train_acc: 0.3496, train_recall: 0.1858, train_f1: 0.1471, val_acc: 0.327586, val_recall: 0.204225, val_f1: 0.160957
Epoch: 291, loss: 21.4185, train_acc: 0.3485, train_recall: 0.1847, train_f1: 0.1427, val_acc: 0.332512, val_recall: 0.206242, val_f1: 0.161768
Epoch: 292, loss: 21.4232, train_acc: 0.3496, train_recall: 0.1858, train_f1: 0.1471, val_acc: 0.330049, val_recall: 0.205112, val_f1: 0.162605
Epoch: 293, loss: 21.4138, train_acc: 0.3506, train_recall: 0.1854, train_f1: 0.1434, val_acc: 0.334975, val_recall: 0.207250, val_f1: 0.162222
Epoch: 294, loss: 21.3943, train_acc: 0.3496, train_recall: 0.1859, train_f1: 0.1485, val_acc: 0.330049, val_recall: 0.205112, val_f1: 0.162662
Epoch: 295, loss: 21.3748, train_acc: 0.3506, train_recall: 0.1854, train_f1: 0.1428, val_acc: 0.337438, val_recall: 0.208258, val_f1: 0.162675
Epoch: 296, loss: 21.4152, train_acc: 0.3231, train_recall: 0.1874, train_f1: 0.1385, val_acc: 0.371921, val_recall: 0.207349, val_f1: 0.168242
Epoch: 297, loss: 21.3824, train_acc: 0.3464, train_recall: 0.1817, train_f1: 0.1362, val_acc: 0.332512, val_recall: 0.200914, val_f1: 0.151704
Epoch: 298, loss: 21.4014, train_acc: 0.3220, train_recall: 0.1875, train_f1: 0.1362, val_acc: 0.364532, val_recall: 0.204203, val_f1: 0.160202
Epoch: 299, loss: 21.4506, train_acc: 0.3506, train_recall: 0.1810, train_f1: 0.1284, val_acc: 0.325123, val_recall: 0.197184, val_f1: 0.141211
epoch: 131, train_acc: 0.315678, val_acc: 0.379310, val_recall: 0.211832, val_f1: 0.165633
Running: year=2015 â†’ downstream_year=2016, seed=3
Random seed set to 42

==============================
PRE-TRAINING
==============================
create PreTrain instance...
pre-training...
(T) | Epoch=001, loss=7.3009, this epoch 0.0463, total 0.0463
+++model saved ! 2015.pth
(T) | Epoch=002, loss=6.3445, this epoch 0.0327, total 0.0790
+++model saved ! 2015.pth
(T) | Epoch=003, loss=6.3445, this epoch 0.0352, total 0.1142
(T) | Epoch=004, loss=6.3444, this epoch 0.0233, total 0.1375
+++model saved ! 2015.pth
(T) | Epoch=005, loss=6.3451, this epoch 0.0226, total 0.1601
(T) | Epoch=006, loss=6.3445, this epoch 0.0276, total 0.1877
(T) | Epoch=007, loss=6.3472, this epoch 0.0280, total 0.2157
(T) | Epoch=008, loss=6.8267, this epoch 0.0227, total 0.2383
(T) | Epoch=009, loss=6.3447, this epoch 0.0276, total 0.2659
(T) | Epoch=010, loss=6.3446, this epoch 0.0283, total 0.2942
(T) | Epoch=011, loss=6.7118, this epoch 0.0224, total 0.3166
(T) | Epoch=012, loss=6.3444, this epoch 0.0266, total 0.3432
(T) | Epoch=013, loss=6.3447, this epoch 0.0279, total 0.3711
(T) | Epoch=014, loss=6.3439, this epoch 0.0299, total 0.4010
+++model saved ! 2015.pth
(T) | Epoch=015, loss=6.3438, this epoch 0.0373, total 0.4383
+++model saved ! 2015.pth
(T) | Epoch=016, loss=6.3444, this epoch 0.0372, total 0.4755
(T) | Epoch=017, loss=6.3436, this epoch 0.0363, total 0.5118
+++model saved ! 2015.pth
(T) | Epoch=018, loss=6.3435, this epoch 0.0371, total 0.5490
+++model saved ! 2015.pth
(T) | Epoch=019, loss=6.5342, this epoch 0.0313, total 0.5803
(T) | Epoch=020, loss=6.3434, this epoch 0.0290, total 0.6093
+++model saved ! 2015.pth
(T) | Epoch=021, loss=6.3474, this epoch 0.0309, total 0.6402
(T) | Epoch=022, loss=6.3466, this epoch 0.0236, total 0.6638
(T) | Epoch=023, loss=6.4797, this epoch 0.0218, total 0.6856
(T) | Epoch=024, loss=6.3427, this epoch 0.0252, total 0.7109
+++model saved ! 2015.pth
(T) | Epoch=025, loss=6.3428, this epoch 0.0243, total 0.7352
(T) | Epoch=026, loss=6.3444, this epoch 0.0216, total 0.7568
(T) | Epoch=027, loss=6.3420, this epoch 0.0214, total 0.7782
+++model saved ! 2015.pth
(T) | Epoch=028, loss=6.3464, this epoch 0.0428, total 0.8209
(T) | Epoch=029, loss=6.3437, this epoch 0.0242, total 0.8451
(T) | Epoch=030, loss=6.4340, this epoch 0.0384, total 0.8835
(T) | Epoch=031, loss=6.3460, this epoch 0.0304, total 0.9139
(T) | Epoch=032, loss=6.3407, this epoch 0.0232, total 0.9371
+++model saved ! 2015.pth
(T) | Epoch=033, loss=6.3406, this epoch 0.0228, total 0.9600
+++model saved ! 2015.pth
(T) | Epoch=034, loss=6.3411, this epoch 0.0520, total 1.0119
(T) | Epoch=035, loss=6.3453, this epoch 0.0365, total 1.0484
(T) | Epoch=036, loss=6.3412, this epoch 0.0377, total 1.0861
(T) | Epoch=037, loss=6.3867, this epoch 0.0330, total 1.1191
(T) | Epoch=038, loss=6.4135, this epoch 0.0300, total 1.1491
(T) | Epoch=039, loss=6.3379, this epoch 0.0377, total 1.1868
+++model saved ! 2015.pth
(T) | Epoch=040, loss=6.3458, this epoch 0.0242, total 1.2110
(T) | Epoch=041, loss=6.3442, this epoch 0.0223, total 1.2333
(T) | Epoch=042, loss=6.3398, this epoch 0.0216, total 1.2549
(T) | Epoch=043, loss=6.3361, this epoch 0.0218, total 1.2766
+++model saved ! 2015.pth
(T) | Epoch=044, loss=6.3448, this epoch 0.0285, total 1.3051
(T) | Epoch=045, loss=6.3339, this epoch 0.0231, total 1.3283
+++model saved ! 2015.pth
(T) | Epoch=046, loss=6.3378, this epoch 0.0288, total 1.3570
(T) | Epoch=047, loss=6.3318, this epoch 0.0225, total 1.3795
+++model saved ! 2015.pth
(T) | Epoch=048, loss=6.3725, this epoch 0.0315, total 1.4111
(T) | Epoch=049, loss=6.3294, this epoch 0.0211, total 1.4322
+++model saved ! 2015.pth
(T) | Epoch=050, loss=6.3329, this epoch 0.0232, total 1.4553
(T) | Epoch=051, loss=6.3295, this epoch 0.0278, total 1.4832
(T) | Epoch=052, loss=6.3292, this epoch 0.0224, total 1.5056
+++model saved ! 2015.pth
(T) | Epoch=053, loss=6.3262, this epoch 0.0232, total 1.5288
+++model saved ! 2015.pth
(T) | Epoch=054, loss=6.3243, this epoch 0.0235, total 1.5522
+++model saved ! 2015.pth
(T) | Epoch=055, loss=6.3177, this epoch 0.0224, total 1.5746
+++model saved ! 2015.pth
(T) | Epoch=056, loss=6.3145, this epoch 0.0298, total 1.6044
+++model saved ! 2015.pth
(T) | Epoch=057, loss=6.3199, this epoch 0.0386, total 1.6430
(T) | Epoch=058, loss=6.3309, this epoch 0.0315, total 1.6745
(T) | Epoch=059, loss=6.3194, this epoch 0.0215, total 1.6961
(T) | Epoch=060, loss=6.2954, this epoch 0.0284, total 1.7244
+++model saved ! 2015.pth
(T) | Epoch=061, loss=6.2872, this epoch 0.0323, total 1.7568
+++model saved ! 2015.pth
(T) | Epoch=062, loss=6.2811, this epoch 0.0383, total 1.7950
+++model saved ! 2015.pth
(T) | Epoch=063, loss=6.2604, this epoch 0.0296, total 1.8246
+++model saved ! 2015.pth
(T) | Epoch=064, loss=6.2564, this epoch 0.0359, total 1.8605
+++model saved ! 2015.pth
(T) | Epoch=065, loss=6.2311, this epoch 0.0324, total 1.8929
+++model saved ! 2015.pth
(T) | Epoch=066, loss=6.2235, this epoch 0.0284, total 1.9213
+++model saved ! 2015.pth
(T) | Epoch=067, loss=6.2264, this epoch 0.0369, total 1.9582
(T) | Epoch=068, loss=6.1711, this epoch 0.0380, total 1.9962
+++model saved ! 2015.pth
(T) | Epoch=069, loss=6.2524, this epoch 0.0333, total 2.0295
(T) | Epoch=070, loss=6.0517, this epoch 0.0225, total 2.0520
+++model saved ! 2015.pth
(T) | Epoch=071, loss=5.9781, this epoch 0.0441, total 2.0960
+++model saved ! 2015.pth
(T) | Epoch=072, loss=5.9220, this epoch 0.0320, total 2.1280
+++model saved ! 2015.pth
(T) | Epoch=073, loss=5.8787, this epoch 0.0352, total 2.1632
+++model saved ! 2015.pth
(T) | Epoch=074, loss=5.8249, this epoch 0.0382, total 2.2013
+++model saved ! 2015.pth
(T) | Epoch=075, loss=5.7711, this epoch 0.0248, total 2.2261
+++model saved ! 2015.pth
(T) | Epoch=076, loss=6.6189, this epoch 0.0415, total 2.2676
(T) | Epoch=077, loss=5.7260, this epoch 0.0241, total 2.2917
+++model saved ! 2015.pth
(T) | Epoch=078, loss=5.6917, this epoch 0.0293, total 2.3211
+++model saved ! 2015.pth
(T) | Epoch=079, loss=5.7709, this epoch 0.0286, total 2.3496
(T) | Epoch=080, loss=5.6984, this epoch 0.0349, total 2.3845
(T) | Epoch=081, loss=5.6744, this epoch 0.0233, total 2.4078
+++model saved ! 2015.pth
(T) | Epoch=082, loss=5.7215, this epoch 0.0220, total 2.4299
(T) | Epoch=083, loss=5.6127, this epoch 0.0276, total 2.4575
+++model saved ! 2015.pth
(T) | Epoch=084, loss=5.9439, this epoch 0.0296, total 2.4871
(T) | Epoch=085, loss=5.6125, this epoch 0.0228, total 2.5099
+++model saved ! 2015.pth
(T) | Epoch=086, loss=5.5824, this epoch 0.0227, total 2.5326
+++model saved ! 2015.pth
(T) | Epoch=087, loss=5.5724, this epoch 0.0231, total 2.5557
+++model saved ! 2015.pth
(T) | Epoch=088, loss=5.9507, this epoch 0.0465, total 2.6022
(T) | Epoch=089, loss=6.1384, this epoch 0.0311, total 2.6333
(T) | Epoch=090, loss=5.6252, this epoch 0.0368, total 2.6701
(T) | Epoch=091, loss=6.0038, this epoch 0.0307, total 2.7008
(T) | Epoch=092, loss=5.6440, this epoch 0.0377, total 2.7384
(T) | Epoch=093, loss=5.6387, this epoch 0.0367, total 2.7751
(T) | Epoch=094, loss=5.8832, this epoch 0.0317, total 2.8068
(T) | Epoch=095, loss=5.7851, this epoch 0.0296, total 2.8365
(T) | Epoch=096, loss=5.5782, this epoch 0.0403, total 2.8768
(T) | Epoch=097, loss=5.6243, this epoch 0.0377, total 2.9144
(T) | Epoch=098, loss=5.7081, this epoch 0.0276, total 2.9421
(T) | Epoch=099, loss=5.6286, this epoch 0.0369, total 2.9790
(T) | Epoch=100, loss=5.6361, this epoch 0.0387, total 3.0176
(T) | Epoch=101, loss=5.9126, this epoch 0.0306, total 3.0483
(T) | Epoch=102, loss=5.6857, this epoch 0.0389, total 3.0872
(T) | Epoch=103, loss=5.6386, this epoch 0.0376, total 3.1248
(T) | Epoch=104, loss=5.6401, this epoch 0.0316, total 3.1564
(T) | Epoch=105, loss=5.6428, this epoch 0.0321, total 3.1884
(T) | Epoch=106, loss=5.7198, this epoch 0.0300, total 3.2184
(T) | Epoch=107, loss=6.5084, this epoch 0.0321, total 3.2506
(T) | Epoch=108, loss=5.6058, this epoch 0.0303, total 3.2809
(T) | Epoch=109, loss=6.0109, this epoch 0.0377, total 3.3187
(T) | Epoch=110, loss=5.7566, this epoch 0.0304, total 3.3491
(T) | Epoch=111, loss=5.7794, this epoch 0.0296, total 3.3786
(T) | Epoch=112, loss=5.8127, this epoch 0.0364, total 3.4150
(T) | Epoch=113, loss=5.8358, this epoch 0.0386, total 3.4536
(T) | Epoch=114, loss=6.3422, this epoch 0.0307, total 3.4843
(T) | Epoch=115, loss=5.9372, this epoch 0.0316, total 3.5159
(T) | Epoch=116, loss=5.8214, this epoch 0.0315, total 3.5474
(T) | Epoch=117, loss=6.2301, this epoch 0.0373, total 3.5847
(T) | Epoch=118, loss=5.8718, this epoch 0.0400, total 3.6248
(T) | Epoch=119, loss=5.7465, this epoch 0.0376, total 3.6624
(T) | Epoch=120, loss=5.9975, this epoch 0.0370, total 3.6994
(T) | Epoch=121, loss=5.7267, this epoch 0.0323, total 3.7317
(T) | Epoch=122, loss=5.7174, this epoch 0.0363, total 3.7680
(T) | Epoch=123, loss=5.6807, this epoch 0.0374, total 3.8054
(T) | Epoch=124, loss=5.6803, this epoch 0.0390, total 3.8444
(T) | Epoch=125, loss=5.7367, this epoch 0.0344, total 3.8788
(T) | Epoch=126, loss=6.0703, this epoch 0.0389, total 3.9177
(T) | Epoch=127, loss=5.8209, this epoch 0.0373, total 3.9550
(T) | Epoch=128, loss=5.6466, this epoch 0.0297, total 3.9847
(T) | Epoch=129, loss=5.6444, this epoch 0.0225, total 4.0072
(T) | Epoch=130, loss=5.6630, this epoch 0.0344, total 4.0416
(T) | Epoch=131, loss=5.6858, this epoch 0.0321, total 4.0737
(T) | Epoch=132, loss=5.6466, this epoch 0.0371, total 4.1108
(T) | Epoch=133, loss=5.7401, this epoch 0.0323, total 4.1431
(T) | Epoch=134, loss=6.0010, this epoch 0.0388, total 4.1819
(T) | Epoch=135, loss=5.8345, this epoch 0.0314, total 4.2133
(T) | Epoch=136, loss=5.6735, this epoch 0.0338, total 4.2471
(T) | Epoch=137, loss=5.5890, this epoch 0.0326, total 4.2797
(T) | Epoch=138, loss=5.5860, this epoch 0.0221, total 4.3019
(T) | Epoch=139, loss=5.6898, this epoch 0.0425, total 4.3444
(T) | Epoch=140, loss=5.6042, this epoch 0.0391, total 4.3834
(T) | Epoch=141, loss=6.1395, this epoch 0.0366, total 4.4200
(T) | Epoch=142, loss=5.6056, this epoch 0.0314, total 4.4514
(T) | Epoch=143, loss=5.6725, this epoch 0.0427, total 4.4941
(T) | Epoch=144, loss=5.6118, this epoch 0.0301, total 4.5242
(T) | Epoch=145, loss=5.6213, this epoch 0.0331, total 4.5573
(T) | Epoch=146, loss=5.7486, this epoch 0.0380, total 4.5953
(T) | Epoch=147, loss=5.6208, this epoch 0.0326, total 4.6280
(T) | Epoch=148, loss=5.6559, this epoch 0.0389, total 4.6668
(T) | Epoch=149, loss=5.6870, this epoch 0.0307, total 4.6975
(T) | Epoch=150, loss=5.6693, this epoch 0.0374, total 4.7349
(T) | Epoch=151, loss=5.5899, this epoch 0.0395, total 4.7744
(T) | Epoch=152, loss=5.7057, this epoch 0.0351, total 4.8095
(T) | Epoch=153, loss=5.5816, this epoch 0.0362, total 4.8457
(T) | Epoch=154, loss=5.5361, this epoch 0.0383, total 4.8840
+++model saved ! 2015.pth
(T) | Epoch=155, loss=5.5700, this epoch 0.0246, total 4.9085
(T) | Epoch=156, loss=5.9975, this epoch 0.0274, total 4.9359
(T) | Epoch=157, loss=5.5145, this epoch 0.0355, total 4.9714
+++model saved ! 2015.pth
(T) | Epoch=158, loss=5.6933, this epoch 0.0302, total 5.0016
(T) | Epoch=159, loss=5.6869, this epoch 0.0347, total 5.0363
(T) | Epoch=160, loss=5.5327, this epoch 0.0367, total 5.0730
(T) | Epoch=161, loss=5.6936, this epoch 0.0296, total 5.1026
(T) | Epoch=162, loss=5.5443, this epoch 0.0309, total 5.1335
(T) | Epoch=163, loss=5.5563, this epoch 0.0324, total 5.1660
(T) | Epoch=164, loss=5.6716, this epoch 0.0369, total 5.2029
(T) | Epoch=165, loss=5.5633, this epoch 0.0317, total 5.2346
(T) | Epoch=166, loss=5.6526, this epoch 0.0371, total 5.2717
(T) | Epoch=167, loss=5.5458, this epoch 0.0275, total 5.2992
(T) | Epoch=168, loss=5.6109, this epoch 0.0342, total 5.3334
(T) | Epoch=169, loss=6.0887, this epoch 0.0350, total 5.3684
(T) | Epoch=170, loss=5.6300, this epoch 0.0375, total 5.4058
(T) | Epoch=171, loss=5.5377, this epoch 0.0359, total 5.4417
(T) | Epoch=172, loss=5.6407, this epoch 0.0329, total 5.4746
(T) | Epoch=173, loss=5.6512, this epoch 0.0283, total 5.5029
(T) | Epoch=174, loss=5.6599, this epoch 0.0363, total 5.5392
(T) | Epoch=175, loss=5.5950, this epoch 0.0372, total 5.5763
(T) | Epoch=176, loss=5.6222, this epoch 0.0368, total 5.6132
(T) | Epoch=177, loss=5.5519, this epoch 0.0355, total 5.6487
(T) | Epoch=178, loss=5.6009, this epoch 0.0315, total 5.6802
(T) | Epoch=179, loss=5.8427, this epoch 0.0369, total 5.7170
(T) | Epoch=180, loss=5.5160, this epoch 0.0376, total 5.7546
(T) | Epoch=181, loss=5.5264, this epoch 0.0290, total 5.7836
(T) | Epoch=182, loss=5.5991, this epoch 0.0354, total 5.8191
(T) | Epoch=183, loss=5.5318, this epoch 0.0298, total 5.8489
(T) | Epoch=184, loss=5.6236, this epoch 0.0384, total 5.8873
(T) | Epoch=185, loss=5.5202, this epoch 0.0340, total 5.9213
(T) | Epoch=186, loss=5.5019, this epoch 0.0378, total 5.9591
+++model saved ! 2015.pth
(T) | Epoch=187, loss=5.5239, this epoch 0.0373, total 5.9963
(T) | Epoch=188, loss=5.4774, this epoch 0.0232, total 6.0196
+++model saved ! 2015.pth
(T) | Epoch=189, loss=5.6433, this epoch 0.0405, total 6.0601
(T) | Epoch=190, loss=5.5375, this epoch 0.0316, total 6.0917
(T) | Epoch=191, loss=5.4963, this epoch 0.0398, total 6.1315
(T) | Epoch=192, loss=5.6377, this epoch 0.0357, total 6.1671
(T) | Epoch=193, loss=5.7552, this epoch 0.0376, total 6.2047
(T) | Epoch=194, loss=5.4859, this epoch 0.0311, total 6.2358
(T) | Epoch=195, loss=5.8543, this epoch 0.0281, total 6.2639
(T) | Epoch=196, loss=5.8948, this epoch 0.0277, total 6.2917
(T) | Epoch=197, loss=5.4898, this epoch 0.0300, total 6.3216
(T) | Epoch=198, loss=5.5865, this epoch 0.0368, total 6.3584
(T) | Epoch=199, loss=5.5759, this epoch 0.0347, total 6.3931
(T) | Epoch=200, loss=5.6207, this epoch 0.0238, total 6.4169
(T) | Epoch=201, loss=5.6858, this epoch 0.0366, total 6.4535
(T) | Epoch=202, loss=5.5701, this epoch 0.0367, total 6.4901
(T) | Epoch=203, loss=5.7228, this epoch 0.0294, total 6.5195
(T) | Epoch=204, loss=5.6139, this epoch 0.0289, total 6.5485
(T) | Epoch=205, loss=5.5377, this epoch 0.0308, total 6.5793
(T) | Epoch=206, loss=5.7066, this epoch 0.0285, total 6.6078
(T) | Epoch=207, loss=5.6557, this epoch 0.0288, total 6.6366
(T) | Epoch=208, loss=5.7168, this epoch 0.0237, total 6.6603
(T) | Epoch=209, loss=5.5084, this epoch 0.0217, total 6.6821
(T) | Epoch=210, loss=5.4506, this epoch 0.0365, total 6.7186
+++model saved ! 2015.pth
(T) | Epoch=211, loss=5.5602, this epoch 0.0382, total 6.7567
(T) | Epoch=212, loss=5.5185, this epoch 0.0366, total 6.7933
(T) | Epoch=213, loss=5.5186, this epoch 0.0295, total 6.8228
(T) | Epoch=214, loss=5.5161, this epoch 0.0349, total 6.8577
(T) | Epoch=215, loss=5.5392, this epoch 0.0324, total 6.8901
(T) | Epoch=216, loss=5.6644, this epoch 0.0321, total 6.9222
(T) | Epoch=217, loss=5.5933, this epoch 0.0381, total 6.9603
(T) | Epoch=218, loss=5.4884, this epoch 0.0311, total 6.9914
(T) | Epoch=219, loss=5.4725, this epoch 0.0385, total 7.0299
(T) | Epoch=220, loss=5.5151, this epoch 0.0319, total 7.0618
(T) | Epoch=221, loss=5.5940, this epoch 0.0360, total 7.0978
(T) | Epoch=222, loss=5.7623, this epoch 0.0281, total 7.1260
(T) | Epoch=223, loss=5.5258, this epoch 0.0370, total 7.1629
(T) | Epoch=224, loss=5.5260, this epoch 0.0310, total 7.1939
(T) | Epoch=225, loss=5.5467, this epoch 0.0284, total 7.2223
(T) | Epoch=226, loss=5.5767, this epoch 0.0296, total 7.2519
(T) | Epoch=227, loss=5.5687, this epoch 0.0344, total 7.2863
(T) | Epoch=228, loss=5.5500, this epoch 0.0283, total 7.3146
(T) | Epoch=229, loss=5.4830, this epoch 0.0278, total 7.3424
(T) | Epoch=230, loss=5.4849, this epoch 0.0290, total 7.3714
(T) | Epoch=231, loss=5.6051, this epoch 0.0353, total 7.4067
(T) | Epoch=232, loss=5.5024, this epoch 0.0349, total 7.4416
(T) | Epoch=233, loss=5.4967, this epoch 0.0323, total 7.4739
(T) | Epoch=234, loss=5.5081, this epoch 0.0290, total 7.5029
(T) | Epoch=235, loss=5.4733, this epoch 0.0326, total 7.5354
(T) | Epoch=236, loss=5.4702, this epoch 0.0384, total 7.5739
(T) | Epoch=237, loss=5.5384, this epoch 0.0361, total 7.6100
(T) | Epoch=238, loss=5.4866, this epoch 0.0373, total 7.6473
(T) | Epoch=239, loss=5.4858, this epoch 0.0324, total 7.6797
(T) | Epoch=240, loss=5.4585, this epoch 0.0294, total 7.7091
(T) | Epoch=241, loss=5.4662, this epoch 0.0363, total 7.7455
(T) | Epoch=242, loss=5.4382, this epoch 0.0387, total 7.7841
+++model saved ! 2015.pth
(T) | Epoch=243, loss=5.5176, this epoch 0.0403, total 7.8244
(T) | Epoch=244, loss=5.4797, this epoch 0.0319, total 7.8564
(T) | Epoch=245, loss=5.4400, this epoch 0.0364, total 7.8927
(T) | Epoch=246, loss=5.7701, this epoch 0.0282, total 7.9209
(T) | Epoch=247, loss=5.5373, this epoch 0.0284, total 7.9493
(T) | Epoch=248, loss=5.5813, this epoch 0.0341, total 7.9834
(T) | Epoch=249, loss=5.5397, this epoch 0.0305, total 8.0140
(T) | Epoch=250, loss=5.6056, this epoch 0.0235, total 8.0375
(T) | Epoch=251, loss=5.5702, this epoch 0.0217, total 8.0592
(T) | Epoch=252, loss=5.7220, this epoch 0.0430, total 8.1022
(T) | Epoch=253, loss=5.5406, this epoch 0.0363, total 8.1385
(T) | Epoch=254, loss=5.5306, this epoch 0.0308, total 8.1693
(T) | Epoch=255, loss=5.4585, this epoch 0.0291, total 8.1984
(T) | Epoch=256, loss=5.4566, this epoch 0.0344, total 8.2329
(T) | Epoch=257, loss=5.8813, this epoch 0.0288, total 8.2617
(T) | Epoch=258, loss=5.5537, this epoch 0.0364, total 8.2981
(T) | Epoch=259, loss=5.4958, this epoch 0.0288, total 8.3269
(T) | Epoch=260, loss=5.4418, this epoch 0.0315, total 8.3584
(T) | Epoch=261, loss=5.4479, this epoch 0.0420, total 8.4005
(T) | Epoch=262, loss=5.4576, this epoch 0.0315, total 8.4320
(T) | Epoch=263, loss=5.5335, this epoch 0.0265, total 8.4585
(T) | Epoch=264, loss=5.4604, this epoch 0.0317, total 8.4902
(T) | Epoch=265, loss=5.5186, this epoch 0.0219, total 8.5121
(T) | Epoch=266, loss=5.4479, this epoch 0.0363, total 8.5484
(T) | Epoch=267, loss=5.4676, this epoch 0.0371, total 8.5855
(T) | Epoch=268, loss=5.8167, this epoch 0.0313, total 8.6168
(T) | Epoch=269, loss=5.6987, this epoch 0.0370, total 8.6538
(T) | Epoch=270, loss=5.4679, this epoch 0.0393, total 8.6931
(T) | Epoch=271, loss=6.1941, this epoch 0.0307, total 8.7238
(T) | Epoch=272, loss=5.4472, this epoch 0.0307, total 8.7545
(T) | Epoch=273, loss=5.6609, this epoch 0.0309, total 8.7854
(T) | Epoch=274, loss=6.1314, this epoch 0.0374, total 8.8228
(T) | Epoch=275, loss=6.0612, this epoch 0.0341, total 8.8569
(T) | Epoch=276, loss=6.0916, this epoch 0.0284, total 8.8853
(T) | Epoch=277, loss=6.2599, this epoch 0.0293, total 8.9146
(T) | Epoch=278, loss=6.5025, this epoch 0.0346, total 8.9492
(T) | Epoch=279, loss=6.5513, this epoch 0.0296, total 8.9788
(T) | Epoch=280, loss=5.4721, this epoch 0.0369, total 9.0157
(T) | Epoch=281, loss=6.2545, this epoch 0.0371, total 9.0527
(T) | Epoch=282, loss=6.2623, this epoch 0.0369, total 9.0896
(T) | Epoch=283, loss=6.6019, this epoch 0.0370, total 9.1266
(T) | Epoch=284, loss=6.2044, this epoch 0.0371, total 9.1638
(T) | Epoch=285, loss=6.2058, this epoch 0.0311, total 9.1948
(T) | Epoch=286, loss=6.2458, this epoch 0.0290, total 9.2238
(T) | Epoch=287, loss=6.6400, this epoch 0.0274, total 9.2512
(T) | Epoch=288, loss=6.5835, this epoch 0.0314, total 9.2827
(T) | Epoch=289, loss=6.2874, this epoch 0.0352, total 9.3178
(T) | Epoch=290, loss=6.2720, this epoch 0.0396, total 9.3574
(T) | Epoch=291, loss=6.5008, this epoch 0.0372, total 9.3946
(T) | Epoch=292, loss=6.2123, this epoch 0.0369, total 9.4315
(T) | Epoch=293, loss=6.2381, this epoch 0.0370, total 9.4686
(T) | Epoch=294, loss=6.2386, this epoch 0.0373, total 9.5059
(T) | Epoch=295, loss=6.2049, this epoch 0.0369, total 9.5427
(T) | Epoch=296, loss=6.2041, this epoch 0.0336, total 9.5763
(T) | Epoch=297, loss=6.2023, this epoch 0.0270, total 9.6033
(T) | Epoch=298, loss=6.1916, this epoch 0.0315, total 9.6348
(T) | Epoch=299, loss=6.1928, this epoch 0.0281, total 9.6629
(T) | Epoch=300, loss=6.2349, this epoch 0.0385, total 9.7014
(T) | Epoch=301, loss=6.1909, this epoch 0.0352, total 9.7366
(T) | Epoch=302, loss=6.2020, this epoch 0.0307, total 9.7673
(T) | Epoch=303, loss=6.6337, this epoch 0.0307, total 9.7980
(T) | Epoch=304, loss=6.2454, this epoch 0.0287, total 9.8266
(T) | Epoch=305, loss=6.1594, this epoch 0.0291, total 9.8557
(T) | Epoch=306, loss=6.2184, this epoch 0.0280, total 9.8837
(T) | Epoch=307, loss=6.2316, this epoch 0.0308, total 9.9145
(T) | Epoch=308, loss=6.1389, this epoch 0.0278, total 9.9423
(T) | Epoch=309, loss=6.1474, this epoch 0.0313, total 9.9736
(T) | Epoch=310, loss=6.1200, this epoch 0.0298, total 10.0034
(T) | Epoch=311, loss=6.2501, this epoch 0.0397, total 10.0431
(T) | Epoch=312, loss=6.3235, this epoch 0.0313, total 10.0744
(T) | Epoch=313, loss=6.0877, this epoch 0.0300, total 10.1044
(T) | Epoch=314, loss=6.1825, this epoch 0.0263, total 10.1307
(T) | Epoch=315, loss=6.1606, this epoch 0.0313, total 10.1620
(T) | Epoch=316, loss=6.1346, this epoch 0.0289, total 10.1909
(T) | Epoch=317, loss=6.0499, this epoch 0.0318, total 10.2226
(T) | Epoch=318, loss=6.1340, this epoch 0.0299, total 10.2526
(T) | Epoch=319, loss=6.1094, this epoch 0.0392, total 10.2918
(T) | Epoch=320, loss=5.9998, this epoch 0.0400, total 10.3318
(T) | Epoch=321, loss=6.0074, this epoch 0.0316, total 10.3634
(T) | Epoch=322, loss=6.1328, this epoch 0.0396, total 10.4030
(T) | Epoch=323, loss=6.2367, this epoch 0.0364, total 10.4395
(T) | Epoch=324, loss=6.0144, this epoch 0.0311, total 10.4706
(T) | Epoch=325, loss=5.9449, this epoch 0.0293, total 10.4999
(T) | Epoch=326, loss=6.0434, this epoch 0.0349, total 10.5349
(T) | Epoch=327, loss=5.9020, this epoch 0.0308, total 10.5656
(T) | Epoch=328, loss=6.1169, this epoch 0.0350, total 10.6006
(T) | Epoch=329, loss=5.8791, this epoch 0.0397, total 10.6403
(T) | Epoch=330, loss=5.8667, this epoch 0.0384, total 10.6787
(T) | Epoch=331, loss=5.8375, this epoch 0.0308, total 10.7095
(T) | Epoch=332, loss=5.8661, this epoch 0.0303, total 10.7398
(T) | Epoch=333, loss=6.2030, this epoch 0.0281, total 10.7679
(T) | Epoch=334, loss=5.8829, this epoch 0.0359, total 10.8038
(T) | Epoch=335, loss=5.8186, this epoch 0.0303, total 10.8341
(T) | Epoch=336, loss=5.8234, this epoch 0.0283, total 10.8624
(T) | Epoch=337, loss=5.8930, this epoch 0.0290, total 10.8915
(T) | Epoch=338, loss=5.7735, this epoch 0.0286, total 10.9201
(T) | Epoch=339, loss=6.1419, this epoch 0.0371, total 10.9572
(T) | Epoch=340, loss=5.7431, this epoch 0.0406, total 10.9978
(T) | Epoch=341, loss=5.7633, this epoch 0.0325, total 11.0303
(T) | Epoch=342, loss=5.8036, this epoch 0.0288, total 11.0591
(T) | Epoch=343, loss=5.7080, this epoch 0.0367, total 11.0958
(T) | Epoch=344, loss=5.7409, this epoch 0.0388, total 11.1346
(T) | Epoch=345, loss=5.7964, this epoch 0.0315, total 11.1661
(T) | Epoch=346, loss=5.7684, this epoch 0.0336, total 11.1997
(T) | Epoch=347, loss=5.7332, this epoch 0.0307, total 11.2305
(T) | Epoch=348, loss=5.8051, this epoch 0.0341, total 11.2646
(T) | Epoch=349, loss=5.6643, this epoch 0.0358, total 11.3004
(T) | Epoch=350, loss=5.6727, this epoch 0.0318, total 11.3321
(T) | Epoch=351, loss=5.5700, this epoch 0.0265, total 11.3587
(T) | Epoch=352, loss=5.6498, this epoch 0.0366, total 11.3952
(T) | Epoch=353, loss=5.8596, this epoch 0.0287, total 11.4240
(T) | Epoch=354, loss=5.5419, this epoch 0.0360, total 11.4600
(T) | Epoch=355, loss=5.5567, this epoch 0.0311, total 11.4911
(T) | Epoch=356, loss=5.6211, this epoch 0.0285, total 11.5195
(T) | Epoch=357, loss=5.6299, this epoch 0.0299, total 11.5495
(T) | Epoch=358, loss=5.6012, this epoch 0.0431, total 11.5926
(T) | Epoch=359, loss=5.5336, this epoch 0.0363, total 11.6289
(T) | Epoch=360, loss=5.6693, this epoch 0.0304, total 11.6593
(T) | Epoch=361, loss=5.4747, this epoch 0.0275, total 11.6867
(T) | Epoch=362, loss=5.4624, this epoch 0.0306, total 11.7174
(T) | Epoch=363, loss=5.4755, this epoch 0.0293, total 11.7467
(T) | Epoch=364, loss=5.9041, this epoch 0.0293, total 11.7759
(T) | Epoch=365, loss=5.8374, this epoch 0.0342, total 11.8101
(T) | Epoch=366, loss=5.6171, this epoch 0.0308, total 11.8409
(T) | Epoch=367, loss=5.9630, this epoch 0.0340, total 11.8749
(T) | Epoch=368, loss=5.5305, this epoch 0.0387, total 11.9136
(T) | Epoch=369, loss=5.5830, this epoch 0.0308, total 11.9444
(T) | Epoch=370, loss=5.7993, this epoch 0.0343, total 11.9787
(T) | Epoch=371, loss=5.7036, this epoch 0.0311, total 12.0097
(T) | Epoch=372, loss=5.8526, this epoch 0.0282, total 12.0379
(T) | Epoch=373, loss=5.6596, this epoch 0.0342, total 12.0722
(T) | Epoch=374, loss=5.8250, this epoch 0.0304, total 12.1026
(T) | Epoch=375, loss=6.0241, this epoch 0.0363, total 12.1389
(T) | Epoch=376, loss=5.6424, this epoch 0.0360, total 12.1749
(T) | Epoch=377, loss=5.6452, this epoch 0.0360, total 12.2108
(T) | Epoch=378, loss=5.7309, this epoch 0.0309, total 12.2417
(T) | Epoch=379, loss=5.7704, this epoch 0.0286, total 12.2703
(T) | Epoch=380, loss=5.6148, this epoch 0.0366, total 12.3069
(T) | Epoch=381, loss=5.6558, this epoch 0.0385, total 12.3455
(T) | Epoch=382, loss=5.6497, this epoch 0.0366, total 12.3821
(T) | Epoch=383, loss=5.6110, this epoch 0.0306, total 12.4127
(T) | Epoch=384, loss=5.7810, this epoch 0.0287, total 12.4414
(T) | Epoch=385, loss=5.9786, this epoch 0.0363, total 12.4778
(T) | Epoch=386, loss=5.6451, this epoch 0.0316, total 12.5094
(T) | Epoch=387, loss=5.5955, this epoch 0.0216, total 12.5310
(T) | Epoch=388, loss=5.6465, this epoch 0.0362, total 12.5673
(T) | Epoch=389, loss=5.4910, this epoch 0.0311, total 12.5983
(T) | Epoch=390, loss=5.6189, this epoch 0.0295, total 12.6279
(T) | Epoch=391, loss=5.4685, this epoch 0.0307, total 12.6586
(T) | Epoch=392, loss=5.4958, this epoch 0.0383, total 12.6969
(T) | Epoch=393, loss=5.5641, this epoch 0.0331, total 12.7299
(T) | Epoch=394, loss=5.4502, this epoch 0.0410, total 12.7709
(T) | Epoch=395, loss=5.4532, this epoch 0.0405, total 12.8114
(T) | Epoch=396, loss=5.4759, this epoch 0.0307, total 12.8422
(T) | Epoch=397, loss=5.4575, this epoch 0.0348, total 12.8770
(T) | Epoch=398, loss=5.6009, this epoch 0.0333, total 12.9102
(T) | Epoch=399, loss=5.5847, this epoch 0.0388, total 12.9490
(T) | Epoch=400, loss=5.4685, this epoch 0.0395, total 12.9886
(T) | Epoch=401, loss=5.5455, this epoch 0.0300, total 13.0186
(T) | Epoch=402, loss=5.6247, this epoch 0.0319, total 13.0505
(T) | Epoch=403, loss=5.4525, this epoch 0.0374, total 13.0879
(T) | Epoch=404, loss=5.4362, this epoch 0.0364, total 13.1243
+++model saved ! 2015.pth
(T) | Epoch=405, loss=5.4400, this epoch 0.0387, total 13.1630
(T) | Epoch=406, loss=5.4798, this epoch 0.0238, total 13.1868
(T) | Epoch=407, loss=5.5529, this epoch 0.0302, total 13.2169
(T) | Epoch=408, loss=5.4490, this epoch 0.0291, total 13.2460
(T) | Epoch=409, loss=5.4630, this epoch 0.0376, total 13.2836
(T) | Epoch=410, loss=5.5495, this epoch 0.0406, total 13.3242
(T) | Epoch=411, loss=5.5500, this epoch 0.0328, total 13.3571
(T) | Epoch=412, loss=5.6063, this epoch 0.0312, total 13.3882
(T) | Epoch=413, loss=5.4396, this epoch 0.0298, total 13.4180
(T) | Epoch=414, loss=5.4364, this epoch 0.0357, total 13.4537
(T) | Epoch=415, loss=5.6077, this epoch 0.0321, total 13.4858
(T) | Epoch=416, loss=5.5016, this epoch 0.0312, total 13.5170
(T) | Epoch=417, loss=5.4417, this epoch 0.0422, total 13.5591
(T) | Epoch=418, loss=5.4503, this epoch 0.0391, total 13.5982
(T) | Epoch=419, loss=5.4448, this epoch 0.0332, total 13.6314
(T) | Epoch=420, loss=5.6126, this epoch 0.0361, total 13.6675
(T) | Epoch=421, loss=5.4671, this epoch 0.0300, total 13.6975
(T) | Epoch=422, loss=5.4307, this epoch 0.0292, total 13.7267
+++model saved ! 2015.pth
(T) | Epoch=423, loss=5.5285, this epoch 0.0362, total 13.7629
(T) | Epoch=424, loss=5.5160, this epoch 0.0312, total 13.7941
(T) | Epoch=425, loss=5.5378, this epoch 0.0316, total 13.8256
(T) | Epoch=426, loss=5.4382, this epoch 0.0294, total 13.8550
(T) | Epoch=427, loss=5.4949, this epoch 0.0293, total 13.8843
(T) | Epoch=428, loss=5.4308, this epoch 0.0288, total 13.9131
(T) | Epoch=429, loss=5.4630, this epoch 0.0314, total 13.9446
(T) | Epoch=430, loss=5.4905, this epoch 0.0287, total 13.9733
(T) | Epoch=431, loss=5.4332, this epoch 0.0294, total 14.0026
(T) | Epoch=432, loss=5.4322, this epoch 0.0286, total 14.0312
(T) | Epoch=433, loss=5.4577, this epoch 0.0350, total 14.0662
(T) | Epoch=434, loss=5.4955, this epoch 0.0245, total 14.0907
(T) | Epoch=435, loss=5.4330, this epoch 0.0286, total 14.1193
(T) | Epoch=436, loss=5.4945, this epoch 0.0348, total 14.1541
(T) | Epoch=437, loss=5.4604, this epoch 0.0311, total 14.1852
(T) | Epoch=438, loss=5.4203, this epoch 0.0292, total 14.2144
+++model saved ! 2015.pth
(T) | Epoch=439, loss=5.5291, this epoch 0.0307, total 14.2451
(T) | Epoch=440, loss=5.4317, this epoch 0.0290, total 14.2740
(T) | Epoch=441, loss=5.5912, this epoch 0.0289, total 14.3030
(T) | Epoch=442, loss=5.4522, this epoch 0.0303, total 14.3333
(T) | Epoch=443, loss=5.4724, this epoch 0.0339, total 14.3672
(T) | Epoch=444, loss=5.5039, this epoch 0.0371, total 14.4042
(T) | Epoch=445, loss=5.4231, this epoch 0.0309, total 14.4351
(T) | Epoch=446, loss=5.7531, this epoch 0.0288, total 14.4639
(T) | Epoch=447, loss=5.7189, this epoch 0.0284, total 14.4924
(T) | Epoch=448, loss=5.4252, this epoch 0.0285, total 14.5209
(T) | Epoch=449, loss=5.5057, this epoch 0.0352, total 14.5561
(T) | Epoch=450, loss=5.5007, this epoch 0.0324, total 14.5885
(T) | Epoch=451, loss=5.4943, this epoch 0.0217, total 14.6102
(T) | Epoch=452, loss=5.4707, this epoch 0.0284, total 14.6386
(T) | Epoch=453, loss=5.4843, this epoch 0.0290, total 14.6676
(T) | Epoch=454, loss=5.4385, this epoch 0.0318, total 14.6994
(T) | Epoch=455, loss=5.4589, this epoch 0.0268, total 14.7263
(T) | Epoch=456, loss=5.5895, this epoch 0.0266, total 14.7529
(T) | Epoch=457, loss=5.4486, this epoch 0.0372, total 14.7901
(T) | Epoch=458, loss=5.5444, this epoch 0.0368, total 14.8268
(T) | Epoch=459, loss=5.4526, this epoch 0.0333, total 14.8601
(T) | Epoch=460, loss=5.4215, this epoch 0.0224, total 14.8825
(T) | Epoch=461, loss=5.4392, this epoch 0.0262, total 14.9087
(T) | Epoch=462, loss=5.5083, this epoch 0.0310, total 14.9397
(T) | Epoch=463, loss=5.4156, this epoch 0.0288, total 14.9686
+++model saved ! 2015.pth
(T) | Epoch=464, loss=5.4269, this epoch 0.0308, total 14.9994
(T) | Epoch=465, loss=5.4246, this epoch 0.0295, total 15.0289
(T) | Epoch=466, loss=5.4477, this epoch 0.0411, total 15.0701
(T) | Epoch=467, loss=5.4322, this epoch 0.0307, total 15.1008
(T) | Epoch=468, loss=5.6179, this epoch 0.0293, total 15.1301
(T) | Epoch=469, loss=5.4339, this epoch 0.0224, total 15.1525
(T) | Epoch=470, loss=5.5125, this epoch 0.0271, total 15.1796
(T) | Epoch=471, loss=5.4552, this epoch 0.0368, total 15.2164
(T) | Epoch=472, loss=5.4483, this epoch 0.0348, total 15.2512
(T) | Epoch=473, loss=5.4204, this epoch 0.0290, total 15.2802
(T) | Epoch=474, loss=5.4519, this epoch 0.0348, total 15.3150
(T) | Epoch=475, loss=5.4931, this epoch 0.0245, total 15.3395
(T) | Epoch=476, loss=5.5540, this epoch 0.0212, total 15.3607
(T) | Epoch=477, loss=5.4611, this epoch 0.0267, total 15.3873
(T) | Epoch=478, loss=5.4274, this epoch 0.0309, total 15.4183
(T) | Epoch=479, loss=5.4271, this epoch 0.0295, total 15.4478
(T) | Epoch=480, loss=5.4793, this epoch 0.0358, total 15.4835
(T) | Epoch=481, loss=5.5025, this epoch 0.0294, total 15.5129
(T) | Epoch=482, loss=5.4387, this epoch 0.0290, total 15.5419
(T) | Epoch=483, loss=5.5007, this epoch 0.0288, total 15.5707
(T) | Epoch=484, loss=5.4266, this epoch 0.0289, total 15.5996
(T) | Epoch=485, loss=5.4199, this epoch 0.0288, total 15.6284
(T) | Epoch=486, loss=5.5012, this epoch 0.0349, total 15.6633
(T) | Epoch=487, loss=5.4642, this epoch 0.0239, total 15.6873
(T) | Epoch=488, loss=5.4254, this epoch 0.0343, total 15.7215
(T) | Epoch=489, loss=5.4601, this epoch 0.0311, total 15.7526
(T) | Epoch=490, loss=5.4232, this epoch 0.0290, total 15.7816
(T) | Epoch=491, loss=5.4730, this epoch 0.0226, total 15.8043
(T) | Epoch=492, loss=5.5828, this epoch 0.0423, total 15.8465
(T) | Epoch=493, loss=5.6463, this epoch 0.0313, total 15.8778
(T) | Epoch=494, loss=5.4714, this epoch 0.0292, total 15.9070
(T) | Epoch=495, loss=5.4351, this epoch 0.0223, total 15.9293
(T) | Epoch=496, loss=5.5540, this epoch 0.0261, total 15.9553
(T) | Epoch=497, loss=5.4227, this epoch 0.0309, total 15.9862
(T) | Epoch=498, loss=5.4147, this epoch 0.0344, total 16.0206
+++model saved ! 2015.pth
(T) | Epoch=499, loss=5.4123, this epoch 0.0411, total 16.0617
+++model saved ! 2015.pth
(T) | Epoch=500, loss=5.4194, this epoch 0.0385, total 16.1003
(T) | Epoch=501, loss=5.5119, this epoch 0.0394, total 16.1396
(T) | Epoch=502, loss=5.5781, this epoch 0.0309, total 16.1705
(T) | Epoch=503, loss=5.4564, this epoch 0.0350, total 16.2055
(T) | Epoch=504, loss=5.4778, this epoch 0.0313, total 16.2368
(T) | Epoch=505, loss=5.5049, this epoch 0.0290, total 16.2659
(T) | Epoch=506, loss=5.4150, this epoch 0.0382, total 16.3041
(T) | Epoch=507, loss=5.4572, this epoch 0.0253, total 16.3293
(T) | Epoch=508, loss=5.4238, this epoch 0.0284, total 16.3577
(T) | Epoch=509, loss=5.4941, this epoch 0.0284, total 16.3861
(T) | Epoch=510, loss=5.5819, this epoch 0.0292, total 16.4154
(T) | Epoch=511, loss=5.4522, this epoch 0.0303, total 16.4457
(T) | Epoch=512, loss=5.4787, this epoch 0.0229, total 16.4686
(T) | Epoch=513, loss=5.4105, this epoch 0.0379, total 16.5065
+++model saved ! 2015.pth
(T) | Epoch=514, loss=5.4909, this epoch 0.0394, total 16.5459
(T) | Epoch=515, loss=5.4507, this epoch 0.0252, total 16.5711
(T) | Epoch=516, loss=5.4115, this epoch 0.0340, total 16.6051
(T) | Epoch=517, loss=5.4083, this epoch 0.0359, total 16.6409
+++model saved ! 2015.pth
(T) | Epoch=518, loss=5.4304, this epoch 0.0230, total 16.6639
(T) | Epoch=519, loss=5.4396, this epoch 0.0292, total 16.6931
(T) | Epoch=520, loss=5.4213, this epoch 0.0213, total 16.7144
(T) | Epoch=521, loss=5.6394, this epoch 0.0352, total 16.7496
(T) | Epoch=522, loss=5.4966, this epoch 0.0233, total 16.7729
(T) | Epoch=523, loss=5.4166, this epoch 0.0215, total 16.7944
(T) | Epoch=524, loss=5.5204, this epoch 0.0285, total 16.8229
(T) | Epoch=525, loss=5.4660, this epoch 0.0225, total 16.8454
(T) | Epoch=526, loss=5.4446, this epoch 0.0316, total 16.8770
(T) | Epoch=527, loss=5.5216, this epoch 0.0298, total 16.9068
(T) | Epoch=528, loss=5.4499, this epoch 0.0403, total 16.9471
(T) | Epoch=529, loss=5.4461, this epoch 0.0232, total 16.9704
(T) | Epoch=530, loss=5.7697, this epoch 0.0217, total 16.9921
(T) | Epoch=531, loss=5.4530, this epoch 0.0211, total 17.0132
(T) | Epoch=532, loss=5.4320, this epoch 0.0218, total 17.0350
(T) | Epoch=533, loss=5.4671, this epoch 0.0215, total 17.0565
(T) | Epoch=534, loss=5.4261, this epoch 0.0217, total 17.0782
(T) | Epoch=535, loss=5.4369, this epoch 0.0216, total 17.0998
(T) | Epoch=536, loss=5.4156, this epoch 0.0217, total 17.1214
(T) | Epoch=537, loss=5.4810, this epoch 0.0213, total 17.1427
(T) | Epoch=538, loss=5.4287, this epoch 0.0213, total 17.1641
(T) | Epoch=539, loss=5.4810, this epoch 0.0214, total 17.1855
(T) | Epoch=540, loss=5.4084, this epoch 0.0346, total 17.2201
(T) | Epoch=541, loss=5.4099, this epoch 0.0302, total 17.2502
(T) | Epoch=542, loss=5.4558, this epoch 0.0224, total 17.2726
(T) | Epoch=543, loss=5.4880, this epoch 0.0272, total 17.2998
(T) | Epoch=544, loss=5.3996, this epoch 0.0235, total 17.3234
+++model saved ! 2015.pth
(T) | Epoch=545, loss=5.4638, this epoch 0.0401, total 17.3634
(T) | Epoch=546, loss=5.4003, this epoch 0.0295, total 17.3929
(T) | Epoch=547, loss=5.4178, this epoch 0.0319, total 17.4249
(T) | Epoch=548, loss=5.4627, this epoch 0.0359, total 17.4608
(T) | Epoch=549, loss=5.4088, this epoch 0.0284, total 17.4892
(T) | Epoch=550, loss=5.4664, this epoch 0.0313, total 17.5205
(T) | Epoch=551, loss=5.4519, this epoch 0.0375, total 17.5580
(T) | Epoch=552, loss=5.4807, this epoch 0.0380, total 17.5961
(T) | Epoch=553, loss=5.4028, this epoch 0.0351, total 17.6311
(T) | Epoch=554, loss=5.4194, this epoch 0.0386, total 17.6698
(T) | Epoch=555, loss=5.4337, this epoch 0.0348, total 17.7046
(T) | Epoch=556, loss=5.4022, this epoch 0.0308, total 17.7354
(T) | Epoch=557, loss=5.4235, this epoch 0.0294, total 17.7648
(T) | Epoch=558, loss=5.3940, this epoch 0.0237, total 17.7885
+++model saved ! 2015.pth
(T) | Epoch=559, loss=5.4017, this epoch 0.0231, total 17.8116
(T) | Epoch=560, loss=5.4002, this epoch 0.0355, total 17.8471
(T) | Epoch=561, loss=5.4339, this epoch 0.0295, total 17.8766
(T) | Epoch=562, loss=5.4761, this epoch 0.0372, total 17.9139
(T) | Epoch=563, loss=5.4846, this epoch 0.0224, total 17.9363
(T) | Epoch=564, loss=5.4196, this epoch 0.0214, total 17.9577
(T) | Epoch=565, loss=5.3929, this epoch 0.0359, total 17.9936
+++model saved ! 2015.pth
(T) | Epoch=566, loss=5.4045, this epoch 0.0370, total 18.0306
(T) | Epoch=567, loss=5.3953, this epoch 0.0305, total 18.0611
(T) | Epoch=568, loss=5.4318, this epoch 0.0302, total 18.0913
(T) | Epoch=569, loss=5.4803, this epoch 0.0374, total 18.1287
(T) | Epoch=570, loss=5.4449, this epoch 0.0228, total 18.1515
(T) | Epoch=571, loss=5.3929, this epoch 0.0346, total 18.1860
(T) | Epoch=572, loss=5.4055, this epoch 0.0343, total 18.2203
(T) | Epoch=573, loss=5.5337, this epoch 0.0217, total 18.2420
(T) | Epoch=574, loss=5.4835, this epoch 0.0216, total 18.2635
(T) | Epoch=575, loss=5.3933, this epoch 0.0296, total 18.2932
(T) | Epoch=576, loss=5.4332, this epoch 0.0393, total 18.3325
(T) | Epoch=577, loss=5.4184, this epoch 0.0366, total 18.3691
(T) | Epoch=578, loss=5.4450, this epoch 0.0398, total 18.4089
(T) | Epoch=579, loss=5.4045, this epoch 0.0312, total 18.4401
(T) | Epoch=580, loss=5.4775, this epoch 0.0283, total 18.4684
(T) | Epoch=581, loss=5.5230, this epoch 0.0291, total 18.4975
(T) | Epoch=582, loss=5.3867, this epoch 0.0289, total 18.5264
+++model saved ! 2015.pth
(T) | Epoch=583, loss=5.4258, this epoch 0.0408, total 18.5672
(T) | Epoch=584, loss=5.4008, this epoch 0.0326, total 18.5998
(T) | Epoch=585, loss=5.4499, this epoch 0.0429, total 18.6427
(T) | Epoch=586, loss=5.3967, this epoch 0.0290, total 18.6716
(T) | Epoch=587, loss=5.3927, this epoch 0.0395, total 18.7111
(T) | Epoch=588, loss=5.3894, this epoch 0.0231, total 18.7342
(T) | Epoch=589, loss=5.3882, this epoch 0.0381, total 18.7723
(T) | Epoch=590, loss=5.4413, this epoch 0.0316, total 18.8039
(T) | Epoch=591, loss=5.4813, this epoch 0.0317, total 18.8356
(T) | Epoch=592, loss=5.4879, this epoch 0.0267, total 18.8623
(T) | Epoch=593, loss=5.4199, this epoch 0.0303, total 18.8927
(T) | Epoch=594, loss=5.4542, this epoch 0.0282, total 18.9209
(T) | Epoch=595, loss=5.4107, this epoch 0.0309, total 18.9518
(T) | Epoch=596, loss=5.4156, this epoch 0.0216, total 18.9734
(T) | Epoch=597, loss=5.4049, this epoch 0.0271, total 19.0005
(T) | Epoch=598, loss=5.4162, this epoch 0.0233, total 19.0238
(T) | Epoch=599, loss=5.4080, this epoch 0.0216, total 19.0454
(T) | Epoch=600, loss=5.3987, this epoch 0.0357, total 19.0811
(T) | Epoch=601, loss=5.4383, this epoch 0.0366, total 19.1177
(T) | Epoch=602, loss=5.4394, this epoch 0.0315, total 19.1492
(T) | Epoch=603, loss=5.4434, this epoch 0.0395, total 19.1887
(T) | Epoch=604, loss=5.3968, this epoch 0.0292, total 19.2179
(T) | Epoch=605, loss=5.4324, this epoch 0.0395, total 19.2574
(T) | Epoch=606, loss=5.3914, this epoch 0.0451, total 19.3025
(T) | Epoch=607, loss=5.4746, this epoch 0.0398, total 19.3424
(T) | Epoch=608, loss=5.4322, this epoch 0.0397, total 19.3820
(T) | Epoch=609, loss=5.4214, this epoch 0.0396, total 19.4217
(T) | Epoch=610, loss=5.3942, this epoch 0.0398, total 19.4615
(T) | Epoch=611, loss=5.3984, this epoch 0.0398, total 19.5013
(T) | Epoch=612, loss=5.4614, this epoch 0.0396, total 19.5408
(T) | Epoch=613, loss=5.3986, this epoch 0.0329, total 19.5737
(T) | Epoch=614, loss=5.3817, this epoch 0.0302, total 19.6039
+++model saved ! 2015.pth
(T) | Epoch=615, loss=5.4214, this epoch 0.0238, total 19.6277
(T) | Epoch=616, loss=5.3858, this epoch 0.0352, total 19.6629
(T) | Epoch=617, loss=5.4838, this epoch 0.0304, total 19.6932
(T) | Epoch=618, loss=5.4272, this epoch 0.0350, total 19.7283
(T) | Epoch=619, loss=5.4520, this epoch 0.0363, total 19.7645
(T) | Epoch=620, loss=5.3841, this epoch 0.0232, total 19.7878
(T) | Epoch=621, loss=5.4481, this epoch 0.0379, total 19.8256
(T) | Epoch=622, loss=5.3893, this epoch 0.0226, total 19.8482
(T) | Epoch=623, loss=5.4128, this epoch 0.0211, total 19.8693
(T) | Epoch=624, loss=5.4245, this epoch 0.0216, total 19.8908
(T) | Epoch=625, loss=5.4436, this epoch 0.0285, total 19.9193
(T) | Epoch=626, loss=5.4665, this epoch 0.0275, total 19.9468
(T) | Epoch=627, loss=5.4290, this epoch 0.0363, total 19.9831
(T) | Epoch=628, loss=5.4125, this epoch 0.0231, total 20.0062
(T) | Epoch=629, loss=5.5287, this epoch 0.0347, total 20.0409
(T) | Epoch=630, loss=5.4578, this epoch 0.0294, total 20.0703
(T) | Epoch=631, loss=5.4757, this epoch 0.0234, total 20.0937
(T) | Epoch=632, loss=5.4147, this epoch 0.0223, total 20.1160
(T) | Epoch=633, loss=5.4140, this epoch 0.0286, total 20.1446
(T) | Epoch=634, loss=5.3873, this epoch 0.0280, total 20.1726
(T) | Epoch=635, loss=5.3738, this epoch 0.0347, total 20.2073
+++model saved ! 2015.pth
(T) | Epoch=636, loss=5.4164, this epoch 0.0317, total 20.2390
(T) | Epoch=637, loss=5.4186, this epoch 0.0293, total 20.2683
(T) | Epoch=638, loss=5.3951, this epoch 0.0275, total 20.2958
(T) | Epoch=639, loss=5.3783, this epoch 0.0241, total 20.3199
(T) | Epoch=640, loss=5.4784, this epoch 0.0215, total 20.3414
(T) | Epoch=641, loss=5.3808, this epoch 0.0223, total 20.3636
(T) | Epoch=642, loss=5.4367, this epoch 0.0355, total 20.3991
(T) | Epoch=643, loss=5.3911, this epoch 0.0291, total 20.4282
(T) | Epoch=644, loss=5.4279, this epoch 0.0287, total 20.4570
(T) | Epoch=645, loss=5.4048, this epoch 0.0219, total 20.4788
(T) | Epoch=646, loss=5.3812, this epoch 0.0304, total 20.5093
(T) | Epoch=647, loss=5.3814, this epoch 0.0295, total 20.5388
(T) | Epoch=648, loss=5.3960, this epoch 0.0273, total 20.5661
(T) | Epoch=649, loss=5.3807, this epoch 0.0301, total 20.5962
(T) | Epoch=650, loss=5.3812, this epoch 0.0302, total 20.6264
(T) | Epoch=651, loss=5.3959, this epoch 0.0348, total 20.6612
(T) | Epoch=652, loss=5.3792, this epoch 0.0310, total 20.6922
(T) | Epoch=653, loss=5.4079, this epoch 0.0287, total 20.7209
(T) | Epoch=654, loss=5.4052, this epoch 0.0289, total 20.7498
(T) | Epoch=655, loss=5.4203, this epoch 0.0344, total 20.7842
(T) | Epoch=656, loss=5.4435, this epoch 0.0307, total 20.8149
(T) | Epoch=657, loss=5.4626, this epoch 0.0286, total 20.8435
(T) | Epoch=658, loss=5.3795, this epoch 0.0292, total 20.8727
(T) | Epoch=659, loss=5.4804, this epoch 0.0224, total 20.8950
(T) | Epoch=660, loss=5.4538, this epoch 0.0285, total 20.9235
(T) | Epoch=661, loss=5.4180, this epoch 0.0289, total 20.9524
(T) | Epoch=662, loss=5.4859, this epoch 0.0287, total 20.9811
(T) | Epoch=663, loss=5.3740, this epoch 0.0286, total 21.0097
(T) | Epoch=664, loss=5.4250, this epoch 0.0290, total 21.0387
(T) | Epoch=665, loss=5.4059, this epoch 0.0346, total 21.0733
(T) | Epoch=666, loss=5.3698, this epoch 0.0310, total 21.1043
+++model saved ! 2015.pth
(T) | Epoch=667, loss=5.4287, this epoch 0.0360, total 21.1403
(T) | Epoch=668, loss=5.4953, this epoch 0.0366, total 21.1769
(T) | Epoch=669, loss=5.3876, this epoch 0.0335, total 21.2105
(T) | Epoch=670, loss=5.3914, this epoch 0.0289, total 21.2393
(T) | Epoch=671, loss=5.3856, this epoch 0.0371, total 21.2764
(T) | Epoch=672, loss=5.3890, this epoch 0.0365, total 21.3130
(T) | Epoch=673, loss=5.3692, this epoch 0.0389, total 21.3519
+++model saved ! 2015.pth
(T) | Epoch=674, loss=5.4544, this epoch 0.0386, total 21.3905
(T) | Epoch=675, loss=5.3751, this epoch 0.0241, total 21.4146
(T) | Epoch=676, loss=5.4191, this epoch 0.0280, total 21.4426
(T) | Epoch=677, loss=5.3874, this epoch 0.0295, total 21.4721
(T) | Epoch=678, loss=5.3973, this epoch 0.0316, total 21.5037
(T) | Epoch=679, loss=5.3596, this epoch 0.0220, total 21.5258
+++model saved ! 2015.pth
(T) | Epoch=680, loss=5.4302, this epoch 0.0227, total 21.5485
(T) | Epoch=681, loss=5.4128, this epoch 0.0275, total 21.5761
(T) | Epoch=682, loss=5.3741, this epoch 0.0223, total 21.5984
(T) | Epoch=683, loss=5.3642, this epoch 0.0211, total 21.6195
(T) | Epoch=684, loss=5.3765, this epoch 0.0217, total 21.6412
(T) | Epoch=685, loss=5.4445, this epoch 0.0217, total 21.6629
(T) | Epoch=686, loss=5.3769, this epoch 0.0216, total 21.6845
(T) | Epoch=687, loss=5.3681, this epoch 0.0207, total 21.7052
(T) | Epoch=688, loss=5.3696, this epoch 0.0209, total 21.7261
(T) | Epoch=689, loss=5.4498, this epoch 0.0210, total 21.7471
(T) | Epoch=690, loss=5.4072, this epoch 0.0205, total 21.7677
(T) | Epoch=691, loss=5.4061, this epoch 0.0206, total 21.7882
(T) | Epoch=692, loss=5.3829, this epoch 0.0280, total 21.8162
(T) | Epoch=693, loss=5.3655, this epoch 0.0222, total 21.8385
(T) | Epoch=694, loss=5.3944, this epoch 0.0214, total 21.8598
(T) | Epoch=695, loss=5.5199, this epoch 0.0217, total 21.8815
(T) | Epoch=696, loss=5.4869, this epoch 0.0289, total 21.9104
(T) | Epoch=697, loss=5.3692, this epoch 0.0212, total 21.9317
(T) | Epoch=698, loss=5.3732, this epoch 0.0216, total 21.9533
(T) | Epoch=699, loss=5.4472, this epoch 0.0217, total 21.9750
(T) | Epoch=700, loss=5.4123, this epoch 0.0210, total 21.9959
(T) | Epoch=701, loss=5.4575, this epoch 0.0290, total 22.0249
(T) | Epoch=702, loss=5.5159, this epoch 0.0210, total 22.0460
(T) | Epoch=703, loss=5.4134, this epoch 0.0284, total 22.0744
(T) | Epoch=704, loss=5.4620, this epoch 0.0277, total 22.1021
(T) | Epoch=705, loss=5.4137, this epoch 0.0293, total 22.1315
(T) | Epoch=706, loss=5.4683, this epoch 0.0209, total 22.1523
(T) | Epoch=707, loss=5.5749, this epoch 0.0209, total 22.1732
(T) | Epoch=708, loss=5.5047, this epoch 0.0212, total 22.1944
(T) | Epoch=709, loss=5.3673, this epoch 0.0207, total 22.2151
(T) | Epoch=710, loss=5.3699, this epoch 0.0206, total 22.2357
(T) | Epoch=711, loss=5.4399, this epoch 0.0282, total 22.2638
(T) | Epoch=712, loss=5.3803, this epoch 0.0222, total 22.2860
(T) | Epoch=713, loss=5.3843, this epoch 0.0223, total 22.3083
(T) | Epoch=714, loss=5.3793, this epoch 0.0269, total 22.3352
(T) | Epoch=715, loss=5.3783, this epoch 0.0226, total 22.3578
(T) | Epoch=716, loss=5.4011, this epoch 0.0279, total 22.3857
(T) | Epoch=717, loss=5.3734, this epoch 0.0225, total 22.4081
(T) | Epoch=718, loss=5.4123, this epoch 0.0287, total 22.4368
(T) | Epoch=719, loss=5.4271, this epoch 0.0349, total 22.4717
(T) | Epoch=720, loss=5.4221, this epoch 0.0308, total 22.5025
(T) | Epoch=721, loss=5.3811, this epoch 0.0294, total 22.5319
(T) | Epoch=722, loss=5.4150, this epoch 0.0293, total 22.5612
(T) | Epoch=723, loss=5.4235, this epoch 0.0352, total 22.5964
(T) | Epoch=724, loss=5.4279, this epoch 0.0314, total 22.6278
(T) | Epoch=725, loss=5.3678, this epoch 0.0367, total 22.6645
(T) | Epoch=726, loss=5.4044, this epoch 0.0312, total 22.6956
(T) | Epoch=727, loss=5.3635, this epoch 0.0292, total 22.7248
(T) | Epoch=728, loss=5.5054, this epoch 0.0291, total 22.7539
(T) | Epoch=729, loss=5.3890, this epoch 0.0345, total 22.7884
(T) | Epoch=730, loss=5.4012, this epoch 0.0380, total 22.8264
(T) | Epoch=731, loss=5.4674, this epoch 0.0358, total 22.8622
(T) | Epoch=732, loss=5.3972, this epoch 0.0310, total 22.8932
(T) | Epoch=733, loss=5.4267, this epoch 0.0312, total 22.9244
(T) | Epoch=734, loss=5.4719, this epoch 0.0358, total 22.9602
(T) | Epoch=735, loss=5.4134, this epoch 0.0359, total 22.9961
(T) | Epoch=736, loss=5.3880, this epoch 0.0309, total 23.0270
(T) | Epoch=737, loss=5.3600, this epoch 0.0292, total 23.0562
(T) | Epoch=738, loss=5.3930, this epoch 0.0272, total 23.0835
(T) | Epoch=739, loss=5.3802, this epoch 0.0333, total 23.1167
(T) | Epoch=740, loss=5.3979, this epoch 0.0351, total 23.1519
(T) | Epoch=741, loss=5.4039, this epoch 0.0313, total 23.1832
(T) | Epoch=742, loss=5.3895, this epoch 0.0387, total 23.2219
(T) | Epoch=743, loss=5.4181, this epoch 0.0233, total 23.2452
(T) | Epoch=744, loss=5.3665, this epoch 0.0212, total 23.2664
(T) | Epoch=745, loss=5.4168, this epoch 0.0290, total 23.2954
(T) | Epoch=746, loss=5.3958, this epoch 0.0290, total 23.3244
(T) | Epoch=747, loss=5.4048, this epoch 0.0346, total 23.3590
(T) | Epoch=748, loss=5.4325, this epoch 0.0290, total 23.3880
(T) | Epoch=749, loss=5.3737, this epoch 0.0229, total 23.4109
(T) | Epoch=750, loss=5.4135, this epoch 0.0214, total 23.4322
(T) | Epoch=751, loss=5.3698, this epoch 0.0206, total 23.4529
(T) | Epoch=752, loss=5.4782, this epoch 0.0276, total 23.4805
(T) | Epoch=753, loss=5.4658, this epoch 0.0222, total 23.5027
(T) | Epoch=754, loss=5.3614, this epoch 0.0219, total 23.5246
(T) | Epoch=755, loss=5.4456, this epoch 0.0214, total 23.5460
(T) | Epoch=756, loss=5.4411, this epoch 0.0214, total 23.5674
(T) | Epoch=757, loss=5.3949, this epoch 0.0218, total 23.5892
(T) | Epoch=758, loss=5.4387, this epoch 0.0214, total 23.6106
(T) | Epoch=759, loss=5.4691, this epoch 0.0290, total 23.6396
(T) | Epoch=760, loss=5.4357, this epoch 0.0216, total 23.6611
(T) | Epoch=761, loss=5.3539, this epoch 0.0218, total 23.6830
+++model saved ! 2015.pth
(T) | Epoch=762, loss=5.4288, this epoch 0.0415, total 23.7245
(T) | Epoch=763, loss=5.3986, this epoch 0.0281, total 23.7526
(T) | Epoch=764, loss=5.3650, this epoch 0.0335, total 23.7861
(T) | Epoch=765, loss=5.3671, this epoch 0.0327, total 23.8188
(T) | Epoch=766, loss=5.4010, this epoch 0.0225, total 23.8413
(T) | Epoch=767, loss=5.3996, this epoch 0.0279, total 23.8692
(T) | Epoch=768, loss=5.4140, this epoch 0.0286, total 23.8978
(T) | Epoch=769, loss=5.3702, this epoch 0.0365, total 23.9343
(T) | Epoch=770, loss=5.4556, this epoch 0.0223, total 23.9566
(T) | Epoch=771, loss=5.3688, this epoch 0.0224, total 23.9790
(T) | Epoch=772, loss=5.3623, this epoch 0.0276, total 24.0065
(T) | Epoch=773, loss=5.4249, this epoch 0.0309, total 24.0374
(T) | Epoch=774, loss=5.3988, this epoch 0.0282, total 24.0656
(T) | Epoch=775, loss=5.3867, this epoch 0.0218, total 24.0874
(T) | Epoch=776, loss=5.3730, this epoch 0.0363, total 24.1237
(T) | Epoch=777, loss=5.4105, this epoch 0.0300, total 24.1538
(T) | Epoch=778, loss=5.3803, this epoch 0.0294, total 24.1832
(T) | Epoch=779, loss=5.3936, this epoch 0.0396, total 24.2228
(T) | Epoch=780, loss=5.3704, this epoch 0.0385, total 24.2613
(T) | Epoch=781, loss=5.4121, this epoch 0.0396, total 24.3009
(T) | Epoch=782, loss=5.3946, this epoch 0.0323, total 24.3332
(T) | Epoch=783, loss=5.3889, this epoch 0.0342, total 24.3674
(T) | Epoch=784, loss=5.3704, this epoch 0.0386, total 24.4060
(T) | Epoch=785, loss=5.4164, this epoch 0.0305, total 24.4365
(T) | Epoch=786, loss=5.4374, this epoch 0.0291, total 24.4656
(T) | Epoch=787, loss=5.3925, this epoch 0.0289, total 24.4945
(T) | Epoch=788, loss=5.3876, this epoch 0.0373, total 24.5318
(T) | Epoch=789, loss=5.3820, this epoch 0.0403, total 24.5722
(T) | Epoch=790, loss=5.3742, this epoch 0.0397, total 24.6118
(T) | Epoch=791, loss=5.3743, this epoch 0.0306, total 24.6425
(T) | Epoch=792, loss=5.4089, this epoch 0.0373, total 24.6798
(T) | Epoch=793, loss=5.3674, this epoch 0.0395, total 24.7193
(T) | Epoch=794, loss=5.3959, this epoch 0.0403, total 24.7596
(T) | Epoch=795, loss=5.4421, this epoch 0.0398, total 24.7994
(T) | Epoch=796, loss=5.4073, this epoch 0.0318, total 24.8312
(T) | Epoch=797, loss=5.3637, this epoch 0.0355, total 24.8668
(T) | Epoch=798, loss=5.3967, this epoch 0.0407, total 24.9074
(T) | Epoch=799, loss=5.4872, this epoch 0.0398, total 24.9472
(T) | Epoch=800, loss=5.3723, this epoch 0.0357, total 24.9829
(T) | Epoch=801, loss=5.3682, this epoch 0.0397, total 25.0226
(T) | Epoch=802, loss=5.4193, this epoch 0.0312, total 25.0538
(T) | Epoch=803, loss=5.4744, this epoch 0.0334, total 25.0872
(T) | Epoch=804, loss=5.3613, this epoch 0.0371, total 25.1243
(T) | Epoch=805, loss=5.4679, this epoch 0.0364, total 25.1608
(T) | Epoch=806, loss=5.3684, this epoch 0.0391, total 25.1999
(T) | Epoch=807, loss=5.3621, this epoch 0.0332, total 25.2331
(T) | Epoch=808, loss=5.3791, this epoch 0.0373, total 25.2704
(T) | Epoch=809, loss=5.4098, this epoch 0.0368, total 25.3071
(T) | Epoch=810, loss=5.4331, this epoch 0.0332, total 25.3403
(T) | Epoch=811, loss=5.4196, this epoch 0.0319, total 25.3722
(T) | Epoch=812, loss=5.3887, this epoch 0.0383, total 25.4105
(T) | Epoch=813, loss=5.4669, this epoch 0.0329, total 25.4434
(T) | Epoch=814, loss=5.3837, this epoch 0.0418, total 25.4851
(T) | Epoch=815, loss=5.4138, this epoch 0.0318, total 25.5170
(T) | Epoch=816, loss=5.3783, this epoch 0.0389, total 25.5559
(T) | Epoch=817, loss=5.4120, this epoch 0.0392, total 25.5951
(T) | Epoch=818, loss=5.3779, this epoch 0.0405, total 25.6356
(T) | Epoch=819, loss=5.4079, this epoch 0.0384, total 25.6740
(T) | Epoch=820, loss=5.3661, this epoch 0.0396, total 25.7136
(T) | Epoch=821, loss=5.3759, this epoch 0.0403, total 25.7539
(T) | Epoch=822, loss=5.3918, this epoch 0.0362, total 25.7901
(T) | Epoch=823, loss=5.3698, this epoch 0.0334, total 25.8235
(T) | Epoch=824, loss=5.3619, this epoch 0.0374, total 25.8608
(T) | Epoch=825, loss=5.4502, this epoch 0.0411, total 25.9020
(T) | Epoch=826, loss=5.3947, this epoch 0.0402, total 25.9422
(T) | Epoch=827, loss=5.3738, this epoch 0.0385, total 25.9807
(T) | Epoch=828, loss=5.3934, this epoch 0.0365, total 26.0171
(T) | Epoch=829, loss=5.3747, this epoch 0.0309, total 26.0480
(T) | Epoch=830, loss=5.4158, this epoch 0.0372, total 26.0852
(T) | Epoch=831, loss=5.3569, this epoch 0.0320, total 26.1172
(T) | Epoch=832, loss=5.3737, this epoch 0.0285, total 26.1457
(T) | Epoch=833, loss=5.4222, this epoch 0.0384, total 26.1841
(T) | Epoch=834, loss=5.3644, this epoch 0.0328, total 26.2169
(T) | Epoch=835, loss=5.6100, this epoch 0.0375, total 26.2544
(T) | Epoch=836, loss=5.3763, this epoch 0.0392, total 26.2936
(T) | Epoch=837, loss=5.4399, this epoch 0.0306, total 26.3242
(T) | Epoch=838, loss=5.4819, this epoch 0.0398, total 26.3640
(T) | Epoch=839, loss=5.4625, this epoch 0.0313, total 26.3952
(T) | Epoch=840, loss=5.4507, this epoch 0.0392, total 26.4344
(T) | Epoch=841, loss=5.4639, this epoch 0.0390, total 26.4734
(T) | Epoch=842, loss=5.5462, this epoch 0.0308, total 26.5042
(T) | Epoch=843, loss=5.4335, this epoch 0.0374, total 26.5415
(T) | Epoch=844, loss=5.4010, this epoch 0.0400, total 26.5815
(T) | Epoch=845, loss=5.4892, this epoch 0.0323, total 26.6139
(T) | Epoch=846, loss=5.3663, this epoch 0.0353, total 26.6492
(T) | Epoch=847, loss=5.4921, this epoch 0.0391, total 26.6883
(T) | Epoch=848, loss=5.3733, this epoch 0.0336, total 26.7219
(T) | Epoch=849, loss=5.3962, this epoch 0.0383, total 26.7602
(T) | Epoch=850, loss=5.3674, this epoch 0.0387, total 26.7989
(T) | Epoch=851, loss=5.4591, this epoch 0.0388, total 26.8377
(T) | Epoch=852, loss=5.4907, this epoch 0.0402, total 26.8779
(T) | Epoch=853, loss=5.4064, this epoch 0.0358, total 26.9137
(T) | Epoch=854, loss=5.4593, this epoch 0.0331, total 26.9468
(T) | Epoch=855, loss=5.3664, this epoch 0.0383, total 26.9851
(T) | Epoch=856, loss=5.4139, this epoch 0.0308, total 27.0159
(T) | Epoch=857, loss=5.4293, this epoch 0.0396, total 27.0555
(T) | Epoch=858, loss=5.3614, this epoch 0.0377, total 27.0932
(T) | Epoch=859, loss=5.3615, this epoch 0.0310, total 27.1242
(T) | Epoch=860, loss=5.4114, this epoch 0.0219, total 27.1461
(T) | Epoch=861, loss=5.3956, this epoch 0.0273, total 27.1734
(T) | Epoch=862, loss=5.4147, this epoch 0.0404, total 27.2138
(T) | Epoch=863, loss=5.4119, this epoch 0.0386, total 27.2524
(T) | Epoch=864, loss=5.4430, this epoch 0.0383, total 27.2906
(T) | Epoch=865, loss=5.4192, this epoch 0.0389, total 27.3295
(T) | Epoch=866, loss=5.4617, this epoch 0.0380, total 27.3675
(T) | Epoch=867, loss=5.3594, this epoch 0.0309, total 27.3984
(T) | Epoch=868, loss=5.3947, this epoch 0.0392, total 27.4376
(T) | Epoch=869, loss=5.3578, this epoch 0.0328, total 27.4704
(T) | Epoch=870, loss=5.3803, this epoch 0.0323, total 27.5027
(T) | Epoch=871, loss=5.4395, this epoch 0.0385, total 27.5412
(T) | Epoch=872, loss=5.5254, this epoch 0.0418, total 27.5830
(T) | Epoch=873, loss=5.4054, this epoch 0.0307, total 27.6137
(T) | Epoch=874, loss=5.3704, this epoch 0.0374, total 27.6511
(T) | Epoch=875, loss=5.3595, this epoch 0.0401, total 27.6912
(T) | Epoch=876, loss=5.3830, this epoch 0.0329, total 27.7241
(T) | Epoch=877, loss=5.3606, this epoch 0.0305, total 27.7546
(T) | Epoch=878, loss=5.3745, this epoch 0.0323, total 27.7869
(T) | Epoch=879, loss=5.4077, this epoch 0.0367, total 27.8236
(T) | Epoch=880, loss=5.4641, this epoch 0.0334, total 27.8570
(T) | Epoch=881, loss=5.3625, this epoch 0.0379, total 27.8949
(T) | Epoch=882, loss=5.5027, this epoch 0.0395, total 27.9344
(T) | Epoch=883, loss=5.4419, this epoch 0.0403, total 27.9747
(T) | Epoch=884, loss=5.4442, this epoch 0.0386, total 28.0133
(T) | Epoch=885, loss=5.4889, this epoch 0.0394, total 28.0526
(T) | Epoch=886, loss=5.3904, this epoch 0.0405, total 28.0931
(T) | Epoch=887, loss=5.3811, this epoch 0.0383, total 28.1315
(T) | Epoch=888, loss=5.3536, this epoch 0.0356, total 28.1671
+++model saved ! 2015.pth
(T) | Epoch=889, loss=5.3893, this epoch 0.0327, total 28.1998
(T) | Epoch=890, loss=5.3615, this epoch 0.0288, total 28.2286
(T) | Epoch=891, loss=5.3730, this epoch 0.0233, total 28.2519
(T) | Epoch=892, loss=5.4291, this epoch 0.0289, total 28.2808
(T) | Epoch=893, loss=5.4850, this epoch 0.0288, total 28.3096
(T) | Epoch=894, loss=5.3885, this epoch 0.0346, total 28.3442
(T) | Epoch=895, loss=5.4378, this epoch 0.0306, total 28.3748
(T) | Epoch=896, loss=5.4141, this epoch 0.0299, total 28.4047
(T) | Epoch=897, loss=5.4474, this epoch 0.0346, total 28.4393
(T) | Epoch=898, loss=5.3651, this epoch 0.0316, total 28.4709
(T) | Epoch=899, loss=5.3834, this epoch 0.0312, total 28.5021
(T) | Epoch=900, loss=5.3587, this epoch 0.0221, total 28.5242
(T) | Epoch=901, loss=5.4534, this epoch 0.0292, total 28.5535
(T) | Epoch=902, loss=5.3763, this epoch 0.0218, total 28.5753
(T) | Epoch=903, loss=5.3644, this epoch 0.0333, total 28.6086
(T) | Epoch=904, loss=5.4079, this epoch 0.0294, total 28.6380
(T) | Epoch=905, loss=5.4267, this epoch 0.0292, total 28.6672
(T) | Epoch=906, loss=5.3807, this epoch 0.0304, total 28.6976
(T) | Epoch=907, loss=5.4625, this epoch 0.0274, total 28.7249
(T) | Epoch=908, loss=5.3605, this epoch 0.0290, total 28.7539
(T) | Epoch=909, loss=5.3567, this epoch 0.0282, total 28.7821
(T) | Epoch=910, loss=5.4293, this epoch 0.0300, total 28.8121
(T) | Epoch=911, loss=5.3916, this epoch 0.0286, total 28.8407
(T) | Epoch=912, loss=5.3558, this epoch 0.0368, total 28.8775
(T) | Epoch=913, loss=5.3560, this epoch 0.0306, total 28.9081
(T) | Epoch=914, loss=5.3572, this epoch 0.0288, total 28.9369
(T) | Epoch=915, loss=5.3912, this epoch 0.0373, total 28.9742
(T) | Epoch=916, loss=5.3535, this epoch 0.0311, total 29.0053
+++model saved ! 2015.pth
(T) | Epoch=917, loss=5.3664, this epoch 0.0235, total 29.0288
(T) | Epoch=918, loss=5.3662, this epoch 0.0276, total 29.0564
(T) | Epoch=919, loss=5.4028, this epoch 0.0223, total 29.0787
(T) | Epoch=920, loss=5.4187, this epoch 0.0218, total 29.1005
(T) | Epoch=921, loss=5.3978, this epoch 0.0219, total 29.1225
(T) | Epoch=922, loss=5.3634, this epoch 0.0216, total 29.1440
(T) | Epoch=923, loss=5.4691, this epoch 0.0215, total 29.1655
(T) | Epoch=924, loss=5.3935, this epoch 0.0219, total 29.1874
(T) | Epoch=925, loss=5.3615, this epoch 0.0352, total 29.2226
(T) | Epoch=926, loss=5.4377, this epoch 0.0365, total 29.2590
(T) | Epoch=927, loss=5.3582, this epoch 0.0309, total 29.2899
(T) | Epoch=928, loss=5.4260, this epoch 0.0345, total 29.3244
(T) | Epoch=929, loss=5.3660, this epoch 0.0307, total 29.3551
(T) | Epoch=930, loss=5.3632, this epoch 0.0285, total 29.3836
(T) | Epoch=931, loss=5.3678, this epoch 0.0225, total 29.4060
(T) | Epoch=932, loss=5.4873, this epoch 0.0211, total 29.4272
(T) | Epoch=933, loss=5.3579, this epoch 0.0282, total 29.4553
(T) | Epoch=934, loss=5.4084, this epoch 0.0303, total 29.4856
(T) | Epoch=935, loss=5.3909, this epoch 0.0281, total 29.5137
(T) | Epoch=936, loss=5.5196, this epoch 0.0343, total 29.5480
(T) | Epoch=937, loss=5.3984, this epoch 0.0311, total 29.5791
(T) | Epoch=938, loss=5.3748, this epoch 0.0230, total 29.6021
(T) | Epoch=939, loss=5.3695, this epoch 0.0332, total 29.6353
(T) | Epoch=940, loss=5.4638, this epoch 0.0276, total 29.6629
(T) | Epoch=941, loss=5.3916, this epoch 0.0301, total 29.6930
(T) | Epoch=942, loss=5.4531, this epoch 0.0299, total 29.7229
(T) | Epoch=943, loss=5.3814, this epoch 0.0295, total 29.7524
(T) | Epoch=944, loss=5.4165, this epoch 0.0377, total 29.7901
(T) | Epoch=945, loss=5.4232, this epoch 0.0451, total 29.8352
(T) | Epoch=946, loss=5.3983, this epoch 0.0402, total 29.8754
(T) | Epoch=947, loss=5.3631, this epoch 0.0314, total 29.9067
(T) | Epoch=948, loss=5.3577, this epoch 0.0208, total 29.9275
(T) | Epoch=949, loss=5.4128, this epoch 0.0280, total 29.9555
(T) | Epoch=950, loss=5.4152, this epoch 0.0293, total 29.9848
(T) | Epoch=951, loss=5.5175, this epoch 0.0225, total 30.0073
(T) | Epoch=952, loss=5.4276, this epoch 0.0215, total 30.0289
(T) | Epoch=953, loss=5.3794, this epoch 0.0287, total 30.0576
(T) | Epoch=954, loss=5.3561, this epoch 0.0290, total 30.0866
(T) | Epoch=955, loss=5.5146, this epoch 0.0321, total 30.1187
(T) | Epoch=956, loss=5.4424, this epoch 0.0349, total 30.1536
(T) | Epoch=957, loss=5.4077, this epoch 0.0245, total 30.1781
(T) | Epoch=958, loss=5.3551, this epoch 0.0282, total 30.2064
(T) | Epoch=959, loss=5.3544, this epoch 0.0210, total 30.2273
(T) | Epoch=960, loss=5.3548, this epoch 0.0351, total 30.2624
(T) | Epoch=961, loss=5.3850, this epoch 0.0312, total 30.2936
(T) | Epoch=962, loss=5.3611, this epoch 0.0218, total 30.3155
(T) | Epoch=963, loss=5.3906, this epoch 0.0213, total 30.3368
(T) | Epoch=964, loss=5.4360, this epoch 0.0290, total 30.3658
(T) | Epoch=965, loss=5.3804, this epoch 0.0279, total 30.3937
(T) | Epoch=966, loss=5.3631, this epoch 0.0234, total 30.4171
(T) | Epoch=967, loss=5.3901, this epoch 0.0296, total 30.4467
(T) | Epoch=968, loss=5.3902, this epoch 0.0217, total 30.4684
(T) | Epoch=969, loss=5.7143, this epoch 0.0286, total 30.4970
(T) | Epoch=970, loss=5.3718, this epoch 0.0389, total 30.5359
(T) | Epoch=971, loss=5.3679, this epoch 0.0309, total 30.5668
(T) | Epoch=972, loss=5.3615, this epoch 0.0294, total 30.5962
(T) | Epoch=973, loss=5.3878, this epoch 0.0222, total 30.6184
(T) | Epoch=974, loss=5.4083, this epoch 0.0356, total 30.6540
(T) | Epoch=975, loss=5.3535, this epoch 0.0314, total 30.6854
(T) | Epoch=976, loss=5.4206, this epoch 0.0348, total 30.7203
(T) | Epoch=977, loss=5.4463, this epoch 0.0229, total 30.7432
(T) | Epoch=978, loss=5.3644, this epoch 0.0295, total 30.7727
(T) | Epoch=979, loss=5.4362, this epoch 0.0272, total 30.7999
(T) | Epoch=980, loss=5.3540, this epoch 0.0220, total 30.8219
(T) | Epoch=981, loss=5.4379, this epoch 0.0292, total 30.8512
(T) | Epoch=982, loss=5.3980, this epoch 0.0296, total 30.8807
(T) | Epoch=983, loss=5.3550, this epoch 0.0293, total 30.9100
(T) | Epoch=984, loss=5.3559, this epoch 0.0215, total 30.9315
(T) | Epoch=985, loss=5.3511, this epoch 0.0289, total 30.9604
+++model saved ! 2015.pth
(T) | Epoch=986, loss=5.3598, this epoch 0.0283, total 30.9887
(T) | Epoch=987, loss=5.4850, this epoch 0.0289, total 31.0175
(T) | Epoch=988, loss=5.4163, this epoch 0.0277, total 31.0452
(T) | Epoch=989, loss=5.3497, this epoch 0.0294, total 31.0746
+++model saved ! 2015.pth
(T) | Epoch=990, loss=5.3871, this epoch 0.0412, total 31.1158
(T) | Epoch=991, loss=5.4118, this epoch 0.0379, total 31.1537
(T) | Epoch=992, loss=5.4212, this epoch 0.0323, total 31.1860
(T) | Epoch=993, loss=5.4191, this epoch 0.0311, total 31.2171
(T) | Epoch=994, loss=5.4329, this epoch 0.0313, total 31.2484
(T) | Epoch=995, loss=5.3584, this epoch 0.0391, total 31.2875
(T) | Epoch=996, loss=5.3596, this epoch 0.0352, total 31.3227
(T) | Epoch=997, loss=5.4289, this epoch 0.0387, total 31.3613
(T) | Epoch=998, loss=5.3952, this epoch 0.0327, total 31.3941
(T) | Epoch=999, loss=5.4049, this epoch 0.0315, total 31.4256
(T) | Epoch=1000, loss=5.4135, this epoch 0.0326, total 31.4582
=== Final ===

==============================
LoRA FINE-TUNING
==============================
Random seed set to 3
Epoch: 0, loss: 24.6812, train_acc: 0.3104, train_recall: 0.1071, train_f1: 0.0532, val_acc: 0.295566, val_recall: 0.104987, val_f1: 0.051480
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed3.csv
Epoch: 1, loss: 71.1587, train_acc: 0.3030, train_recall: 0.1111, train_f1: 0.0517, val_acc: 0.320197, val_recall: 0.125000, val_f1: 0.060748
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed3.csv
Epoch: 2, loss: 118.9642, train_acc: 0.2129, train_recall: 0.1095, train_f1: 0.0391, val_acc: 0.226601, val_recall: 0.121053, val_f1: 0.046748
Epoch: 3, loss: 164.5558, train_acc: 0.0244, train_recall: 0.1111, train_f1: 0.0053, val_acc: 0.032020, val_recall: 0.125000, val_f1: 0.007757
Epoch: 4, loss: 112.5814, train_acc: 0.0244, train_recall: 0.1111, train_f1: 0.0053, val_acc: 0.032020, val_recall: 0.125000, val_f1: 0.007757
Epoch: 5, loss: 91.0434, train_acc: 0.2161, train_recall: 0.1111, train_f1: 0.0395, val_acc: 0.233990, val_recall: 0.125000, val_f1: 0.047405
Epoch: 6, loss: 113.3289, train_acc: 0.0720, train_recall: 0.1111, train_f1: 0.0149, val_acc: 0.046798, val_recall: 0.125000, val_f1: 0.011176
Epoch: 7, loss: 93.7915, train_acc: 0.0201, train_recall: 0.1111, train_f1: 0.0044, val_acc: 0.019704, val_recall: 0.125000, val_f1: 0.004831
Epoch: 8, loss: 48.9925, train_acc: 0.3051, train_recall: 0.1392, train_f1: 0.0687, val_acc: 0.322660, val_recall: 0.199279, val_f1: 0.095148
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed3.csv
Epoch: 9, loss: 70.5232, train_acc: 0.0011, train_recall: 0.1111, train_f1: 0.0002, val_acc: 0.002463, val_recall: 0.125000, val_f1: 0.000614
Epoch: 10, loss: 70.0716, train_acc: 0.3220, train_recall: 0.1111, train_f1: 0.0541, val_acc: 0.312808, val_recall: 0.125000, val_f1: 0.059568
Epoch: 11, loss: 75.0685, train_acc: 0.3220, train_recall: 0.1111, train_f1: 0.0541, val_acc: 0.312808, val_recall: 0.125000, val_f1: 0.059568
Epoch: 12, loss: 67.3175, train_acc: 0.3220, train_recall: 0.1111, train_f1: 0.0541, val_acc: 0.312808, val_recall: 0.125000, val_f1: 0.059568
Epoch: 13, loss: 66.4797, train_acc: 0.3030, train_recall: 0.1111, train_f1: 0.0517, val_acc: 0.320197, val_recall: 0.125000, val_f1: 0.060634
Epoch: 14, loss: 67.6712, train_acc: 0.3030, train_recall: 0.1111, train_f1: 0.0517, val_acc: 0.320197, val_recall: 0.125000, val_f1: 0.060634
Epoch: 15, loss: 58.7789, train_acc: 0.3030, train_recall: 0.1111, train_f1: 0.0517, val_acc: 0.320197, val_recall: 0.125000, val_f1: 0.060634
Epoch: 16, loss: 58.7976, train_acc: 0.2161, train_recall: 0.1111, train_f1: 0.0395, val_acc: 0.233990, val_recall: 0.125000, val_f1: 0.047405
Epoch: 17, loss: 60.8230, train_acc: 0.2161, train_recall: 0.1111, train_f1: 0.0395, val_acc: 0.233990, val_recall: 0.125000, val_f1: 0.047405
Epoch: 18, loss: 57.4049, train_acc: 0.3220, train_recall: 0.1111, train_f1: 0.0541, val_acc: 0.312808, val_recall: 0.125000, val_f1: 0.059568
Epoch: 19, loss: 57.9544, train_acc: 0.3220, train_recall: 0.1111, train_f1: 0.0541, val_acc: 0.312808, val_recall: 0.125000, val_f1: 0.059568
Epoch: 20, loss: 50.9979, train_acc: 0.3220, train_recall: 0.1111, train_f1: 0.0541, val_acc: 0.312808, val_recall: 0.125000, val_f1: 0.059568
Epoch: 21, loss: 46.7931, train_acc: 0.2172, train_recall: 0.1115, train_f1: 0.0403, val_acc: 0.233990, val_recall: 0.125000, val_f1: 0.047500
Epoch: 22, loss: 47.2742, train_acc: 0.3019, train_recall: 0.1447, train_f1: 0.0776, val_acc: 0.315271, val_recall: 0.166346, val_f1: 0.088307
Epoch: 23, loss: 52.9671, train_acc: 0.3030, train_recall: 0.1111, train_f1: 0.0517, val_acc: 0.320197, val_recall: 0.125000, val_f1: 0.060634
Epoch: 24, loss: 51.0644, train_acc: 0.3030, train_recall: 0.1111, train_f1: 0.0517, val_acc: 0.320197, val_recall: 0.125000, val_f1: 0.060634
Epoch: 25, loss: 42.5290, train_acc: 0.3072, train_recall: 0.1126, train_f1: 0.0547, val_acc: 0.327586, val_recall: 0.127975, val_f1: 0.068368
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed3.csv
Epoch: 26, loss: 38.3862, train_acc: 0.2161, train_recall: 0.1091, train_f1: 0.0456, val_acc: 0.238916, val_recall: 0.125311, val_f1: 0.058630
Epoch: 27, loss: 45.1586, train_acc: 0.3220, train_recall: 0.1111, train_f1: 0.0541, val_acc: 0.312808, val_recall: 0.125000, val_f1: 0.059568
Epoch: 28, loss: 52.1606, train_acc: 0.0837, train_recall: 0.1151, train_f1: 0.0227, val_acc: 0.064039, val_recall: 0.131890, val_f1: 0.023795
Epoch: 29, loss: 50.2629, train_acc: 0.3220, train_recall: 0.1111, train_f1: 0.0541, val_acc: 0.312808, val_recall: 0.125000, val_f1: 0.059568
Epoch: 30, loss: 47.5035, train_acc: 0.3220, train_recall: 0.1111, train_f1: 0.0541, val_acc: 0.312808, val_recall: 0.125000, val_f1: 0.059568
Epoch: 31, loss: 49.7118, train_acc: 0.0392, train_recall: 0.1111, train_f1: 0.0084, val_acc: 0.036946, val_recall: 0.126969, val_f1: 0.011659
Epoch: 32, loss: 46.0772, train_acc: 0.2161, train_recall: 0.1111, train_f1: 0.0395, val_acc: 0.233990, val_recall: 0.125000, val_f1: 0.047405
Epoch: 33, loss: 42.6586, train_acc: 0.2161, train_recall: 0.1111, train_f1: 0.0395, val_acc: 0.233990, val_recall: 0.125000, val_f1: 0.047405
Epoch: 34, loss: 36.1038, train_acc: 0.3136, train_recall: 0.1147, train_f1: 0.0600, val_acc: 0.332512, val_recall: 0.129967, val_f1: 0.073744
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed3.csv
Epoch: 35, loss: 37.9768, train_acc: 0.3157, train_recall: 0.1155, train_f1: 0.0617, val_acc: 0.327586, val_recall: 0.128044, val_f1: 0.073113
Epoch: 36, loss: 41.0522, train_acc: 0.3220, train_recall: 0.1111, train_f1: 0.0541, val_acc: 0.312808, val_recall: 0.125000, val_f1: 0.059568
Epoch: 37, loss: 41.2024, train_acc: 0.3220, train_recall: 0.1111, train_f1: 0.0541, val_acc: 0.312808, val_recall: 0.125000, val_f1: 0.059568
Epoch: 38, loss: 43.0617, train_acc: 0.0837, train_recall: 0.1151, train_f1: 0.0227, val_acc: 0.064039, val_recall: 0.131890, val_f1: 0.023851
Epoch: 39, loss: 39.5348, train_acc: 0.3136, train_recall: 0.1147, train_f1: 0.0600, val_acc: 0.332512, val_recall: 0.129967, val_f1: 0.073744
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed3.csv
Epoch: 40, loss: 37.4923, train_acc: 0.3136, train_recall: 0.1147, train_f1: 0.0600, val_acc: 0.330049, val_recall: 0.129005, val_f1: 0.073295
Epoch: 41, loss: 36.6037, train_acc: 0.3220, train_recall: 0.1111, train_f1: 0.0541, val_acc: 0.312808, val_recall: 0.125000, val_f1: 0.059568
Epoch: 42, loss: 42.5863, train_acc: 0.2161, train_recall: 0.1111, train_f1: 0.0395, val_acc: 0.233990, val_recall: 0.125000, val_f1: 0.047405
Epoch: 43, loss: 42.1472, train_acc: 0.2161, train_recall: 0.1111, train_f1: 0.0395, val_acc: 0.233990, val_recall: 0.125000, val_f1: 0.047500
Epoch: 44, loss: 38.0571, train_acc: 0.3220, train_recall: 0.1111, train_f1: 0.0541, val_acc: 0.312808, val_recall: 0.125000, val_f1: 0.059568
Epoch: 45, loss: 35.5318, train_acc: 0.3220, train_recall: 0.1111, train_f1: 0.0541, val_acc: 0.312808, val_recall: 0.125000, val_f1: 0.059568
Epoch: 46, loss: 37.2288, train_acc: 0.3136, train_recall: 0.1167, train_f1: 0.0632, val_acc: 0.327586, val_recall: 0.129656, val_f1: 0.072771
Epoch: 47, loss: 37.8706, train_acc: 0.3136, train_recall: 0.1167, train_f1: 0.0632, val_acc: 0.327586, val_recall: 0.129656, val_f1: 0.072771
Epoch: 48, loss: 33.9733, train_acc: 0.3019, train_recall: 0.1124, train_f1: 0.0615, val_acc: 0.312808, val_recall: 0.124241, val_f1: 0.072021
Epoch: 49, loss: 34.4570, train_acc: 0.2161, train_recall: 0.1111, train_f1: 0.0395, val_acc: 0.233990, val_recall: 0.125000, val_f1: 0.047405
Epoch: 50, loss: 34.3902, train_acc: 0.0837, train_recall: 0.1171, train_f1: 0.0264, val_acc: 0.059113, val_recall: 0.116959, val_f1: 0.020856
Epoch: 51, loss: 34.6967, train_acc: 0.3220, train_recall: 0.1111, train_f1: 0.0552, val_acc: 0.312808, val_recall: 0.111111, val_f1: 0.054169
Epoch: 52, loss: 34.5579, train_acc: 0.3220, train_recall: 0.1111, train_f1: 0.0552, val_acc: 0.312808, val_recall: 0.111111, val_f1: 0.054169
Epoch: 53, loss: 30.0850, train_acc: 0.3189, train_recall: 0.1102, train_f1: 0.0548, val_acc: 0.300493, val_recall: 0.120079, val_f1: 0.057985
Epoch: 54, loss: 32.1779, train_acc: 0.3114, train_recall: 0.1156, train_f1: 0.0617, val_acc: 0.322660, val_recall: 0.127024, val_f1: 0.068446
Epoch: 55, loss: 32.5277, train_acc: 0.3104, train_recall: 0.1362, train_f1: 0.0872, val_acc: 0.322660, val_recall: 0.161994, val_f1: 0.101558
Epoch: 56, loss: 35.9859, train_acc: 0.2172, train_recall: 0.1362, train_f1: 0.0655, val_acc: 0.236453, val_recall: 0.159514, val_f1: 0.075895
Epoch: 57, loss: 36.6422, train_acc: 0.0392, train_recall: 0.1111, train_f1: 0.0084, val_acc: 0.034483, val_recall: 0.126316, val_f1: 0.010357
Epoch: 58, loss: 32.4652, train_acc: 0.2225, train_recall: 0.1369, train_f1: 0.0770, val_acc: 0.236453, val_recall: 0.134413, val_f1: 0.069633
Epoch: 59, loss: 30.5896, train_acc: 0.3104, train_recall: 0.1416, train_f1: 0.0748, val_acc: 0.330049, val_recall: 0.180331, val_f1: 0.095997
Epoch: 60, loss: 28.5683, train_acc: 0.3072, train_recall: 0.1510, train_f1: 0.0740, val_acc: 0.330049, val_recall: 0.217181, val_f1: 0.108279
Epoch: 61, loss: 31.5718, train_acc: 0.3136, train_recall: 0.1356, train_f1: 0.0697, val_acc: 0.312808, val_recall: 0.198204, val_f1: 0.098984
Epoch: 62, loss: 33.8296, train_acc: 0.3136, train_recall: 0.1356, train_f1: 0.0697, val_acc: 0.312808, val_recall: 0.198204, val_f1: 0.098984
Epoch: 63, loss: 33.8383, train_acc: 0.0805, train_recall: 0.1524, train_f1: 0.0362, val_acc: 0.064039, val_recall: 0.219734, val_f1: 0.058146
Epoch: 64, loss: 31.0497, train_acc: 0.3136, train_recall: 0.1466, train_f1: 0.0734, val_acc: 0.310345, val_recall: 0.211860, val_f1: 0.102381
Epoch: 65, loss: 27.0882, train_acc: 0.3242, train_recall: 0.1683, train_f1: 0.0883, val_acc: 0.312808, val_recall: 0.213839, val_f1: 0.104780
Epoch: 66, loss: 28.6519, train_acc: 0.2214, train_recall: 0.1404, train_f1: 0.0628, val_acc: 0.243842, val_recall: 0.201809, val_f1: 0.102702
Epoch: 67, loss: 31.6747, train_acc: 0.3189, train_recall: 0.1460, train_f1: 0.0907, val_acc: 0.339901, val_recall: 0.207781, val_f1: 0.136713
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed3.csv
Epoch: 68, loss: 32.3111, train_acc: 0.3178, train_recall: 0.1401, train_f1: 0.0900, val_acc: 0.332512, val_recall: 0.160906, val_f1: 0.109198
Epoch: 69, loss: 28.7285, train_acc: 0.3114, train_recall: 0.1214, train_f1: 0.0734, val_acc: 0.325123, val_recall: 0.129049, val_f1: 0.074253
Epoch: 70, loss: 28.7896, train_acc: 0.2161, train_recall: 0.1111, train_f1: 0.0395, val_acc: 0.233990, val_recall: 0.125000, val_f1: 0.047405
Epoch: 71, loss: 30.2503, train_acc: 0.3220, train_recall: 0.1116, train_f1: 0.0571, val_acc: 0.302956, val_recall: 0.121395, val_f1: 0.060436
Epoch: 72, loss: 31.2330, train_acc: 0.3231, train_recall: 0.1159, train_f1: 0.0634, val_acc: 0.307882, val_recall: 0.123031, val_f1: 0.058851
Epoch: 73, loss: 27.8789, train_acc: 0.3242, train_recall: 0.1183, train_f1: 0.0741, val_acc: 0.305419, val_recall: 0.123373, val_f1: 0.068314
Epoch: 74, loss: 29.6549, train_acc: 0.0943, train_recall: 0.1654, train_f1: 0.0745, val_acc: 0.071429, val_recall: 0.163057, val_f1: 0.057407
Epoch: 75, loss: 30.1367, train_acc: 0.2267, train_recall: 0.1594, train_f1: 0.0886, val_acc: 0.241379, val_recall: 0.153846, val_f1: 0.078569
Epoch: 76, loss: 27.6594, train_acc: 0.3136, train_recall: 0.1662, train_f1: 0.1170, val_acc: 0.325123, val_recall: 0.155719, val_f1: 0.106228
Epoch: 77, loss: 30.5885, train_acc: 0.3231, train_recall: 0.1688, train_f1: 0.1126, val_acc: 0.327586, val_recall: 0.154909, val_f1: 0.098366
Epoch: 78, loss: 29.8770, train_acc: 0.3199, train_recall: 0.1800, train_f1: 0.1100, val_acc: 0.325123, val_recall: 0.153593, val_f1: 0.094459
Epoch: 79, loss: 29.1288, train_acc: 0.0191, train_recall: 0.1797, train_f1: 0.0525, val_acc: 0.007389, val_recall: 0.025641, val_f1: 0.023810
Epoch: 80, loss: 27.1444, train_acc: 0.0413, train_recall: 0.1427, train_f1: 0.0614, val_acc: 0.029557, val_recall: 0.106753, val_f1: 0.033719
Epoch: 81, loss: 29.5888, train_acc: 0.3326, train_recall: 0.1773, train_f1: 0.1106, val_acc: 0.315271, val_recall: 0.151878, val_f1: 0.087465
Epoch: 82, loss: 29.5096, train_acc: 0.3337, train_recall: 0.1643, train_f1: 0.1052, val_acc: 0.315271, val_recall: 0.151878, val_f1: 0.089174
Epoch: 83, loss: 25.9092, train_acc: 0.3337, train_recall: 0.1662, train_f1: 0.1150, val_acc: 0.315271, val_recall: 0.153535, val_f1: 0.099603
Epoch: 84, loss: 28.1553, train_acc: 0.2278, train_recall: 0.1643, train_f1: 0.0923, val_acc: 0.241379, val_recall: 0.153846, val_f1: 0.077514
Epoch: 85, loss: 30.6754, train_acc: 0.3252, train_recall: 0.1699, train_f1: 0.1163, val_acc: 0.332512, val_recall: 0.157540, val_f1: 0.103845
Epoch: 86, loss: 31.3212, train_acc: 0.3263, train_recall: 0.1767, train_f1: 0.1294, val_acc: 0.334975, val_recall: 0.173165, val_f1: 0.126818
Epoch: 87, loss: 28.7712, train_acc: 0.3093, train_recall: 0.1582, train_f1: 0.1099, val_acc: 0.325123, val_recall: 0.196356, val_f1: 0.129129
Epoch: 88, loss: 28.4800, train_acc: 0.2256, train_recall: 0.1576, train_f1: 0.0918, val_acc: 0.246305, val_recall: 0.197115, val_f1: 0.117683
Epoch: 89, loss: 25.2794, train_acc: 0.2659, train_recall: 0.1742, train_f1: 0.1360, val_acc: 0.288177, val_recall: 0.166964, val_f1: 0.130689
Epoch: 90, loss: 28.0946, train_acc: 0.3337, train_recall: 0.1643, train_f1: 0.1072, val_acc: 0.320197, val_recall: 0.153846, val_f1: 0.090940
Epoch: 91, loss: 28.6543, train_acc: 0.1028, train_recall: 0.1664, train_f1: 0.0763, val_acc: 0.071429, val_recall: 0.155141, val_f1: 0.054488
Epoch: 92, loss: 27.5503, train_acc: 0.1028, train_recall: 0.1664, train_f1: 0.0763, val_acc: 0.071429, val_recall: 0.155141, val_f1: 0.054570
Epoch: 93, loss: 28.5651, train_acc: 0.3242, train_recall: 0.1631, train_f1: 0.1093, val_acc: 0.337438, val_recall: 0.157851, val_f1: 0.104738
Epoch: 94, loss: 28.1560, train_acc: 0.3263, train_recall: 0.1638, train_f1: 0.1107, val_acc: 0.334975, val_recall: 0.156890, val_f1: 0.104322
Epoch: 95, loss: 27.2352, train_acc: 0.3326, train_recall: 0.1594, train_f1: 0.1035, val_acc: 0.317734, val_recall: 0.152862, val_f1: 0.090577
Epoch: 96, loss: 25.9099, train_acc: 0.3347, train_recall: 0.1621, train_f1: 0.1136, val_acc: 0.315271, val_recall: 0.153867, val_f1: 0.102424
Epoch: 97, loss: 28.6339, train_acc: 0.2267, train_recall: 0.1594, train_f1: 0.0886, val_acc: 0.241379, val_recall: 0.153846, val_f1: 0.078569
Epoch: 98, loss: 26.7939, train_acc: 0.2267, train_recall: 0.1594, train_f1: 0.0886, val_acc: 0.241379, val_recall: 0.153846, val_f1: 0.078569
Epoch: 99, loss: 27.4556, train_acc: 0.3263, train_recall: 0.1658, train_f1: 0.1141, val_acc: 0.334975, val_recall: 0.158502, val_f1: 0.104213
Epoch: 100, loss: 26.5115, train_acc: 0.3199, train_recall: 0.1660, train_f1: 0.1114, val_acc: 0.330049, val_recall: 0.154921, val_f1: 0.101056
Epoch: 101, loss: 28.4932, train_acc: 0.3284, train_recall: 0.1580, train_f1: 0.1029, val_acc: 0.317734, val_recall: 0.152862, val_f1: 0.090694
Epoch: 102, loss: 28.3927, train_acc: 0.3284, train_recall: 0.1632, train_f1: 0.1121, val_acc: 0.317734, val_recall: 0.152862, val_f1: 0.091047
Epoch: 103, loss: 29.4351, train_acc: 0.0869, train_recall: 0.1532, train_f1: 0.0454, val_acc: 0.061576, val_recall: 0.174061, val_f1: 0.043110
Epoch: 104, loss: 29.9378, train_acc: 0.0392, train_recall: 0.1111, train_f1: 0.0084, val_acc: 0.032020, val_recall: 0.125000, val_f1: 0.007831
Epoch: 105, loss: 25.4592, train_acc: 0.3263, train_recall: 0.1804, train_f1: 0.1319, val_acc: 0.325123, val_recall: 0.153593, val_f1: 0.097431
Epoch: 106, loss: 24.4494, train_acc: 0.3157, train_recall: 0.1663, train_f1: 0.1160, val_acc: 0.320197, val_recall: 0.152733, val_f1: 0.101918
Epoch: 107, loss: 26.3522, train_acc: 0.2278, train_recall: 0.1643, train_f1: 0.0923, val_acc: 0.241379, val_recall: 0.153846, val_f1: 0.078569
Epoch: 108, loss: 27.2273, train_acc: 0.3347, train_recall: 0.1662, train_f1: 0.1156, val_acc: 0.310345, val_recall: 0.150904, val_f1: 0.095899
Epoch: 109, loss: 26.4564, train_acc: 0.3358, train_recall: 0.1670, train_f1: 0.1174, val_acc: 0.315271, val_recall: 0.153867, val_f1: 0.102424
Epoch: 110, loss: 26.1695, train_acc: 0.2659, train_recall: 0.1559, train_f1: 0.1231, val_acc: 0.300493, val_recall: 0.155668, val_f1: 0.121943
Epoch: 111, loss: 27.6606, train_acc: 0.3252, train_recall: 0.1699, train_f1: 0.1163, val_acc: 0.334975, val_recall: 0.158502, val_f1: 0.104232
Epoch: 112, loss: 26.4176, train_acc: 0.3242, train_recall: 0.1695, train_f1: 0.1161, val_acc: 0.330049, val_recall: 0.156579, val_f1: 0.103291
Epoch: 113, loss: 25.8095, train_acc: 0.3369, train_recall: 0.1738, train_f1: 0.1304, val_acc: 0.317734, val_recall: 0.169492, val_f1: 0.127659
Epoch: 114, loss: 26.5409, train_acc: 0.3294, train_recall: 0.1485, train_f1: 0.0879, val_acc: 0.322660, val_recall: 0.218771, val_f1: 0.130824
Epoch: 115, loss: 26.5015, train_acc: 0.2214, train_recall: 0.1610, train_f1: 0.0619, val_acc: 0.236453, val_recall: 0.212171, val_f1: 0.086849
Epoch: 116, loss: 25.7023, train_acc: 0.1864, train_recall: 0.1729, train_f1: 0.1202, val_acc: 0.140394, val_recall: 0.139269, val_f1: 0.084127
Epoch: 117, loss: 25.4106, train_acc: 0.1081, train_recall: 0.1877, train_f1: 0.0892, val_acc: 0.064039, val_recall: 0.153515, val_f1: 0.050567
Epoch: 118, loss: 25.5502, train_acc: 0.3337, train_recall: 0.1858, train_f1: 0.1263, val_acc: 0.325123, val_recall: 0.153616, val_f1: 0.096957
Epoch: 119, loss: 25.4388, train_acc: 0.3273, train_recall: 0.1823, train_f1: 0.1242, val_acc: 0.322660, val_recall: 0.152391, val_f1: 0.099890
Epoch: 120, loss: 25.8721, train_acc: 0.3369, train_recall: 0.1659, train_f1: 0.1106, val_acc: 0.317734, val_recall: 0.153525, val_f1: 0.095078
Epoch: 121, loss: 26.5433, train_acc: 0.2299, train_recall: 0.1650, train_f1: 0.0939, val_acc: 0.241379, val_recall: 0.153846, val_f1: 0.078569
Epoch: 122, loss: 25.6999, train_acc: 0.3400, train_recall: 0.1804, train_f1: 0.1165, val_acc: 0.317734, val_recall: 0.153525, val_f1: 0.093089
Epoch: 123, loss: 25.9903, train_acc: 0.3316, train_recall: 0.1840, train_f1: 0.1234, val_acc: 0.332512, val_recall: 0.156237, val_f1: 0.101632
Epoch: 124, loss: 25.3073, train_acc: 0.3273, train_recall: 0.1823, train_f1: 0.1232, val_acc: 0.325123, val_recall: 0.153352, val_f1: 0.100327
Epoch: 125, loss: 25.0680, train_acc: 0.3411, train_recall: 0.1818, train_f1: 0.1165, val_acc: 0.312808, val_recall: 0.152220, val_f1: 0.094614
Epoch: 126, loss: 25.0628, train_acc: 0.2352, train_recall: 0.1802, train_f1: 0.1003, val_acc: 0.238916, val_recall: 0.152530, val_f1: 0.076132
Epoch: 127, loss: 24.0296, train_acc: 0.3411, train_recall: 0.1818, train_f1: 0.1210, val_acc: 0.312808, val_recall: 0.152220, val_f1: 0.096295
Epoch: 128, loss: 23.6690, train_acc: 0.3294, train_recall: 0.1850, train_f1: 0.1325, val_acc: 0.325123, val_recall: 0.153970, val_f1: 0.098886
Epoch: 129, loss: 23.7291, train_acc: 0.1038, train_recall: 0.1651, train_f1: 0.0752, val_acc: 0.068966, val_recall: 0.172368, val_f1: 0.060435
Epoch: 130, loss: 23.9762, train_acc: 0.0742, train_recall: 0.1363, train_f1: 0.0363, val_acc: 0.054187, val_recall: 0.156275, val_f1: 0.030350
Epoch: 131, loss: 23.4379, train_acc: 0.3337, train_recall: 0.1537, train_f1: 0.1009, val_acc: 0.315271, val_recall: 0.170466, val_f1: 0.108814
Epoch: 132, loss: 23.4649, train_acc: 0.2309, train_recall: 0.1479, train_f1: 0.0814, val_acc: 0.241379, val_recall: 0.162146, val_f1: 0.086815
Epoch: 133, loss: 23.5108, train_acc: 0.3189, train_recall: 0.1447, train_f1: 0.1002, val_acc: 0.330049, val_recall: 0.165233, val_f1: 0.111706
Epoch: 134, loss: 23.3842, train_acc: 0.3167, train_recall: 0.1382, train_f1: 0.0988, val_acc: 0.322660, val_recall: 0.161708, val_f1: 0.113844
Epoch: 135, loss: 23.2421, train_acc: 0.3347, train_recall: 0.1514, train_f1: 0.1133, val_acc: 0.322660, val_recall: 0.220729, val_f1: 0.171063
Epoch: 136, loss: 23.0922, train_acc: 0.2288, train_recall: 0.1583, train_f1: 0.0847, val_acc: 0.246305, val_recall: 0.217434, val_f1: 0.116635
Epoch: 137, loss: 23.2203, train_acc: 0.1038, train_recall: 0.1842, train_f1: 0.0777, val_acc: 0.076355, val_recall: 0.221027, val_f1: 0.084720
Epoch: 138, loss: 23.1867, train_acc: 0.3210, train_recall: 0.1832, train_f1: 0.1228, val_acc: 0.322660, val_recall: 0.215028, val_f1: 0.130066
Epoch: 139, loss: 22.9696, train_acc: 0.3369, train_recall: 0.1714, train_f1: 0.1589, val_acc: 0.332512, val_recall: 0.204524, val_f1: 0.162816
Epoch: 140, loss: 22.8985, train_acc: 0.3390, train_recall: 0.1728, train_f1: 0.1389, val_acc: 0.317734, val_recall: 0.171405, val_f1: 0.133515
Epoch: 141, loss: 23.1129, train_acc: 0.2331, train_recall: 0.1583, train_f1: 0.0924, val_acc: 0.243842, val_recall: 0.171761, val_f1: 0.091557
Epoch: 142, loss: 22.8904, train_acc: 0.3167, train_recall: 0.1585, train_f1: 0.1117, val_acc: 0.325123, val_recall: 0.171655, val_f1: 0.113909
Epoch: 143, loss: 22.9864, train_acc: 0.3284, train_recall: 0.1554, train_f1: 0.1075, val_acc: 0.320197, val_recall: 0.171372, val_f1: 0.104450
Epoch: 144, loss: 22.9429, train_acc: 0.3316, train_recall: 0.1564, train_f1: 0.1041, val_acc: 0.315271, val_recall: 0.169471, val_f1: 0.098781
Epoch: 145, loss: 22.8562, train_acc: 0.3199, train_recall: 0.1868, train_f1: 0.1361, val_acc: 0.322660, val_recall: 0.153408, val_f1: 0.104452
Epoch: 146, loss: 22.8554, train_acc: 0.2373, train_recall: 0.1870, train_f1: 0.1157, val_acc: 0.238916, val_recall: 0.152530, val_f1: 0.076432
Epoch: 147, loss: 22.8326, train_acc: 0.3294, train_recall: 0.1900, train_f1: 0.1462, val_acc: 0.322660, val_recall: 0.153431, val_f1: 0.105559
Epoch: 148, loss: 22.8667, train_acc: 0.3400, train_recall: 0.1823, train_f1: 0.1298, val_acc: 0.307882, val_recall: 0.149920, val_f1: 0.093998
Epoch: 149, loss: 22.7600, train_acc: 0.3252, train_recall: 0.1886, train_f1: 0.1425, val_acc: 0.320197, val_recall: 0.152092, val_f1: 0.101782
Epoch: 150, loss: 22.7094, train_acc: 0.3189, train_recall: 0.1665, train_f1: 0.1169, val_acc: 0.327586, val_recall: 0.217659, val_f1: 0.137010
Epoch: 151, loss: 22.7771, train_acc: 0.2712, train_recall: 0.1678, train_f1: 0.1200, val_acc: 0.305419, val_recall: 0.218593, val_f1: 0.145677
Epoch: 152, loss: 22.7003, train_acc: 0.3358, train_recall: 0.1721, train_f1: 0.1412, val_acc: 0.317734, val_recall: 0.183470, val_f1: 0.144789
Epoch: 153, loss: 22.6988, train_acc: 0.3400, train_recall: 0.1703, train_f1: 0.1339, val_acc: 0.317734, val_recall: 0.180081, val_f1: 0.136591
Epoch: 154, loss: 22.7006, train_acc: 0.3263, train_recall: 0.1809, train_f1: 0.1406, val_acc: 0.320197, val_recall: 0.161055, val_f1: 0.115925
Epoch: 155, loss: 22.7220, train_acc: 0.3220, train_recall: 0.1883, train_f1: 0.1398, val_acc: 0.322660, val_recall: 0.154403, val_f1: 0.104228
Epoch: 156, loss: 22.7014, train_acc: 0.3443, train_recall: 0.1935, train_f1: 0.1410, val_acc: 0.312808, val_recall: 0.152220, val_f1: 0.096594
Epoch: 157, loss: 22.6916, train_acc: 0.3443, train_recall: 0.1935, train_f1: 0.1410, val_acc: 0.312808, val_recall: 0.152220, val_f1: 0.096412
Epoch: 158, loss: 22.6900, train_acc: 0.3273, train_recall: 0.1939, train_f1: 0.1443, val_acc: 0.317734, val_recall: 0.151440, val_f1: 0.100356
Epoch: 159, loss: 22.6252, train_acc: 0.3284, train_recall: 0.1898, train_f1: 0.1408, val_acc: 0.320197, val_recall: 0.152401, val_f1: 0.100134
Epoch: 160, loss: 22.5998, train_acc: 0.3242, train_recall: 0.1882, train_f1: 0.1414, val_acc: 0.327586, val_recall: 0.155399, val_f1: 0.109131
Epoch: 161, loss: 22.6375, train_acc: 0.3369, train_recall: 0.1712, train_f1: 0.1238, val_acc: 0.320197, val_recall: 0.217123, val_f1: 0.137083
Epoch: 162, loss: 22.6262, train_acc: 0.3379, train_recall: 0.1716, train_f1: 0.1250, val_acc: 0.320197, val_recall: 0.217123, val_f1: 0.134205
Epoch: 163, loss: 22.6011, train_acc: 0.3220, train_recall: 0.1670, train_f1: 0.1237, val_acc: 0.327586, val_recall: 0.217305, val_f1: 0.140538
Epoch: 164, loss: 22.5587, train_acc: 0.3284, train_recall: 0.1898, train_f1: 0.1417, val_acc: 0.320197, val_recall: 0.152401, val_f1: 0.100172
Epoch: 165, loss: 22.5836, train_acc: 0.3422, train_recall: 0.1883, train_f1: 0.1375, val_acc: 0.312808, val_recall: 0.152220, val_f1: 0.096594
Epoch: 166, loss: 22.6004, train_acc: 0.3422, train_recall: 0.1883, train_f1: 0.1375, val_acc: 0.312808, val_recall: 0.152220, val_f1: 0.096594
Epoch: 167, loss: 22.5619, train_acc: 0.3242, train_recall: 0.1883, train_f1: 0.1413, val_acc: 0.322660, val_recall: 0.153408, val_f1: 0.104197
Epoch: 168, loss: 22.5569, train_acc: 0.3273, train_recall: 0.1895, train_f1: 0.1389, val_acc: 0.320197, val_recall: 0.152401, val_f1: 0.100134
Epoch: 169, loss: 22.5556, train_acc: 0.3284, train_recall: 0.1939, train_f1: 0.1608, val_acc: 0.332512, val_recall: 0.174630, val_f1: 0.139254
Epoch: 170, loss: 22.5434, train_acc: 0.3443, train_recall: 0.1908, train_f1: 0.1631, val_acc: 0.320197, val_recall: 0.193085, val_f1: 0.162189
Epoch: 171, loss: 22.5409, train_acc: 0.3369, train_recall: 0.1658, train_f1: 0.1209, val_acc: 0.320197, val_recall: 0.217123, val_f1: 0.137236
Epoch: 172, loss: 22.5502, train_acc: 0.3231, train_recall: 0.1674, train_f1: 0.1233, val_acc: 0.327586, val_recall: 0.217305, val_f1: 0.137573
Epoch: 173, loss: 22.5161, train_acc: 0.3263, train_recall: 0.1846, train_f1: 0.1381, val_acc: 0.322660, val_recall: 0.153363, val_f1: 0.101581
Epoch: 174, loss: 22.5248, train_acc: 0.3422, train_recall: 0.1883, train_f1: 0.1375, val_acc: 0.312808, val_recall: 0.152220, val_f1: 0.096594
Epoch: 175, loss: 22.5238, train_acc: 0.3422, train_recall: 0.1883, train_f1: 0.1375, val_acc: 0.312808, val_recall: 0.152220, val_f1: 0.096594
Epoch: 176, loss: 22.5032, train_acc: 0.3273, train_recall: 0.1937, train_f1: 0.1555, val_acc: 0.322660, val_recall: 0.170648, val_f1: 0.126352
Epoch: 177, loss: 22.4986, train_acc: 0.3284, train_recall: 0.1941, train_f1: 0.1541, val_acc: 0.322660, val_recall: 0.170648, val_f1: 0.125564
Epoch: 178, loss: 22.5012, train_acc: 0.3400, train_recall: 0.1918, train_f1: 0.1528, val_acc: 0.315271, val_recall: 0.170466, val_f1: 0.123973
Epoch: 179, loss: 22.5018, train_acc: 0.3443, train_recall: 0.1924, train_f1: 0.1494, val_acc: 0.320197, val_recall: 0.181066, val_f1: 0.139659
Epoch: 180, loss: 22.4833, train_acc: 0.3242, train_recall: 0.1782, train_f1: 0.1718, val_acc: 0.327586, val_recall: 0.191349, val_f1: 0.174639
Epoch: 181, loss: 22.4769, train_acc: 0.3242, train_recall: 0.1800, train_f1: 0.1483, val_acc: 0.327586, val_recall: 0.190622, val_f1: 0.145964
Epoch: 182, loss: 22.4615, train_acc: 0.3252, train_recall: 0.1887, train_f1: 0.1415, val_acc: 0.327586, val_recall: 0.155354, val_f1: 0.106430
Epoch: 183, loss: 22.4555, train_acc: 0.3432, train_recall: 0.1887, train_f1: 0.1384, val_acc: 0.312808, val_recall: 0.152220, val_f1: 0.096594
Epoch: 184, loss: 22.4695, train_acc: 0.3432, train_recall: 0.1887, train_f1: 0.1404, val_acc: 0.307882, val_recall: 0.150251, val_f1: 0.096308
Epoch: 185, loss: 22.4409, train_acc: 0.3284, train_recall: 0.1943, train_f1: 0.1453, val_acc: 0.325123, val_recall: 0.154370, val_f1: 0.104660
Epoch: 186, loss: 22.4465, train_acc: 0.3263, train_recall: 0.1935, train_f1: 0.1450, val_acc: 0.325123, val_recall: 0.154370, val_f1: 0.104660
Epoch: 187, loss: 22.4438, train_acc: 0.3379, train_recall: 0.1914, train_f1: 0.1448, val_acc: 0.312808, val_recall: 0.152174, val_f1: 0.100266
Epoch: 188, loss: 22.4283, train_acc: 0.3443, train_recall: 0.1936, train_f1: 0.1458, val_acc: 0.310345, val_recall: 0.151213, val_f1: 0.098360
Epoch: 189, loss: 22.4364, train_acc: 0.3252, train_recall: 0.1931, train_f1: 0.1448, val_acc: 0.325123, val_recall: 0.154392, val_f1: 0.105998
Epoch: 190, loss: 22.4266, train_acc: 0.3252, train_recall: 0.1931, train_f1: 0.1448, val_acc: 0.322660, val_recall: 0.153408, val_f1: 0.104239
Epoch: 191, loss: 22.4102, train_acc: 0.3263, train_recall: 0.1934, train_f1: 0.1461, val_acc: 0.327586, val_recall: 0.155399, val_f1: 0.109131
Epoch: 192, loss: 22.4068, train_acc: 0.3400, train_recall: 0.1961, train_f1: 0.1733, val_acc: 0.322660, val_recall: 0.154294, val_f1: 0.132450
Epoch: 193, loss: 22.4072, train_acc: 0.3263, train_recall: 0.1934, train_f1: 0.1460, val_acc: 0.327586, val_recall: 0.155399, val_f1: 0.109131
Epoch: 194, loss: 22.3906, train_acc: 0.3252, train_recall: 0.1931, train_f1: 0.1448, val_acc: 0.325123, val_recall: 0.154392, val_f1: 0.106044
Epoch: 195, loss: 22.3912, train_acc: 0.3273, train_recall: 0.1938, train_f1: 0.1462, val_acc: 0.327586, val_recall: 0.155399, val_f1: 0.109064
Epoch: 196, loss: 22.3818, train_acc: 0.3273, train_recall: 0.1938, train_f1: 0.1467, val_acc: 0.330049, val_recall: 0.156384, val_f1: 0.110782
Epoch: 197, loss: 22.4095, train_acc: 0.3263, train_recall: 0.1934, train_f1: 0.1460, val_acc: 0.327586, val_recall: 0.155399, val_f1: 0.109131
Epoch: 198, loss: 22.3567, train_acc: 0.3263, train_recall: 0.1935, train_f1: 0.1455, val_acc: 0.325123, val_recall: 0.154415, val_f1: 0.107382
Epoch: 199, loss: 22.3537, train_acc: 0.3263, train_recall: 0.1934, train_f1: 0.1460, val_acc: 0.327586, val_recall: 0.155399, val_f1: 0.109131
Epoch: 200, loss: 22.3651, train_acc: 0.3263, train_recall: 0.1934, train_f1: 0.1461, val_acc: 0.327586, val_recall: 0.155399, val_f1: 0.109064
Epoch: 201, loss: 22.3595, train_acc: 0.3242, train_recall: 0.1928, train_f1: 0.1426, val_acc: 0.325123, val_recall: 0.154415, val_f1: 0.107567
Epoch: 202, loss: 22.3396, train_acc: 0.3252, train_recall: 0.1931, train_f1: 0.1433, val_acc: 0.325123, val_recall: 0.154415, val_f1: 0.107567
Epoch: 203, loss: 22.3547, train_acc: 0.3231, train_recall: 0.1923, train_f1: 0.1430, val_acc: 0.325123, val_recall: 0.154415, val_f1: 0.107567
Epoch: 204, loss: 22.3321, train_acc: 0.3242, train_recall: 0.1927, train_f1: 0.1432, val_acc: 0.325123, val_recall: 0.154415, val_f1: 0.107510
Epoch: 205, loss: 22.3435, train_acc: 0.3242, train_recall: 0.1928, train_f1: 0.1426, val_acc: 0.322660, val_recall: 0.153431, val_f1: 0.105738
Epoch: 206, loss: 22.3130, train_acc: 0.3242, train_recall: 0.1928, train_f1: 0.1426, val_acc: 0.325123, val_recall: 0.154415, val_f1: 0.107567
Epoch: 207, loss: 22.3172, train_acc: 0.3242, train_recall: 0.1927, train_f1: 0.1439, val_acc: 0.325123, val_recall: 0.154438, val_f1: 0.108994
Epoch: 208, loss: 22.3174, train_acc: 0.3242, train_recall: 0.1927, train_f1: 0.1432, val_acc: 0.325123, val_recall: 0.154415, val_f1: 0.107567
Epoch: 209, loss: 22.3054, train_acc: 0.3242, train_recall: 0.1928, train_f1: 0.1426, val_acc: 0.322660, val_recall: 0.153431, val_f1: 0.105785
Epoch: 210, loss: 22.3070, train_acc: 0.3242, train_recall: 0.1928, train_f1: 0.1426, val_acc: 0.322660, val_recall: 0.153431, val_f1: 0.105785
Epoch: 211, loss: 22.2930, train_acc: 0.3242, train_recall: 0.1927, train_f1: 0.1439, val_acc: 0.325123, val_recall: 0.154438, val_f1: 0.108929
Epoch: 212, loss: 22.2740, train_acc: 0.3242, train_recall: 0.1927, train_f1: 0.1439, val_acc: 0.325123, val_recall: 0.154438, val_f1: 0.108929
Epoch: 213, loss: 22.2908, train_acc: 0.3242, train_recall: 0.1928, train_f1: 0.1426, val_acc: 0.322660, val_recall: 0.153431, val_f1: 0.105738
Epoch: 214, loss: 22.2831, train_acc: 0.3242, train_recall: 0.1928, train_f1: 0.1426, val_acc: 0.322660, val_recall: 0.153431, val_f1: 0.105785
Epoch: 215, loss: 22.2553, train_acc: 0.3242, train_recall: 0.1927, train_f1: 0.1439, val_acc: 0.325123, val_recall: 0.154438, val_f1: 0.108994
Epoch: 216, loss: 22.2551, train_acc: 0.3178, train_recall: 0.1902, train_f1: 0.1472, val_acc: 0.320197, val_recall: 0.152538, val_f1: 0.109530
Epoch: 217, loss: 22.2535, train_acc: 0.3242, train_recall: 0.1928, train_f1: 0.1426, val_acc: 0.322660, val_recall: 0.153431, val_f1: 0.105785
Epoch: 218, loss: 22.2462, train_acc: 0.3284, train_recall: 0.1942, train_f1: 0.1459, val_acc: 0.325123, val_recall: 0.154415, val_f1: 0.107382
Epoch: 219, loss: 22.2416, train_acc: 0.3263, train_recall: 0.1934, train_f1: 0.1460, val_acc: 0.327586, val_recall: 0.155399, val_f1: 0.109064
Epoch: 220, loss: 22.2507, train_acc: 0.3242, train_recall: 0.1925, train_f1: 0.1501, val_acc: 0.322660, val_recall: 0.153522, val_f1: 0.110869
Epoch: 221, loss: 22.2351, train_acc: 0.3284, train_recall: 0.1942, train_f1: 0.1459, val_acc: 0.325123, val_recall: 0.154415, val_f1: 0.107382
Epoch: 222, loss: 22.2408, train_acc: 0.3273, train_recall: 0.1939, train_f1: 0.1431, val_acc: 0.322660, val_recall: 0.153431, val_f1: 0.105785
Epoch: 223, loss: 22.2250, train_acc: 0.3252, train_recall: 0.1931, train_f1: 0.1433, val_acc: 0.325123, val_recall: 0.154415, val_f1: 0.107567
Epoch: 224, loss: 22.2370, train_acc: 0.3189, train_recall: 0.1907, train_f1: 0.1456, val_acc: 0.317734, val_recall: 0.151553, val_f1: 0.107682
Epoch: 225, loss: 22.2073, train_acc: 0.3242, train_recall: 0.1928, train_f1: 0.1426, val_acc: 0.322660, val_recall: 0.153431, val_f1: 0.105785
Epoch: 226, loss: 22.1911, train_acc: 0.3273, train_recall: 0.1939, train_f1: 0.1431, val_acc: 0.322660, val_recall: 0.153431, val_f1: 0.105785
Epoch: 227, loss: 22.2147, train_acc: 0.3242, train_recall: 0.1928, train_f1: 0.1426, val_acc: 0.322660, val_recall: 0.153431, val_f1: 0.105785
Epoch: 228, loss: 22.1919, train_acc: 0.3252, train_recall: 0.1931, train_f1: 0.1440, val_acc: 0.322660, val_recall: 0.153454, val_f1: 0.107265
Epoch: 229, loss: 22.1875, train_acc: 0.3242, train_recall: 0.1928, train_f1: 0.1426, val_acc: 0.325123, val_recall: 0.154415, val_f1: 0.107567
Epoch: 230, loss: 22.1722, train_acc: 0.3273, train_recall: 0.1939, train_f1: 0.1431, val_acc: 0.322660, val_recall: 0.153431, val_f1: 0.105785
Epoch: 231, loss: 22.1679, train_acc: 0.3242, train_recall: 0.1928, train_f1: 0.1426, val_acc: 0.322660, val_recall: 0.153431, val_f1: 0.105785
Epoch: 232, loss: 22.1531, train_acc: 0.3242, train_recall: 0.1927, train_f1: 0.1433, val_acc: 0.322660, val_recall: 0.153454, val_f1: 0.107265
Epoch: 233, loss: 22.1381, train_acc: 0.3242, train_recall: 0.1928, train_f1: 0.1426, val_acc: 0.322660, val_recall: 0.153431, val_f1: 0.105785
Epoch: 234, loss: 22.1399, train_acc: 0.3263, train_recall: 0.1935, train_f1: 0.1429, val_acc: 0.322660, val_recall: 0.153431, val_f1: 0.105785
Epoch: 235, loss: 22.1157, train_acc: 0.3242, train_recall: 0.1928, train_f1: 0.1427, val_acc: 0.322660, val_recall: 0.153431, val_f1: 0.105785
Epoch: 236, loss: 22.1284, train_acc: 0.3242, train_recall: 0.1928, train_f1: 0.1427, val_acc: 0.325123, val_recall: 0.154415, val_f1: 0.107567
Epoch: 237, loss: 22.1087, train_acc: 0.3242, train_recall: 0.1928, train_f1: 0.1426, val_acc: 0.322660, val_recall: 0.153431, val_f1: 0.105785
Epoch: 238, loss: 22.1069, train_acc: 0.3263, train_recall: 0.1935, train_f1: 0.1429, val_acc: 0.322660, val_recall: 0.153431, val_f1: 0.105704
Epoch: 239, loss: 22.1115, train_acc: 0.3263, train_recall: 0.1935, train_f1: 0.1430, val_acc: 0.322660, val_recall: 0.153431, val_f1: 0.105785
Epoch: 240, loss: 22.1063, train_acc: 0.3242, train_recall: 0.1928, train_f1: 0.1427, val_acc: 0.325123, val_recall: 0.154415, val_f1: 0.107567
Epoch: 241, loss: 22.0780, train_acc: 0.3263, train_recall: 0.1935, train_f1: 0.1430, val_acc: 0.322660, val_recall: 0.153431, val_f1: 0.105785
Epoch: 242, loss: 22.1054, train_acc: 0.3263, train_recall: 0.1935, train_f1: 0.1430, val_acc: 0.322660, val_recall: 0.153431, val_f1: 0.105704
Epoch: 243, loss: 22.0769, train_acc: 0.3263, train_recall: 0.1935, train_f1: 0.1430, val_acc: 0.322660, val_recall: 0.153431, val_f1: 0.105704
Epoch: 244, loss: 22.0710, train_acc: 0.3263, train_recall: 0.1935, train_f1: 0.1430, val_acc: 0.322660, val_recall: 0.153431, val_f1: 0.105704
Epoch: 245, loss: 22.0580, train_acc: 0.3263, train_recall: 0.1935, train_f1: 0.1430, val_acc: 0.322660, val_recall: 0.153431, val_f1: 0.105704
Epoch: 246, loss: 22.0606, train_acc: 0.3263, train_recall: 0.1935, train_f1: 0.1430, val_acc: 0.322660, val_recall: 0.153431, val_f1: 0.105704
Epoch: 247, loss: 22.0490, train_acc: 0.3263, train_recall: 0.1935, train_f1: 0.1430, val_acc: 0.322660, val_recall: 0.153431, val_f1: 0.105704
Epoch: 248, loss: 22.0448, train_acc: 0.3263, train_recall: 0.1935, train_f1: 0.1430, val_acc: 0.322660, val_recall: 0.153431, val_f1: 0.105704
Epoch: 249, loss: 22.0408, train_acc: 0.3263, train_recall: 0.1935, train_f1: 0.1430, val_acc: 0.322660, val_recall: 0.153431, val_f1: 0.105704
Epoch: 250, loss: 22.0475, train_acc: 0.3263, train_recall: 0.1935, train_f1: 0.1430, val_acc: 0.322660, val_recall: 0.153431, val_f1: 0.105704
Epoch: 251, loss: 22.0135, train_acc: 0.3263, train_recall: 0.1935, train_f1: 0.1427, val_acc: 0.322660, val_recall: 0.153431, val_f1: 0.105752
Epoch: 252, loss: 22.0199, train_acc: 0.3263, train_recall: 0.1935, train_f1: 0.1427, val_acc: 0.322660, val_recall: 0.153431, val_f1: 0.105752
Epoch: 253, loss: 21.9999, train_acc: 0.3263, train_recall: 0.1935, train_f1: 0.1427, val_acc: 0.322660, val_recall: 0.153431, val_f1: 0.105752
Epoch: 254, loss: 22.0128, train_acc: 0.3263, train_recall: 0.1935, train_f1: 0.1427, val_acc: 0.322660, val_recall: 0.153431, val_f1: 0.105752
Epoch: 255, loss: 22.0025, train_acc: 0.3263, train_recall: 0.1935, train_f1: 0.1427, val_acc: 0.322660, val_recall: 0.153431, val_f1: 0.105752
Epoch: 256, loss: 21.9876, train_acc: 0.3263, train_recall: 0.1935, train_f1: 0.1427, val_acc: 0.322660, val_recall: 0.153431, val_f1: 0.105752
Epoch: 257, loss: 21.9826, train_acc: 0.3263, train_recall: 0.1935, train_f1: 0.1427, val_acc: 0.322660, val_recall: 0.153431, val_f1: 0.105752
Epoch: 258, loss: 21.9720, train_acc: 0.3263, train_recall: 0.1935, train_f1: 0.1427, val_acc: 0.322660, val_recall: 0.153431, val_f1: 0.105752
Epoch: 259, loss: 21.9633, train_acc: 0.3263, train_recall: 0.1935, train_f1: 0.1427, val_acc: 0.322660, val_recall: 0.153431, val_f1: 0.105752
Epoch: 260, loss: 21.9600, train_acc: 0.3263, train_recall: 0.1935, train_f1: 0.1427, val_acc: 0.322660, val_recall: 0.153431, val_f1: 0.105752
Epoch: 261, loss: 21.9437, train_acc: 0.3263, train_recall: 0.1935, train_f1: 0.1427, val_acc: 0.322660, val_recall: 0.153431, val_f1: 0.105752
Epoch: 262, loss: 21.9493, train_acc: 0.3263, train_recall: 0.1935, train_f1: 0.1427, val_acc: 0.322660, val_recall: 0.153431, val_f1: 0.105752
Epoch: 263, loss: 21.9433, train_acc: 0.3263, train_recall: 0.1935, train_f1: 0.1427, val_acc: 0.322660, val_recall: 0.153431, val_f1: 0.105752
Epoch: 264, loss: 21.9389, train_acc: 0.3263, train_recall: 0.1935, train_f1: 0.1420, val_acc: 0.322660, val_recall: 0.153431, val_f1: 0.105752
Epoch: 265, loss: 21.9146, train_acc: 0.3263, train_recall: 0.1935, train_f1: 0.1420, val_acc: 0.322660, val_recall: 0.153431, val_f1: 0.105752
Epoch: 266, loss: 21.9129, train_acc: 0.3263, train_recall: 0.1935, train_f1: 0.1420, val_acc: 0.320197, val_recall: 0.152469, val_f1: 0.105316
Epoch: 267, loss: 21.9056, train_acc: 0.3263, train_recall: 0.1935, train_f1: 0.1420, val_acc: 0.322660, val_recall: 0.153431, val_f1: 0.105752
Epoch: 268, loss: 21.8798, train_acc: 0.3273, train_recall: 0.1939, train_f1: 0.1421, val_acc: 0.322660, val_recall: 0.153431, val_f1: 0.105752
Epoch: 269, loss: 21.8962, train_acc: 0.3273, train_recall: 0.1939, train_f1: 0.1421, val_acc: 0.320197, val_recall: 0.152469, val_f1: 0.105316
Epoch: 270, loss: 21.8711, train_acc: 0.3294, train_recall: 0.1999, train_f1: 0.1491, val_acc: 0.322660, val_recall: 0.162085, val_f1: 0.116431
Epoch: 271, loss: 21.8643, train_acc: 0.3294, train_recall: 0.1999, train_f1: 0.1491, val_acc: 0.325123, val_recall: 0.163046, val_f1: 0.116872
Epoch: 272, loss: 21.8554, train_acc: 0.3294, train_recall: 0.1999, train_f1: 0.1491, val_acc: 0.325123, val_recall: 0.163046, val_f1: 0.116872
Epoch: 273, loss: 21.8693, train_acc: 0.3294, train_recall: 0.1999, train_f1: 0.1492, val_acc: 0.322660, val_recall: 0.162085, val_f1: 0.116586
Epoch: 274, loss: 21.8543, train_acc: 0.3294, train_recall: 0.1999, train_f1: 0.1491, val_acc: 0.322660, val_recall: 0.162085, val_f1: 0.116431
Epoch: 275, loss: 21.8368, train_acc: 0.3294, train_recall: 0.1999, train_f1: 0.1491, val_acc: 0.322660, val_recall: 0.162085, val_f1: 0.116431
Epoch: 276, loss: 21.8463, train_acc: 0.3294, train_recall: 0.1999, train_f1: 0.1491, val_acc: 0.322660, val_recall: 0.162085, val_f1: 0.116431
Epoch: 277, loss: 21.8215, train_acc: 0.3294, train_recall: 0.1999, train_f1: 0.1492, val_acc: 0.322660, val_recall: 0.162085, val_f1: 0.116586
Epoch: 278, loss: 21.8175, train_acc: 0.3294, train_recall: 0.1999, train_f1: 0.1492, val_acc: 0.322660, val_recall: 0.162085, val_f1: 0.116586
Epoch: 279, loss: 21.8158, train_acc: 0.3294, train_recall: 0.1999, train_f1: 0.1491, val_acc: 0.322660, val_recall: 0.162085, val_f1: 0.116431
/home/ADS/cyang314/ucr_work/HINI_Baseline/GraphLoRA/model/GraphLoRA.py:218: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.
Consider using tensor.detach() first. (Triggered internally at /pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:836.)
  f.write(f'{pre_dataset} to {downstream_dataset}: seed: %d, epoch: %d, train_loss: %f, train_acc: %f, train_recall: %f, train_f1: %f, val_acc: %f, val_recall: %f, val_f1: %f\n' %
Epoch: 280, loss: 21.7914, train_acc: 0.3294, train_recall: 0.1999, train_f1: 0.1492, val_acc: 0.322660, val_recall: 0.162085, val_f1: 0.116586
Epoch: 281, loss: 21.7989, train_acc: 0.3294, train_recall: 0.1999, train_f1: 0.1492, val_acc: 0.322660, val_recall: 0.162085, val_f1: 0.116586
Epoch: 282, loss: 21.7719, train_acc: 0.3294, train_recall: 0.1999, train_f1: 0.1492, val_acc: 0.322660, val_recall: 0.162085, val_f1: 0.116586
Epoch: 283, loss: 21.7603, train_acc: 0.3294, train_recall: 0.1999, train_f1: 0.1492, val_acc: 0.322660, val_recall: 0.162085, val_f1: 0.116586
Epoch: 284, loss: 21.7683, train_acc: 0.3294, train_recall: 0.1999, train_f1: 0.1492, val_acc: 0.322660, val_recall: 0.162085, val_f1: 0.116586
Epoch: 285, loss: 21.7664, train_acc: 0.3294, train_recall: 0.1999, train_f1: 0.1492, val_acc: 0.322660, val_recall: 0.162085, val_f1: 0.116586
Epoch: 286, loss: 21.7682, train_acc: 0.3294, train_recall: 0.1999, train_f1: 0.1492, val_acc: 0.322660, val_recall: 0.162085, val_f1: 0.116586
Epoch: 287, loss: 21.7297, train_acc: 0.3294, train_recall: 0.1999, train_f1: 0.1492, val_acc: 0.322660, val_recall: 0.162085, val_f1: 0.116586
Epoch: 288, loss: 21.7297, train_acc: 0.3294, train_recall: 0.1999, train_f1: 0.1492, val_acc: 0.322660, val_recall: 0.162085, val_f1: 0.116586
Epoch: 289, loss: 21.7306, train_acc: 0.3294, train_recall: 0.1999, train_f1: 0.1492, val_acc: 0.322660, val_recall: 0.162085, val_f1: 0.116586
Epoch: 290, loss: 21.7105, train_acc: 0.3294, train_recall: 0.1999, train_f1: 0.1492, val_acc: 0.322660, val_recall: 0.162085, val_f1: 0.116586
Epoch: 291, loss: 21.6994, train_acc: 0.3294, train_recall: 0.1999, train_f1: 0.1492, val_acc: 0.322660, val_recall: 0.162085, val_f1: 0.116586
Epoch: 292, loss: 21.7014, train_acc: 0.3294, train_recall: 0.1999, train_f1: 0.1492, val_acc: 0.322660, val_recall: 0.162085, val_f1: 0.116586
Epoch: 293, loss: 21.6893, train_acc: 0.3294, train_recall: 0.1999, train_f1: 0.1492, val_acc: 0.322660, val_recall: 0.162085, val_f1: 0.116586
Epoch: 294, loss: 21.6794, train_acc: 0.3294, train_recall: 0.1999, train_f1: 0.1492, val_acc: 0.322660, val_recall: 0.162085, val_f1: 0.116586
Epoch: 295, loss: 21.6560, train_acc: 0.3294, train_recall: 0.1999, train_f1: 0.1492, val_acc: 0.322660, val_recall: 0.162085, val_f1: 0.116586
Epoch: 296, loss: 21.6377, train_acc: 0.3294, train_recall: 0.1999, train_f1: 0.1492, val_acc: 0.322660, val_recall: 0.162085, val_f1: 0.116586
Epoch: 297, loss: 21.6540, train_acc: 0.3294, train_recall: 0.1999, train_f1: 0.1492, val_acc: 0.322660, val_recall: 0.162085, val_f1: 0.116586
Epoch: 298, loss: 21.6394, train_acc: 0.3294, train_recall: 0.1999, train_f1: 0.1492, val_acc: 0.322660, val_recall: 0.162085, val_f1: 0.116586
Epoch: 299, loss: 21.6311, train_acc: 0.3294, train_recall: 0.1999, train_f1: 0.1492, val_acc: 0.322660, val_recall: 0.162085, val_f1: 0.116586
epoch: 68, train_acc: 0.318856, val_acc: 0.339901, val_recall: 0.207781, val_f1: 0.136713
Running: year=2015 â†’ downstream_year=2016, seed=4
Random seed set to 42

==============================
PRE-TRAINING
==============================
create PreTrain instance...
pre-training...
(T) | Epoch=001, loss=7.3009, this epoch 0.0455, total 0.0455
+++model saved ! 2015.pth
(T) | Epoch=002, loss=6.3445, this epoch 0.0349, total 0.0804
+++model saved ! 2015.pth
(T) | Epoch=003, loss=6.3445, this epoch 0.0319, total 0.1123
(T) | Epoch=004, loss=6.3444, this epoch 0.0332, total 0.1455
+++model saved ! 2015.pth
(T) | Epoch=005, loss=6.3451, this epoch 0.0318, total 0.1773
(T) | Epoch=006, loss=6.3445, this epoch 0.0283, total 0.2055
(T) | Epoch=007, loss=6.3472, this epoch 0.0288, total 0.2343
(T) | Epoch=008, loss=6.8267, this epoch 0.0236, total 0.2579
(T) | Epoch=009, loss=6.3447, this epoch 0.0345, total 0.2924
(T) | Epoch=010, loss=6.3446, this epoch 0.0363, total 0.3287
(T) | Epoch=011, loss=6.7118, this epoch 0.0303, total 0.3591
(T) | Epoch=012, loss=6.3444, this epoch 0.0292, total 0.3883
(T) | Epoch=013, loss=6.3447, this epoch 0.0330, total 0.4213
(T) | Epoch=014, loss=6.3439, this epoch 0.0292, total 0.4505
+++model saved ! 2015.pth
(T) | Epoch=015, loss=6.3438, this epoch 0.0395, total 0.4900
+++model saved ! 2015.pth
(T) | Epoch=016, loss=6.3444, this epoch 0.0250, total 0.5150
(T) | Epoch=017, loss=6.3436, this epoch 0.0343, total 0.5493
+++model saved ! 2015.pth
(T) | Epoch=018, loss=6.3435, this epoch 0.0248, total 0.5741
+++model saved ! 2015.pth
(T) | Epoch=019, loss=6.5342, this epoch 0.0421, total 0.6162
(T) | Epoch=020, loss=6.3434, this epoch 0.0244, total 0.6406
+++model saved ! 2015.pth
(T) | Epoch=021, loss=6.3474, this epoch 0.0301, total 0.6707
(T) | Epoch=022, loss=6.3466, this epoch 0.0289, total 0.6996
(T) | Epoch=023, loss=6.4797, this epoch 0.0369, total 0.7365
(T) | Epoch=024, loss=6.3427, this epoch 0.0306, total 0.7671
+++model saved ! 2015.pth
(T) | Epoch=025, loss=6.3428, this epoch 0.0383, total 0.8054
(T) | Epoch=026, loss=6.3444, this epoch 0.0354, total 0.8408
(T) | Epoch=027, loss=6.3420, this epoch 0.0321, total 0.8729
+++model saved ! 2015.pth
(T) | Epoch=028, loss=6.3464, this epoch 0.0345, total 0.9074
(T) | Epoch=029, loss=6.3437, this epoch 0.0300, total 0.9375
(T) | Epoch=030, loss=6.4340, this epoch 0.0400, total 0.9774
(T) | Epoch=031, loss=6.3460, this epoch 0.0321, total 1.0095
(T) | Epoch=032, loss=6.3407, this epoch 0.0315, total 1.0410
+++model saved ! 2015.pth
(T) | Epoch=033, loss=6.3406, this epoch 0.0300, total 1.0709
+++model saved ! 2015.pth
(T) | Epoch=034, loss=6.3411, this epoch 0.0422, total 1.1131
(T) | Epoch=035, loss=6.3453, this epoch 0.0368, total 1.1499
(T) | Epoch=036, loss=6.3412, this epoch 0.0366, total 1.1865
(T) | Epoch=037, loss=6.3867, this epoch 0.0324, total 1.2189
(T) | Epoch=038, loss=6.4135, this epoch 0.0294, total 1.2483
(T) | Epoch=039, loss=6.3379, this epoch 0.0378, total 1.2861
+++model saved ! 2015.pth
(T) | Epoch=040, loss=6.3458, this epoch 0.0247, total 1.3108
(T) | Epoch=041, loss=6.3442, this epoch 0.0264, total 1.3372
(T) | Epoch=042, loss=6.3398, this epoch 0.0219, total 1.3592
(T) | Epoch=043, loss=6.3361, this epoch 0.0214, total 1.3806
+++model saved ! 2015.pth
(T) | Epoch=044, loss=6.3448, this epoch 0.0290, total 1.4096
(T) | Epoch=045, loss=6.3339, this epoch 0.0223, total 1.4319
+++model saved ! 2015.pth
(T) | Epoch=046, loss=6.3378, this epoch 0.0349, total 1.4667
(T) | Epoch=047, loss=6.3318, this epoch 0.0226, total 1.4893
+++model saved ! 2015.pth
(T) | Epoch=048, loss=6.3725, this epoch 0.0451, total 1.5344
(T) | Epoch=049, loss=6.3294, this epoch 0.0374, total 1.5717
+++model saved ! 2015.pth
(T) | Epoch=050, loss=6.3329, this epoch 0.0401, total 1.6119
(T) | Epoch=051, loss=6.3295, this epoch 0.0312, total 1.6430
(T) | Epoch=052, loss=6.3292, this epoch 0.0306, total 1.6736
+++model saved ! 2015.pth
(T) | Epoch=053, loss=6.3262, this epoch 0.0344, total 1.7080
+++model saved ! 2015.pth
(T) | Epoch=054, loss=6.3243, this epoch 0.0373, total 1.7454
+++model saved ! 2015.pth
(T) | Epoch=055, loss=6.3177, this epoch 0.0396, total 1.7850
+++model saved ! 2015.pth
(T) | Epoch=056, loss=6.3145, this epoch 0.0409, total 1.8259
+++model saved ! 2015.pth
(T) | Epoch=057, loss=6.3199, this epoch 0.0314, total 1.8573
(T) | Epoch=058, loss=6.3309, this epoch 0.0232, total 1.8805
(T) | Epoch=059, loss=6.3194, this epoch 0.0210, total 1.9015
(T) | Epoch=060, loss=6.2954, this epoch 0.0212, total 1.9227
+++model saved ! 2015.pth
(T) | Epoch=061, loss=6.2872, this epoch 0.0335, total 1.9563
+++model saved ! 2015.pth
(T) | Epoch=062, loss=6.2811, this epoch 0.0333, total 1.9896
+++model saved ! 2015.pth
(T) | Epoch=063, loss=6.2604, this epoch 0.0250, total 2.0146
+++model saved ! 2015.pth
(T) | Epoch=064, loss=6.2564, this epoch 0.0259, total 2.0405
+++model saved ! 2015.pth
(T) | Epoch=065, loss=6.2311, this epoch 0.0298, total 2.0703
+++model saved ! 2015.pth
(T) | Epoch=066, loss=6.2235, this epoch 0.0237, total 2.0940
+++model saved ! 2015.pth
(T) | Epoch=067, loss=6.2264, this epoch 0.0314, total 2.1254
(T) | Epoch=068, loss=6.1711, this epoch 0.0278, total 2.1532
+++model saved ! 2015.pth
(T) | Epoch=069, loss=6.2524, this epoch 0.0294, total 2.1826
(T) | Epoch=070, loss=6.0517, this epoch 0.0231, total 2.2057
+++model saved ! 2015.pth
(T) | Epoch=071, loss=5.9781, this epoch 0.0423, total 2.2480
+++model saved ! 2015.pth
(T) | Epoch=072, loss=5.9220, this epoch 0.0304, total 2.2784
+++model saved ! 2015.pth
(T) | Epoch=073, loss=5.8787, this epoch 0.0346, total 2.3130
+++model saved ! 2015.pth
(T) | Epoch=074, loss=5.8249, this epoch 0.0333, total 2.3463
+++model saved ! 2015.pth
(T) | Epoch=075, loss=5.7711, this epoch 0.0331, total 2.3794
+++model saved ! 2015.pth
(T) | Epoch=076, loss=6.6189, this epoch 0.0352, total 2.4145
(T) | Epoch=077, loss=5.7260, this epoch 0.0327, total 2.4473
+++model saved ! 2015.pth
(T) | Epoch=078, loss=5.6917, this epoch 0.0309, total 2.4782
+++model saved ! 2015.pth
(T) | Epoch=079, loss=5.7709, this epoch 0.0347, total 2.5129
(T) | Epoch=080, loss=5.6984, this epoch 0.0334, total 2.5463
(T) | Epoch=081, loss=5.6744, this epoch 0.0353, total 2.5816
+++model saved ! 2015.pth
(T) | Epoch=082, loss=5.7215, this epoch 0.0264, total 2.6080
(T) | Epoch=083, loss=5.6127, this epoch 0.0265, total 2.6345
+++model saved ! 2015.pth
(T) | Epoch=084, loss=5.9439, this epoch 0.0339, total 2.6684
(T) | Epoch=085, loss=5.6125, this epoch 0.0280, total 2.6964
+++model saved ! 2015.pth
(T) | Epoch=086, loss=5.5824, this epoch 0.0233, total 2.7197
+++model saved ! 2015.pth
(T) | Epoch=087, loss=5.5724, this epoch 0.0366, total 2.7564
+++model saved ! 2015.pth
(T) | Epoch=088, loss=5.9507, this epoch 0.0376, total 2.7940
(T) | Epoch=089, loss=6.1384, this epoch 0.0388, total 2.8327
(T) | Epoch=090, loss=5.6252, this epoch 0.0346, total 2.8674
(T) | Epoch=091, loss=6.0038, this epoch 0.0299, total 2.8973
(T) | Epoch=092, loss=5.6440, this epoch 0.0357, total 2.9330
(T) | Epoch=093, loss=5.6387, this epoch 0.0309, total 2.9638
(T) | Epoch=094, loss=5.8832, this epoch 0.0309, total 2.9948
(T) | Epoch=095, loss=5.7851, this epoch 0.0272, total 3.0220
(T) | Epoch=096, loss=5.5782, this epoch 0.0307, total 3.0527
(T) | Epoch=097, loss=5.6243, this epoch 0.0379, total 3.0906
(T) | Epoch=098, loss=5.7081, this epoch 0.0273, total 3.1179
(T) | Epoch=099, loss=5.6286, this epoch 0.0343, total 3.1522
(T) | Epoch=100, loss=5.6361, this epoch 0.0354, total 3.1876
(T) | Epoch=101, loss=5.9126, this epoch 0.0291, total 3.2167
(T) | Epoch=102, loss=5.6857, this epoch 0.0351, total 3.2518
(T) | Epoch=103, loss=5.6386, this epoch 0.0310, total 3.2828
(T) | Epoch=104, loss=5.6401, this epoch 0.0297, total 3.3126
(T) | Epoch=105, loss=5.6428, this epoch 0.0305, total 3.3431
(T) | Epoch=106, loss=5.7198, this epoch 0.0273, total 3.3704
(T) | Epoch=107, loss=6.5084, this epoch 0.0358, total 3.4063
(T) | Epoch=108, loss=5.6058, this epoch 0.0391, total 3.4454
(T) | Epoch=109, loss=6.0109, this epoch 0.0364, total 3.4818
(T) | Epoch=110, loss=5.7566, this epoch 0.0359, total 3.5177
(T) | Epoch=111, loss=5.7794, this epoch 0.0362, total 3.5539
(T) | Epoch=112, loss=5.8127, this epoch 0.0305, total 3.5844
(T) | Epoch=113, loss=5.8358, this epoch 0.0363, total 3.6207
(T) | Epoch=114, loss=6.3422, this epoch 0.0277, total 3.6484
(T) | Epoch=115, loss=5.9372, this epoch 0.0372, total 3.6856
(T) | Epoch=116, loss=5.8214, this epoch 0.0358, total 3.7214
(T) | Epoch=117, loss=6.2301, this epoch 0.0371, total 3.7586
(T) | Epoch=118, loss=5.8718, this epoch 0.0392, total 3.7978
(T) | Epoch=119, loss=5.7465, this epoch 0.0366, total 3.8344
(T) | Epoch=120, loss=5.9975, this epoch 0.0235, total 3.8579
(T) | Epoch=121, loss=5.7267, this epoch 0.0278, total 3.8857
(T) | Epoch=122, loss=5.7174, this epoch 0.0368, total 3.9225
(T) | Epoch=123, loss=5.6807, this epoch 0.0346, total 3.9571
(T) | Epoch=124, loss=5.6803, this epoch 0.0384, total 3.9955
(T) | Epoch=125, loss=5.7367, this epoch 0.0340, total 4.0295
(T) | Epoch=126, loss=6.0703, this epoch 0.0331, total 4.0626
(T) | Epoch=127, loss=5.8209, this epoch 0.0367, total 4.0993
(T) | Epoch=128, loss=5.6466, this epoch 0.0364, total 4.1357
(T) | Epoch=129, loss=5.6444, this epoch 0.0314, total 4.1671
(T) | Epoch=130, loss=5.6630, this epoch 0.0359, total 4.2030
(T) | Epoch=131, loss=5.6858, this epoch 0.0321, total 4.2351
(T) | Epoch=132, loss=5.6466, this epoch 0.0219, total 4.2570
(T) | Epoch=133, loss=5.7401, this epoch 0.0418, total 4.2988
(T) | Epoch=134, loss=6.0010, this epoch 0.0393, total 4.3381
(T) | Epoch=135, loss=5.8345, this epoch 0.0377, total 4.3758
(T) | Epoch=136, loss=5.6735, this epoch 0.0354, total 4.4111
(T) | Epoch=137, loss=5.5890, this epoch 0.0313, total 4.4425
(T) | Epoch=138, loss=5.5860, this epoch 0.0280, total 4.4705
(T) | Epoch=139, loss=5.6898, this epoch 0.0295, total 4.4999
(T) | Epoch=140, loss=5.6042, this epoch 0.0385, total 4.5385
(T) | Epoch=141, loss=6.1395, this epoch 0.0375, total 4.5759
(T) | Epoch=142, loss=5.6056, this epoch 0.0388, total 4.6147
(T) | Epoch=143, loss=5.6725, this epoch 0.0307, total 4.6454
(T) | Epoch=144, loss=5.6118, this epoch 0.0213, total 4.6667
(T) | Epoch=145, loss=5.6213, this epoch 0.0292, total 4.6959
(T) | Epoch=146, loss=5.7486, this epoch 0.0362, total 4.7321
(T) | Epoch=147, loss=5.6208, this epoch 0.0358, total 4.7678
(T) | Epoch=148, loss=5.6559, this epoch 0.0388, total 4.8066
(T) | Epoch=149, loss=5.6870, this epoch 0.0304, total 4.8370
(T) | Epoch=150, loss=5.6693, this epoch 0.0345, total 4.8715
(T) | Epoch=151, loss=5.5899, this epoch 0.0444, total 4.9160
(T) | Epoch=152, loss=5.7057, this epoch 0.0389, total 4.9549
(T) | Epoch=153, loss=5.5816, this epoch 0.0398, total 4.9947
(T) | Epoch=154, loss=5.5361, this epoch 0.0326, total 5.0273
+++model saved ! 2015.pth
(T) | Epoch=155, loss=5.5700, this epoch 0.0397, total 5.0670
(T) | Epoch=156, loss=5.9975, this epoch 0.0334, total 5.1004
(T) | Epoch=157, loss=5.5145, this epoch 0.0302, total 5.1306
+++model saved ! 2015.pth
(T) | Epoch=158, loss=5.6933, this epoch 0.0235, total 5.1541
(T) | Epoch=159, loss=5.6869, this epoch 0.0354, total 5.1895
(T) | Epoch=160, loss=5.5327, this epoch 0.0231, total 5.2126
(T) | Epoch=161, loss=5.6936, this epoch 0.0363, total 5.2489
(T) | Epoch=162, loss=5.5443, this epoch 0.0303, total 5.2793
(T) | Epoch=163, loss=5.5563, this epoch 0.0224, total 5.3016
(T) | Epoch=164, loss=5.6716, this epoch 0.0264, total 5.3280
(T) | Epoch=165, loss=5.5633, this epoch 0.0315, total 5.3595
(T) | Epoch=166, loss=5.6526, this epoch 0.0360, total 5.3955
(T) | Epoch=167, loss=5.5458, this epoch 0.0288, total 5.4243
(T) | Epoch=168, loss=5.6109, this epoch 0.0312, total 5.4556
(T) | Epoch=169, loss=6.0887, this epoch 0.0354, total 5.4909
(T) | Epoch=170, loss=5.6300, this epoch 0.0319, total 5.5228
(T) | Epoch=171, loss=5.5377, this epoch 0.0356, total 5.5584
(T) | Epoch=172, loss=5.6407, this epoch 0.0317, total 5.5901
(T) | Epoch=173, loss=5.6512, this epoch 0.0302, total 5.6203
(T) | Epoch=174, loss=5.6599, this epoch 0.0308, total 5.6511
(T) | Epoch=175, loss=5.5950, this epoch 0.0365, total 5.6876
(T) | Epoch=176, loss=5.6222, this epoch 0.0220, total 5.7096
(T) | Epoch=177, loss=5.5519, this epoch 0.0294, total 5.7390
(T) | Epoch=178, loss=5.6009, this epoch 0.0311, total 5.7701
(T) | Epoch=179, loss=5.8427, this epoch 0.0368, total 5.8068
(T) | Epoch=180, loss=5.5160, this epoch 0.0338, total 5.8407
(T) | Epoch=181, loss=5.5264, this epoch 0.0311, total 5.8718
(T) | Epoch=182, loss=5.5991, this epoch 0.0324, total 5.9042
(T) | Epoch=183, loss=5.5318, this epoch 0.0311, total 5.9353
(T) | Epoch=184, loss=5.6236, this epoch 0.0393, total 5.9746
(T) | Epoch=185, loss=5.5202, this epoch 0.0300, total 6.0046
(T) | Epoch=186, loss=5.5019, this epoch 0.0358, total 6.0404
+++model saved ! 2015.pth
(T) | Epoch=187, loss=5.5239, this epoch 0.0351, total 6.0755
(T) | Epoch=188, loss=5.4774, this epoch 0.0258, total 6.1014
+++model saved ! 2015.pth
(T) | Epoch=189, loss=5.6433, this epoch 0.0339, total 6.1353
(T) | Epoch=190, loss=5.5375, this epoch 0.0354, total 6.1707
(T) | Epoch=191, loss=5.4963, this epoch 0.0354, total 6.2061
(T) | Epoch=192, loss=5.6377, this epoch 0.0234, total 6.2294
(T) | Epoch=193, loss=5.7552, this epoch 0.0331, total 6.2625
(T) | Epoch=194, loss=5.4859, this epoch 0.0301, total 6.2926
(T) | Epoch=195, loss=5.8543, this epoch 0.0254, total 6.3180
(T) | Epoch=196, loss=5.8948, this epoch 0.0306, total 6.3486
(T) | Epoch=197, loss=5.4898, this epoch 0.0344, total 6.3830
(T) | Epoch=198, loss=5.5865, this epoch 0.0340, total 6.4171
(T) | Epoch=199, loss=5.5759, this epoch 0.0382, total 6.4552
(T) | Epoch=200, loss=5.6207, this epoch 0.0252, total 6.4804
(T) | Epoch=201, loss=5.6858, this epoch 0.0369, total 6.5172
(T) | Epoch=202, loss=5.5701, this epoch 0.0358, total 6.5531
(T) | Epoch=203, loss=5.7228, this epoch 0.0296, total 6.5826
(T) | Epoch=204, loss=5.6139, this epoch 0.0289, total 6.6116
(T) | Epoch=205, loss=5.5377, this epoch 0.0317, total 6.6432
(T) | Epoch=206, loss=5.7066, this epoch 0.0288, total 6.6721
(T) | Epoch=207, loss=5.6557, this epoch 0.0279, total 6.7000
(T) | Epoch=208, loss=5.7168, this epoch 0.0296, total 6.7296
(T) | Epoch=209, loss=5.5084, this epoch 0.0286, total 6.7582
(T) | Epoch=210, loss=5.4506, this epoch 0.0224, total 6.7806
+++model saved ! 2015.pth
(T) | Epoch=211, loss=5.5602, this epoch 0.0362, total 6.8168
(T) | Epoch=212, loss=5.5185, this epoch 0.0316, total 6.8484
(T) | Epoch=213, loss=5.5186, this epoch 0.0230, total 6.8713
(T) | Epoch=214, loss=5.5161, this epoch 0.0339, total 6.9052
(T) | Epoch=215, loss=5.5392, this epoch 0.0362, total 6.9414
(T) | Epoch=216, loss=5.6644, this epoch 0.0309, total 6.9722
(T) | Epoch=217, loss=5.5933, this epoch 0.0285, total 7.0008
(T) | Epoch=218, loss=5.4884, this epoch 0.0341, total 7.0349
(T) | Epoch=219, loss=5.4725, this epoch 0.0334, total 7.0683
(T) | Epoch=220, loss=5.5151, this epoch 0.0223, total 7.0906
(T) | Epoch=221, loss=5.5940, this epoch 0.0265, total 7.1171
(T) | Epoch=222, loss=5.7623, this epoch 0.0231, total 7.1402
(T) | Epoch=223, loss=5.5258, this epoch 0.0342, total 7.1745
(T) | Epoch=224, loss=5.5260, this epoch 0.0235, total 7.1979
(T) | Epoch=225, loss=5.5467, this epoch 0.0219, total 7.2198
(T) | Epoch=226, loss=5.5767, this epoch 0.0316, total 7.2513
(T) | Epoch=227, loss=5.5687, this epoch 0.0209, total 7.2722
(T) | Epoch=228, loss=5.5500, this epoch 0.0209, total 7.2931
(T) | Epoch=229, loss=5.4830, this epoch 0.0209, total 7.3140
(T) | Epoch=230, loss=5.4849, this epoch 0.0276, total 7.3416
(T) | Epoch=231, loss=5.6051, this epoch 0.0361, total 7.3777
(T) | Epoch=232, loss=5.5024, this epoch 0.0230, total 7.4008
(T) | Epoch=233, loss=5.4967, this epoch 0.0281, total 7.4289
(T) | Epoch=234, loss=5.5081, this epoch 0.0319, total 7.4608
(T) | Epoch=235, loss=5.4733, this epoch 0.0345, total 7.4953
(T) | Epoch=236, loss=5.4702, this epoch 0.0312, total 7.5265
(T) | Epoch=237, loss=5.5384, this epoch 0.0214, total 7.5479
(T) | Epoch=238, loss=5.4866, this epoch 0.0216, total 7.5695
(T) | Epoch=239, loss=5.4858, this epoch 0.0278, total 7.5972
(T) | Epoch=240, loss=5.4585, this epoch 0.0291, total 7.6264
(T) | Epoch=241, loss=5.4662, this epoch 0.0303, total 7.6566
(T) | Epoch=242, loss=5.4382, this epoch 0.0350, total 7.6917
+++model saved ! 2015.pth
(T) | Epoch=243, loss=5.5176, this epoch 0.0406, total 7.7323
(T) | Epoch=244, loss=5.4797, this epoch 0.0310, total 7.7632
(T) | Epoch=245, loss=5.4400, this epoch 0.0318, total 7.7950
(T) | Epoch=246, loss=5.7701, this epoch 0.0291, total 7.8241
(T) | Epoch=247, loss=5.5373, this epoch 0.0286, total 7.8527
(T) | Epoch=248, loss=5.5813, this epoch 0.0294, total 7.8821
(T) | Epoch=249, loss=5.5397, this epoch 0.0338, total 7.9158
(T) | Epoch=250, loss=5.6056, this epoch 0.0316, total 7.9474
(T) | Epoch=251, loss=5.5702, this epoch 0.0269, total 7.9743
(T) | Epoch=252, loss=5.7220, this epoch 0.0299, total 8.0042
(T) | Epoch=253, loss=5.5406, this epoch 0.0311, total 8.0353
(T) | Epoch=254, loss=5.5306, this epoch 0.0313, total 8.0666
(T) | Epoch=255, loss=5.4585, this epoch 0.0345, total 8.1011
(T) | Epoch=256, loss=5.4566, this epoch 0.0348, total 8.1359
(T) | Epoch=257, loss=5.8813, this epoch 0.0314, total 8.1673
(T) | Epoch=258, loss=5.5537, this epoch 0.0315, total 8.1989
(T) | Epoch=259, loss=5.4958, this epoch 0.0316, total 8.2304
(T) | Epoch=260, loss=5.4418, this epoch 0.0326, total 8.2630
(T) | Epoch=261, loss=5.4479, this epoch 0.0436, total 8.3066
(T) | Epoch=262, loss=5.4576, this epoch 0.0323, total 8.3389
(T) | Epoch=263, loss=5.5335, this epoch 0.0217, total 8.3606
(T) | Epoch=264, loss=5.4604, this epoch 0.0311, total 8.3917
(T) | Epoch=265, loss=5.5186, this epoch 0.0321, total 8.4238
(T) | Epoch=266, loss=5.4479, this epoch 0.0318, total 8.4557
(T) | Epoch=267, loss=5.4676, this epoch 0.0384, total 8.4941
(T) | Epoch=268, loss=5.8167, this epoch 0.0353, total 8.5294
(T) | Epoch=269, loss=5.6987, this epoch 0.0385, total 8.5679
(T) | Epoch=270, loss=5.4679, this epoch 0.0315, total 8.5994
(T) | Epoch=271, loss=6.1941, this epoch 0.0366, total 8.6359
(T) | Epoch=272, loss=5.4472, this epoch 0.0383, total 8.6743
(T) | Epoch=273, loss=5.6609, this epoch 0.0353, total 8.7095
(T) | Epoch=274, loss=6.1314, this epoch 0.0386, total 8.7482
(T) | Epoch=275, loss=6.0612, this epoch 0.0402, total 8.7883
(T) | Epoch=276, loss=6.0916, this epoch 0.0290, total 8.8173
(T) | Epoch=277, loss=6.2599, this epoch 0.0319, total 8.8492
(T) | Epoch=278, loss=6.5025, this epoch 0.0293, total 8.8785
(T) | Epoch=279, loss=6.5513, this epoch 0.0319, total 8.9105
(T) | Epoch=280, loss=5.4721, this epoch 0.0317, total 8.9422
(T) | Epoch=281, loss=6.2545, this epoch 0.0313, total 8.9734
(T) | Epoch=282, loss=6.2623, this epoch 0.0308, total 9.0042
(T) | Epoch=283, loss=6.6019, this epoch 0.0293, total 9.0336
(T) | Epoch=284, loss=6.2044, this epoch 0.0374, total 9.0710
(T) | Epoch=285, loss=6.2058, this epoch 0.0333, total 9.1042
(T) | Epoch=286, loss=6.2458, this epoch 0.0320, total 9.1363
(T) | Epoch=287, loss=6.6400, this epoch 0.0375, total 9.1738
(T) | Epoch=288, loss=6.5835, this epoch 0.0393, total 9.2131
(T) | Epoch=289, loss=6.2874, this epoch 0.0315, total 9.2446
(T) | Epoch=290, loss=6.2720, this epoch 0.0399, total 9.2845
(T) | Epoch=291, loss=6.5008, this epoch 0.0310, total 9.3155
(T) | Epoch=292, loss=6.2123, this epoch 0.0316, total 9.3470
(T) | Epoch=293, loss=6.2381, this epoch 0.0316, total 9.3786
(T) | Epoch=294, loss=6.2386, this epoch 0.0317, total 9.4103
(T) | Epoch=295, loss=6.2049, this epoch 0.0306, total 9.4410
(T) | Epoch=296, loss=6.2041, this epoch 0.0377, total 9.4786
(T) | Epoch=297, loss=6.2023, this epoch 0.0373, total 9.5160
(T) | Epoch=298, loss=6.1916, this epoch 0.0307, total 9.5467
(T) | Epoch=299, loss=6.1928, this epoch 0.0298, total 9.5765
(T) | Epoch=300, loss=6.2349, this epoch 0.0407, total 9.6171
(T) | Epoch=301, loss=6.1909, this epoch 0.0374, total 9.6545
(T) | Epoch=302, loss=6.2020, this epoch 0.0331, total 9.6876
(T) | Epoch=303, loss=6.6337, this epoch 0.0346, total 9.7222
(T) | Epoch=304, loss=6.2454, this epoch 0.0303, total 9.7525
(T) | Epoch=305, loss=6.1594, this epoch 0.0317, total 9.7842
(T) | Epoch=306, loss=6.2184, this epoch 0.0303, total 9.8145
(T) | Epoch=307, loss=6.2316, this epoch 0.0313, total 9.8458
(T) | Epoch=308, loss=6.1389, this epoch 0.0298, total 9.8755
(T) | Epoch=309, loss=6.1474, this epoch 0.0339, total 9.9094
(T) | Epoch=310, loss=6.1200, this epoch 0.0392, total 9.9486
(T) | Epoch=311, loss=6.2501, this epoch 0.0401, total 9.9887
(T) | Epoch=312, loss=6.3235, this epoch 0.0298, total 10.0185
(T) | Epoch=313, loss=6.0877, this epoch 0.0377, total 10.0562
(T) | Epoch=314, loss=6.1825, this epoch 0.0314, total 10.0877
(T) | Epoch=315, loss=6.1606, this epoch 0.0336, total 10.1213
(T) | Epoch=316, loss=6.1346, this epoch 0.0291, total 10.1504
(T) | Epoch=317, loss=6.0499, this epoch 0.0374, total 10.1878
(T) | Epoch=318, loss=6.1340, this epoch 0.0407, total 10.2286
(T) | Epoch=319, loss=6.1094, this epoch 0.0436, total 10.2721
(T) | Epoch=320, loss=5.9998, this epoch 0.0408, total 10.3130
(T) | Epoch=321, loss=6.0074, this epoch 0.0333, total 10.3463
(T) | Epoch=322, loss=6.1328, this epoch 0.0402, total 10.3865
(T) | Epoch=323, loss=6.2367, this epoch 0.0299, total 10.4164
(T) | Epoch=324, loss=6.0144, this epoch 0.0293, total 10.4458
(T) | Epoch=325, loss=5.9449, this epoch 0.0290, total 10.4748
(T) | Epoch=326, loss=6.0434, this epoch 0.0375, total 10.5122
(T) | Epoch=327, loss=5.9020, this epoch 0.0341, total 10.5464
(T) | Epoch=328, loss=6.1169, this epoch 0.0300, total 10.5764
(T) | Epoch=329, loss=5.8791, this epoch 0.0409, total 10.6173
(T) | Epoch=330, loss=5.8667, this epoch 0.0410, total 10.6582
(T) | Epoch=331, loss=5.8375, this epoch 0.0310, total 10.6893
(T) | Epoch=332, loss=5.8661, this epoch 0.0385, total 10.7278
(T) | Epoch=333, loss=6.2030, this epoch 0.0303, total 10.7581
(T) | Epoch=334, loss=5.8829, this epoch 0.0378, total 10.7959
(T) | Epoch=335, loss=5.8186, this epoch 0.0317, total 10.8277
(T) | Epoch=336, loss=5.8234, this epoch 0.0305, total 10.8581
(T) | Epoch=337, loss=5.8930, this epoch 0.0355, total 10.8936
(T) | Epoch=338, loss=5.7735, this epoch 0.0305, total 10.9241
(T) | Epoch=339, loss=6.1419, this epoch 0.0334, total 10.9575
(T) | Epoch=340, loss=5.7431, this epoch 0.0403, total 10.9978
(T) | Epoch=341, loss=5.7633, this epoch 0.0365, total 11.0343
(T) | Epoch=342, loss=5.8036, this epoch 0.0357, total 11.0700
(T) | Epoch=343, loss=5.7080, this epoch 0.0394, total 11.1093
(T) | Epoch=344, loss=5.7409, this epoch 0.0391, total 11.1485
(T) | Epoch=345, loss=5.7964, this epoch 0.0395, total 11.1880
(T) | Epoch=346, loss=5.7684, this epoch 0.0391, total 11.2271
(T) | Epoch=347, loss=5.7332, this epoch 0.0328, total 11.2599
(T) | Epoch=348, loss=5.8051, this epoch 0.0292, total 11.2892
(T) | Epoch=349, loss=5.6643, this epoch 0.0308, total 11.3200
(T) | Epoch=350, loss=5.6727, this epoch 0.0395, total 11.3595
(T) | Epoch=351, loss=5.5700, this epoch 0.0297, total 11.3892
(T) | Epoch=352, loss=5.6498, this epoch 0.0306, total 11.4198
(T) | Epoch=353, loss=5.8596, this epoch 0.0306, total 11.4504
(T) | Epoch=354, loss=5.5419, this epoch 0.0301, total 11.4806
(T) | Epoch=355, loss=5.5567, this epoch 0.0370, total 11.5176
(T) | Epoch=356, loss=5.6211, this epoch 0.0386, total 11.5562
(T) | Epoch=357, loss=5.6299, this epoch 0.0383, total 11.5945
(T) | Epoch=358, loss=5.6012, this epoch 0.0415, total 11.6359
(T) | Epoch=359, loss=5.5336, this epoch 0.0383, total 11.6743
(T) | Epoch=360, loss=5.6693, this epoch 0.0294, total 11.7037
(T) | Epoch=361, loss=5.4747, this epoch 0.0289, total 11.7325
(T) | Epoch=362, loss=5.4624, this epoch 0.0325, total 11.7651
(T) | Epoch=363, loss=5.4755, this epoch 0.0287, total 11.7938
(T) | Epoch=364, loss=5.9041, this epoch 0.0311, total 11.8248
(T) | Epoch=365, loss=5.8374, this epoch 0.0286, total 11.8534
(T) | Epoch=366, loss=5.6171, this epoch 0.0328, total 11.8861
(T) | Epoch=367, loss=5.9630, this epoch 0.0292, total 11.9154
(T) | Epoch=368, loss=5.5305, this epoch 0.0391, total 11.9544
(T) | Epoch=369, loss=5.5830, this epoch 0.0298, total 11.9842
(T) | Epoch=370, loss=5.7993, this epoch 0.0344, total 12.0186
(T) | Epoch=371, loss=5.7036, this epoch 0.0384, total 12.0570
(T) | Epoch=372, loss=5.8526, this epoch 0.0335, total 12.0905
(T) | Epoch=373, loss=5.6596, this epoch 0.0287, total 12.1193
(T) | Epoch=374, loss=5.8250, this epoch 0.0314, total 12.1507
(T) | Epoch=375, loss=6.0241, this epoch 0.0383, total 12.1890
(T) | Epoch=376, loss=5.6424, this epoch 0.0303, total 12.2193
(T) | Epoch=377, loss=5.6452, this epoch 0.0359, total 12.2552
(T) | Epoch=378, loss=5.7309, this epoch 0.0336, total 12.2888
(T) | Epoch=379, loss=5.7704, this epoch 0.0312, total 12.3200
(T) | Epoch=380, loss=5.6148, this epoch 0.0315, total 12.3515
(T) | Epoch=381, loss=5.6558, this epoch 0.0380, total 12.3896
(T) | Epoch=382, loss=5.6497, this epoch 0.0303, total 12.4199
(T) | Epoch=383, loss=5.6110, this epoch 0.0333, total 12.4532
(T) | Epoch=384, loss=5.7810, this epoch 0.0289, total 12.4821
(T) | Epoch=385, loss=5.9786, this epoch 0.0292, total 12.5113
(T) | Epoch=386, loss=5.6451, this epoch 0.0359, total 12.5472
(T) | Epoch=387, loss=5.5955, this epoch 0.0233, total 12.5705
(T) | Epoch=388, loss=5.6465, this epoch 0.0373, total 12.6078
(T) | Epoch=389, loss=5.4910, this epoch 0.0285, total 12.6364
(T) | Epoch=390, loss=5.6189, this epoch 0.0367, total 12.6731
(T) | Epoch=391, loss=5.4685, this epoch 0.0304, total 12.7035
(T) | Epoch=392, loss=5.4958, this epoch 0.0382, total 12.7417
(T) | Epoch=393, loss=5.5641, this epoch 0.0328, total 12.7745
(T) | Epoch=394, loss=5.4502, this epoch 0.0395, total 12.8140
(T) | Epoch=395, loss=5.4532, this epoch 0.0347, total 12.8487
(T) | Epoch=396, loss=5.4759, this epoch 0.0290, total 12.8776
(T) | Epoch=397, loss=5.4575, this epoch 0.0345, total 12.9121
(T) | Epoch=398, loss=5.6009, this epoch 0.0303, total 12.9424
(T) | Epoch=399, loss=5.5847, this epoch 0.0296, total 12.9719
(T) | Epoch=400, loss=5.4685, this epoch 0.0399, total 13.0119
(T) | Epoch=401, loss=5.5455, this epoch 0.0301, total 13.0419
(T) | Epoch=402, loss=5.6247, this epoch 0.0290, total 13.0709
(T) | Epoch=403, loss=5.4525, this epoch 0.0312, total 13.1021
(T) | Epoch=404, loss=5.4362, this epoch 0.0299, total 13.1320
+++model saved ! 2015.pth
(T) | Epoch=405, loss=5.4400, this epoch 0.0384, total 13.1704
(T) | Epoch=406, loss=5.4798, this epoch 0.0312, total 13.2016
(T) | Epoch=407, loss=5.5529, this epoch 0.0298, total 13.2314
(T) | Epoch=408, loss=5.4490, this epoch 0.0423, total 13.2737
(T) | Epoch=409, loss=5.4630, this epoch 0.0380, total 13.3117
(T) | Epoch=410, loss=5.5495, this epoch 0.0318, total 13.3435
(T) | Epoch=411, loss=5.5500, this epoch 0.0289, total 13.3724
(T) | Epoch=412, loss=5.6063, this epoch 0.0323, total 13.4047
(T) | Epoch=413, loss=5.4396, this epoch 0.0291, total 13.4337
(T) | Epoch=414, loss=5.4364, this epoch 0.0300, total 13.4637
(T) | Epoch=415, loss=5.6077, this epoch 0.0377, total 13.5014
(T) | Epoch=416, loss=5.5016, this epoch 0.0302, total 13.5316
(T) | Epoch=417, loss=5.4417, this epoch 0.0392, total 13.5708
(T) | Epoch=418, loss=5.4503, this epoch 0.0321, total 13.6028
(T) | Epoch=419, loss=5.4448, this epoch 0.0399, total 13.6428
(T) | Epoch=420, loss=5.6126, this epoch 0.0248, total 13.6676
(T) | Epoch=421, loss=5.4671, this epoch 0.0285, total 13.6961
(T) | Epoch=422, loss=5.4307, this epoch 0.0317, total 13.7278
+++model saved ! 2015.pth
(T) | Epoch=423, loss=5.5285, this epoch 0.0325, total 13.7603
(T) | Epoch=424, loss=5.5160, this epoch 0.0331, total 13.7935
(T) | Epoch=425, loss=5.5378, this epoch 0.0379, total 13.8314
(T) | Epoch=426, loss=5.4382, this epoch 0.0307, total 13.8621
(T) | Epoch=427, loss=5.4949, this epoch 0.0369, total 13.8991
(T) | Epoch=428, loss=5.4308, this epoch 0.0299, total 13.9290
(T) | Epoch=429, loss=5.4630, this epoch 0.0431, total 13.9720
(T) | Epoch=430, loss=5.4905, this epoch 0.0312, total 14.0032
(T) | Epoch=431, loss=5.4332, this epoch 0.0383, total 14.0415
(T) | Epoch=432, loss=5.4322, this epoch 0.0305, total 14.0720
(T) | Epoch=433, loss=5.4577, this epoch 0.0299, total 14.1019
(T) | Epoch=434, loss=5.4955, this epoch 0.0312, total 14.1332
(T) | Epoch=435, loss=5.4330, this epoch 0.0232, total 14.1563
(T) | Epoch=436, loss=5.4945, this epoch 0.0389, total 14.1953
(T) | Epoch=437, loss=5.4604, this epoch 0.0410, total 14.2363
(T) | Epoch=438, loss=5.4203, this epoch 0.0407, total 14.2771
+++model saved ! 2015.pth
(T) | Epoch=439, loss=5.5291, this epoch 0.0325, total 14.3096
(T) | Epoch=440, loss=5.4317, this epoch 0.0225, total 14.3321
(T) | Epoch=441, loss=5.5912, this epoch 0.0281, total 14.3602
(T) | Epoch=442, loss=5.4522, this epoch 0.0296, total 14.3898
(T) | Epoch=443, loss=5.4724, this epoch 0.0291, total 14.4188
(T) | Epoch=444, loss=5.5039, this epoch 0.0210, total 14.4399
(T) | Epoch=445, loss=5.4231, this epoch 0.0216, total 14.4615
(T) | Epoch=446, loss=5.7531, this epoch 0.0211, total 14.4826
(T) | Epoch=447, loss=5.7189, this epoch 0.0217, total 14.5043
(T) | Epoch=448, loss=5.4252, this epoch 0.0215, total 14.5258
(T) | Epoch=449, loss=5.5057, this epoch 0.0294, total 14.5553
(T) | Epoch=450, loss=5.5007, this epoch 0.0289, total 14.5842
(T) | Epoch=451, loss=5.4943, this epoch 0.0213, total 14.6055
(T) | Epoch=452, loss=5.4707, this epoch 0.0215, total 14.6271
(T) | Epoch=453, loss=5.4843, this epoch 0.0212, total 14.6483
(T) | Epoch=454, loss=5.4385, this epoch 0.0292, total 14.6775
(T) | Epoch=455, loss=5.4589, this epoch 0.0216, total 14.6991
(T) | Epoch=456, loss=5.5895, this epoch 0.0212, total 14.7203
(T) | Epoch=457, loss=5.4486, this epoch 0.0219, total 14.7422
(T) | Epoch=458, loss=5.5444, this epoch 0.0220, total 14.7641
(T) | Epoch=459, loss=5.4526, this epoch 0.0220, total 14.7861
(T) | Epoch=460, loss=5.4215, this epoch 0.0292, total 14.8153
(T) | Epoch=461, loss=5.4392, this epoch 0.0216, total 14.8369
(T) | Epoch=462, loss=5.5083, this epoch 0.0298, total 14.8666
(T) | Epoch=463, loss=5.4156, this epoch 0.0221, total 14.8887
+++model saved ! 2015.pth
(T) | Epoch=464, loss=5.4269, this epoch 0.0397, total 14.9284
(T) | Epoch=465, loss=5.4246, this epoch 0.0234, total 14.9518
(T) | Epoch=466, loss=5.4477, this epoch 0.0339, total 14.9857
(T) | Epoch=467, loss=5.4322, this epoch 0.0313, total 15.0170
(T) | Epoch=468, loss=5.6179, this epoch 0.0357, total 15.0527
(T) | Epoch=469, loss=5.4339, this epoch 0.0370, total 15.0897
(T) | Epoch=470, loss=5.5125, this epoch 0.0370, total 15.1267
(T) | Epoch=471, loss=5.4552, this epoch 0.0313, total 15.1581
(T) | Epoch=472, loss=5.4483, this epoch 0.0320, total 15.1901
(T) | Epoch=473, loss=5.4204, this epoch 0.0295, total 15.2196
(T) | Epoch=474, loss=5.4519, this epoch 0.0377, total 15.2572
(T) | Epoch=475, loss=5.4931, this epoch 0.0249, total 15.2821
(T) | Epoch=476, loss=5.5540, this epoch 0.0349, total 15.3170
(T) | Epoch=477, loss=5.4611, this epoch 0.0289, total 15.3459
(T) | Epoch=478, loss=5.4274, this epoch 0.0299, total 15.3758
(T) | Epoch=479, loss=5.4271, this epoch 0.0294, total 15.4051
(T) | Epoch=480, loss=5.4793, this epoch 0.0364, total 15.4416
(T) | Epoch=481, loss=5.5025, this epoch 0.0295, total 15.4711
(T) | Epoch=482, loss=5.4387, this epoch 0.0379, total 15.5090
(T) | Epoch=483, loss=5.5007, this epoch 0.0312, total 15.5401
(T) | Epoch=484, loss=5.4266, this epoch 0.0292, total 15.5694
(T) | Epoch=485, loss=5.4199, this epoch 0.0345, total 15.6039
(T) | Epoch=486, loss=5.5012, this epoch 0.0371, total 15.6410
(T) | Epoch=487, loss=5.4642, this epoch 0.0294, total 15.6704
(T) | Epoch=488, loss=5.4254, this epoch 0.0316, total 15.7020
(T) | Epoch=489, loss=5.4601, this epoch 0.0380, total 15.7399
(T) | Epoch=490, loss=5.4232, this epoch 0.0331, total 15.7730
(T) | Epoch=491, loss=5.4730, this epoch 0.0348, total 15.8079
(T) | Epoch=492, loss=5.5828, this epoch 0.0397, total 15.8476
(T) | Epoch=493, loss=5.6463, this epoch 0.0309, total 15.8784
(T) | Epoch=494, loss=5.4714, this epoch 0.0295, total 15.9079
(T) | Epoch=495, loss=5.4351, this epoch 0.0291, total 15.9370
(T) | Epoch=496, loss=5.5540, this epoch 0.0410, total 15.9780
(T) | Epoch=497, loss=5.4227, this epoch 0.0332, total 16.0112
(T) | Epoch=498, loss=5.4147, this epoch 0.0268, total 16.0380
+++model saved ! 2015.pth
(T) | Epoch=499, loss=5.4123, this epoch 0.0419, total 16.0799
+++model saved ! 2015.pth
(T) | Epoch=500, loss=5.4194, this epoch 0.0367, total 16.1167
(T) | Epoch=501, loss=5.5119, this epoch 0.0387, total 16.1554
(T) | Epoch=502, loss=5.5781, this epoch 0.0358, total 16.1912
(T) | Epoch=503, loss=5.4564, this epoch 0.0374, total 16.2286
(T) | Epoch=504, loss=5.4778, this epoch 0.0310, total 16.2596
(T) | Epoch=505, loss=5.5049, this epoch 0.0293, total 16.2888
(T) | Epoch=506, loss=5.4150, this epoch 0.0323, total 16.3212
(T) | Epoch=507, loss=5.4572, this epoch 0.0236, total 16.3447
(T) | Epoch=508, loss=5.4238, this epoch 0.0289, total 16.3736
(T) | Epoch=509, loss=5.4941, this epoch 0.0289, total 16.4025
(T) | Epoch=510, loss=5.5819, this epoch 0.0293, total 16.4318
(T) | Epoch=511, loss=5.4522, this epoch 0.0227, total 16.4546
(T) | Epoch=512, loss=5.4787, this epoch 0.0392, total 16.4938
(T) | Epoch=513, loss=5.4105, this epoch 0.0243, total 16.5181
+++model saved ! 2015.pth
(T) | Epoch=514, loss=5.4909, this epoch 0.0239, total 16.5420
(T) | Epoch=515, loss=5.4507, this epoch 0.0289, total 16.5709
(T) | Epoch=516, loss=5.4115, this epoch 0.0218, total 16.5927
(T) | Epoch=517, loss=5.4083, this epoch 0.0290, total 16.6217
+++model saved ! 2015.pth
(T) | Epoch=518, loss=5.4304, this epoch 0.0230, total 16.6446
(T) | Epoch=519, loss=5.4396, this epoch 0.0285, total 16.6731
(T) | Epoch=520, loss=5.4213, this epoch 0.0362, total 16.7093
(T) | Epoch=521, loss=5.6394, this epoch 0.0302, total 16.7395
(T) | Epoch=522, loss=5.4966, this epoch 0.0222, total 16.7618
(T) | Epoch=523, loss=5.4166, this epoch 0.0216, total 16.7833
(T) | Epoch=524, loss=5.5204, this epoch 0.0285, total 16.8118
(T) | Epoch=525, loss=5.4660, this epoch 0.0236, total 16.8355
(T) | Epoch=526, loss=5.4446, this epoch 0.0394, total 16.8748
(T) | Epoch=527, loss=5.5216, this epoch 0.0287, total 16.9035
(T) | Epoch=528, loss=5.4499, this epoch 0.0411, total 16.9446
(T) | Epoch=529, loss=5.4461, this epoch 0.0343, total 16.9789
(T) | Epoch=530, loss=5.7697, this epoch 0.0288, total 17.0077
(T) | Epoch=531, loss=5.4530, this epoch 0.0285, total 17.0362
(T) | Epoch=532, loss=5.4320, this epoch 0.0307, total 17.0669
(T) | Epoch=533, loss=5.4671, this epoch 0.0223, total 17.0892
(T) | Epoch=534, loss=5.4261, this epoch 0.0217, total 17.1108
(T) | Epoch=535, loss=5.4369, this epoch 0.0214, total 17.1322
(T) | Epoch=536, loss=5.4156, this epoch 0.0219, total 17.1541
(T) | Epoch=537, loss=5.4810, this epoch 0.0273, total 17.1814
(T) | Epoch=538, loss=5.4287, this epoch 0.0315, total 17.2129
(T) | Epoch=539, loss=5.4810, this epoch 0.0287, total 17.2415
(T) | Epoch=540, loss=5.4084, this epoch 0.0295, total 17.2711
(T) | Epoch=541, loss=5.4099, this epoch 0.0319, total 17.3030
(T) | Epoch=542, loss=5.4558, this epoch 0.0224, total 17.3255
(T) | Epoch=543, loss=5.4880, this epoch 0.0278, total 17.3532
(T) | Epoch=544, loss=5.3996, this epoch 0.0232, total 17.3764
+++model saved ! 2015.pth
(T) | Epoch=545, loss=5.4638, this epoch 0.0374, total 17.4139
(T) | Epoch=546, loss=5.4003, this epoch 0.0293, total 17.4432
(T) | Epoch=547, loss=5.4178, this epoch 0.0226, total 17.4658
(T) | Epoch=548, loss=5.4627, this epoch 0.0357, total 17.5015
(T) | Epoch=549, loss=5.4088, this epoch 0.0283, total 17.5298
(T) | Epoch=550, loss=5.4664, this epoch 0.0312, total 17.5610
(T) | Epoch=551, loss=5.4519, this epoch 0.0293, total 17.5903
(T) | Epoch=552, loss=5.4807, this epoch 0.0295, total 17.6198
(T) | Epoch=553, loss=5.4028, this epoch 0.0291, total 17.6489
(T) | Epoch=554, loss=5.4194, this epoch 0.0294, total 17.6783
(T) | Epoch=555, loss=5.4337, this epoch 0.0356, total 17.7139
(T) | Epoch=556, loss=5.4022, this epoch 0.0321, total 17.7460
(T) | Epoch=557, loss=5.4235, this epoch 0.0291, total 17.7750
(T) | Epoch=558, loss=5.3940, this epoch 0.0285, total 17.8035
+++model saved ! 2015.pth
(T) | Epoch=559, loss=5.4017, this epoch 0.0259, total 17.8295
(T) | Epoch=560, loss=5.4002, this epoch 0.0366, total 17.8661
(T) | Epoch=561, loss=5.4339, this epoch 0.0293, total 17.8954
(T) | Epoch=562, loss=5.4761, this epoch 0.0334, total 17.9288
(T) | Epoch=563, loss=5.4846, this epoch 0.0236, total 17.9524
(T) | Epoch=564, loss=5.4196, this epoch 0.0213, total 17.9738
(T) | Epoch=565, loss=5.3929, this epoch 0.0300, total 18.0038
+++model saved ! 2015.pth
(T) | Epoch=566, loss=5.4045, this epoch 0.0318, total 18.0356
(T) | Epoch=567, loss=5.3953, this epoch 0.0222, total 18.0578
(T) | Epoch=568, loss=5.4318, this epoch 0.0343, total 18.0920
(T) | Epoch=569, loss=5.4803, this epoch 0.0248, total 18.1168
(T) | Epoch=570, loss=5.4449, this epoch 0.0218, total 18.1385
(T) | Epoch=571, loss=5.3929, this epoch 0.0459, total 18.1845
(T) | Epoch=572, loss=5.4055, this epoch 0.0313, total 18.2158
(T) | Epoch=573, loss=5.5337, this epoch 0.0291, total 18.2449
(T) | Epoch=574, loss=5.4835, this epoch 0.0293, total 18.2743
(T) | Epoch=575, loss=5.3933, this epoch 0.0291, total 18.3034
(T) | Epoch=576, loss=5.4332, this epoch 0.0294, total 18.3327
(T) | Epoch=577, loss=5.4184, this epoch 0.0352, total 18.3679
(T) | Epoch=578, loss=5.4450, this epoch 0.0404, total 18.4083
(T) | Epoch=579, loss=5.4045, this epoch 0.0312, total 18.4395
(T) | Epoch=580, loss=5.4775, this epoch 0.0356, total 18.4752
(T) | Epoch=581, loss=5.5230, this epoch 0.0375, total 18.5127
(T) | Epoch=582, loss=5.3867, this epoch 0.0296, total 18.5423
+++model saved ! 2015.pth
(T) | Epoch=583, loss=5.4258, this epoch 0.0413, total 18.5836
(T) | Epoch=584, loss=5.4008, this epoch 0.0323, total 18.6159
(T) | Epoch=585, loss=5.4499, this epoch 0.0214, total 18.6373
(T) | Epoch=586, loss=5.3967, this epoch 0.0354, total 18.6727
(T) | Epoch=587, loss=5.3927, this epoch 0.0407, total 18.7134
(T) | Epoch=588, loss=5.3894, this epoch 0.0355, total 18.7489
(T) | Epoch=589, loss=5.3882, this epoch 0.0346, total 18.7836
(T) | Epoch=590, loss=5.4413, this epoch 0.0292, total 18.8127
(T) | Epoch=591, loss=5.4813, this epoch 0.0418, total 18.8546
(T) | Epoch=592, loss=5.4879, this epoch 0.0373, total 18.8919
(T) | Epoch=593, loss=5.4199, this epoch 0.0312, total 18.9231
(T) | Epoch=594, loss=5.4542, this epoch 0.0353, total 18.9584
(T) | Epoch=595, loss=5.4107, this epoch 0.0316, total 18.9900
(T) | Epoch=596, loss=5.4156, this epoch 0.0348, total 19.0248
(T) | Epoch=597, loss=5.4049, this epoch 0.0289, total 19.0537
(T) | Epoch=598, loss=5.4162, this epoch 0.0373, total 19.0910
(T) | Epoch=599, loss=5.4080, this epoch 0.0401, total 19.1311
(T) | Epoch=600, loss=5.3987, this epoch 0.0247, total 19.1558
(T) | Epoch=601, loss=5.4383, this epoch 0.0334, total 19.1892
(T) | Epoch=602, loss=5.4394, this epoch 0.0379, total 19.2271
(T) | Epoch=603, loss=5.4434, this epoch 0.0311, total 19.2582
(T) | Epoch=604, loss=5.3968, this epoch 0.0424, total 19.3006
(T) | Epoch=605, loss=5.4324, this epoch 0.0398, total 19.3404
(T) | Epoch=606, loss=5.3914, this epoch 0.0430, total 19.3833
(T) | Epoch=607, loss=5.4746, this epoch 0.0396, total 19.4229
(T) | Epoch=608, loss=5.4322, this epoch 0.0397, total 19.4626
(T) | Epoch=609, loss=5.4214, this epoch 0.0397, total 19.5023
(T) | Epoch=610, loss=5.3942, this epoch 0.0395, total 19.5418
(T) | Epoch=611, loss=5.3984, this epoch 0.0399, total 19.5817
(T) | Epoch=612, loss=5.4614, this epoch 0.0319, total 19.6136
(T) | Epoch=613, loss=5.3986, this epoch 0.0369, total 19.6505
(T) | Epoch=614, loss=5.3817, this epoch 0.0294, total 19.6798
+++model saved ! 2015.pth
(T) | Epoch=615, loss=5.4214, this epoch 0.0383, total 19.7181
(T) | Epoch=616, loss=5.3858, this epoch 0.0302, total 19.7483
(T) | Epoch=617, loss=5.4838, this epoch 0.0427, total 19.7910
(T) | Epoch=618, loss=5.4272, this epoch 0.0409, total 19.8319
(T) | Epoch=619, loss=5.4520, this epoch 0.0456, total 19.8774
(T) | Epoch=620, loss=5.3841, this epoch 0.0313, total 19.9087
(T) | Epoch=621, loss=5.4481, this epoch 0.0365, total 19.9453
(T) | Epoch=622, loss=5.3893, this epoch 0.0373, total 19.9825
(T) | Epoch=623, loss=5.4128, this epoch 0.0293, total 20.0119
(T) | Epoch=624, loss=5.4245, this epoch 0.0242, total 20.0361
(T) | Epoch=625, loss=5.4436, this epoch 0.0352, total 20.0713
(T) | Epoch=626, loss=5.4665, this epoch 0.0363, total 20.1076
(T) | Epoch=627, loss=5.4290, this epoch 0.0395, total 20.1470
(T) | Epoch=628, loss=5.4125, this epoch 0.0379, total 20.1849
(T) | Epoch=629, loss=5.5287, this epoch 0.0296, total 20.2145
(T) | Epoch=630, loss=5.4578, this epoch 0.0384, total 20.2530
(T) | Epoch=631, loss=5.4757, this epoch 0.0302, total 20.2831
(T) | Epoch=632, loss=5.4147, this epoch 0.0367, total 20.3199
(T) | Epoch=633, loss=5.4140, this epoch 0.0302, total 20.3500
(T) | Epoch=634, loss=5.3873, this epoch 0.0366, total 20.3866
(T) | Epoch=635, loss=5.3738, this epoch 0.0414, total 20.4281
+++model saved ! 2015.pth
(T) | Epoch=636, loss=5.4164, this epoch 0.0392, total 20.4672
(T) | Epoch=637, loss=5.4186, this epoch 0.0242, total 20.4915
(T) | Epoch=638, loss=5.3951, this epoch 0.0353, total 20.5268
(T) | Epoch=639, loss=5.3783, this epoch 0.0366, total 20.5633
(T) | Epoch=640, loss=5.4784, this epoch 0.0314, total 20.5947
(T) | Epoch=641, loss=5.3808, this epoch 0.0232, total 20.6180
(T) | Epoch=642, loss=5.4367, this epoch 0.0369, total 20.6548
(T) | Epoch=643, loss=5.3911, this epoch 0.0320, total 20.6868
(T) | Epoch=644, loss=5.4279, this epoch 0.0293, total 20.7161
(T) | Epoch=645, loss=5.4048, this epoch 0.0296, total 20.7456
(T) | Epoch=646, loss=5.3812, this epoch 0.0302, total 20.7759
(T) | Epoch=647, loss=5.3814, this epoch 0.0375, total 20.8134
(T) | Epoch=648, loss=5.3960, this epoch 0.0295, total 20.8430
(T) | Epoch=649, loss=5.3807, this epoch 0.0360, total 20.8790
(T) | Epoch=650, loss=5.3812, this epoch 0.0308, total 20.9098
(T) | Epoch=651, loss=5.3959, this epoch 0.0357, total 20.9455
(T) | Epoch=652, loss=5.3792, this epoch 0.0342, total 20.9797
(T) | Epoch=653, loss=5.4079, this epoch 0.0355, total 21.0151
(T) | Epoch=654, loss=5.4052, this epoch 0.0315, total 21.0467
(T) | Epoch=655, loss=5.4203, this epoch 0.0356, total 21.0823
(T) | Epoch=656, loss=5.4435, this epoch 0.0336, total 21.1159
(T) | Epoch=657, loss=5.4626, this epoch 0.0363, total 21.1521
(T) | Epoch=658, loss=5.3795, this epoch 0.0331, total 21.1852
(T) | Epoch=659, loss=5.4804, this epoch 0.0293, total 21.2146
(T) | Epoch=660, loss=5.4538, this epoch 0.0295, total 21.2441
(T) | Epoch=661, loss=5.4180, this epoch 0.0309, total 21.2750
(T) | Epoch=662, loss=5.4859, this epoch 0.0348, total 21.3098
(T) | Epoch=663, loss=5.3740, this epoch 0.0312, total 21.3410
(T) | Epoch=664, loss=5.4250, this epoch 0.0360, total 21.3770
(T) | Epoch=665, loss=5.4059, this epoch 0.0376, total 21.4146
(T) | Epoch=666, loss=5.3698, this epoch 0.0401, total 21.4546
+++model saved ! 2015.pth
(T) | Epoch=667, loss=5.4287, this epoch 0.0406, total 21.4953
(T) | Epoch=668, loss=5.4953, this epoch 0.0303, total 21.5255
(T) | Epoch=669, loss=5.3876, this epoch 0.0306, total 21.5561
(T) | Epoch=670, loss=5.3914, this epoch 0.0301, total 21.5862
(T) | Epoch=671, loss=5.3856, this epoch 0.0438, total 21.6300
(T) | Epoch=672, loss=5.3890, this epoch 0.0318, total 21.6618
(T) | Epoch=673, loss=5.3692, this epoch 0.0411, total 21.7030
+++model saved ! 2015.pth
(T) | Epoch=674, loss=5.4544, this epoch 0.0309, total 21.7339
(T) | Epoch=675, loss=5.3751, this epoch 0.0239, total 21.7578
(T) | Epoch=676, loss=5.4191, this epoch 0.0286, total 21.7864
(T) | Epoch=677, loss=5.3874, this epoch 0.0289, total 21.8153
(T) | Epoch=678, loss=5.3973, this epoch 0.0291, total 21.8444
(T) | Epoch=679, loss=5.3596, this epoch 0.0220, total 21.8665
+++model saved ! 2015.pth
(T) | Epoch=680, loss=5.4302, this epoch 0.0246, total 21.8911
(T) | Epoch=681, loss=5.4128, this epoch 0.0376, total 21.9287
(T) | Epoch=682, loss=5.3741, this epoch 0.0224, total 21.9511
(T) | Epoch=683, loss=5.3642, this epoch 0.0215, total 21.9725
(T) | Epoch=684, loss=5.3765, this epoch 0.0215, total 21.9941
(T) | Epoch=685, loss=5.4445, this epoch 0.0384, total 22.0325
(T) | Epoch=686, loss=5.3769, this epoch 0.0319, total 22.0644
(T) | Epoch=687, loss=5.3681, this epoch 0.0313, total 22.0957
(T) | Epoch=688, loss=5.3696, this epoch 0.0386, total 22.1344
(T) | Epoch=689, loss=5.4498, this epoch 0.0291, total 22.1635
(T) | Epoch=690, loss=5.4072, this epoch 0.0379, total 22.2014
(T) | Epoch=691, loss=5.4061, this epoch 0.0386, total 22.2400
(T) | Epoch=692, loss=5.3829, this epoch 0.0352, total 22.2752
(T) | Epoch=693, loss=5.3655, this epoch 0.0294, total 22.3046
(T) | Epoch=694, loss=5.3944, this epoch 0.0290, total 22.3337
(T) | Epoch=695, loss=5.5199, this epoch 0.0292, total 22.3629
(T) | Epoch=696, loss=5.4869, this epoch 0.0392, total 22.4021
(T) | Epoch=697, loss=5.3692, this epoch 0.0361, total 22.4382
(T) | Epoch=698, loss=5.3732, this epoch 0.0314, total 22.4696
(T) | Epoch=699, loss=5.4472, this epoch 0.0325, total 22.5021
(T) | Epoch=700, loss=5.4123, this epoch 0.0299, total 22.5320
(T) | Epoch=701, loss=5.4575, this epoch 0.0381, total 22.5701
(T) | Epoch=702, loss=5.5159, this epoch 0.0286, total 22.5987
(T) | Epoch=703, loss=5.4134, this epoch 0.0399, total 22.6386
(T) | Epoch=704, loss=5.4620, this epoch 0.0408, total 22.6794
(T) | Epoch=705, loss=5.4137, this epoch 0.0333, total 22.7127
(T) | Epoch=706, loss=5.4683, this epoch 0.0322, total 22.7449
(T) | Epoch=707, loss=5.5749, this epoch 0.0404, total 22.7854
(T) | Epoch=708, loss=5.5047, this epoch 0.0310, total 22.8164
(T) | Epoch=709, loss=5.3673, this epoch 0.0297, total 22.8460
(T) | Epoch=710, loss=5.3699, this epoch 0.0384, total 22.8845
(T) | Epoch=711, loss=5.4399, this epoch 0.0340, total 22.9185
(T) | Epoch=712, loss=5.3803, this epoch 0.0297, total 22.9482
(T) | Epoch=713, loss=5.3843, this epoch 0.0363, total 22.9844
(T) | Epoch=714, loss=5.3793, this epoch 0.0303, total 23.0147
(T) | Epoch=715, loss=5.3783, this epoch 0.0357, total 23.0504
(T) | Epoch=716, loss=5.4011, this epoch 0.0376, total 23.0880
(T) | Epoch=717, loss=5.3734, this epoch 0.0338, total 23.1218
(T) | Epoch=718, loss=5.4123, this epoch 0.0390, total 23.1608
(T) | Epoch=719, loss=5.4271, this epoch 0.0406, total 23.2014
(T) | Epoch=720, loss=5.4221, this epoch 0.0382, total 23.2397
(T) | Epoch=721, loss=5.3811, this epoch 0.0311, total 23.2708
(T) | Epoch=722, loss=5.4150, this epoch 0.0322, total 23.3030
(T) | Epoch=723, loss=5.4235, this epoch 0.0382, total 23.3411
(T) | Epoch=724, loss=5.4279, this epoch 0.0397, total 23.3808
(T) | Epoch=725, loss=5.3678, this epoch 0.0320, total 23.4127
(T) | Epoch=726, loss=5.4044, this epoch 0.0342, total 23.4470
(T) | Epoch=727, loss=5.3635, this epoch 0.0304, total 23.4773
(T) | Epoch=728, loss=5.5054, this epoch 0.0336, total 23.5110
(T) | Epoch=729, loss=5.3890, this epoch 0.0380, total 23.5489
(T) | Epoch=730, loss=5.4012, this epoch 0.0424, total 23.5913
(T) | Epoch=731, loss=5.4674, this epoch 0.0322, total 23.6235
(T) | Epoch=732, loss=5.3972, this epoch 0.0340, total 23.6575
(T) | Epoch=733, loss=5.4267, this epoch 0.0294, total 23.6869
(T) | Epoch=734, loss=5.4719, this epoch 0.0297, total 23.7166
(T) | Epoch=735, loss=5.4134, this epoch 0.0321, total 23.7488
(T) | Epoch=736, loss=5.3880, this epoch 0.0311, total 23.7798
(T) | Epoch=737, loss=5.3600, this epoch 0.0318, total 23.8116
(T) | Epoch=738, loss=5.3930, this epoch 0.0301, total 23.8417
(T) | Epoch=739, loss=5.3802, this epoch 0.0407, total 23.8824
(T) | Epoch=740, loss=5.3979, this epoch 0.0315, total 23.9140
(T) | Epoch=741, loss=5.4039, this epoch 0.0318, total 23.9458
(T) | Epoch=742, loss=5.3895, this epoch 0.0317, total 23.9774
(T) | Epoch=743, loss=5.4181, this epoch 0.0315, total 24.0089
(T) | Epoch=744, loss=5.3665, this epoch 0.0323, total 24.0412
(T) | Epoch=745, loss=5.4168, this epoch 0.0298, total 24.0710
(T) | Epoch=746, loss=5.3958, this epoch 0.0391, total 24.1101
(T) | Epoch=747, loss=5.4048, this epoch 0.0230, total 24.1331
(T) | Epoch=748, loss=5.4325, this epoch 0.0383, total 24.1715
(T) | Epoch=749, loss=5.3737, this epoch 0.0369, total 24.2083
(T) | Epoch=750, loss=5.4135, this epoch 0.0375, total 24.2458
(T) | Epoch=751, loss=5.3698, this epoch 0.0377, total 24.2835
(T) | Epoch=752, loss=5.4782, this epoch 0.0409, total 24.3244
(T) | Epoch=753, loss=5.4658, this epoch 0.0307, total 24.3551
(T) | Epoch=754, loss=5.3614, this epoch 0.0295, total 24.3846
(T) | Epoch=755, loss=5.4456, this epoch 0.0301, total 24.4147
(T) | Epoch=756, loss=5.4411, this epoch 0.0403, total 24.4550
(T) | Epoch=757, loss=5.3949, this epoch 0.0314, total 24.4864
(T) | Epoch=758, loss=5.4387, this epoch 0.0312, total 24.5176
(T) | Epoch=759, loss=5.4691, this epoch 0.0387, total 24.5564
(T) | Epoch=760, loss=5.4357, this epoch 0.0365, total 24.5928
(T) | Epoch=761, loss=5.3539, this epoch 0.0298, total 24.6226
+++model saved ! 2015.pth
(T) | Epoch=762, loss=5.4288, this epoch 0.0418, total 24.6644
(T) | Epoch=763, loss=5.3986, this epoch 0.0319, total 24.6963
(T) | Epoch=764, loss=5.3650, this epoch 0.0366, total 24.7329
(T) | Epoch=765, loss=5.3671, this epoch 0.0397, total 24.7725
(T) | Epoch=766, loss=5.4010, this epoch 0.0315, total 24.8040
(T) | Epoch=767, loss=5.3996, this epoch 0.0366, total 24.8406
(T) | Epoch=768, loss=5.4140, this epoch 0.0334, total 24.8740
(T) | Epoch=769, loss=5.3702, this epoch 0.0380, total 24.9120
(T) | Epoch=770, loss=5.4556, this epoch 0.0359, total 24.9479
(T) | Epoch=771, loss=5.3688, this epoch 0.0310, total 24.9789
(T) | Epoch=772, loss=5.3623, this epoch 0.0295, total 25.0084
(T) | Epoch=773, loss=5.4249, this epoch 0.0318, total 25.0402
(T) | Epoch=774, loss=5.3988, this epoch 0.0447, total 25.0849
(T) | Epoch=775, loss=5.3867, this epoch 0.0377, total 25.1226
(T) | Epoch=776, loss=5.3730, this epoch 0.0406, total 25.1632
(T) | Epoch=777, loss=5.4105, this epoch 0.0308, total 25.1941
(T) | Epoch=778, loss=5.3803, this epoch 0.0324, total 25.2265
(T) | Epoch=779, loss=5.3936, this epoch 0.0371, total 25.2636
(T) | Epoch=780, loss=5.3704, this epoch 0.0383, total 25.3019
(T) | Epoch=781, loss=5.4121, this epoch 0.0384, total 25.3402
(T) | Epoch=782, loss=5.3946, this epoch 0.0329, total 25.3731
(T) | Epoch=783, loss=5.3889, this epoch 0.0358, total 25.4089
(T) | Epoch=784, loss=5.3704, this epoch 0.0397, total 25.4486
(T) | Epoch=785, loss=5.4164, this epoch 0.0318, total 25.4804
(T) | Epoch=786, loss=5.4374, this epoch 0.0361, total 25.5165
(T) | Epoch=787, loss=5.3925, this epoch 0.0297, total 25.5462
(T) | Epoch=788, loss=5.3876, this epoch 0.0407, total 25.5869
(T) | Epoch=789, loss=5.3820, this epoch 0.0386, total 25.6255
(T) | Epoch=790, loss=5.3742, this epoch 0.0391, total 25.6646
(T) | Epoch=791, loss=5.3743, this epoch 0.0327, total 25.6973
(T) | Epoch=792, loss=5.4089, this epoch 0.0321, total 25.7294
(T) | Epoch=793, loss=5.3674, this epoch 0.0308, total 25.7602
(T) | Epoch=794, loss=5.3959, this epoch 0.0366, total 25.7968
(T) | Epoch=795, loss=5.4421, this epoch 0.0403, total 25.8372
(T) | Epoch=796, loss=5.4073, this epoch 0.0296, total 25.8668
(T) | Epoch=797, loss=5.3637, this epoch 0.0379, total 25.9047
(T) | Epoch=798, loss=5.3967, this epoch 0.0405, total 25.9453
(T) | Epoch=799, loss=5.4872, this epoch 0.0409, total 25.9861
(T) | Epoch=800, loss=5.3723, this epoch 0.0374, total 26.0235
(T) | Epoch=801, loss=5.3682, this epoch 0.0399, total 26.0635
(T) | Epoch=802, loss=5.4193, this epoch 0.0399, total 26.1033
(T) | Epoch=803, loss=5.4744, this epoch 0.0344, total 26.1377
(T) | Epoch=804, loss=5.3613, this epoch 0.0382, total 26.1759
(T) | Epoch=805, loss=5.4679, this epoch 0.0295, total 26.2054
(T) | Epoch=806, loss=5.3684, this epoch 0.0398, total 26.2453
(T) | Epoch=807, loss=5.3621, this epoch 0.0342, total 26.2795
(T) | Epoch=808, loss=5.3791, this epoch 0.0379, total 26.3173
(T) | Epoch=809, loss=5.4098, this epoch 0.0376, total 26.3549
(T) | Epoch=810, loss=5.4331, this epoch 0.0338, total 26.3888
(T) | Epoch=811, loss=5.4196, this epoch 0.0378, total 26.4265
(T) | Epoch=812, loss=5.3887, this epoch 0.0421, total 26.4687
(T) | Epoch=813, loss=5.4669, this epoch 0.0401, total 26.5088
(T) | Epoch=814, loss=5.3837, this epoch 0.0405, total 26.5493
(T) | Epoch=815, loss=5.4138, this epoch 0.0415, total 26.5908
(T) | Epoch=816, loss=5.3783, this epoch 0.0406, total 26.6314
(T) | Epoch=817, loss=5.4120, this epoch 0.0376, total 26.6690
(T) | Epoch=818, loss=5.3779, this epoch 0.0414, total 26.7104
(T) | Epoch=819, loss=5.4079, this epoch 0.0395, total 26.7499
(T) | Epoch=820, loss=5.3661, this epoch 0.0400, total 26.7899
(T) | Epoch=821, loss=5.3759, this epoch 0.0345, total 26.8245
(T) | Epoch=822, loss=5.3918, this epoch 0.0349, total 26.8594
(T) | Epoch=823, loss=5.3698, this epoch 0.0318, total 26.8911
(T) | Epoch=824, loss=5.3619, this epoch 0.0380, total 26.9292
(T) | Epoch=825, loss=5.4502, this epoch 0.0345, total 26.9637
(T) | Epoch=826, loss=5.3947, this epoch 0.0393, total 27.0029
(T) | Epoch=827, loss=5.3738, this epoch 0.0399, total 27.0429
(T) | Epoch=828, loss=5.3934, this epoch 0.0318, total 27.0747
(T) | Epoch=829, loss=5.3747, this epoch 0.0311, total 27.1058
(T) | Epoch=830, loss=5.4158, this epoch 0.0355, total 27.1413
(T) | Epoch=831, loss=5.3569, this epoch 0.0398, total 27.1811
(T) | Epoch=832, loss=5.3737, this epoch 0.0316, total 27.2126
(T) | Epoch=833, loss=5.4222, this epoch 0.0384, total 27.2510
(T) | Epoch=834, loss=5.3644, this epoch 0.0337, total 27.2847
(T) | Epoch=835, loss=5.6100, this epoch 0.0381, total 27.3228
(T) | Epoch=836, loss=5.3763, this epoch 0.0317, total 27.3545
(T) | Epoch=837, loss=5.4399, this epoch 0.0312, total 27.3857
(T) | Epoch=838, loss=5.4819, this epoch 0.0326, total 27.4184
(T) | Epoch=839, loss=5.4625, this epoch 0.0263, total 27.4446
(T) | Epoch=840, loss=5.4507, this epoch 0.0398, total 27.4844
(T) | Epoch=841, loss=5.4639, this epoch 0.0318, total 27.5162
(T) | Epoch=842, loss=5.5462, this epoch 0.0314, total 27.5476
(T) | Epoch=843, loss=5.4335, this epoch 0.0382, total 27.5858
(T) | Epoch=844, loss=5.4010, this epoch 0.0406, total 27.6264
(T) | Epoch=845, loss=5.4892, this epoch 0.0330, total 27.6593
(T) | Epoch=846, loss=5.3663, this epoch 0.0357, total 27.6950
(T) | Epoch=847, loss=5.4921, this epoch 0.0317, total 27.7267
(T) | Epoch=848, loss=5.3733, this epoch 0.0340, total 27.7607
(T) | Epoch=849, loss=5.3962, this epoch 0.0384, total 27.7992
(T) | Epoch=850, loss=5.3674, this epoch 0.0387, total 27.8379
(T) | Epoch=851, loss=5.4591, this epoch 0.0402, total 27.8781
(T) | Epoch=852, loss=5.4907, this epoch 0.0331, total 27.9112
(T) | Epoch=853, loss=5.4064, this epoch 0.0372, total 27.9484
(T) | Epoch=854, loss=5.4593, this epoch 0.0338, total 27.9822
(T) | Epoch=855, loss=5.3664, this epoch 0.0390, total 28.0211
(T) | Epoch=856, loss=5.4139, this epoch 0.0308, total 28.0520
(T) | Epoch=857, loss=5.4293, this epoch 0.0395, total 28.0914
(T) | Epoch=858, loss=5.3614, this epoch 0.0401, total 28.1315
(T) | Epoch=859, loss=5.3615, this epoch 0.0312, total 28.1628
(T) | Epoch=860, loss=5.4114, this epoch 0.0217, total 28.1845
(T) | Epoch=861, loss=5.3956, this epoch 0.0275, total 28.2119
(T) | Epoch=862, loss=5.4147, this epoch 0.0400, total 28.2520
(T) | Epoch=863, loss=5.4119, this epoch 0.0318, total 28.2838
(T) | Epoch=864, loss=5.4430, this epoch 0.0393, total 28.3231
(T) | Epoch=865, loss=5.4192, this epoch 0.0395, total 28.3626
(T) | Epoch=866, loss=5.4617, this epoch 0.0379, total 28.4006
(T) | Epoch=867, loss=5.3594, this epoch 0.0388, total 28.4394
(T) | Epoch=868, loss=5.3947, this epoch 0.0344, total 28.4737
(T) | Epoch=869, loss=5.3578, this epoch 0.0384, total 28.5121
(T) | Epoch=870, loss=5.3803, this epoch 0.0325, total 28.5446
(T) | Epoch=871, loss=5.4395, this epoch 0.0381, total 28.5827
(T) | Epoch=872, loss=5.5254, this epoch 0.0394, total 28.6220
(T) | Epoch=873, loss=5.4054, this epoch 0.0309, total 28.6530
(T) | Epoch=874, loss=5.3704, this epoch 0.0375, total 28.6905
(T) | Epoch=875, loss=5.3595, this epoch 0.0370, total 28.7275
(T) | Epoch=876, loss=5.3830, this epoch 0.0373, total 28.7647
(T) | Epoch=877, loss=5.3606, this epoch 0.0311, total 28.7958
(T) | Epoch=878, loss=5.3745, this epoch 0.0321, total 28.8279
(T) | Epoch=879, loss=5.4077, this epoch 0.0378, total 28.8658
(T) | Epoch=880, loss=5.4641, this epoch 0.0340, total 28.8998
(T) | Epoch=881, loss=5.3625, this epoch 0.0383, total 28.9381
(T) | Epoch=882, loss=5.5027, this epoch 0.0403, total 28.9784
(T) | Epoch=883, loss=5.4419, this epoch 0.0416, total 29.0200
(T) | Epoch=884, loss=5.4442, this epoch 0.0313, total 29.0513
(T) | Epoch=885, loss=5.4889, this epoch 0.0329, total 29.0842
(T) | Epoch=886, loss=5.3904, this epoch 0.0418, total 29.1260
(T) | Epoch=887, loss=5.3811, this epoch 0.0380, total 29.1640
(T) | Epoch=888, loss=5.3536, this epoch 0.0393, total 29.2033
+++model saved ! 2015.pth
(T) | Epoch=889, loss=5.3893, this epoch 0.0329, total 29.2362
(T) | Epoch=890, loss=5.3615, this epoch 0.0293, total 29.2655
(T) | Epoch=891, loss=5.3730, this epoch 0.0223, total 29.2878
(T) | Epoch=892, loss=5.4291, this epoch 0.0283, total 29.3161
(T) | Epoch=893, loss=5.4850, this epoch 0.0289, total 29.3451
(T) | Epoch=894, loss=5.3885, this epoch 0.0357, total 29.3808
(T) | Epoch=895, loss=5.4378, this epoch 0.0382, total 29.4190
(T) | Epoch=896, loss=5.4141, this epoch 0.0306, total 29.4496
(T) | Epoch=897, loss=5.4474, this epoch 0.0359, total 29.4855
(T) | Epoch=898, loss=5.3651, this epoch 0.0318, total 29.5173
(T) | Epoch=899, loss=5.3834, this epoch 0.0342, total 29.5515
(T) | Epoch=900, loss=5.3587, this epoch 0.0224, total 29.5739
(T) | Epoch=901, loss=5.4534, this epoch 0.0282, total 29.6021
(T) | Epoch=902, loss=5.3763, this epoch 0.0223, total 29.6244
(T) | Epoch=903, loss=5.3644, this epoch 0.0420, total 29.6664
(T) | Epoch=904, loss=5.4079, this epoch 0.0314, total 29.6978
(T) | Epoch=905, loss=5.4267, this epoch 0.0361, total 29.7339
(T) | Epoch=906, loss=5.3807, this epoch 0.0351, total 29.7690
(T) | Epoch=907, loss=5.4625, this epoch 0.0285, total 29.7974
(T) | Epoch=908, loss=5.3605, this epoch 0.0299, total 29.8274
(T) | Epoch=909, loss=5.3567, this epoch 0.0299, total 29.8572
(T) | Epoch=910, loss=5.4293, this epoch 0.0295, total 29.8868
(T) | Epoch=911, loss=5.3916, this epoch 0.0366, total 29.9234
(T) | Epoch=912, loss=5.3558, this epoch 0.0362, total 29.9596
(T) | Epoch=913, loss=5.3560, this epoch 0.0310, total 29.9906
(T) | Epoch=914, loss=5.3572, this epoch 0.0295, total 30.0201
(T) | Epoch=915, loss=5.3912, this epoch 0.0278, total 30.0479
(T) | Epoch=916, loss=5.3535, this epoch 0.0306, total 30.0785
+++model saved ! 2015.pth
(T) | Epoch=917, loss=5.3664, this epoch 0.0237, total 30.1022
(T) | Epoch=918, loss=5.3662, this epoch 0.0211, total 30.1234
(T) | Epoch=919, loss=5.4028, this epoch 0.0287, total 30.1521
(T) | Epoch=920, loss=5.4187, this epoch 0.0216, total 30.1736
(T) | Epoch=921, loss=5.3978, this epoch 0.0211, total 30.1947
(T) | Epoch=922, loss=5.3634, this epoch 0.0290, total 30.2237
(T) | Epoch=923, loss=5.4691, this epoch 0.0300, total 30.2537
(T) | Epoch=924, loss=5.3935, this epoch 0.0297, total 30.2834
(T) | Epoch=925, loss=5.3615, this epoch 0.0318, total 30.3152
(T) | Epoch=926, loss=5.4377, this epoch 0.0354, total 30.3506
(T) | Epoch=927, loss=5.3582, this epoch 0.0372, total 30.3879
(T) | Epoch=928, loss=5.4260, this epoch 0.0378, total 30.4256
(T) | Epoch=929, loss=5.3660, this epoch 0.0314, total 30.4570
(T) | Epoch=930, loss=5.3632, this epoch 0.0221, total 30.4792
(T) | Epoch=931, loss=5.3678, this epoch 0.0215, total 30.5007
(T) | Epoch=932, loss=5.4873, this epoch 0.0215, total 30.5221
(T) | Epoch=933, loss=5.3579, this epoch 0.0299, total 30.5520
(T) | Epoch=934, loss=5.4084, this epoch 0.0293, total 30.5814
(T) | Epoch=935, loss=5.3909, this epoch 0.0215, total 30.6029
(T) | Epoch=936, loss=5.5196, this epoch 0.0214, total 30.6243
(T) | Epoch=937, loss=5.3984, this epoch 0.0298, total 30.6541
(T) | Epoch=938, loss=5.3748, this epoch 0.0298, total 30.6838
(T) | Epoch=939, loss=5.3695, this epoch 0.0297, total 30.7136
(T) | Epoch=940, loss=5.4638, this epoch 0.0212, total 30.7347
(T) | Epoch=941, loss=5.3916, this epoch 0.0217, total 30.7564
(T) | Epoch=942, loss=5.4531, this epoch 0.0299, total 30.7863
(T) | Epoch=943, loss=5.3814, this epoch 0.0291, total 30.8154
(T) | Epoch=944, loss=5.4165, this epoch 0.0360, total 30.8514
(T) | Epoch=945, loss=5.4232, this epoch 0.0388, total 30.8902
(T) | Epoch=946, loss=5.3983, this epoch 0.0387, total 30.9289
(T) | Epoch=947, loss=5.3631, this epoch 0.0309, total 30.9597
(T) | Epoch=948, loss=5.3577, this epoch 0.0349, total 30.9947
(T) | Epoch=949, loss=5.4128, this epoch 0.0299, total 31.0245
(T) | Epoch=950, loss=5.4152, this epoch 0.0230, total 31.0476
(T) | Epoch=951, loss=5.5175, this epoch 0.0216, total 31.0692
(T) | Epoch=952, loss=5.4276, this epoch 0.0215, total 31.0907
(T) | Epoch=953, loss=5.3794, this epoch 0.0287, total 31.1195
(T) | Epoch=954, loss=5.3561, this epoch 0.0291, total 31.1486
(T) | Epoch=955, loss=5.5146, this epoch 0.0289, total 31.1775
(T) | Epoch=956, loss=5.4424, this epoch 0.0216, total 31.1991
(T) | Epoch=957, loss=5.4077, this epoch 0.0274, total 31.2265
(T) | Epoch=958, loss=5.3551, this epoch 0.0311, total 31.2577
(T) | Epoch=959, loss=5.3544, this epoch 0.0210, total 31.2786
(T) | Epoch=960, loss=5.3548, this epoch 0.0349, total 31.3135
(T) | Epoch=961, loss=5.3850, this epoch 0.0299, total 31.3435
(T) | Epoch=962, loss=5.3611, this epoch 0.0294, total 31.3729
(T) | Epoch=963, loss=5.3906, this epoch 0.0224, total 31.3952
(T) | Epoch=964, loss=5.4360, this epoch 0.0293, total 31.4245
(T) | Epoch=965, loss=5.3804, this epoch 0.0347, total 31.4592
(T) | Epoch=966, loss=5.3631, this epoch 0.0299, total 31.4891
(T) | Epoch=967, loss=5.3901, this epoch 0.0300, total 31.5191
(T) | Epoch=968, loss=5.3902, this epoch 0.0213, total 31.5404
(T) | Epoch=969, loss=5.7143, this epoch 0.0282, total 31.5686
(T) | Epoch=970, loss=5.3718, this epoch 0.0321, total 31.6007
(T) | Epoch=971, loss=5.3679, this epoch 0.0345, total 31.6352
(T) | Epoch=972, loss=5.3615, this epoch 0.0313, total 31.6664
(T) | Epoch=973, loss=5.3878, this epoch 0.0217, total 31.6882
(T) | Epoch=974, loss=5.4083, this epoch 0.0351, total 31.7232
(T) | Epoch=975, loss=5.3535, this epoch 0.0312, total 31.7544
(T) | Epoch=976, loss=5.4206, this epoch 0.0290, total 31.7834
(T) | Epoch=977, loss=5.4463, this epoch 0.0215, total 31.8049
(T) | Epoch=978, loss=5.3644, this epoch 0.0289, total 31.8339
(T) | Epoch=979, loss=5.4362, this epoch 0.0216, total 31.8554
(T) | Epoch=980, loss=5.3540, this epoch 0.0352, total 31.8906
(T) | Epoch=981, loss=5.4379, this epoch 0.0351, total 31.9257
(T) | Epoch=982, loss=5.3980, this epoch 0.0290, total 31.9547
(T) | Epoch=983, loss=5.3550, this epoch 0.0289, total 31.9836
(T) | Epoch=984, loss=5.3559, this epoch 0.0215, total 32.0051
(T) | Epoch=985, loss=5.3511, this epoch 0.0284, total 32.0335
+++model saved ! 2015.pth
(T) | Epoch=986, loss=5.3598, this epoch 0.0304, total 32.0639
(T) | Epoch=987, loss=5.4850, this epoch 0.0299, total 32.0938
(T) | Epoch=988, loss=5.4163, this epoch 0.0283, total 32.1221
(T) | Epoch=989, loss=5.3497, this epoch 0.0220, total 32.1441
+++model saved ! 2015.pth
(T) | Epoch=990, loss=5.3871, this epoch 0.0221, total 32.1662
(T) | Epoch=991, loss=5.4118, this epoch 0.0283, total 32.1946
(T) | Epoch=992, loss=5.4212, this epoch 0.0211, total 32.2156
(T) | Epoch=993, loss=5.4191, this epoch 0.0205, total 32.2361
(T) | Epoch=994, loss=5.4329, this epoch 0.0207, total 32.2568
(T) | Epoch=995, loss=5.3584, this epoch 0.0282, total 32.2850
(T) | Epoch=996, loss=5.3596, this epoch 0.0214, total 32.3064
(T) | Epoch=997, loss=5.4289, this epoch 0.0207, total 32.3271
(T) | Epoch=998, loss=5.3952, this epoch 0.0209, total 32.3480
(T) | Epoch=999, loss=5.4049, this epoch 0.0287, total 32.3767
(T) | Epoch=1000, loss=5.4135, this epoch 0.0212, total 32.3979
=== Final ===

==============================
LoRA FINE-TUNING
==============================
Random seed set to 4
Epoch: 0, loss: 36.0866, train_acc: 0.0000, train_recall: 0.0000, train_f1: 0.0000, val_acc: 0.000000, val_recall: 0.000000, val_f1: 0.000000
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed4.csv
Epoch: 1, loss: 31.7249, train_acc: 0.0339, train_recall: 0.1250, train_f1: 0.0082, val_acc: 0.044335, val_recall: 0.125000, val_f1: 0.010613
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed4.csv
Epoch: 2, loss: 31.0722, train_acc: 0.0233, train_recall: 0.1250, train_f1: 0.0059, val_acc: 0.034483, val_recall: 0.125000, val_f1: 0.008537
Epoch: 3, loss: 39.5576, train_acc: 0.3178, train_recall: 0.1250, train_f1: 0.0603, val_acc: 0.322660, val_recall: 0.125000, val_f1: 0.060987
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed4.csv
Epoch: 4, loss: 70.3599, train_acc: 0.3083, train_recall: 0.1250, train_f1: 0.0589, val_acc: 0.307882, val_recall: 0.125000, val_f1: 0.058851
Epoch: 5, loss: 111.7604, train_acc: 0.2235, train_recall: 0.1250, train_f1: 0.0457, val_acc: 0.216749, val_recall: 0.125000, val_f1: 0.044534
Epoch: 6, loss: 99.8110, train_acc: 0.2235, train_recall: 0.1250, train_f1: 0.0457, val_acc: 0.216749, val_recall: 0.125000, val_f1: 0.044534
Epoch: 7, loss: 74.2945, train_acc: 0.3083, train_recall: 0.1250, train_f1: 0.0589, val_acc: 0.307882, val_recall: 0.125000, val_f1: 0.058851
Epoch: 8, loss: 64.8364, train_acc: 0.3061, train_recall: 0.1241, train_f1: 0.0597, val_acc: 0.305419, val_recall: 0.110222, val_f1: 0.052788
Epoch: 9, loss: 91.3206, train_acc: 0.0244, train_recall: 0.1250, train_f1: 0.0059, val_acc: 0.009852, val_recall: 0.125000, val_f1: 0.002439
Epoch: 10, loss: 84.9294, train_acc: 0.3178, train_recall: 0.1250, train_f1: 0.0603, val_acc: 0.322660, val_recall: 0.125000, val_f1: 0.060987
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed4.csv
Epoch: 11, loss: 90.2653, train_acc: 0.3178, train_recall: 0.1250, train_f1: 0.0603, val_acc: 0.322660, val_recall: 0.125000, val_f1: 0.060987
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed4.csv
Epoch: 12, loss: 82.5591, train_acc: 0.3178, train_recall: 0.1250, train_f1: 0.0603, val_acc: 0.322660, val_recall: 0.125000, val_f1: 0.060987
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed4.csv
Epoch: 13, loss: 65.5491, train_acc: 0.3178, train_recall: 0.1250, train_f1: 0.0603, val_acc: 0.322660, val_recall: 0.125000, val_f1: 0.060987
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed4.csv
Epoch: 14, loss: 54.4699, train_acc: 0.0689, train_recall: 0.1262, train_f1: 0.0180, val_acc: 0.059113, val_recall: 0.125000, val_f1: 0.014019
Epoch: 15, loss: 59.3609, train_acc: 0.2235, train_recall: 0.1250, train_f1: 0.0457, val_acc: 0.216749, val_recall: 0.125000, val_f1: 0.044534
Epoch: 16, loss: 65.0609, train_acc: 0.3083, train_recall: 0.1250, train_f1: 0.0589, val_acc: 0.307882, val_recall: 0.125000, val_f1: 0.058851
Epoch: 17, loss: 68.9254, train_acc: 0.3083, train_recall: 0.1250, train_f1: 0.0589, val_acc: 0.307882, val_recall: 0.125000, val_f1: 0.058851
Epoch: 18, loss: 65.5348, train_acc: 0.2235, train_recall: 0.1250, train_f1: 0.0457, val_acc: 0.216749, val_recall: 0.125000, val_f1: 0.044534
Epoch: 19, loss: 61.9953, train_acc: 0.3125, train_recall: 0.1283, train_f1: 0.0699, val_acc: 0.320197, val_recall: 0.132943, val_f1: 0.076276
Epoch: 20, loss: 56.5096, train_acc: 0.2235, train_recall: 0.1250, train_f1: 0.0457, val_acc: 0.216749, val_recall: 0.125000, val_f1: 0.044534
Epoch: 21, loss: 49.9620, train_acc: 0.3167, train_recall: 0.1301, train_f1: 0.0703, val_acc: 0.320197, val_recall: 0.132523, val_f1: 0.074807
Epoch: 22, loss: 43.7808, train_acc: 0.3178, train_recall: 0.1250, train_f1: 0.0603, val_acc: 0.322660, val_recall: 0.125000, val_f1: 0.060987
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed4.csv
Epoch: 23, loss: 53.1175, train_acc: 0.0180, train_recall: 0.1313, train_f1: 0.0119, val_acc: 0.007389, val_recall: 0.002545, val_f1: 0.004662
Epoch: 24, loss: 51.8885, train_acc: 0.3178, train_recall: 0.1250, train_f1: 0.0603, val_acc: 0.322660, val_recall: 0.125000, val_f1: 0.060987
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed4.csv
Epoch: 25, loss: 49.0442, train_acc: 0.3178, train_recall: 0.1250, train_f1: 0.0603, val_acc: 0.322660, val_recall: 0.125000, val_f1: 0.060987
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed4.csv
Epoch: 26, loss: 43.7009, train_acc: 0.2235, train_recall: 0.1250, train_f1: 0.0457, val_acc: 0.216749, val_recall: 0.125000, val_f1: 0.044534
Epoch: 27, loss: 38.5126, train_acc: 0.2246, train_recall: 0.1254, train_f1: 0.0465, val_acc: 0.216749, val_recall: 0.125000, val_f1: 0.044534
Epoch: 28, loss: 37.4022, train_acc: 0.3220, train_recall: 0.1304, train_f1: 0.0716, val_acc: 0.312808, val_recall: 0.126863, val_f1: 0.065280
Epoch: 29, loss: 42.4953, train_acc: 0.0847, train_recall: 0.1321, train_f1: 0.0293, val_acc: 0.066502, val_recall: 0.123654, val_f1: 0.021061
Epoch: 30, loss: 39.5942, train_acc: 0.3157, train_recall: 0.1278, train_f1: 0.0727, val_acc: 0.312808, val_recall: 0.126817, val_f1: 0.067261
Epoch: 31, loss: 42.3861, train_acc: 0.3178, train_recall: 0.1250, train_f1: 0.0603, val_acc: 0.322660, val_recall: 0.125000, val_f1: 0.060987
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed4.csv
Epoch: 32, loss: 40.1915, train_acc: 0.3178, train_recall: 0.1250, train_f1: 0.0603, val_acc: 0.322660, val_recall: 0.125000, val_f1: 0.060987
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed4.csv
Epoch: 33, loss: 41.4678, train_acc: 0.3231, train_recall: 0.1361, train_f1: 0.0815, val_acc: 0.312808, val_recall: 0.126863, val_f1: 0.064744
Epoch: 34, loss: 38.6394, train_acc: 0.3189, train_recall: 0.1449, train_f1: 0.0904, val_acc: 0.312808, val_recall: 0.126863, val_f1: 0.064783
Epoch: 35, loss: 33.5466, train_acc: 0.0403, train_recall: 0.1265, train_f1: 0.0187, val_acc: 0.064039, val_recall: 0.128842, val_f1: 0.031028
Epoch: 36, loss: 31.5396, train_acc: 0.3242, train_recall: 0.1854, train_f1: 0.1077, val_acc: 0.342365, val_recall: 0.196429, val_f1: 0.127497
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed4.csv
Epoch: 37, loss: 33.5703, train_acc: 0.2278, train_recall: 0.1372, train_f1: 0.0683, val_acc: 0.216749, val_recall: 0.125000, val_f1: 0.044534
Epoch: 38, loss: 33.5674, train_acc: 0.2267, train_recall: 0.1315, train_f1: 0.0583, val_acc: 0.216749, val_recall: 0.125000, val_f1: 0.044534
Epoch: 39, loss: 28.9635, train_acc: 0.3189, train_recall: 0.1307, train_f1: 0.0713, val_acc: 0.322660, val_recall: 0.125000, val_f1: 0.060987
Epoch: 40, loss: 28.8893, train_acc: 0.3167, train_recall: 0.1283, train_f1: 0.0677, val_acc: 0.312808, val_recall: 0.126863, val_f1: 0.064820
Epoch: 41, loss: 28.0113, train_acc: 0.3242, train_recall: 0.1714, train_f1: 0.1017, val_acc: 0.317734, val_recall: 0.189363, val_f1: 0.095755
Epoch: 42, loss: 27.8003, train_acc: 0.3157, train_recall: 0.1643, train_f1: 0.0939, val_acc: 0.325123, val_recall: 0.186546, val_f1: 0.092888
Epoch: 43, loss: 28.5998, train_acc: 0.0900, train_recall: 0.1743, train_f1: 0.0595, val_acc: 0.073892, val_recall: 0.187063, val_f1: 0.052156
Epoch: 44, loss: 28.2089, train_acc: 0.2352, train_recall: 0.1830, train_f1: 0.0915, val_acc: 0.216749, val_recall: 0.212624, val_f1: 0.079861
Epoch: 45, loss: 27.8200, train_acc: 0.3210, train_recall: 0.1814, train_f1: 0.0952, val_acc: 0.330049, val_recall: 0.218750, val_f1: 0.089827
Epoch: 46, loss: 26.3514, train_acc: 0.3210, train_recall: 0.1762, train_f1: 0.1136, val_acc: 0.330049, val_recall: 0.182119, val_f1: 0.109727
Epoch: 47, loss: 29.1714, train_acc: 0.3263, train_recall: 0.1774, train_f1: 0.1078, val_acc: 0.330049, val_recall: 0.175474, val_f1: 0.112144
Epoch: 48, loss: 28.9335, train_acc: 0.3284, train_recall: 0.1991, train_f1: 0.1190, val_acc: 0.337438, val_recall: 0.196307, val_f1: 0.125784
Epoch: 49, loss: 29.8641, train_acc: 0.0487, train_recall: 0.1286, train_f1: 0.0215, val_acc: 0.064039, val_recall: 0.134965, val_f1: 0.027817
Epoch: 50, loss: 30.0142, train_acc: 0.3231, train_recall: 0.1724, train_f1: 0.1021, val_acc: 0.339901, val_recall: 0.173611, val_f1: 0.108712
Epoch: 51, loss: 30.6209, train_acc: 0.3273, train_recall: 0.1636, train_f1: 0.1043, val_acc: 0.332512, val_recall: 0.152778, val_f1: 0.096680
Epoch: 52, loss: 30.2589, train_acc: 0.2352, train_recall: 0.1488, train_f1: 0.0868, val_acc: 0.219212, val_recall: 0.141594, val_f1: 0.076781
Epoch: 53, loss: 28.4552, train_acc: 0.3167, train_recall: 0.1246, train_f1: 0.0602, val_acc: 0.325123, val_recall: 0.156250, val_f1: 0.111215
Epoch: 54, loss: 31.8257, train_acc: 0.3220, train_recall: 0.1304, train_f1: 0.0714, val_acc: 0.315271, val_recall: 0.158113, val_f1: 0.114935
Epoch: 55, loss: 31.1057, train_acc: 0.3305, train_recall: 0.1739, train_f1: 0.1134, val_acc: 0.317734, val_recall: 0.189363, val_f1: 0.097473
Epoch: 56, loss: 28.2833, train_acc: 0.3263, train_recall: 0.1685, train_f1: 0.1016, val_acc: 0.327586, val_recall: 0.187500, val_f1: 0.093631
Epoch: 57, loss: 28.2579, train_acc: 0.2362, train_recall: 0.1684, train_f1: 0.0964, val_acc: 0.216749, val_recall: 0.183260, val_f1: 0.080587
Epoch: 58, loss: 27.0969, train_acc: 0.2511, train_recall: 0.1880, train_f1: 0.1143, val_acc: 0.221675, val_recall: 0.212667, val_f1: 0.090194
Epoch: 59, loss: 25.7763, train_acc: 0.3231, train_recall: 0.1858, train_f1: 0.1153, val_acc: 0.312808, val_recall: 0.217384, val_f1: 0.102656
Epoch: 60, loss: 27.8204, train_acc: 0.0953, train_recall: 0.1916, train_f1: 0.0676, val_acc: 0.076355, val_recall: 0.218404, val_f1: 0.053768
Epoch: 61, loss: 27.5640, train_acc: 0.3231, train_recall: 0.1860, train_f1: 0.1043, val_acc: 0.320197, val_recall: 0.220613, val_f1: 0.096118
Epoch: 62, loss: 25.1361, train_acc: 0.3178, train_recall: 0.1838, train_f1: 0.1035, val_acc: 0.312808, val_recall: 0.217567, val_f1: 0.094688
Epoch: 63, loss: 26.2116, train_acc: 0.3273, train_recall: 0.1839, train_f1: 0.1029, val_acc: 0.332512, val_recall: 0.220170, val_f1: 0.096682
Epoch: 64, loss: 26.7921, train_acc: 0.2405, train_recall: 0.1860, train_f1: 0.1006, val_acc: 0.219212, val_recall: 0.214044, val_f1: 0.082663
Epoch: 65, loss: 26.0375, train_acc: 0.3294, train_recall: 0.1848, train_f1: 0.1088, val_acc: 0.327586, val_recall: 0.187500, val_f1: 0.085085
Epoch: 66, loss: 23.6151, train_acc: 0.2426, train_recall: 0.1814, train_f1: 0.1198, val_acc: 0.226601, val_recall: 0.180322, val_f1: 0.107742
Epoch: 67, loss: 25.6199, train_acc: 0.3294, train_recall: 0.1843, train_f1: 0.1310, val_acc: 0.334975, val_recall: 0.191775, val_f1: 0.146893
Epoch: 68, loss: 25.4751, train_acc: 0.1028, train_recall: 0.1872, train_f1: 0.0930, val_acc: 0.100985, val_recall: 0.193146, val_f1: 0.107733
Epoch: 69, loss: 25.1694, train_acc: 0.0424, train_recall: 0.1081, train_f1: 0.0545, val_acc: 0.049261, val_recall: 0.099095, val_f1: 0.054045
Epoch: 70, loss: 24.2787, train_acc: 0.3284, train_recall: 0.1815, train_f1: 0.1080, val_acc: 0.342365, val_recall: 0.175032, val_f1: 0.111273
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed4.csv
Epoch: 71, loss: 24.9333, train_acc: 0.2405, train_recall: 0.1770, train_f1: 0.0964, val_acc: 0.233990, val_recall: 0.173611, val_f1: 0.092564
Epoch: 72, loss: 25.2339, train_acc: 0.3305, train_recall: 0.1790, train_f1: 0.1204, val_acc: 0.330049, val_recall: 0.175894, val_f1: 0.115767
Epoch: 73, loss: 25.6854, train_acc: 0.3305, train_recall: 0.1789, train_f1: 0.1245, val_acc: 0.322660, val_recall: 0.172336, val_f1: 0.116244
Epoch: 74, loss: 25.4307, train_acc: 0.3316, train_recall: 0.1758, train_f1: 0.1099, val_acc: 0.342365, val_recall: 0.175032, val_f1: 0.112454
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed4.csv
Epoch: 75, loss: 26.1575, train_acc: 0.2373, train_recall: 0.1758, train_f1: 0.0950, val_acc: 0.233990, val_recall: 0.173611, val_f1: 0.092466
Epoch: 76, loss: 25.5001, train_acc: 0.3242, train_recall: 0.1812, train_f1: 0.1303, val_acc: 0.325123, val_recall: 0.186311, val_f1: 0.120087
Epoch: 77, loss: 25.8595, train_acc: 0.3305, train_recall: 0.1800, train_f1: 0.1219, val_acc: 0.334975, val_recall: 0.184028, val_f1: 0.112798
Epoch: 78, loss: 25.2016, train_acc: 0.3284, train_recall: 0.1780, train_f1: 0.1254, val_acc: 0.322660, val_recall: 0.172336, val_f1: 0.116618
Epoch: 79, loss: 25.4876, train_acc: 0.1038, train_recall: 0.1901, train_f1: 0.0895, val_acc: 0.103448, val_recall: 0.183766, val_f1: 0.086730
Epoch: 80, loss: 25.7380, train_acc: 0.2362, train_recall: 0.1750, train_f1: 0.0953, val_acc: 0.231527, val_recall: 0.172191, val_f1: 0.092527
Epoch: 81, loss: 25.2059, train_acc: 0.3284, train_recall: 0.1851, train_f1: 0.1265, val_acc: 0.322660, val_recall: 0.172848, val_f1: 0.115963
Epoch: 82, loss: 25.7573, train_acc: 0.3316, train_recall: 0.1758, train_f1: 0.1092, val_acc: 0.339901, val_recall: 0.173611, val_f1: 0.109399
Epoch: 83, loss: 25.4560, train_acc: 0.3337, train_recall: 0.1890, train_f1: 0.1273, val_acc: 0.339901, val_recall: 0.182088, val_f1: 0.125287
Epoch: 84, loss: 25.8003, train_acc: 0.2373, train_recall: 0.1758, train_f1: 0.0943, val_acc: 0.233990, val_recall: 0.173611, val_f1: 0.092466
Epoch: 85, loss: 25.3663, train_acc: 0.3305, train_recall: 0.1754, train_f1: 0.1102, val_acc: 0.339901, val_recall: 0.173611, val_f1: 0.109769
Epoch: 86, loss: 25.5932, train_acc: 0.3284, train_recall: 0.1782, train_f1: 0.1202, val_acc: 0.327586, val_recall: 0.174474, val_f1: 0.113368
Epoch: 87, loss: 25.4474, train_acc: 0.1144, train_recall: 0.1901, train_f1: 0.0948, val_acc: 0.110837, val_recall: 0.183265, val_f1: 0.086437
Epoch: 88, loss: 24.7865, train_acc: 0.3337, train_recall: 0.1836, train_f1: 0.1144, val_acc: 0.342365, val_recall: 0.175032, val_f1: 0.112331
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed4.csv
Epoch: 89, loss: 25.5223, train_acc: 0.2394, train_recall: 0.1836, train_f1: 0.1001, val_acc: 0.233990, val_recall: 0.173611, val_f1: 0.092466
Epoch: 90, loss: 25.0508, train_acc: 0.3347, train_recall: 0.1877, train_f1: 0.1272, val_acc: 0.325123, val_recall: 0.173474, val_f1: 0.112506
Epoch: 91, loss: 25.2590, train_acc: 0.3326, train_recall: 0.1868, train_f1: 0.1273, val_acc: 0.327586, val_recall: 0.174428, val_f1: 0.114394
Epoch: 92, loss: 24.6209, train_acc: 0.3358, train_recall: 0.1867, train_f1: 0.1516, val_acc: 0.342365, val_recall: 0.178458, val_f1: 0.146758
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed4.csv
Epoch: 93, loss: 24.6709, train_acc: 0.2383, train_recall: 0.1762, train_f1: 0.0956, val_acc: 0.233990, val_recall: 0.173611, val_f1: 0.092762
Epoch: 94, loss: 24.5553, train_acc: 0.1081, train_recall: 0.1883, train_f1: 0.0886, val_acc: 0.100985, val_recall: 0.181039, val_f1: 0.084530
Epoch: 95, loss: 24.2220, train_acc: 0.3252, train_recall: 0.1837, train_f1: 0.1272, val_acc: 0.322660, val_recall: 0.172291, val_f1: 0.117618
Epoch: 96, loss: 24.1677, train_acc: 0.3358, train_recall: 0.1881, train_f1: 0.1267, val_acc: 0.325123, val_recall: 0.173474, val_f1: 0.112506
Epoch: 97, loss: 23.9760, train_acc: 0.2500, train_recall: 0.1877, train_f1: 0.1088, val_acc: 0.236453, val_recall: 0.173770, val_f1: 0.097773
Epoch: 98, loss: 23.8161, train_acc: 0.2458, train_recall: 0.1860, train_f1: 0.1054, val_acc: 0.231527, val_recall: 0.171770, val_f1: 0.093946
Epoch: 99, loss: 23.6876, train_acc: 0.3369, train_recall: 0.1849, train_f1: 0.1178, val_acc: 0.342365, val_recall: 0.174611, val_f1: 0.111613
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed4.csv
Epoch: 100, loss: 23.6631, train_acc: 0.3347, train_recall: 0.1877, train_f1: 0.1251, val_acc: 0.332512, val_recall: 0.176894, val_f1: 0.116468
Epoch: 101, loss: 23.1595, train_acc: 0.3422, train_recall: 0.2035, train_f1: 0.1637, val_acc: 0.362069, val_recall: 0.204804, val_f1: 0.167449
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed4.csv
Epoch: 102, loss: 23.1771, train_acc: 0.2447, train_recall: 0.1927, train_f1: 0.1077, val_acc: 0.238916, val_recall: 0.193024, val_f1: 0.107450
Epoch: 103, loss: 22.9592, train_acc: 0.3347, train_recall: 0.1738, train_f1: 0.1236, val_acc: 0.349754, val_recall: 0.183984, val_f1: 0.140389
Epoch: 104, loss: 23.4005, train_acc: 0.3199, train_recall: 0.1804, train_f1: 0.1059, val_acc: 0.337438, val_recall: 0.212601, val_f1: 0.151877
Epoch: 105, loss: 23.5992, train_acc: 0.0551, train_recall: 0.1324, train_f1: 0.0290, val_acc: 0.068966, val_recall: 0.131080, val_f1: 0.034839
Epoch: 106, loss: 23.1138, train_acc: 0.3284, train_recall: 0.1767, train_f1: 0.1160, val_acc: 0.347291, val_recall: 0.200825, val_f1: 0.153096
Epoch: 107, loss: 23.2243, train_acc: 0.3189, train_recall: 0.1779, train_f1: 0.1274, val_acc: 0.330049, val_recall: 0.178508, val_f1: 0.124991
Epoch: 108, loss: 23.5433, train_acc: 0.3347, train_recall: 0.1809, train_f1: 0.1221, val_acc: 0.332512, val_recall: 0.176894, val_f1: 0.116468
Epoch: 109, loss: 23.7783, train_acc: 0.3305, train_recall: 0.1754, train_f1: 0.1101, val_acc: 0.342365, val_recall: 0.175032, val_f1: 0.112702
Epoch: 110, loss: 23.7250, train_acc: 0.2500, train_recall: 0.1733, train_f1: 0.1053, val_acc: 0.241379, val_recall: 0.169360, val_f1: 0.102763
Epoch: 111, loss: 23.8361, train_acc: 0.3294, train_recall: 0.1734, train_f1: 0.1178, val_acc: 0.334975, val_recall: 0.173723, val_f1: 0.120486
Epoch: 112, loss: 23.8392, train_acc: 0.3379, train_recall: 0.1784, train_f1: 0.1152, val_acc: 0.342365, val_recall: 0.175032, val_f1: 0.112454
Epoch: 113, loss: 23.6853, train_acc: 0.3273, train_recall: 0.1778, train_f1: 0.1251, val_acc: 0.330049, val_recall: 0.175711, val_f1: 0.121400
Epoch: 114, loss: 23.6701, train_acc: 0.3273, train_recall: 0.1797, train_f1: 0.1219, val_acc: 0.339901, val_recall: 0.182088, val_f1: 0.125598
Epoch: 115, loss: 23.5120, train_acc: 0.3294, train_recall: 0.1710, train_f1: 0.1293, val_acc: 0.349754, val_recall: 0.190815, val_f1: 0.142431
Epoch: 116, loss: 23.4447, train_acc: 0.3400, train_recall: 0.2017, train_f1: 0.1433, val_acc: 0.347291, val_recall: 0.255984, val_f1: 0.159809
Epoch: 117, loss: 23.6009, train_acc: 0.3231, train_recall: 0.1862, train_f1: 0.1142, val_acc: 0.330049, val_recall: 0.239162, val_f1: 0.129001
Epoch: 118, loss: 23.4003, train_acc: 0.3294, train_recall: 0.1904, train_f1: 0.1329, val_acc: 0.347291, val_recall: 0.227227, val_f1: 0.171822
Epoch: 119, loss: 23.3649, train_acc: 0.3443, train_recall: 0.2020, train_f1: 0.1299, val_acc: 0.349754, val_recall: 0.195865, val_f1: 0.127749
Epoch: 120, loss: 23.2732, train_acc: 0.3337, train_recall: 0.1960, train_f1: 0.1296, val_acc: 0.347291, val_recall: 0.202921, val_f1: 0.140729
Epoch: 121, loss: 23.2467, train_acc: 0.3284, train_recall: 0.1801, train_f1: 0.1207, val_acc: 0.337438, val_recall: 0.181134, val_f1: 0.123553
Epoch: 122, loss: 23.1262, train_acc: 0.3358, train_recall: 0.1736, train_f1: 0.1260, val_acc: 0.339901, val_recall: 0.186540, val_f1: 0.126918
Epoch: 123, loss: 23.0484, train_acc: 0.3263, train_recall: 0.1735, train_f1: 0.1271, val_acc: 0.332512, val_recall: 0.219515, val_f1: 0.147128
Epoch: 124, loss: 22.9292, train_acc: 0.3284, train_recall: 0.1905, train_f1: 0.1438, val_acc: 0.332512, val_recall: 0.215856, val_f1: 0.137197
Epoch: 125, loss: 22.7796, train_acc: 0.3358, train_recall: 0.1856, train_f1: 0.1540, val_acc: 0.352217, val_recall: 0.211155, val_f1: 0.175041
Epoch: 126, loss: 22.7150, train_acc: 0.3358, train_recall: 0.1939, train_f1: 0.1472, val_acc: 0.369458, val_recall: 0.227750, val_f1: 0.191270
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed4.csv
Epoch: 127, loss: 22.7267, train_acc: 0.3294, train_recall: 0.2051, train_f1: 0.1436, val_acc: 0.354680, val_recall: 0.239628, val_f1: 0.192821
Epoch: 128, loss: 22.9777, train_acc: 0.3072, train_recall: 0.1870, train_f1: 0.1329, val_acc: 0.337438, val_recall: 0.214632, val_f1: 0.161521
Epoch: 129, loss: 22.7699, train_acc: 0.3379, train_recall: 0.1895, train_f1: 0.1207, val_acc: 0.354680, val_recall: 0.213830, val_f1: 0.157593
Epoch: 130, loss: 22.7896, train_acc: 0.3284, train_recall: 0.1955, train_f1: 0.1419, val_acc: 0.342365, val_recall: 0.231233, val_f1: 0.188770
Epoch: 131, loss: 22.7372, train_acc: 0.3284, train_recall: 0.1988, train_f1: 0.1468, val_acc: 0.334975, val_recall: 0.210820, val_f1: 0.130300
Epoch: 132, loss: 22.7890, train_acc: 0.3390, train_recall: 0.1927, train_f1: 0.1351, val_acc: 0.342365, val_recall: 0.236151, val_f1: 0.134487
Epoch: 133, loss: 22.7421, train_acc: 0.3326, train_recall: 0.2052, train_f1: 0.1599, val_acc: 0.334975, val_recall: 0.232450, val_f1: 0.153389
Epoch: 134, loss: 22.7219, train_acc: 0.3305, train_recall: 0.1957, train_f1: 0.1505, val_acc: 0.342365, val_recall: 0.243168, val_f1: 0.171970
Epoch: 135, loss: 22.8040, train_acc: 0.3464, train_recall: 0.2085, train_f1: 0.1444, val_acc: 0.349754, val_recall: 0.196048, val_f1: 0.131658
Epoch: 136, loss: 22.7205, train_acc: 0.3347, train_recall: 0.1961, train_f1: 0.1535, val_acc: 0.339901, val_recall: 0.240348, val_f1: 0.166972
Epoch: 137, loss: 22.6660, train_acc: 0.3347, train_recall: 0.2122, train_f1: 0.1719, val_acc: 0.339901, val_recall: 0.236223, val_f1: 0.160247
Epoch: 138, loss: 22.6787, train_acc: 0.3464, train_recall: 0.2108, train_f1: 0.1761, val_acc: 0.354680, val_recall: 0.252520, val_f1: 0.197139
Epoch: 139, loss: 22.6497, train_acc: 0.3390, train_recall: 0.2089, train_f1: 0.1683, val_acc: 0.366995, val_recall: 0.251102, val_f1: 0.220322
Epoch: 140, loss: 22.6092, train_acc: 0.3305, train_recall: 0.2052, train_f1: 0.1487, val_acc: 0.349754, val_recall: 0.236787, val_f1: 0.191406
Epoch: 141, loss: 22.6212, train_acc: 0.3369, train_recall: 0.2035, train_f1: 0.1513, val_acc: 0.371921, val_recall: 0.238800, val_f1: 0.202342
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed4.csv
Epoch: 142, loss: 22.6470, train_acc: 0.3400, train_recall: 0.2046, train_f1: 0.1467, val_acc: 0.357143, val_recall: 0.232754, val_f1: 0.186553
Epoch: 143, loss: 22.5926, train_acc: 0.3358, train_recall: 0.2141, train_f1: 0.1512, val_acc: 0.349754, val_recall: 0.240755, val_f1: 0.183983
Epoch: 144, loss: 22.5748, train_acc: 0.3316, train_recall: 0.2126, train_f1: 0.1495, val_acc: 0.352217, val_recall: 0.241709, val_f1: 0.185933
Epoch: 145, loss: 22.5563, train_acc: 0.3443, train_recall: 0.2086, train_f1: 0.1641, val_acc: 0.359606, val_recall: 0.224683, val_f1: 0.188516
Epoch: 146, loss: 22.5484, train_acc: 0.3316, train_recall: 0.2063, train_f1: 0.1702, val_acc: 0.344828, val_recall: 0.231759, val_f1: 0.191198
Epoch: 147, loss: 22.5635, train_acc: 0.3316, train_recall: 0.2010, train_f1: 0.1603, val_acc: 0.339901, val_recall: 0.241747, val_f1: 0.169436
Epoch: 148, loss: 22.5631, train_acc: 0.3453, train_recall: 0.2171, train_f1: 0.1775, val_acc: 0.359606, val_recall: 0.225615, val_f1: 0.183072
Epoch: 149, loss: 22.5617, train_acc: 0.3358, train_recall: 0.2073, train_f1: 0.1550, val_acc: 0.349754, val_recall: 0.203409, val_f1: 0.142148
Epoch: 150, loss: 22.5218, train_acc: 0.3369, train_recall: 0.2175, train_f1: 0.1710, val_acc: 0.344828, val_recall: 0.225806, val_f1: 0.172197
Epoch: 151, loss: 22.5052, train_acc: 0.3390, train_recall: 0.2131, train_f1: 0.1643, val_acc: 0.366995, val_recall: 0.230856, val_f1: 0.190637
Epoch: 152, loss: 22.5082, train_acc: 0.3326, train_recall: 0.2100, train_f1: 0.1628, val_acc: 0.376847, val_recall: 0.246679, val_f1: 0.208399
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed4.csv
Epoch: 153, loss: 22.5185, train_acc: 0.3263, train_recall: 0.1966, train_f1: 0.1179, val_acc: 0.344828, val_recall: 0.221342, val_f1: 0.144840
Epoch: 154, loss: 22.4919, train_acc: 0.3369, train_recall: 0.2174, train_f1: 0.1650, val_acc: 0.369458, val_recall: 0.241860, val_f1: 0.195575
Epoch: 155, loss: 22.5067, train_acc: 0.3422, train_recall: 0.2199, train_f1: 0.1804, val_acc: 0.366995, val_recall: 0.208553, val_f1: 0.178375
Epoch: 156, loss: 22.4680, train_acc: 0.3369, train_recall: 0.2267, train_f1: 0.1795, val_acc: 0.344828, val_recall: 0.205469, val_f1: 0.159104
Epoch: 157, loss: 22.4985, train_acc: 0.3443, train_recall: 0.2291, train_f1: 0.2059, val_acc: 0.366995, val_recall: 0.232858, val_f1: 0.211466
Epoch: 158, loss: 22.4755, train_acc: 0.3411, train_recall: 0.2205, train_f1: 0.2018, val_acc: 0.362069, val_recall: 0.218969, val_f1: 0.188675
Epoch: 159, loss: 22.4871, train_acc: 0.3390, train_recall: 0.2230, train_f1: 0.1951, val_acc: 0.342365, val_recall: 0.216840, val_f1: 0.171408
Epoch: 160, loss: 22.4692, train_acc: 0.3390, train_recall: 0.2065, train_f1: 0.1687, val_acc: 0.366995, val_recall: 0.208553, val_f1: 0.178335
Epoch: 161, loss: 22.4780, train_acc: 0.3411, train_recall: 0.2143, train_f1: 0.1723, val_acc: 0.366995, val_recall: 0.208553, val_f1: 0.178335
Epoch: 162, loss: 22.4667, train_acc: 0.3400, train_recall: 0.2225, train_f1: 0.1734, val_acc: 0.347291, val_recall: 0.206423, val_f1: 0.161068
Epoch: 163, loss: 22.4622, train_acc: 0.3231, train_recall: 0.2077, train_f1: 0.1804, val_acc: 0.349754, val_recall: 0.212160, val_f1: 0.187826
Epoch: 164, loss: 22.4674, train_acc: 0.3411, train_recall: 0.2245, train_f1: 0.1954, val_acc: 0.366995, val_recall: 0.242779, val_f1: 0.218243
Epoch: 165, loss: 22.4350, train_acc: 0.3347, train_recall: 0.2074, train_f1: 0.1729, val_acc: 0.334975, val_recall: 0.196007, val_f1: 0.144291
Epoch: 166, loss: 22.4471, train_acc: 0.3252, train_recall: 0.2171, train_f1: 0.2117, val_acc: 0.347291, val_recall: 0.226049, val_f1: 0.214683
Epoch: 167, loss: 22.4548, train_acc: 0.3411, train_recall: 0.2143, train_f1: 0.1733, val_acc: 0.366995, val_recall: 0.208553, val_f1: 0.178335
Epoch: 168, loss: 22.4366, train_acc: 0.3379, train_recall: 0.2164, train_f1: 0.1654, val_acc: 0.347291, val_recall: 0.206423, val_f1: 0.161068
Epoch: 169, loss: 22.4297, train_acc: 0.3453, train_recall: 0.2243, train_f1: 0.2036, val_acc: 0.354680, val_recall: 0.228316, val_f1: 0.208573
Epoch: 170, loss: 22.4164, train_acc: 0.3390, train_recall: 0.2143, train_f1: 0.1902, val_acc: 0.362069, val_recall: 0.218969, val_f1: 0.188675
Epoch: 171, loss: 22.4263, train_acc: 0.3358, train_recall: 0.2184, train_f1: 0.1749, val_acc: 0.342365, val_recall: 0.226761, val_f1: 0.160296
Epoch: 172, loss: 22.4349, train_acc: 0.3242, train_recall: 0.2113, train_f1: 0.1987, val_acc: 0.342365, val_recall: 0.222959, val_f1: 0.190888
Epoch: 173, loss: 22.4082, train_acc: 0.3432, train_recall: 0.2287, train_f1: 0.2035, val_acc: 0.369458, val_recall: 0.234279, val_f1: 0.209413
Epoch: 174, loss: 22.4094, train_acc: 0.3411, train_recall: 0.2257, train_f1: 0.1889, val_acc: 0.349754, val_recall: 0.232149, val_f1: 0.192072
Epoch: 175, loss: 22.4042, train_acc: 0.3294, train_recall: 0.2133, train_f1: 0.2091, val_acc: 0.337438, val_recall: 0.211130, val_f1: 0.200649
Epoch: 176, loss: 22.4211, train_acc: 0.3379, train_recall: 0.2139, train_f1: 0.1901, val_acc: 0.366995, val_recall: 0.251640, val_f1: 0.211718
Epoch: 177, loss: 22.4040, train_acc: 0.3379, train_recall: 0.2226, train_f1: 0.1927, val_acc: 0.347291, val_recall: 0.249510, val_f1: 0.194377
Epoch: 178, loss: 22.3963, train_acc: 0.3294, train_recall: 0.2238, train_f1: 0.2250, val_acc: 0.344828, val_recall: 0.214459, val_f1: 0.204608
Epoch: 179, loss: 22.4015, train_acc: 0.3453, train_recall: 0.2347, train_f1: 0.2220, val_acc: 0.357143, val_recall: 0.235689, val_f1: 0.218077
Epoch: 180, loss: 22.3922, train_acc: 0.3411, train_recall: 0.2308, train_f1: 0.2019, val_acc: 0.347291, val_recall: 0.219215, val_f1: 0.175912
Epoch: 181, loss: 22.3817, train_acc: 0.3337, train_recall: 0.2256, train_f1: 0.2273, val_acc: 0.334975, val_recall: 0.210176, val_f1: 0.199933
Epoch: 182, loss: 22.3767, train_acc: 0.3305, train_recall: 0.2173, train_f1: 0.2179, val_acc: 0.339901, val_recall: 0.242846, val_f1: 0.222920
Epoch: 183, loss: 22.3806, train_acc: 0.3390, train_recall: 0.2230, train_f1: 0.1954, val_acc: 0.349754, val_recall: 0.250465, val_f1: 0.196314
Epoch: 184, loss: 22.3722, train_acc: 0.3337, train_recall: 0.2255, train_f1: 0.2285, val_acc: 0.334975, val_recall: 0.210176, val_f1: 0.199972
Epoch: 185, loss: 22.3814, train_acc: 0.3358, train_recall: 0.2286, train_f1: 0.2047, val_acc: 0.339901, val_recall: 0.216123, val_f1: 0.177566
Epoch: 186, loss: 22.3600, train_acc: 0.3390, train_recall: 0.2299, train_f1: 0.2013, val_acc: 0.347291, val_recall: 0.219215, val_f1: 0.175912
Epoch: 187, loss: 22.3706, train_acc: 0.3337, train_recall: 0.2309, train_f1: 0.2317, val_acc: 0.339901, val_recall: 0.230017, val_f1: 0.224875
Epoch: 188, loss: 22.3589, train_acc: 0.3358, train_recall: 0.2339, train_f1: 0.2086, val_acc: 0.344828, val_recall: 0.235964, val_f1: 0.202369
Epoch: 189, loss: 22.3528, train_acc: 0.3390, train_recall: 0.2353, train_f1: 0.2061, val_acc: 0.349754, val_recall: 0.238056, val_f1: 0.200301
Epoch: 190, loss: 22.3614, train_acc: 0.3326, train_recall: 0.2251, train_f1: 0.2286, val_acc: 0.334975, val_recall: 0.210596, val_f1: 0.201526
Epoch: 191, loss: 22.3602, train_acc: 0.3400, train_recall: 0.2304, train_f1: 0.2039, val_acc: 0.344828, val_recall: 0.218215, val_f1: 0.175397
Epoch: 192, loss: 22.3422, train_acc: 0.3400, train_recall: 0.2304, train_f1: 0.2039, val_acc: 0.344828, val_recall: 0.218215, val_f1: 0.175372
Epoch: 193, loss: 22.3320, train_acc: 0.3326, train_recall: 0.2251, train_f1: 0.2293, val_acc: 0.334975, val_recall: 0.210596, val_f1: 0.201526
Epoch: 194, loss: 22.3390, train_acc: 0.3400, train_recall: 0.2357, train_f1: 0.2069, val_acc: 0.349754, val_recall: 0.238056, val_f1: 0.200301
Epoch: 195, loss: 22.3366, train_acc: 0.3358, train_recall: 0.2339, train_f1: 0.2080, val_acc: 0.344828, val_recall: 0.236010, val_f1: 0.200981
Epoch: 196, loss: 22.3280, train_acc: 0.3326, train_recall: 0.2305, train_f1: 0.2312, val_acc: 0.339901, val_recall: 0.230438, val_f1: 0.226454
Epoch: 197, loss: 22.3340, train_acc: 0.3400, train_recall: 0.2304, train_f1: 0.2039, val_acc: 0.344828, val_recall: 0.218215, val_f1: 0.175372
Epoch: 198, loss: 22.3198, train_acc: 0.3369, train_recall: 0.2290, train_f1: 0.2072, val_acc: 0.339901, val_recall: 0.216123, val_f1: 0.177440
Epoch: 199, loss: 22.3171, train_acc: 0.3379, train_recall: 0.2294, train_f1: 0.2079, val_acc: 0.339901, val_recall: 0.216123, val_f1: 0.177440
Epoch: 200, loss: 22.3199, train_acc: 0.3400, train_recall: 0.2304, train_f1: 0.2048, val_acc: 0.344828, val_recall: 0.218215, val_f1: 0.175372
Epoch: 201, loss: 22.3032, train_acc: 0.3347, train_recall: 0.2270, train_f1: 0.2299, val_acc: 0.364532, val_recall: 0.224199, val_f1: 0.211587
Epoch: 202, loss: 22.2995, train_acc: 0.3369, train_recall: 0.2290, train_f1: 0.2072, val_acc: 0.339901, val_recall: 0.216123, val_f1: 0.177440
Epoch: 203, loss: 22.3007, train_acc: 0.3411, train_recall: 0.2308, train_f1: 0.2051, val_acc: 0.344828, val_recall: 0.218215, val_f1: 0.175372
Epoch: 204, loss: 22.2925, train_acc: 0.3326, train_recall: 0.2251, train_f1: 0.2289, val_acc: 0.334975, val_recall: 0.210596, val_f1: 0.201526
Epoch: 205, loss: 22.3090, train_acc: 0.3411, train_recall: 0.2308, train_f1: 0.2051, val_acc: 0.344828, val_recall: 0.218215, val_f1: 0.175372
Epoch: 206, loss: 22.2854, train_acc: 0.3411, train_recall: 0.2308, train_f1: 0.2051, val_acc: 0.344828, val_recall: 0.218215, val_f1: 0.175372
Epoch: 207, loss: 22.2917, train_acc: 0.3326, train_recall: 0.2251, train_f1: 0.2293, val_acc: 0.334975, val_recall: 0.210596, val_f1: 0.201551
Epoch: 208, loss: 22.2856, train_acc: 0.3411, train_recall: 0.2308, train_f1: 0.2056, val_acc: 0.344828, val_recall: 0.218215, val_f1: 0.175397
Epoch: 209, loss: 22.2805, train_acc: 0.3379, train_recall: 0.2294, train_f1: 0.2075, val_acc: 0.339901, val_recall: 0.216123, val_f1: 0.177465
Epoch: 210, loss: 22.2818, train_acc: 0.3379, train_recall: 0.2294, train_f1: 0.2075, val_acc: 0.339901, val_recall: 0.216123, val_f1: 0.177440
Epoch: 211, loss: 22.2848, train_acc: 0.3411, train_recall: 0.2308, train_f1: 0.2051, val_acc: 0.344828, val_recall: 0.218215, val_f1: 0.175372
Epoch: 212, loss: 22.2649, train_acc: 0.3379, train_recall: 0.2294, train_f1: 0.2075, val_acc: 0.339901, val_recall: 0.216123, val_f1: 0.177440
Epoch: 213, loss: 22.2606, train_acc: 0.3379, train_recall: 0.2294, train_f1: 0.2075, val_acc: 0.339901, val_recall: 0.216123, val_f1: 0.177465
Epoch: 214, loss: 22.2563, train_acc: 0.3411, train_recall: 0.2308, train_f1: 0.2056, val_acc: 0.344828, val_recall: 0.218215, val_f1: 0.175397
Epoch: 215, loss: 22.2579, train_acc: 0.3379, train_recall: 0.2294, train_f1: 0.2075, val_acc: 0.339901, val_recall: 0.216123, val_f1: 0.177465
Epoch: 216, loss: 22.2466, train_acc: 0.3379, train_recall: 0.2294, train_f1: 0.2069, val_acc: 0.339901, val_recall: 0.216169, val_f1: 0.176052
Epoch: 217, loss: 22.2415, train_acc: 0.3369, train_recall: 0.2290, train_f1: 0.2067, val_acc: 0.339901, val_recall: 0.216123, val_f1: 0.177440
Epoch: 218, loss: 22.2426, train_acc: 0.3379, train_recall: 0.2294, train_f1: 0.2075, val_acc: 0.339901, val_recall: 0.216123, val_f1: 0.177408
Epoch: 219, loss: 22.2325, train_acc: 0.3422, train_recall: 0.2312, train_f1: 0.2059, val_acc: 0.344828, val_recall: 0.218215, val_f1: 0.175313
Epoch: 220, loss: 22.2347, train_acc: 0.3379, train_recall: 0.2294, train_f1: 0.2075, val_acc: 0.339901, val_recall: 0.216123, val_f1: 0.177379
Epoch: 221, loss: 22.2416, train_acc: 0.3369, train_recall: 0.2290, train_f1: 0.2068, val_acc: 0.339901, val_recall: 0.216123, val_f1: 0.177379
Epoch: 222, loss: 22.2356, train_acc: 0.3379, train_recall: 0.2294, train_f1: 0.2069, val_acc: 0.339901, val_recall: 0.216123, val_f1: 0.177379
Epoch: 223, loss: 22.2326, train_acc: 0.3379, train_recall: 0.2294, train_f1: 0.2075, val_acc: 0.339901, val_recall: 0.216123, val_f1: 0.177379
Epoch: 224, loss: 22.2165, train_acc: 0.3379, train_recall: 0.2294, train_f1: 0.2069, val_acc: 0.339901, val_recall: 0.216169, val_f1: 0.175992
Epoch: 225, loss: 22.2261, train_acc: 0.3379, train_recall: 0.2294, train_f1: 0.2075, val_acc: 0.339901, val_recall: 0.216123, val_f1: 0.177379
Epoch: 226, loss: 22.2332, train_acc: 0.3379, train_recall: 0.2294, train_f1: 0.2075, val_acc: 0.339901, val_recall: 0.216123, val_f1: 0.177379
Epoch: 227, loss: 22.1995, train_acc: 0.3379, train_recall: 0.2294, train_f1: 0.2069, val_acc: 0.339901, val_recall: 0.216169, val_f1: 0.175992
Epoch: 228, loss: 22.2014, train_acc: 0.3379, train_recall: 0.2294, train_f1: 0.2075, val_acc: 0.339901, val_recall: 0.216123, val_f1: 0.177379
Epoch: 229, loss: 22.2069, train_acc: 0.3379, train_recall: 0.2294, train_f1: 0.2074, val_acc: 0.342365, val_recall: 0.217123, val_f1: 0.177829
Epoch: 230, loss: 22.2051, train_acc: 0.3369, train_recall: 0.2290, train_f1: 0.2072, val_acc: 0.339901, val_recall: 0.216123, val_f1: 0.177379
Epoch: 231, loss: 22.1854, train_acc: 0.3379, train_recall: 0.2294, train_f1: 0.2080, val_acc: 0.339901, val_recall: 0.216123, val_f1: 0.177379
Epoch: 232, loss: 22.1968, train_acc: 0.3422, train_recall: 0.2312, train_f1: 0.2059, val_acc: 0.344828, val_recall: 0.218215, val_f1: 0.175313
Epoch: 233, loss: 22.1978, train_acc: 0.3379, train_recall: 0.2294, train_f1: 0.2075, val_acc: 0.339901, val_recall: 0.216123, val_f1: 0.177379
Epoch: 234, loss: 22.2054, train_acc: 0.3379, train_recall: 0.2294, train_f1: 0.2069, val_acc: 0.339901, val_recall: 0.216123, val_f1: 0.177379
Epoch: 235, loss: 22.1922, train_acc: 0.3379, train_recall: 0.2294, train_f1: 0.2074, val_acc: 0.339901, val_recall: 0.216123, val_f1: 0.177379
Epoch: 236, loss: 22.1694, train_acc: 0.3379, train_recall: 0.2294, train_f1: 0.2080, val_acc: 0.339901, val_recall: 0.216123, val_f1: 0.177405
Epoch: 237, loss: 22.1941, train_acc: 0.3379, train_recall: 0.2294, train_f1: 0.2074, val_acc: 0.339901, val_recall: 0.216123, val_f1: 0.177405
Epoch: 238, loss: 22.1773, train_acc: 0.3379, train_recall: 0.2294, train_f1: 0.2080, val_acc: 0.339901, val_recall: 0.216123, val_f1: 0.177405
Epoch: 239, loss: 22.1721, train_acc: 0.3379, train_recall: 0.2294, train_f1: 0.2074, val_acc: 0.339901, val_recall: 0.216123, val_f1: 0.177379
Epoch: 240, loss: 22.1823, train_acc: 0.3379, train_recall: 0.2294, train_f1: 0.2069, val_acc: 0.339901, val_recall: 0.216123, val_f1: 0.177379
Epoch: 241, loss: 22.1781, train_acc: 0.3369, train_recall: 0.2290, train_f1: 0.2068, val_acc: 0.339901, val_recall: 0.216123, val_f1: 0.177379
Epoch: 242, loss: 22.1831, train_acc: 0.3379, train_recall: 0.2294, train_f1: 0.2069, val_acc: 0.339901, val_recall: 0.216123, val_f1: 0.177379
Epoch: 243, loss: 22.1778, train_acc: 0.3369, train_recall: 0.2290, train_f1: 0.2072, val_acc: 0.339901, val_recall: 0.216123, val_f1: 0.177405
Epoch: 244, loss: 22.1817, train_acc: 0.3369, train_recall: 0.2237, train_f1: 0.1991, val_acc: 0.339901, val_recall: 0.216123, val_f1: 0.177405
Epoch: 245, loss: 22.1989, train_acc: 0.3358, train_recall: 0.2233, train_f1: 0.1989, val_acc: 0.339901, val_recall: 0.216123, val_f1: 0.177405
Epoch: 246, loss: 22.1860, train_acc: 0.3379, train_recall: 0.2294, train_f1: 0.2074, val_acc: 0.339901, val_recall: 0.216123, val_f1: 0.177379
Epoch: 247, loss: 22.1763, train_acc: 0.3379, train_recall: 0.2294, train_f1: 0.2069, val_acc: 0.339901, val_recall: 0.216123, val_f1: 0.177379
Epoch: 248, loss: 22.1281, train_acc: 0.3369, train_recall: 0.2290, train_f1: 0.2068, val_acc: 0.339901, val_recall: 0.216123, val_f1: 0.177355
Epoch: 249, loss: 22.1731, train_acc: 0.3379, train_recall: 0.2294, train_f1: 0.2074, val_acc: 0.339901, val_recall: 0.216169, val_f1: 0.175967
Epoch: 250, loss: 22.1517, train_acc: 0.3358, train_recall: 0.2233, train_f1: 0.1989, val_acc: 0.339901, val_recall: 0.216123, val_f1: 0.177380
Epoch: 251, loss: 22.1582, train_acc: 0.3369, train_recall: 0.2237, train_f1: 0.1991, val_acc: 0.339901, val_recall: 0.216123, val_f1: 0.177380
Epoch: 252, loss: 22.1597, train_acc: 0.3379, train_recall: 0.2294, train_f1: 0.2074, val_acc: 0.339901, val_recall: 0.216123, val_f1: 0.177355
Epoch: 253, loss: 22.1419, train_acc: 0.3379, train_recall: 0.2294, train_f1: 0.2074, val_acc: 0.339901, val_recall: 0.216123, val_f1: 0.177355
Epoch: 254, loss: 22.1748, train_acc: 0.3379, train_recall: 0.2294, train_f1: 0.2074, val_acc: 0.339901, val_recall: 0.216123, val_f1: 0.177355
Epoch: 255, loss: 22.1424, train_acc: 0.3379, train_recall: 0.2294, train_f1: 0.2074, val_acc: 0.339901, val_recall: 0.216123, val_f1: 0.177355
Epoch: 256, loss: 22.1446, train_acc: 0.3379, train_recall: 0.2294, train_f1: 0.2074, val_acc: 0.339901, val_recall: 0.216123, val_f1: 0.177355
Epoch: 257, loss: 22.1520, train_acc: 0.3379, train_recall: 0.2294, train_f1: 0.2074, val_acc: 0.339901, val_recall: 0.216123, val_f1: 0.177380
Epoch: 258, loss: 22.1414, train_acc: 0.3369, train_recall: 0.2237, train_f1: 0.1991, val_acc: 0.339901, val_recall: 0.216123, val_f1: 0.177380
Epoch: 259, loss: 22.1438, train_acc: 0.3379, train_recall: 0.2294, train_f1: 0.2072, val_acc: 0.339901, val_recall: 0.216123, val_f1: 0.177355
Epoch: 260, loss: 22.1633, train_acc: 0.3379, train_recall: 0.2294, train_f1: 0.2072, val_acc: 0.339901, val_recall: 0.216123, val_f1: 0.177355
Epoch: 261, loss: 22.1367, train_acc: 0.3379, train_recall: 0.2294, train_f1: 0.2072, val_acc: 0.339901, val_recall: 0.216123, val_f1: 0.177355
Epoch: 262, loss: 22.1313, train_acc: 0.3379, train_recall: 0.2294, train_f1: 0.2074, val_acc: 0.339901, val_recall: 0.216123, val_f1: 0.177355
Epoch: 263, loss: 22.1522, train_acc: 0.3379, train_recall: 0.2294, train_f1: 0.2072, val_acc: 0.339901, val_recall: 0.216123, val_f1: 0.177355
Epoch: 264, loss: 22.1408, train_acc: 0.3369, train_recall: 0.2237, train_f1: 0.1989, val_acc: 0.339901, val_recall: 0.216123, val_f1: 0.177380
Epoch: 265, loss: 22.1463, train_acc: 0.3411, train_recall: 0.2255, train_f1: 0.1979, val_acc: 0.344828, val_recall: 0.218215, val_f1: 0.175314
Epoch: 266, loss: 22.1393, train_acc: 0.3379, train_recall: 0.2294, train_f1: 0.2072, val_acc: 0.339901, val_recall: 0.216123, val_f1: 0.177355
Epoch: 267, loss: 22.1478, train_acc: 0.3422, train_recall: 0.2312, train_f1: 0.2062, val_acc: 0.344828, val_recall: 0.218215, val_f1: 0.175289
Epoch: 268, loss: 22.1260, train_acc: 0.3379, train_recall: 0.2294, train_f1: 0.2072, val_acc: 0.339901, val_recall: 0.216123, val_f1: 0.177355
Epoch: 269, loss: 22.1300, train_acc: 0.3379, train_recall: 0.2294, train_f1: 0.2072, val_acc: 0.339901, val_recall: 0.216123, val_f1: 0.177355
Epoch: 270, loss: 22.1220, train_acc: 0.3379, train_recall: 0.2294, train_f1: 0.2072, val_acc: 0.339901, val_recall: 0.216123, val_f1: 0.177355
Epoch: 271, loss: 22.1304, train_acc: 0.3369, train_recall: 0.2237, train_f1: 0.1989, val_acc: 0.339901, val_recall: 0.216123, val_f1: 0.177380
/home/ADS/cyang314/ucr_work/HINI_Baseline/GraphLoRA/model/GraphLoRA.py:218: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.
Consider using tensor.detach() first. (Triggered internally at /pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:836.)
  f.write(f'{pre_dataset} to {downstream_dataset}: seed: %d, epoch: %d, train_loss: %f, train_acc: %f, train_recall: %f, train_f1: %f, val_acc: %f, val_recall: %f, val_f1: %f\n' %
Epoch: 272, loss: 22.1030, train_acc: 0.3369, train_recall: 0.2237, train_f1: 0.1989, val_acc: 0.339901, val_recall: 0.216123, val_f1: 0.177380
Epoch: 273, loss: 22.1213, train_acc: 0.3369, train_recall: 0.2237, train_f1: 0.1989, val_acc: 0.334975, val_recall: 0.214123, val_f1: 0.176278
Epoch: 274, loss: 22.1083, train_acc: 0.3379, train_recall: 0.2294, train_f1: 0.2072, val_acc: 0.334975, val_recall: 0.214123, val_f1: 0.176278
Epoch: 275, loss: 22.1087, train_acc: 0.3379, train_recall: 0.2294, train_f1: 0.2072, val_acc: 0.334975, val_recall: 0.214123, val_f1: 0.176278
Epoch: 276, loss: 22.1179, train_acc: 0.3379, train_recall: 0.2294, train_f1: 0.2072, val_acc: 0.334975, val_recall: 0.214123, val_f1: 0.176278
Epoch: 277, loss: 22.1123, train_acc: 0.3379, train_recall: 0.2294, train_f1: 0.2072, val_acc: 0.334975, val_recall: 0.214123, val_f1: 0.176278
Epoch: 278, loss: 22.1062, train_acc: 0.3369, train_recall: 0.2237, train_f1: 0.1989, val_acc: 0.334975, val_recall: 0.214123, val_f1: 0.176304
Epoch: 279, loss: 22.1078, train_acc: 0.3369, train_recall: 0.2237, train_f1: 0.1989, val_acc: 0.334975, val_recall: 0.214123, val_f1: 0.176304
Epoch: 280, loss: 22.0935, train_acc: 0.3411, train_recall: 0.2255, train_f1: 0.1979, val_acc: 0.339901, val_recall: 0.216215, val_f1: 0.174228
Epoch: 281, loss: 22.0940, train_acc: 0.3379, train_recall: 0.2294, train_f1: 0.2072, val_acc: 0.334975, val_recall: 0.214123, val_f1: 0.176278
Epoch: 282, loss: 22.0995, train_acc: 0.3422, train_recall: 0.2312, train_f1: 0.2062, val_acc: 0.339901, val_recall: 0.216215, val_f1: 0.174228
Epoch: 283, loss: 22.1052, train_acc: 0.3379, train_recall: 0.2294, train_f1: 0.2072, val_acc: 0.334975, val_recall: 0.214123, val_f1: 0.176278
Epoch: 284, loss: 22.0732, train_acc: 0.3411, train_recall: 0.2255, train_f1: 0.1979, val_acc: 0.339901, val_recall: 0.216215, val_f1: 0.174253
Epoch: 285, loss: 22.1102, train_acc: 0.3369, train_recall: 0.2237, train_f1: 0.1989, val_acc: 0.334975, val_recall: 0.214123, val_f1: 0.176304
Epoch: 286, loss: 22.0660, train_acc: 0.3400, train_recall: 0.2251, train_f1: 0.1971, val_acc: 0.339901, val_recall: 0.216215, val_f1: 0.174253
Epoch: 287, loss: 22.0842, train_acc: 0.3379, train_recall: 0.2294, train_f1: 0.2078, val_acc: 0.337438, val_recall: 0.215077, val_f1: 0.178080
Epoch: 288, loss: 22.0796, train_acc: 0.3411, train_recall: 0.2308, train_f1: 0.2054, val_acc: 0.339901, val_recall: 0.216215, val_f1: 0.174228
Epoch: 289, loss: 22.0923, train_acc: 0.3326, train_recall: 0.2251, train_f1: 0.2292, val_acc: 0.332512, val_recall: 0.209551, val_f1: 0.200220
Epoch: 290, loss: 22.0709, train_acc: 0.3400, train_recall: 0.2251, train_f1: 0.1971, val_acc: 0.339901, val_recall: 0.216215, val_f1: 0.174253
Epoch: 291, loss: 22.0736, train_acc: 0.3379, train_recall: 0.2209, train_f1: 0.1986, val_acc: 0.359606, val_recall: 0.218764, val_f1: 0.187661
Epoch: 292, loss: 22.0787, train_acc: 0.3400, train_recall: 0.2251, train_f1: 0.1965, val_acc: 0.339901, val_recall: 0.216215, val_f1: 0.174253
Epoch: 293, loss: 22.0743, train_acc: 0.3400, train_recall: 0.2217, train_f1: 0.1990, val_acc: 0.354680, val_recall: 0.216298, val_f1: 0.182140
Epoch: 294, loss: 22.0722, train_acc: 0.3400, train_recall: 0.2304, train_f1: 0.2027, val_acc: 0.334975, val_recall: 0.214306, val_f1: 0.170327
Epoch: 295, loss: 22.0882, train_acc: 0.3453, train_recall: 0.2290, train_f1: 0.2067, val_acc: 0.352217, val_recall: 0.215161, val_f1: 0.175864
Epoch: 296, loss: 22.0710, train_acc: 0.3400, train_recall: 0.2255, train_f1: 0.1951, val_acc: 0.332512, val_recall: 0.213352, val_f1: 0.168344
Epoch: 297, loss: 22.0868, train_acc: 0.3475, train_recall: 0.2289, train_f1: 0.1983, val_acc: 0.347291, val_recall: 0.211762, val_f1: 0.165402
Epoch: 298, loss: 22.1445, train_acc: 0.3358, train_recall: 0.2240, train_f1: 0.1912, val_acc: 0.334975, val_recall: 0.214773, val_f1: 0.170447
Epoch: 299, loss: 22.1745, train_acc: 0.3422, train_recall: 0.2215, train_f1: 0.1849, val_acc: 0.344828, val_recall: 0.210716, val_f1: 0.161267
epoch: 153, train_acc: 0.332627, val_acc: 0.376847, val_recall: 0.246679, val_f1: 0.208399
Running: year=2016 â†’ downstream_year=2017, seed=0
Random seed set to 42

==============================
PRE-TRAINING
==============================
create PreTrain instance...
pre-training...
(T) | Epoch=001, loss=7.8972, this epoch 1.6172, total 1.6172
+++model saved ! 2016.pth
(T) | Epoch=002, loss=7.8960, this epoch 1.5203, total 3.1376
+++model saved ! 2016.pth
(T) | Epoch=003, loss=7.8942, this epoch 1.4825, total 4.6201
+++model saved ! 2016.pth
(T) | Epoch=004, loss=7.8973, this epoch 1.5332, total 6.1532
(T) | Epoch=005, loss=7.8891, this epoch 1.4979, total 7.6511
+++model saved ! 2016.pth
(T) | Epoch=006, loss=7.8855, this epoch 1.4792, total 9.1303
+++model saved ! 2016.pth
(T) | Epoch=007, loss=7.8808, this epoch 1.4307, total 10.5609
+++model saved ! 2016.pth
(T) | Epoch=008, loss=7.8751, this epoch 1.4425, total 12.0034
+++model saved ! 2016.pth
(T) | Epoch=009, loss=7.8690, this epoch 1.5212, total 13.5247
+++model saved ! 2016.pth
(T) | Epoch=010, loss=7.8849, this epoch 1.5542, total 15.0789
(T) | Epoch=011, loss=7.8509, this epoch 1.5251, total 16.6040
+++model saved ! 2016.pth
(T) | Epoch=012, loss=7.8414, this epoch 1.5064, total 18.1104
+++model saved ! 2016.pth
(T) | Epoch=013, loss=7.8309, this epoch 1.4321, total 19.5425
+++model saved ! 2016.pth
(T) | Epoch=014, loss=7.8687, this epoch 1.4372, total 20.9797
(T) | Epoch=015, loss=7.8100, this epoch 1.4640, total 22.4437
+++model saved ! 2016.pth
(T) | Epoch=016, loss=8.9353, this epoch 1.4787, total 23.9223
(T) | Epoch=017, loss=7.7978, this epoch 1.4440, total 25.3664
+++model saved ! 2016.pth
(T) | Epoch=018, loss=7.8477, this epoch 1.4631, total 26.8295
(T) | Epoch=019, loss=8.3957, this epoch 1.5090, total 28.3384
(T) | Epoch=020, loss=7.8094, this epoch 1.4456, total 29.7840
(T) | Epoch=021, loss=8.1407, this epoch 1.5401, total 31.3241
(T) | Epoch=022, loss=8.0648, this epoch 1.4530, total 32.7771
(T) | Epoch=023, loss=7.8291, this epoch 1.5354, total 34.3125
(T) | Epoch=024, loss=7.8650, this epoch 1.4493, total 35.7618
(T) | Epoch=025, loss=7.9818, this epoch 1.4793, total 37.2411
(T) | Epoch=026, loss=7.8394, this epoch 1.5283, total 38.7694
(T) | Epoch=027, loss=7.9440, this epoch 1.4889, total 40.2582
(T) | Epoch=028, loss=7.9133, this epoch 1.4700, total 41.7283
(T) | Epoch=029, loss=7.8532, this epoch 1.4852, total 43.2135
(T) | Epoch=030, loss=7.8385, this epoch 1.4886, total 44.7020
(T) | Epoch=031, loss=7.8355, this epoch 1.4764, total 46.1785
(T) | Epoch=032, loss=7.8339, this epoch 1.4439, total 47.6224
(T) | Epoch=033, loss=7.8334, this epoch 1.4947, total 49.1171
(T) | Epoch=034, loss=7.8813, this epoch 1.4735, total 50.5906
(T) | Epoch=035, loss=7.8675, this epoch 1.4616, total 52.0522
(T) | Epoch=036, loss=7.8694, this epoch 1.5302, total 53.5825
(T) | Epoch=037, loss=7.8295, this epoch 1.4999, total 55.0824
(T) | Epoch=038, loss=7.8461, this epoch 1.4671, total 56.5494
(T) | Epoch=039, loss=7.8733, this epoch 1.5532, total 58.1026
(T) | Epoch=040, loss=7.9009, this epoch 1.5615, total 59.6641
(T) | Epoch=041, loss=7.8444, this epoch 1.4854, total 61.1495
(T) | Epoch=042, loss=7.8238, this epoch 1.4649, total 62.6145
(T) | Epoch=043, loss=7.8633, this epoch 1.4675, total 64.0819
(T) | Epoch=044, loss=7.8205, this epoch 1.4807, total 65.5627
(T) | Epoch=045, loss=7.8970, this epoch 1.4980, total 67.0607
(T) | Epoch=046, loss=7.8833, this epoch 1.4454, total 68.5061
(T) | Epoch=047, loss=7.8183, this epoch 1.4601, total 69.9661
(T) | Epoch=048, loss=7.8181, this epoch 1.4387, total 71.4049
(T) | Epoch=049, loss=7.8869, this epoch 1.4515, total 72.8564
(T) | Epoch=050, loss=7.8172, this epoch 1.4843, total 74.3407
(T) | Epoch=051, loss=7.8158, this epoch 1.4898, total 75.8305
(T) | Epoch=052, loss=7.8338, this epoch 1.4737, total 77.3042
(T) | Epoch=053, loss=7.8124, this epoch 1.4525, total 78.7567
(T) | Epoch=054, loss=7.8297, this epoch 1.5096, total 80.2662
(T) | Epoch=055, loss=7.8500, this epoch 1.4877, total 81.7539
(T) | Epoch=056, loss=7.8050, this epoch 1.4329, total 83.1868
(T) | Epoch=057, loss=7.8455, this epoch 1.4170, total 84.6038
(T) | Epoch=058, loss=7.7608, this epoch 1.4351, total 86.0389
+++model saved ! 2016.pth
(T) | Epoch=059, loss=7.7941, this epoch 1.4403, total 87.4792
(T) | Epoch=060, loss=7.7920, this epoch 1.4815, total 88.9607
(T) | Epoch=061, loss=7.8088, this epoch 1.4923, total 90.4529
(T) | Epoch=062, loss=7.7835, this epoch 1.4681, total 91.9211
(T) | Epoch=063, loss=7.8721, this epoch 1.4518, total 93.3729
(T) | Epoch=064, loss=7.8320, this epoch 1.5062, total 94.8791
(T) | Epoch=065, loss=7.8550, this epoch 1.5031, total 96.3822
(T) | Epoch=066, loss=7.7719, this epoch 1.4665, total 97.8487
(T) | Epoch=067, loss=7.7767, this epoch 1.5160, total 99.3647
(T) | Epoch=068, loss=7.8208, this epoch 1.4512, total 100.8159
(T) | Epoch=069, loss=7.7684, this epoch 1.5019, total 102.3178
(T) | Epoch=070, loss=7.7887, this epoch 1.5120, total 103.8298
(T) | Epoch=071, loss=7.8050, this epoch 1.5474, total 105.3772
(T) | Epoch=072, loss=7.7990, this epoch 1.4498, total 106.8269
(T) | Epoch=073, loss=7.7964, this epoch 1.4541, total 108.2810
(T) | Epoch=074, loss=7.8045, this epoch 1.5258, total 109.8068
(T) | Epoch=075, loss=7.7370, this epoch 1.4893, total 111.2960
+++model saved ! 2016.pth
(T) | Epoch=076, loss=7.7841, this epoch 1.4653, total 112.7613
(T) | Epoch=077, loss=7.7808, this epoch 1.4678, total 114.2290
(T) | Epoch=078, loss=7.7223, this epoch 1.4708, total 115.6999
+++model saved ! 2016.pth
(T) | Epoch=079, loss=7.7388, this epoch 1.4741, total 117.1739
(T) | Epoch=080, loss=7.7078, this epoch 1.4718, total 118.6458
+++model saved ! 2016.pth
(T) | Epoch=081, loss=7.7200, this epoch 1.5133, total 120.1591
(T) | Epoch=082, loss=7.7504, this epoch 1.5224, total 121.6815
(T) | Epoch=083, loss=7.7256, this epoch 1.5200, total 123.2015
(T) | Epoch=084, loss=7.7405, this epoch 1.4810, total 124.6825
(T) | Epoch=085, loss=7.9853, this epoch 1.4979, total 126.1804
(T) | Epoch=086, loss=7.7604, this epoch 1.4846, total 127.6650
(T) | Epoch=087, loss=7.6827, this epoch 1.4704, total 129.1354
+++model saved ! 2016.pth
(T) | Epoch=088, loss=7.6728, this epoch 1.5223, total 130.6577
+++model saved ! 2016.pth
(T) | Epoch=089, loss=7.7032, this epoch 1.4743, total 132.1320
(T) | Epoch=090, loss=7.6639, this epoch 1.4651, total 133.5971
+++model saved ! 2016.pth
(T) | Epoch=091, loss=7.7604, this epoch 1.5134, total 135.1105
(T) | Epoch=092, loss=7.7533, this epoch 1.5159, total 136.6264
(T) | Epoch=093, loss=7.6967, this epoch 1.4788, total 138.1053
(T) | Epoch=094, loss=7.6565, this epoch 1.4498, total 139.5550
+++model saved ! 2016.pth
(T) | Epoch=095, loss=7.7766, this epoch 1.4825, total 141.0375
(T) | Epoch=096, loss=7.7297, this epoch 1.5003, total 142.5378
(T) | Epoch=097, loss=7.6908, this epoch 1.4792, total 144.0170
(T) | Epoch=098, loss=7.7321, this epoch 1.5110, total 145.5280
(T) | Epoch=099, loss=7.6779, this epoch 1.5522, total 147.0802
(T) | Epoch=100, loss=7.6936, this epoch 1.4909, total 148.5711
(T) | Epoch=101, loss=7.7002, this epoch 1.5143, total 150.0854
(T) | Epoch=102, loss=7.6657, this epoch 1.5082, total 151.5937
(T) | Epoch=103, loss=7.6628, this epoch 1.4833, total 153.0770
(T) | Epoch=104, loss=7.6648, this epoch 1.4256, total 154.5026
(T) | Epoch=105, loss=7.8314, this epoch 1.4838, total 155.9864
(T) | Epoch=106, loss=7.6576, this epoch 1.4948, total 157.4812
(T) | Epoch=107, loss=7.9045, this epoch 1.4845, total 158.9658
(T) | Epoch=108, loss=7.6804, this epoch 1.4575, total 160.4233
(T) | Epoch=109, loss=7.6935, this epoch 1.4923, total 161.9156
(T) | Epoch=110, loss=7.6577, this epoch 1.5649, total 163.4805
(T) | Epoch=111, loss=7.6718, this epoch 1.5441, total 165.0246
(T) | Epoch=112, loss=8.1404, this epoch 1.5115, total 166.5361
(T) | Epoch=113, loss=7.6954, this epoch 1.4914, total 168.0275
(T) | Epoch=114, loss=7.7348, this epoch 1.4644, total 169.4919
(T) | Epoch=115, loss=7.6981, this epoch 1.4592, total 170.9511
(T) | Epoch=116, loss=7.9329, this epoch 1.4822, total 172.4333
(T) | Epoch=117, loss=7.9696, this epoch 1.4956, total 173.9289
(T) | Epoch=118, loss=7.7740, this epoch 1.4933, total 175.4223
(T) | Epoch=119, loss=7.7467, this epoch 1.5062, total 176.9285
(T) | Epoch=120, loss=7.8114, this epoch 1.5260, total 178.4545
(T) | Epoch=121, loss=7.7820, this epoch 1.5283, total 179.9829
(T) | Epoch=122, loss=7.8905, this epoch 1.4671, total 181.4499
(T) | Epoch=123, loss=7.8847, this epoch 1.4893, total 182.9392
(T) | Epoch=124, loss=7.7380, this epoch 1.5144, total 184.4536
(T) | Epoch=125, loss=7.7314, this epoch 1.5264, total 185.9800
(T) | Epoch=126, loss=7.7788, this epoch 1.5059, total 187.4859
(T) | Epoch=127, loss=7.7780, this epoch 1.4743, total 188.9602
(T) | Epoch=128, loss=7.7069, this epoch 1.4523, total 190.4125
(T) | Epoch=129, loss=7.6958, this epoch 1.4423, total 191.8548
(T) | Epoch=130, loss=7.6921, this epoch 1.5378, total 193.3925
(T) | Epoch=131, loss=7.7212, this epoch 1.5519, total 194.9444
(T) | Epoch=132, loss=7.6800, this epoch 1.5006, total 196.4450
(T) | Epoch=133, loss=7.6819, this epoch 1.5355, total 197.9805
(T) | Epoch=134, loss=7.7725, this epoch 1.5217, total 199.5021
(T) | Epoch=135, loss=8.0296, this epoch 1.5372, total 201.0393
(T) | Epoch=136, loss=7.7016, this epoch 1.5228, total 202.5621
(T) | Epoch=137, loss=7.7240, this epoch 1.4973, total 204.0594
(T) | Epoch=138, loss=7.6803, this epoch 1.5496, total 205.6090
(T) | Epoch=139, loss=7.6857, this epoch 1.4799, total 207.0889
(T) | Epoch=140, loss=7.7758, this epoch 1.5093, total 208.5982
(T) | Epoch=141, loss=7.6878, this epoch 1.4606, total 210.0588
(T) | Epoch=142, loss=7.6937, this epoch 1.4531, total 211.5119
(T) | Epoch=143, loss=7.6953, this epoch 1.4702, total 212.9821
(T) | Epoch=144, loss=7.6950, this epoch 1.4772, total 214.4593
(T) | Epoch=145, loss=7.6916, this epoch 1.4668, total 215.9261
(T) | Epoch=146, loss=7.8707, this epoch 1.4338, total 217.3598
(T) | Epoch=147, loss=7.9587, this epoch 1.4262, total 218.7860
(T) | Epoch=148, loss=7.7416, this epoch 1.4139, total 220.1999
(T) | Epoch=149, loss=7.6807, this epoch 1.4387, total 221.6386
(T) | Epoch=150, loss=7.6698, this epoch 1.5058, total 223.1444
(T) | Epoch=151, loss=7.7041, this epoch 1.4197, total 224.5641
(T) | Epoch=152, loss=7.6688, this epoch 1.4393, total 226.0035
(T) | Epoch=153, loss=7.6618, this epoch 1.4462, total 227.4497
(T) | Epoch=154, loss=7.7258, this epoch 1.4465, total 228.8962
(T) | Epoch=155, loss=7.6446, this epoch 1.5019, total 230.3981
+++model saved ! 2016.pth
(T) | Epoch=156, loss=7.6350, this epoch 1.5068, total 231.9049
+++model saved ! 2016.pth
(T) | Epoch=157, loss=7.6350, this epoch 1.4913, total 233.3963
+++model saved ! 2016.pth
(T) | Epoch=158, loss=7.6295, this epoch 1.4642, total 234.8605
+++model saved ! 2016.pth
(T) | Epoch=159, loss=7.9806, this epoch 1.4971, total 236.3576
(T) | Epoch=160, loss=7.6389, this epoch 1.4171, total 237.7747
(T) | Epoch=161, loss=7.7209, this epoch 1.4956, total 239.2702
(T) | Epoch=162, loss=7.7459, this epoch 1.5026, total 240.7728
(T) | Epoch=163, loss=7.7678, this epoch 1.5256, total 242.2984
(T) | Epoch=164, loss=7.7021, this epoch 1.5016, total 243.8000
(T) | Epoch=165, loss=7.7692, this epoch 1.4802, total 245.2801
(T) | Epoch=166, loss=7.9191, this epoch 1.4416, total 246.7217
(T) | Epoch=167, loss=7.7395, this epoch 1.4735, total 248.1952
(T) | Epoch=168, loss=7.8386, this epoch 1.4814, total 249.6766
(T) | Epoch=169, loss=7.7457, this epoch 1.5098, total 251.1864
(T) | Epoch=170, loss=7.8090, this epoch 1.4637, total 252.6501
(T) | Epoch=171, loss=7.8330, this epoch 1.5280, total 254.1781
(T) | Epoch=172, loss=7.7389, this epoch 1.5146, total 255.6926
(T) | Epoch=173, loss=7.7918, this epoch 1.4822, total 257.1749
(T) | Epoch=174, loss=7.7982, this epoch 1.4115, total 258.5863
(T) | Epoch=175, loss=7.7083, this epoch 1.4298, total 260.0162
(T) | Epoch=176, loss=7.7061, this epoch 1.4546, total 261.4707
(T) | Epoch=177, loss=7.6974, this epoch 1.4765, total 262.9472
(T) | Epoch=178, loss=7.7521, this epoch 1.4758, total 264.4230
(T) | Epoch=179, loss=7.7281, this epoch 1.4741, total 265.8971
(T) | Epoch=180, loss=7.7283, this epoch 1.4358, total 267.3328
(T) | Epoch=181, loss=7.7021, this epoch 1.4553, total 268.7882
(T) | Epoch=182, loss=7.6751, this epoch 1.4640, total 270.2522
(T) | Epoch=183, loss=7.7060, this epoch 1.4822, total 271.7344
(T) | Epoch=184, loss=7.6700, this epoch 1.4926, total 273.2271
(T) | Epoch=185, loss=7.7197, this epoch 1.5190, total 274.7461
(T) | Epoch=186, loss=7.6522, this epoch 1.5182, total 276.2643
(T) | Epoch=187, loss=7.7096, this epoch 1.5224, total 277.7867
(T) | Epoch=188, loss=7.6425, this epoch 1.4285, total 279.2152
(T) | Epoch=189, loss=7.6440, this epoch 1.4309, total 280.6461
(T) | Epoch=190, loss=7.6702, this epoch 1.4507, total 282.0968
(T) | Epoch=191, loss=7.6335, this epoch 1.4919, total 283.5887
(T) | Epoch=192, loss=7.6225, this epoch 1.5098, total 285.0984
+++model saved ! 2016.pth
(T) | Epoch=193, loss=7.6214, this epoch 1.4620, total 286.5605
+++model saved ! 2016.pth
(T) | Epoch=194, loss=7.6194, this epoch 1.4981, total 288.0586
+++model saved ! 2016.pth
(T) | Epoch=195, loss=7.6500, this epoch 1.4440, total 289.5026
(T) | Epoch=196, loss=7.6092, this epoch 1.4845, total 290.9871
+++model saved ! 2016.pth
(T) | Epoch=197, loss=7.5999, this epoch 1.4833, total 292.4704
+++model saved ! 2016.pth
(T) | Epoch=198, loss=7.6908, this epoch 1.4877, total 293.9581
(T) | Epoch=199, loss=7.6461, this epoch 1.4148, total 295.3729
(T) | Epoch=200, loss=7.6914, this epoch 1.4888, total 296.8617
(T) | Epoch=201, loss=7.7265, this epoch 1.4631, total 298.3248
(T) | Epoch=202, loss=7.6334, this epoch 1.5203, total 299.8451
(T) | Epoch=203, loss=7.6409, this epoch 1.5329, total 301.3780
(T) | Epoch=204, loss=7.6671, this epoch 1.5201, total 302.8982
(T) | Epoch=205, loss=7.6712, this epoch 1.4785, total 304.3767
(T) | Epoch=206, loss=7.6833, this epoch 1.5074, total 305.8841
(T) | Epoch=207, loss=7.7145, this epoch 1.4633, total 307.3474
(T) | Epoch=208, loss=7.6406, this epoch 1.5014, total 308.8487
(T) | Epoch=209, loss=7.6699, this epoch 1.4591, total 310.3078
(T) | Epoch=210, loss=7.6666, this epoch 1.4889, total 311.7967
(T) | Epoch=211, loss=7.6291, this epoch 1.4940, total 313.2907
(T) | Epoch=212, loss=7.7429, this epoch 1.4786, total 314.7693
(T) | Epoch=213, loss=7.6209, this epoch 1.4625, total 316.2318
(T) | Epoch=214, loss=7.6179, this epoch 1.4967, total 317.7285
(T) | Epoch=215, loss=7.6074, this epoch 1.5018, total 319.2304
(T) | Epoch=216, loss=7.7853, this epoch 1.4475, total 320.6779
(T) | Epoch=217, loss=7.6730, this epoch 1.4486, total 322.1265
(T) | Epoch=218, loss=7.6273, this epoch 1.4294, total 323.5559
(T) | Epoch=219, loss=7.6329, this epoch 1.4378, total 324.9937
(T) | Epoch=220, loss=7.6359, this epoch 1.4846, total 326.4783
(T) | Epoch=221, loss=7.7191, this epoch 1.4836, total 327.9619
(T) | Epoch=222, loss=7.6743, this epoch 1.5046, total 329.4665
(T) | Epoch=223, loss=7.6385, this epoch 1.5381, total 331.0046
(T) | Epoch=224, loss=7.6429, this epoch 1.4691, total 332.4737
(T) | Epoch=225, loss=7.6478, this epoch 1.4581, total 333.9317
(T) | Epoch=226, loss=7.6565, this epoch 1.4780, total 335.4097
(T) | Epoch=227, loss=7.7008, this epoch 1.4424, total 336.8521
(T) | Epoch=228, loss=7.6370, this epoch 1.4367, total 338.2888
(T) | Epoch=229, loss=7.6407, this epoch 1.4953, total 339.7841
(T) | Epoch=230, loss=7.6681, this epoch 1.4994, total 341.2835
(T) | Epoch=231, loss=7.6297, this epoch 1.4404, total 342.7239
(T) | Epoch=232, loss=7.6327, this epoch 1.4652, total 344.1891
(T) | Epoch=233, loss=7.6318, this epoch 1.5182, total 345.7073
(T) | Epoch=234, loss=7.6505, this epoch 1.4224, total 347.1297
(T) | Epoch=235, loss=7.6506, this epoch 1.4662, total 348.5959
(T) | Epoch=236, loss=7.6233, this epoch 1.4613, total 350.0572
(T) | Epoch=237, loss=7.6656, this epoch 1.4822, total 351.5394
(T) | Epoch=238, loss=7.6206, this epoch 1.4952, total 353.0347
(T) | Epoch=239, loss=7.6069, this epoch 1.4686, total 354.5032
(T) | Epoch=240, loss=7.6020, this epoch 1.4959, total 355.9991
(T) | Epoch=241, loss=7.6014, this epoch 1.4530, total 357.4521
(T) | Epoch=242, loss=7.5974, this epoch 1.5018, total 358.9539
+++model saved ! 2016.pth
(T) | Epoch=243, loss=7.7122, this epoch 1.4962, total 360.4500
(T) | Epoch=244, loss=7.5825, this epoch 1.4565, total 361.9065
+++model saved ! 2016.pth
(T) | Epoch=245, loss=7.6316, this epoch 1.4553, total 363.3618
(T) | Epoch=246, loss=7.6575, this epoch 1.4154, total 364.7772
(T) | Epoch=247, loss=7.6442, this epoch 1.5167, total 366.2939
(T) | Epoch=248, loss=7.6549, this epoch 1.4063, total 367.7002
(T) | Epoch=249, loss=7.6258, this epoch 1.4822, total 369.1824
(T) | Epoch=250, loss=7.6161, this epoch 1.4268, total 370.6093
(T) | Epoch=251, loss=7.6353, this epoch 1.4956, total 372.1048
(T) | Epoch=252, loss=7.6374, this epoch 1.4459, total 373.5508
(T) | Epoch=253, loss=7.6581, this epoch 1.4400, total 374.9908
(T) | Epoch=254, loss=7.6275, this epoch 1.4654, total 376.4563
(T) | Epoch=255, loss=7.6231, this epoch 1.4912, total 377.9474
(T) | Epoch=256, loss=7.6265, this epoch 1.4703, total 379.4178
(T) | Epoch=257, loss=7.6247, this epoch 1.5099, total 380.9277
(T) | Epoch=258, loss=7.6480, this epoch 1.4377, total 382.3654
(T) | Epoch=259, loss=7.6726, this epoch 1.4937, total 383.8591
(T) | Epoch=260, loss=7.7362, this epoch 1.4942, total 385.3533
(T) | Epoch=261, loss=7.6112, this epoch 1.4915, total 386.8449
(T) | Epoch=262, loss=7.5926, this epoch 1.4895, total 388.3344
(T) | Epoch=263, loss=7.6275, this epoch 1.4721, total 389.8065
(T) | Epoch=264, loss=7.5969, this epoch 1.4315, total 391.2380
(T) | Epoch=265, loss=7.5855, this epoch 1.5698, total 392.8079
(T) | Epoch=266, loss=7.5856, this epoch 1.5290, total 394.3368
(T) | Epoch=267, loss=7.5699, this epoch 1.4912, total 395.8281
+++model saved ! 2016.pth
(T) | Epoch=268, loss=7.6041, this epoch 1.4443, total 397.2724
(T) | Epoch=269, loss=7.5493, this epoch 1.4636, total 398.7360
+++model saved ! 2016.pth
(T) | Epoch=270, loss=7.5517, this epoch 1.4751, total 400.2111
(T) | Epoch=271, loss=7.5466, this epoch 1.4273, total 401.6384
+++model saved ! 2016.pth
(T) | Epoch=272, loss=8.0264, this epoch 1.4718, total 403.1102
(T) | Epoch=273, loss=7.6125, this epoch 1.4752, total 404.5854
(T) | Epoch=274, loss=7.6927, this epoch 1.4764, total 406.0619
(T) | Epoch=275, loss=7.7581, this epoch 1.5129, total 407.5748
(T) | Epoch=276, loss=7.6928, this epoch 1.5116, total 409.0864
(T) | Epoch=277, loss=7.7092, this epoch 1.5060, total 410.5924
(T) | Epoch=278, loss=7.7423, this epoch 1.4933, total 412.0858
(T) | Epoch=279, loss=7.7906, this epoch 1.4958, total 413.5815
(T) | Epoch=280, loss=7.9144, this epoch 1.4798, total 415.0614
(T) | Epoch=281, loss=7.8076, this epoch 1.4934, total 416.5547
(T) | Epoch=282, loss=7.8167, this epoch 1.5260, total 418.0808
(T) | Epoch=283, loss=7.7798, this epoch 1.4780, total 419.5588
(T) | Epoch=284, loss=7.8240, this epoch 1.4924, total 421.0512
(T) | Epoch=285, loss=7.7826, this epoch 1.4486, total 422.4998
(T) | Epoch=286, loss=7.9037, this epoch 1.4438, total 423.9436
(T) | Epoch=287, loss=7.9312, this epoch 1.4833, total 425.4269
(T) | Epoch=288, loss=7.9051, this epoch 1.4455, total 426.8724
(T) | Epoch=289, loss=7.9388, this epoch 1.4925, total 428.3649
(T) | Epoch=290, loss=7.8462, this epoch 1.4738, total 429.8387
(T) | Epoch=291, loss=7.7874, this epoch 1.4480, total 431.2867
(T) | Epoch=292, loss=7.7891, this epoch 1.4656, total 432.7523
(T) | Epoch=293, loss=7.9099, this epoch 1.4322, total 434.1845
(T) | Epoch=294, loss=7.7901, this epoch 1.4556, total 435.6400
(T) | Epoch=295, loss=7.7890, this epoch 1.4663, total 437.1064
(T) | Epoch=296, loss=7.7900, this epoch 1.5000, total 438.6063
(T) | Epoch=297, loss=7.8543, this epoch 1.5151, total 440.1214
(T) | Epoch=298, loss=8.0895, this epoch 1.4965, total 441.6179
(T) | Epoch=299, loss=7.7896, this epoch 1.4889, total 443.1068
(T) | Epoch=300, loss=7.9353, this epoch 1.5475, total 444.6543
(T) | Epoch=301, loss=7.7884, this epoch 1.4690, total 446.1233
(T) | Epoch=302, loss=7.9216, this epoch 1.4345, total 447.5578
(T) | Epoch=303, loss=7.7897, this epoch 1.4392, total 448.9970
(T) | Epoch=304, loss=7.7893, this epoch 1.4714, total 450.4684
(T) | Epoch=305, loss=7.7887, this epoch 1.5049, total 451.9733
(T) | Epoch=306, loss=7.9136, this epoch 1.4651, total 453.4384
(T) | Epoch=307, loss=7.9213, this epoch 1.4597, total 454.8980
(T) | Epoch=308, loss=7.7882, this epoch 1.5112, total 456.4092
(T) | Epoch=309, loss=7.8438, this epoch 1.4984, total 457.9076
(T) | Epoch=310, loss=7.7876, this epoch 1.4752, total 459.3828
(T) | Epoch=311, loss=7.7879, this epoch 1.5186, total 460.9014
(T) | Epoch=312, loss=7.7863, this epoch 1.4618, total 462.3632
(T) | Epoch=313, loss=7.7874, this epoch 1.4430, total 463.8062
(T) | Epoch=314, loss=7.8393, this epoch 1.4454, total 465.2516
(T) | Epoch=315, loss=7.8384, this epoch 1.5035, total 466.7551
(T) | Epoch=316, loss=7.7841, this epoch 1.4178, total 468.1729
(T) | Epoch=317, loss=7.8805, this epoch 1.4710, total 469.6439
(T) | Epoch=318, loss=7.7858, this epoch 1.4619, total 471.1059
(T) | Epoch=319, loss=7.7838, this epoch 1.5007, total 472.6066
(T) | Epoch=320, loss=7.7820, this epoch 1.5487, total 474.1553
(T) | Epoch=321, loss=7.8743, this epoch 1.5358, total 475.6911
(T) | Epoch=322, loss=7.8903, this epoch 1.4748, total 477.1660
(T) | Epoch=323, loss=7.8891, this epoch 1.4553, total 478.6212
(T) | Epoch=324, loss=7.7777, this epoch 1.5350, total 480.1562
(T) | Epoch=325, loss=7.8938, this epoch 1.4885, total 481.6447
(T) | Epoch=326, loss=7.7803, this epoch 1.4779, total 483.1226
(T) | Epoch=327, loss=7.7766, this epoch 1.5126, total 484.6352
(T) | Epoch=328, loss=7.7773, this epoch 1.4748, total 486.1100
(T) | Epoch=329, loss=7.7776, this epoch 1.5236, total 487.6336
(T) | Epoch=330, loss=7.8904, this epoch 1.4898, total 489.1234
(T) | Epoch=331, loss=7.8145, this epoch 1.4498, total 490.5731
(T) | Epoch=332, loss=7.7720, this epoch 1.4602, total 492.0333
(T) | Epoch=333, loss=7.8152, this epoch 1.4939, total 493.5272
(T) | Epoch=334, loss=7.7734, this epoch 1.4681, total 494.9953
(T) | Epoch=335, loss=7.7695, this epoch 1.4231, total 496.4184
(T) | Epoch=336, loss=7.7696, this epoch 1.4696, total 497.8880
(T) | Epoch=337, loss=7.8588, this epoch 1.4731, total 499.3611
(T) | Epoch=338, loss=7.8664, this epoch 1.4708, total 500.8319
(T) | Epoch=339, loss=7.8717, this epoch 1.4853, total 502.3172
(T) | Epoch=340, loss=7.8483, this epoch 1.4990, total 503.8162
(T) | Epoch=341, loss=7.7959, this epoch 1.4500, total 505.2662
(T) | Epoch=342, loss=7.7556, this epoch 1.5625, total 506.8287
(T) | Epoch=343, loss=7.9333, this epoch 1.4822, total 508.3109
(T) | Epoch=344, loss=7.8531, this epoch 1.4908, total 509.8017
(T) | Epoch=345, loss=7.9464, this epoch 1.4447, total 511.2465
(T) | Epoch=346, loss=7.7494, this epoch 1.4802, total 512.7267
(T) | Epoch=347, loss=7.8198, this epoch 1.4311, total 514.1578
(T) | Epoch=348, loss=7.8303, this epoch 1.4350, total 515.5928
(T) | Epoch=349, loss=7.8083, this epoch 1.5304, total 517.1231
(T) | Epoch=350, loss=7.7799, this epoch 1.5401, total 518.6633
(T) | Epoch=351, loss=7.7349, this epoch 1.5607, total 520.2240
(T) | Epoch=352, loss=7.8048, this epoch 1.5935, total 521.8174
(T) | Epoch=353, loss=7.7255, this epoch 1.5152, total 523.3326
(T) | Epoch=354, loss=7.7234, this epoch 1.5220, total 524.8547
(T) | Epoch=355, loss=7.7990, this epoch 1.4876, total 526.3423
(T) | Epoch=356, loss=7.7167, this epoch 1.5195, total 527.8617
(T) | Epoch=357, loss=7.7518, this epoch 1.4812, total 529.3429
(T) | Epoch=358, loss=7.7096, this epoch 1.5216, total 530.8645
(T) | Epoch=359, loss=7.7178, this epoch 1.4725, total 532.3370
(T) | Epoch=360, loss=7.7386, this epoch 1.4966, total 533.8336
(T) | Epoch=361, loss=7.6995, this epoch 1.4902, total 535.3238
(T) | Epoch=362, loss=7.7580, this epoch 1.5330, total 536.8568
(T) | Epoch=363, loss=7.7012, this epoch 1.4609, total 538.3177
(T) | Epoch=364, loss=7.7008, this epoch 1.5074, total 539.8251
(T) | Epoch=365, loss=7.7237, this epoch 1.5050, total 541.3301
(T) | Epoch=366, loss=7.6910, this epoch 1.5293, total 542.8594
(T) | Epoch=367, loss=7.7336, this epoch 1.4334, total 544.2928
(T) | Epoch=368, loss=7.6818, this epoch 1.4635, total 545.7563
(T) | Epoch=369, loss=7.8351, this epoch 1.4992, total 547.2555
(T) | Epoch=370, loss=7.7580, this epoch 1.4784, total 548.7339
(T) | Epoch=371, loss=7.6759, this epoch 1.4473, total 550.1812
(T) | Epoch=372, loss=7.6740, this epoch 1.4976, total 551.6789
(T) | Epoch=373, loss=7.6948, this epoch 1.4991, total 553.1780
(T) | Epoch=374, loss=7.6699, this epoch 1.5157, total 554.6937
(T) | Epoch=375, loss=7.6610, this epoch 1.4487, total 556.1424
(T) | Epoch=376, loss=7.8949, this epoch 1.4231, total 557.5654
(T) | Epoch=377, loss=7.6607, this epoch 1.4627, total 559.0281
(T) | Epoch=378, loss=7.6540, this epoch 1.5254, total 560.5535
(T) | Epoch=379, loss=7.6588, this epoch 1.5106, total 562.0641
(T) | Epoch=380, loss=7.6600, this epoch 1.4448, total 563.5090
(T) | Epoch=381, loss=7.6927, this epoch 1.4605, total 564.9695
(T) | Epoch=382, loss=7.6894, this epoch 1.4369, total 566.4064
(T) | Epoch=383, loss=7.6950, this epoch 1.4388, total 567.8452
(T) | Epoch=384, loss=7.6705, this epoch 1.4346, total 569.2798
(T) | Epoch=385, loss=7.6422, this epoch 1.4492, total 570.7290
(T) | Epoch=386, loss=7.6419, this epoch 1.4631, total 572.1921
(T) | Epoch=387, loss=7.6368, this epoch 1.4363, total 573.6284
(T) | Epoch=388, loss=7.6347, this epoch 1.4878, total 575.1162
(T) | Epoch=389, loss=7.6315, this epoch 1.5743, total 576.6905
(T) | Epoch=390, loss=7.6342, this epoch 1.5523, total 578.2428
(T) | Epoch=391, loss=7.6326, this epoch 1.5094, total 579.7522
(T) | Epoch=392, loss=7.6625, this epoch 1.4506, total 581.2028
(T) | Epoch=393, loss=7.6289, this epoch 1.4449, total 582.6478
(T) | Epoch=394, loss=7.6257, this epoch 1.4604, total 584.1081
(T) | Epoch=395, loss=7.6796, this epoch 1.4598, total 585.5680
(T) | Epoch=396, loss=7.6172, this epoch 1.4360, total 587.0040
(T) | Epoch=397, loss=7.6148, this epoch 1.4315, total 588.4355
(T) | Epoch=398, loss=7.6550, this epoch 1.4595, total 589.8949
(T) | Epoch=399, loss=7.6177, this epoch 1.5034, total 591.3983
(T) | Epoch=400, loss=7.6535, this epoch 1.5068, total 592.9051
(T) | Epoch=401, loss=7.6070, this epoch 1.5205, total 594.4256
(T) | Epoch=402, loss=7.6853, this epoch 1.4934, total 595.9190
(T) | Epoch=403, loss=7.7738, this epoch 1.5003, total 597.4193
(T) | Epoch=404, loss=7.6505, this epoch 1.4400, total 598.8593
(T) | Epoch=405, loss=7.6475, this epoch 1.4799, total 600.3392
(T) | Epoch=406, loss=7.6837, this epoch 1.4883, total 601.8275
(T) | Epoch=407, loss=7.6217, this epoch 1.4859, total 603.3134
(T) | Epoch=408, loss=7.6186, this epoch 1.4889, total 604.8023
(T) | Epoch=409, loss=7.6610, this epoch 1.4723, total 606.2746
(T) | Epoch=410, loss=7.6358, this epoch 1.4947, total 607.7693
(T) | Epoch=411, loss=7.6181, this epoch 1.5102, total 609.2795
(T) | Epoch=412, loss=7.6446, this epoch 1.4327, total 610.7122
(T) | Epoch=413, loss=7.6595, this epoch 1.4424, total 612.1546
(T) | Epoch=414, loss=7.6464, this epoch 1.4413, total 613.5959
(T) | Epoch=415, loss=7.6406, this epoch 1.5390, total 615.1349
(T) | Epoch=416, loss=7.6049, this epoch 1.4825, total 616.6174
(T) | Epoch=417, loss=7.6072, this epoch 1.4317, total 618.0491
(T) | Epoch=418, loss=7.6069, this epoch 1.4149, total 619.4639
(T) | Epoch=419, loss=7.6294, this epoch 1.4465, total 620.9105
(T) | Epoch=420, loss=7.5997, this epoch 1.4265, total 622.3370
(T) | Epoch=421, loss=7.6541, this epoch 1.4963, total 623.8333
(T) | Epoch=422, loss=7.6473, this epoch 1.4487, total 625.2820
(T) | Epoch=423, loss=7.6440, this epoch 1.4768, total 626.7588
(T) | Epoch=424, loss=7.6294, this epoch 1.4451, total 628.2039
(T) | Epoch=425, loss=7.5835, this epoch 1.4785, total 629.6824
(T) | Epoch=426, loss=7.6154, this epoch 1.4426, total 631.1250
(T) | Epoch=427, loss=7.5951, this epoch 1.4512, total 632.5762
(T) | Epoch=428, loss=7.6772, this epoch 1.5649, total 634.1410
(T) | Epoch=429, loss=7.5929, this epoch 1.5398, total 635.6808
(T) | Epoch=430, loss=7.5881, this epoch 1.4816, total 637.1624
(T) | Epoch=431, loss=7.5790, this epoch 1.5407, total 638.7031
(T) | Epoch=432, loss=7.6305, this epoch 1.5307, total 640.2339
(T) | Epoch=433, loss=7.5647, this epoch 1.5251, total 641.7589
(T) | Epoch=434, loss=7.5661, this epoch 1.5196, total 643.2785
(T) | Epoch=435, loss=8.8555, this epoch 1.5129, total 644.7914
(T) | Epoch=436, loss=7.5970, this epoch 1.4649, total 646.2563
(T) | Epoch=437, loss=7.6141, this epoch 1.4891, total 647.7454
(T) | Epoch=438, loss=7.6272, this epoch 1.4885, total 649.2339
(T) | Epoch=439, loss=7.6476, this epoch 1.5198, total 650.7537
(T) | Epoch=440, loss=7.6558, this epoch 1.5720, total 652.3257
(T) | Epoch=441, loss=7.6616, this epoch 1.6182, total 653.9439
(T) | Epoch=442, loss=7.7359, this epoch 1.5005, total 655.4444
(T) | Epoch=443, loss=7.6688, this epoch 1.5210, total 656.9653
(T) | Epoch=444, loss=7.7759, this epoch 1.5014, total 658.4667
(T) | Epoch=445, loss=7.6888, this epoch 1.4655, total 659.9323
(T) | Epoch=446, loss=7.6943, this epoch 1.4831, total 661.4154
(T) | Epoch=447, loss=7.6920, this epoch 1.5017, total 662.9171
(T) | Epoch=448, loss=7.7044, this epoch 1.5376, total 664.4547
(T) | Epoch=449, loss=7.7629, this epoch 1.4586, total 665.9133
(T) | Epoch=450, loss=7.7834, this epoch 1.5406, total 667.4538
(T) | Epoch=451, loss=7.7058, this epoch 1.4852, total 668.9391
(T) | Epoch=452, loss=7.7130, this epoch 1.4659, total 670.4050
(T) | Epoch=453, loss=7.7006, this epoch 1.5288, total 671.9338
(T) | Epoch=454, loss=7.8071, this epoch 1.4788, total 673.4125
(T) | Epoch=455, loss=7.7116, this epoch 1.4621, total 674.8746
(T) | Epoch=456, loss=7.6989, this epoch 1.4980, total 676.3726
(T) | Epoch=457, loss=7.7952, this epoch 1.5123, total 677.8849
(T) | Epoch=458, loss=7.7380, this epoch 1.5123, total 679.3972
(T) | Epoch=459, loss=7.6934, this epoch 1.4650, total 680.8622
(T) | Epoch=460, loss=7.7487, this epoch 1.4627, total 682.3250
(T) | Epoch=461, loss=7.6262, this epoch 1.4955, total 683.8204
(T) | Epoch=462, loss=7.6856, this epoch 1.4964, total 685.3169
(T) | Epoch=463, loss=7.7327, this epoch 1.4675, total 686.7843
(T) | Epoch=464, loss=7.7354, this epoch 1.4688, total 688.2531
(T) | Epoch=465, loss=7.6769, this epoch 1.4504, total 689.7034
(T) | Epoch=466, loss=7.6861, this epoch 1.4417, total 691.1451
(T) | Epoch=467, loss=7.6730, this epoch 1.4307, total 692.5758
(T) | Epoch=468, loss=7.7102, this epoch 1.4662, total 694.0420
(T) | Epoch=469, loss=7.7181, this epoch 1.4562, total 695.4983
(T) | Epoch=470, loss=7.6789, this epoch 1.4703, total 696.9686
(T) | Epoch=471, loss=7.6631, this epoch 1.5220, total 698.4906
(T) | Epoch=472, loss=7.6953, this epoch 1.5131, total 700.0036
(T) | Epoch=473, loss=7.6945, this epoch 1.4487, total 701.4523
(T) | Epoch=474, loss=7.6669, this epoch 1.4677, total 702.9200
(T) | Epoch=475, loss=7.7069, this epoch 1.4887, total 704.4087
(T) | Epoch=476, loss=7.6591, this epoch 1.5027, total 705.9114
(T) | Epoch=477, loss=7.8788, this epoch 1.4885, total 707.3999
(T) | Epoch=478, loss=7.6518, this epoch 1.4725, total 708.8725
(T) | Epoch=479, loss=7.6897, this epoch 1.4703, total 710.3428
(T) | Epoch=480, loss=7.6492, this epoch 1.5202, total 711.8630
(T) | Epoch=481, loss=7.6469, this epoch 1.5242, total 713.3872
(T) | Epoch=482, loss=7.6953, this epoch 1.5041, total 714.8913
(T) | Epoch=483, loss=7.6424, this epoch 1.4711, total 716.3624
(T) | Epoch=484, loss=7.6413, this epoch 1.5229, total 717.8853
(T) | Epoch=485, loss=7.6418, this epoch 1.5110, total 719.3963
(T) | Epoch=486, loss=7.6354, this epoch 1.4884, total 720.8847
(T) | Epoch=487, loss=7.6761, this epoch 1.4668, total 722.3516
(T) | Epoch=488, loss=7.6678, this epoch 1.5090, total 723.8606
(T) | Epoch=489, loss=7.6692, this epoch 1.4697, total 725.3303
(T) | Epoch=490, loss=7.6350, this epoch 1.5337, total 726.8640
(T) | Epoch=491, loss=7.7432, this epoch 1.5608, total 728.4248
(T) | Epoch=492, loss=7.6367, this epoch 1.5255, total 729.9503
(T) | Epoch=493, loss=7.6636, this epoch 1.4815, total 731.4318
(T) | Epoch=494, loss=7.6340, this epoch 1.5196, total 732.9513
(T) | Epoch=495, loss=7.7777, this epoch 1.4918, total 734.4431
(T) | Epoch=496, loss=7.6382, this epoch 1.4970, total 735.9401
(T) | Epoch=497, loss=7.6332, this epoch 1.5484, total 737.4885
(T) | Epoch=498, loss=7.6808, this epoch 1.5657, total 739.0542
(T) | Epoch=499, loss=7.6218, this epoch 1.5366, total 740.5908
(T) | Epoch=500, loss=7.6256, this epoch 1.5113, total 742.1021
(T) | Epoch=501, loss=7.6146, this epoch 1.4850, total 743.5871
(T) | Epoch=502, loss=7.6180, this epoch 1.4901, total 745.0772
(T) | Epoch=503, loss=7.6192, this epoch 1.4644, total 746.5416
(T) | Epoch=504, loss=7.6568, this epoch 1.4417, total 747.9833
(T) | Epoch=505, loss=7.6222, this epoch 1.4901, total 749.4734
(T) | Epoch=506, loss=7.6270, this epoch 1.4106, total 750.8839
(T) | Epoch=507, loss=7.7611, this epoch 1.4297, total 752.3137
(T) | Epoch=508, loss=7.6683, this epoch 1.5653, total 753.8790
(T) | Epoch=509, loss=7.6216, this epoch 1.5230, total 755.4020
(T) | Epoch=510, loss=7.6057, this epoch 1.4587, total 756.8606
(T) | Epoch=511, loss=7.5973, this epoch 1.5390, total 758.3996
(T) | Epoch=512, loss=7.6436, this epoch 1.5394, total 759.9390
(T) | Epoch=513, loss=7.6028, this epoch 1.5195, total 761.4585
(T) | Epoch=514, loss=7.6421, this epoch 1.4458, total 762.9043
(T) | Epoch=515, loss=7.6110, this epoch 1.4539, total 764.3582
(T) | Epoch=516, loss=7.5971, this epoch 1.4583, total 765.8165
(T) | Epoch=517, loss=7.5968, this epoch 1.4864, total 767.3029
(T) | Epoch=518, loss=7.5976, this epoch 1.5261, total 768.8289
(T) | Epoch=519, loss=7.6575, this epoch 1.4450, total 770.2739
(T) | Epoch=520, loss=7.5918, this epoch 1.4695, total 771.7434
(T) | Epoch=521, loss=7.8254, this epoch 1.4456, total 773.1889
(T) | Epoch=522, loss=7.5873, this epoch 1.4380, total 774.6269
(T) | Epoch=523, loss=7.6075, this epoch 1.4642, total 776.0912
(T) | Epoch=524, loss=7.5892, this epoch 1.4460, total 777.5371
(T) | Epoch=525, loss=7.5842, this epoch 1.4703, total 779.0075
(T) | Epoch=526, loss=7.5914, this epoch 1.4872, total 780.4947
(T) | Epoch=527, loss=7.5829, this epoch 1.4978, total 781.9925
(T) | Epoch=528, loss=7.6304, this epoch 1.4776, total 783.4700
(T) | Epoch=529, loss=7.6188, this epoch 1.4738, total 784.9438
(T) | Epoch=530, loss=7.5749, this epoch 1.5406, total 786.4844
(T) | Epoch=531, loss=7.5748, this epoch 1.4872, total 787.9717
(T) | Epoch=532, loss=7.5735, this epoch 1.5346, total 789.5062
(T) | Epoch=533, loss=7.5773, this epoch 1.5065, total 791.0128
(T) | Epoch=534, loss=7.5610, this epoch 1.5357, total 792.5485
(T) | Epoch=535, loss=7.5635, this epoch 1.5424, total 794.0908
(T) | Epoch=536, loss=7.5694, this epoch 1.5560, total 795.6468
(T) | Epoch=537, loss=7.5600, this epoch 1.5148, total 797.1617
(T) | Epoch=538, loss=7.5680, this epoch 1.5472, total 798.7089
(T) | Epoch=539, loss=7.6645, this epoch 1.5598, total 800.2686
(T) | Epoch=540, loss=7.5580, this epoch 1.5631, total 801.8318
(T) | Epoch=541, loss=7.6246, this epoch 1.5356, total 803.3674
(T) | Epoch=542, loss=7.6086, this epoch 1.5653, total 804.9327
(T) | Epoch=543, loss=7.5702, this epoch 1.4869, total 806.4196
(T) | Epoch=544, loss=7.5940, this epoch 1.5394, total 807.9590
(T) | Epoch=545, loss=7.6040, this epoch 1.5091, total 809.4681
(T) | Epoch=546, loss=7.6244, this epoch 1.5402, total 811.0082
(T) | Epoch=547, loss=7.5849, this epoch 1.5303, total 812.5386
(T) | Epoch=548, loss=7.6130, this epoch 1.4859, total 814.0245
(T) | Epoch=549, loss=7.5738, this epoch 1.5205, total 815.5450
(T) | Epoch=550, loss=7.5826, this epoch 1.5331, total 817.0781
(T) | Epoch=551, loss=7.6111, this epoch 1.5036, total 818.5817
(T) | Epoch=552, loss=7.5868, this epoch 1.4929, total 820.0746
(T) | Epoch=553, loss=7.5866, this epoch 1.4346, total 821.5092
(T) | Epoch=554, loss=7.5646, this epoch 1.4690, total 822.9782
(T) | Epoch=555, loss=7.5692, this epoch 1.4923, total 824.4705
(T) | Epoch=556, loss=7.6203, this epoch 1.5031, total 825.9736
(T) | Epoch=557, loss=7.5584, this epoch 1.4856, total 827.4592
(T) | Epoch=558, loss=7.5464, this epoch 1.4931, total 828.9522
+++model saved ! 2016.pth
(T) | Epoch=559, loss=7.5851, this epoch 1.4561, total 830.4084
(T) | Epoch=560, loss=7.6410, this epoch 1.4697, total 831.8781
(T) | Epoch=561, loss=7.6574, this epoch 1.4898, total 833.3679
(T) | Epoch=562, loss=7.6625, this epoch 1.5125, total 834.8804
(T) | Epoch=563, loss=7.5632, this epoch 1.5333, total 836.4137
(T) | Epoch=564, loss=7.5562, this epoch 1.5102, total 837.9239
(T) | Epoch=565, loss=7.5854, this epoch 1.5208, total 839.4447
(T) | Epoch=566, loss=7.6233, this epoch 1.4784, total 840.9231
(T) | Epoch=567, loss=7.6616, this epoch 1.5589, total 842.4820
(T) | Epoch=568, loss=7.6135, this epoch 1.5052, total 843.9872
(T) | Epoch=569, loss=7.5859, this epoch 1.5264, total 845.5136
(T) | Epoch=570, loss=7.6316, this epoch 1.5096, total 847.0232
(T) | Epoch=571, loss=7.6261, this epoch 1.5204, total 848.5436
(T) | Epoch=572, loss=7.5836, this epoch 1.4758, total 850.0194
(T) | Epoch=573, loss=7.5971, this epoch 1.4579, total 851.4773
(T) | Epoch=574, loss=7.5732, this epoch 1.4513, total 852.9286
(T) | Epoch=575, loss=7.6232, this epoch 1.5089, total 854.4375
(T) | Epoch=576, loss=7.5735, this epoch 1.4909, total 855.9284
(T) | Epoch=577, loss=7.6003, this epoch 1.4862, total 857.4145
(T) | Epoch=578, loss=7.5478, this epoch 1.5009, total 858.9155
(T) | Epoch=579, loss=7.5747, this epoch 1.4679, total 860.3834
(T) | Epoch=580, loss=7.5865, this epoch 1.4718, total 861.8553
(T) | Epoch=581, loss=7.5419, this epoch 1.5257, total 863.3810
+++model saved ! 2016.pth
(T) | Epoch=582, loss=7.5377, this epoch 1.5526, total 864.9336
+++model saved ! 2016.pth
(T) | Epoch=583, loss=7.5394, this epoch 1.5528, total 866.4864
(T) | Epoch=584, loss=7.5436, this epoch 1.4965, total 867.9829
(T) | Epoch=585, loss=7.6660, this epoch 1.5055, total 869.4884
(T) | Epoch=586, loss=7.6943, this epoch 1.5122, total 871.0006
(T) | Epoch=587, loss=7.5414, this epoch 1.4779, total 872.4785
(T) | Epoch=588, loss=7.5496, this epoch 1.5234, total 874.0019
(T) | Epoch=589, loss=7.6015, this epoch 1.5161, total 875.5180
(T) | Epoch=590, loss=7.5909, this epoch 1.5258, total 877.0438
(T) | Epoch=591, loss=7.5737, this epoch 1.5087, total 878.5525
(T) | Epoch=592, loss=7.5666, this epoch 1.4902, total 880.0427
(T) | Epoch=593, loss=7.6187, this epoch 1.5137, total 881.5564
(T) | Epoch=594, loss=7.5772, this epoch 1.4323, total 882.9887
(T) | Epoch=595, loss=7.5701, this epoch 1.4471, total 884.4358
(T) | Epoch=596, loss=7.5685, this epoch 1.4930, total 885.9288
(T) | Epoch=597, loss=7.7428, this epoch 1.4752, total 887.4039
(T) | Epoch=598, loss=7.6816, this epoch 1.4846, total 888.8885
(T) | Epoch=599, loss=7.5591, this epoch 1.5022, total 890.3907
(T) | Epoch=600, loss=7.5543, this epoch 1.5350, total 891.9257
(T) | Epoch=601, loss=7.5813, this epoch 1.4962, total 893.4219
(T) | Epoch=602, loss=7.5786, this epoch 1.4690, total 894.8909
(T) | Epoch=603, loss=7.5394, this epoch 1.4714, total 896.3623
(T) | Epoch=604, loss=7.5837, this epoch 1.4634, total 897.8257
(T) | Epoch=605, loss=7.6497, this epoch 1.4822, total 899.3080
(T) | Epoch=606, loss=7.6468, this epoch 1.5232, total 900.8312
(T) | Epoch=607, loss=7.5772, this epoch 1.4671, total 902.2983
(T) | Epoch=608, loss=7.5956, this epoch 1.4504, total 903.7487
(T) | Epoch=609, loss=7.5851, this epoch 1.5006, total 905.2493
(T) | Epoch=610, loss=7.5447, this epoch 1.4906, total 906.7399
(T) | Epoch=611, loss=7.5901, this epoch 1.4211, total 908.1610
(T) | Epoch=612, loss=7.5597, this epoch 1.5101, total 909.6711
(T) | Epoch=613, loss=7.5652, this epoch 1.5222, total 911.1933
(T) | Epoch=614, loss=7.6075, this epoch 1.4356, total 912.6289
(T) | Epoch=615, loss=7.5424, this epoch 1.4875, total 914.1163
(T) | Epoch=616, loss=7.5880, this epoch 1.5284, total 915.6448
(T) | Epoch=617, loss=7.5568, this epoch 1.4714, total 917.1162
(T) | Epoch=618, loss=7.9246, this epoch 1.4710, total 918.5872
(T) | Epoch=619, loss=7.5886, this epoch 1.4609, total 920.0482
(T) | Epoch=620, loss=7.5779, this epoch 1.5300, total 921.5781
(T) | Epoch=621, loss=7.6063, this epoch 1.5052, total 923.0833
(T) | Epoch=622, loss=7.7040, this epoch 1.5065, total 924.5898
(T) | Epoch=623, loss=7.6573, this epoch 1.5502, total 926.1400
(T) | Epoch=624, loss=7.6209, this epoch 1.5287, total 927.6687
(T) | Epoch=625, loss=7.7462, this epoch 1.5090, total 929.1777
(T) | Epoch=626, loss=7.7053, this epoch 1.4956, total 930.6733
(T) | Epoch=627, loss=7.7241, this epoch 1.4995, total 932.1728
(T) | Epoch=628, loss=7.6715, this epoch 1.4903, total 933.6631
(T) | Epoch=629, loss=7.7613, this epoch 1.5277, total 935.1909
(T) | Epoch=630, loss=7.6643, this epoch 1.4953, total 936.6862
(T) | Epoch=631, loss=7.6158, this epoch 1.4957, total 938.1819
(T) | Epoch=632, loss=7.6885, this epoch 1.4816, total 939.6635
(T) | Epoch=633, loss=7.7158, this epoch 1.4947, total 941.1581
(T) | Epoch=634, loss=7.6617, this epoch 1.5548, total 942.7129
(T) | Epoch=635, loss=7.6372, this epoch 1.5468, total 944.2597
(T) | Epoch=636, loss=7.5970, this epoch 1.4586, total 945.7183
(T) | Epoch=637, loss=7.5732, this epoch 1.4576, total 947.1759
(T) | Epoch=638, loss=7.5922, this epoch 1.4334, total 948.6094
(T) | Epoch=639, loss=7.5744, this epoch 1.4836, total 950.0929
(T) | Epoch=640, loss=7.5679, this epoch 1.4948, total 951.5877
(T) | Epoch=641, loss=7.5585, this epoch 1.4798, total 953.0675
(T) | Epoch=642, loss=7.5652, this epoch 1.4403, total 954.5078
(T) | Epoch=643, loss=7.5579, this epoch 1.4598, total 955.9675
(T) | Epoch=644, loss=7.5418, this epoch 1.4593, total 957.4269
(T) | Epoch=645, loss=7.5535, this epoch 1.4668, total 958.8936
(T) | Epoch=646, loss=7.6644, this epoch 1.4300, total 960.3236
(T) | Epoch=647, loss=7.5938, this epoch 1.5383, total 961.8619
(T) | Epoch=648, loss=7.5303, this epoch 1.4825, total 963.3444
+++model saved ! 2016.pth
(T) | Epoch=649, loss=7.5369, this epoch 1.5404, total 964.8848
(T) | Epoch=650, loss=7.5302, this epoch 1.5552, total 966.4400
+++model saved ! 2016.pth
(T) | Epoch=651, loss=7.7184, this epoch 1.5086, total 967.9486
(T) | Epoch=652, loss=7.5743, this epoch 1.4788, total 969.4273
(T) | Epoch=653, loss=7.5901, this epoch 1.4617, total 970.8890
(T) | Epoch=654, loss=7.5366, this epoch 1.4714, total 972.3604
(T) | Epoch=655, loss=7.5400, this epoch 1.4447, total 973.8052
(T) | Epoch=656, loss=7.5406, this epoch 1.4915, total 975.2966
(T) | Epoch=657, loss=7.5509, this epoch 1.5355, total 976.8321
(T) | Epoch=658, loss=7.5420, this epoch 1.4830, total 978.3151
(T) | Epoch=659, loss=7.5411, this epoch 1.5126, total 979.8277
(T) | Epoch=660, loss=7.5595, this epoch 1.5329, total 981.3607
(T) | Epoch=661, loss=7.5507, this epoch 1.5021, total 982.8627
(T) | Epoch=662, loss=7.5366, this epoch 1.5010, total 984.3637
(T) | Epoch=663, loss=7.5315, this epoch 1.4760, total 985.8397
(T) | Epoch=664, loss=7.5941, this epoch 1.4969, total 987.3366
(T) | Epoch=665, loss=7.5324, this epoch 1.5088, total 988.8454
(T) | Epoch=666, loss=7.5218, this epoch 1.4777, total 990.3231
+++model saved ! 2016.pth
(T) | Epoch=667, loss=7.5428, this epoch 1.5113, total 991.8344
(T) | Epoch=668, loss=7.5979, this epoch 1.4253, total 993.2596
(T) | Epoch=669, loss=7.5203, this epoch 1.5499, total 994.8096
+++model saved ! 2016.pth
(T) | Epoch=670, loss=7.5354, this epoch 1.4663, total 996.2758
(T) | Epoch=671, loss=7.5235, this epoch 1.4921, total 997.7679
(T) | Epoch=672, loss=7.5090, this epoch 1.4788, total 999.2467
+++model saved ! 2016.pth
(T) | Epoch=673, loss=7.6084, this epoch 1.5081, total 1000.7548
(T) | Epoch=674, loss=7.6919, this epoch 1.5283, total 1002.2831
(T) | Epoch=675, loss=7.6284, this epoch 1.5022, total 1003.7853
(T) | Epoch=676, loss=7.5683, this epoch 1.5060, total 1005.2914
(T) | Epoch=677, loss=7.5386, this epoch 1.4609, total 1006.7523
(T) | Epoch=678, loss=7.5375, this epoch 1.5301, total 1008.2825
(T) | Epoch=679, loss=7.6165, this epoch 1.4945, total 1009.7770
(T) | Epoch=680, loss=7.5592, this epoch 1.4784, total 1011.2553
(T) | Epoch=681, loss=7.5891, this epoch 1.4606, total 1012.7159
(T) | Epoch=682, loss=7.6065, this epoch 1.4723, total 1014.1882
(T) | Epoch=683, loss=7.5375, this epoch 1.4895, total 1015.6777
(T) | Epoch=684, loss=7.5520, this epoch 1.5168, total 1017.1945
(T) | Epoch=685, loss=7.5973, this epoch 1.4533, total 1018.6478
(T) | Epoch=686, loss=7.5370, this epoch 1.4961, total 1020.1439
(T) | Epoch=687, loss=7.5928, this epoch 1.4462, total 1021.5901
(T) | Epoch=688, loss=7.5405, this epoch 1.4920, total 1023.0822
(T) | Epoch=689, loss=7.5507, this epoch 1.4336, total 1024.5158
(T) | Epoch=690, loss=7.5025, this epoch 1.4747, total 1025.9905
+++model saved ! 2016.pth
(T) | Epoch=691, loss=7.5100, this epoch 1.4999, total 1027.4904
(T) | Epoch=692, loss=7.5161, this epoch 1.5034, total 1028.9938
(T) | Epoch=693, loss=7.5075, this epoch 1.4530, total 1030.4468
(T) | Epoch=694, loss=7.5037, this epoch 1.4934, total 1031.9403
(T) | Epoch=695, loss=7.6855, this epoch 1.4617, total 1033.4020
(T) | Epoch=696, loss=7.6191, this epoch 1.4336, total 1034.8356
(T) | Epoch=697, loss=7.4859, this epoch 1.4643, total 1036.2999
+++model saved ! 2016.pth
(T) | Epoch=698, loss=7.4998, this epoch 1.4859, total 1037.7857
(T) | Epoch=699, loss=7.6113, this epoch 1.4957, total 1039.2814
(T) | Epoch=700, loss=7.5673, this epoch 1.4993, total 1040.7807
(T) | Epoch=701, loss=7.6014, this epoch 1.4924, total 1042.2731
(T) | Epoch=702, loss=7.5841, this epoch 1.4871, total 1043.7602
(T) | Epoch=703, loss=7.5623, this epoch 1.4644, total 1045.2246
(T) | Epoch=704, loss=7.5112, this epoch 1.4615, total 1046.6861
(T) | Epoch=705, loss=7.5196, this epoch 1.4851, total 1048.1712
(T) | Epoch=706, loss=7.5050, this epoch 1.4419, total 1049.6131
(T) | Epoch=707, loss=7.5091, this epoch 1.4506, total 1051.0637
(T) | Epoch=708, loss=7.5170, this epoch 1.5492, total 1052.6129
(T) | Epoch=709, loss=7.4916, this epoch 1.5488, total 1054.1617
(T) | Epoch=710, loss=7.4894, this epoch 1.4639, total 1055.6256
(T) | Epoch=711, loss=7.4918, this epoch 1.5316, total 1057.1572
(T) | Epoch=712, loss=7.4801, this epoch 1.4637, total 1058.6209
+++model saved ! 2016.pth
(T) | Epoch=713, loss=7.4883, this epoch 1.4542, total 1060.0751
(T) | Epoch=714, loss=7.5451, this epoch 1.4387, total 1061.5137
(T) | Epoch=715, loss=7.5167, this epoch 1.5124, total 1063.0262
(T) | Epoch=716, loss=7.7756, this epoch 1.5588, total 1064.5850
(T) | Epoch=717, loss=7.5639, this epoch 1.4543, total 1066.0393
(T) | Epoch=718, loss=7.5330, this epoch 1.4619, total 1067.5011
(T) | Epoch=719, loss=7.4973, this epoch 1.4790, total 1068.9802
(T) | Epoch=720, loss=7.5384, this epoch 1.4737, total 1070.4538
(T) | Epoch=721, loss=7.5110, this epoch 1.4613, total 1071.9151
(T) | Epoch=722, loss=7.5924, this epoch 1.4807, total 1073.3958
(T) | Epoch=723, loss=7.6101, this epoch 1.5135, total 1074.9093
(T) | Epoch=724, loss=7.6637, this epoch 1.4922, total 1076.4015
(T) | Epoch=725, loss=7.5691, this epoch 1.4513, total 1077.8528
(T) | Epoch=726, loss=7.5562, this epoch 1.4235, total 1079.2763
(T) | Epoch=727, loss=7.5286, this epoch 1.4300, total 1080.7063
(T) | Epoch=728, loss=7.6278, this epoch 1.4668, total 1082.1731
(T) | Epoch=729, loss=7.4906, this epoch 1.4724, total 1083.6455
(T) | Epoch=730, loss=7.5124, this epoch 1.4989, total 1085.1444
(T) | Epoch=731, loss=7.5713, this epoch 1.5081, total 1086.6525
(T) | Epoch=732, loss=7.5772, this epoch 1.5165, total 1088.1690
(T) | Epoch=733, loss=7.4962, this epoch 1.4813, total 1089.6503
(T) | Epoch=734, loss=7.6442, this epoch 1.4289, total 1091.0792
(T) | Epoch=735, loss=7.5288, this epoch 1.4457, total 1092.5249
(T) | Epoch=736, loss=7.6173, this epoch 1.4790, total 1094.0038
(T) | Epoch=737, loss=7.4920, this epoch 1.4283, total 1095.4322
(T) | Epoch=738, loss=7.4910, this epoch 1.4632, total 1096.8954
(T) | Epoch=739, loss=7.4957, this epoch 1.5471, total 1098.4425
(T) | Epoch=740, loss=7.5523, this epoch 1.4573, total 1099.8998
(T) | Epoch=741, loss=7.4833, this epoch 1.4345, total 1101.3343
(T) | Epoch=742, loss=7.4919, this epoch 1.4539, total 1102.7882
(T) | Epoch=743, loss=7.5959, this epoch 1.4364, total 1104.2246
(T) | Epoch=744, loss=7.5345, this epoch 1.4854, total 1105.7100
(T) | Epoch=745, loss=7.5529, this epoch 1.5474, total 1107.2574
(T) | Epoch=746, loss=7.6533, this epoch 1.5060, total 1108.7634
(T) | Epoch=747, loss=7.5002, this epoch 1.4879, total 1110.2513
(T) | Epoch=748, loss=7.6444, this epoch 1.5215, total 1111.7728
(T) | Epoch=749, loss=7.5042, this epoch 1.4635, total 1113.2363
(T) | Epoch=750, loss=7.4901, this epoch 1.4843, total 1114.7206
(T) | Epoch=751, loss=7.4888, this epoch 1.5112, total 1116.2318
(T) | Epoch=752, loss=7.4750, this epoch 1.4909, total 1117.7227
+++model saved ! 2016.pth
(T) | Epoch=753, loss=7.5470, this epoch 1.5080, total 1119.2307
(T) | Epoch=754, loss=7.5540, this epoch 1.4660, total 1120.6966
(T) | Epoch=755, loss=7.4729, this epoch 1.4607, total 1122.1573
+++model saved ! 2016.pth
(T) | Epoch=756, loss=7.5052, this epoch 1.4577, total 1123.6150
(T) | Epoch=757, loss=7.4819, this epoch 1.4646, total 1125.0796
(T) | Epoch=758, loss=7.6917, this epoch 1.4680, total 1126.5475
(T) | Epoch=759, loss=7.5317, this epoch 1.4994, total 1128.0470
(T) | Epoch=760, loss=7.4822, this epoch 1.4977, total 1129.5447
(T) | Epoch=761, loss=7.4894, this epoch 1.4770, total 1131.0216
(T) | Epoch=762, loss=7.5406, this epoch 1.4599, total 1132.4816
(T) | Epoch=763, loss=7.5323, this epoch 1.4751, total 1133.9566
(T) | Epoch=764, loss=7.6193, this epoch 1.4772, total 1135.4338
(T) | Epoch=765, loss=7.5374, this epoch 1.4993, total 1136.9330
(T) | Epoch=766, loss=7.5751, this epoch 1.5030, total 1138.4360
(T) | Epoch=767, loss=7.5834, this epoch 1.5068, total 1139.9428
(T) | Epoch=768, loss=7.7935, this epoch 1.4649, total 1141.4077
(T) | Epoch=769, loss=7.5117, this epoch 1.4521, total 1142.8598
(T) | Epoch=770, loss=7.5153, this epoch 1.5056, total 1144.3654
(T) | Epoch=771, loss=7.5309, this epoch 1.4858, total 1145.8512
(T) | Epoch=772, loss=7.6173, this epoch 1.4924, total 1147.3436
(T) | Epoch=773, loss=7.5414, this epoch 1.4758, total 1148.8194
(T) | Epoch=774, loss=7.5881, this epoch 1.5137, total 1150.3331
(T) | Epoch=775, loss=7.4857, this epoch 1.5293, total 1151.8624
(T) | Epoch=776, loss=7.4799, this epoch 1.5459, total 1153.4083
(T) | Epoch=777, loss=7.5660, this epoch 1.5302, total 1154.9385
(T) | Epoch=778, loss=7.4705, this epoch 1.4773, total 1156.4159
+++model saved ! 2016.pth
(T) | Epoch=779, loss=7.5744, this epoch 1.5071, total 1157.9230
(T) | Epoch=780, loss=7.4698, this epoch 1.5152, total 1159.4382
+++model saved ! 2016.pth
(T) | Epoch=781, loss=7.5405, this epoch 1.4762, total 1160.9144
(T) | Epoch=782, loss=7.4797, this epoch 1.4702, total 1162.3846
(T) | Epoch=783, loss=7.6766, this epoch 1.4590, total 1163.8436
(T) | Epoch=784, loss=7.4664, this epoch 1.4384, total 1165.2820
+++model saved ! 2016.pth
(T) | Epoch=785, loss=7.5509, this epoch 1.4331, total 1166.7151
(T) | Epoch=786, loss=7.4781, this epoch 1.4614, total 1168.1765
(T) | Epoch=787, loss=7.4693, this epoch 1.4762, total 1169.6527
(T) | Epoch=788, loss=7.4842, this epoch 1.5207, total 1171.1734
(T) | Epoch=789, loss=7.5137, this epoch 1.5346, total 1172.7080
(T) | Epoch=790, loss=7.5145, this epoch 1.4565, total 1174.1645
(T) | Epoch=791, loss=7.6304, this epoch 1.4571, total 1175.6216
(T) | Epoch=792, loss=7.5807, this epoch 1.4660, total 1177.0876
(T) | Epoch=793, loss=7.5676, this epoch 1.4710, total 1178.5586
(T) | Epoch=794, loss=7.5561, this epoch 1.4843, total 1180.0429
(T) | Epoch=795, loss=7.4869, this epoch 1.4993, total 1181.5422
(T) | Epoch=796, loss=7.4931, this epoch 1.5692, total 1183.1114
(T) | Epoch=797, loss=7.5260, this epoch 1.5727, total 1184.6841
(T) | Epoch=798, loss=7.4712, this epoch 1.4408, total 1186.1250
(T) | Epoch=799, loss=7.5479, this epoch 1.4315, total 1187.5565
(T) | Epoch=800, loss=7.4716, this epoch 1.4848, total 1189.0412
(T) | Epoch=801, loss=7.5732, this epoch 1.4903, total 1190.5316
(T) | Epoch=802, loss=7.4689, this epoch 1.4704, total 1192.0020
(T) | Epoch=803, loss=7.4664, this epoch 1.4803, total 1193.4823
+++model saved ! 2016.pth
(T) | Epoch=804, loss=7.4937, this epoch 1.4927, total 1194.9750
(T) | Epoch=805, loss=7.4592, this epoch 1.4685, total 1196.4435
+++model saved ! 2016.pth
(T) | Epoch=806, loss=7.4727, this epoch 1.4869, total 1197.9303
(T) | Epoch=807, loss=7.4544, this epoch 1.4392, total 1199.3696
+++model saved ! 2016.pth
(T) | Epoch=808, loss=7.4552, this epoch 1.4548, total 1200.8244
(T) | Epoch=809, loss=7.4606, this epoch 1.4813, total 1202.3057
(T) | Epoch=810, loss=7.5576, this epoch 1.4425, total 1203.7482
(T) | Epoch=811, loss=7.4504, this epoch 1.4422, total 1205.1904
+++model saved ! 2016.pth
(T) | Epoch=812, loss=7.4696, this epoch 1.4705, total 1206.6609
(T) | Epoch=813, loss=7.4539, this epoch 1.4786, total 1208.1395
(T) | Epoch=814, loss=7.5601, this epoch 1.5799, total 1209.7194
(T) | Epoch=815, loss=7.4713, this epoch 1.4965, total 1211.2159
(T) | Epoch=816, loss=7.4558, this epoch 1.4601, total 1212.6760
(T) | Epoch=817, loss=7.4746, this epoch 1.4767, total 1214.1527
(T) | Epoch=818, loss=7.4876, this epoch 1.4802, total 1215.6329
(T) | Epoch=819, loss=7.4837, this epoch 1.4153, total 1217.0482
(T) | Epoch=820, loss=7.4662, this epoch 1.4493, total 1218.4974
(T) | Epoch=821, loss=7.4507, this epoch 1.4481, total 1219.9455
(T) | Epoch=822, loss=7.5482, this epoch 1.4298, total 1221.3753
(T) | Epoch=823, loss=8.8821, this epoch 1.5054, total 1222.8807
(T) | Epoch=824, loss=7.4677, this epoch 1.4239, total 1224.3046
(T) | Epoch=825, loss=7.5743, this epoch 1.4663, total 1225.7709
(T) | Epoch=826, loss=7.6496, this epoch 1.4516, total 1227.2226
(T) | Epoch=827, loss=7.6390, this epoch 1.4614, total 1228.6839
(T) | Epoch=828, loss=7.5075, this epoch 1.4732, total 1230.1572
(T) | Epoch=829, loss=7.8442, this epoch 1.4725, total 1231.6296
(T) | Epoch=830, loss=7.8455, this epoch 1.4860, total 1233.1156
(T) | Epoch=831, loss=7.6267, this epoch 1.5092, total 1234.6249
(T) | Epoch=832, loss=7.8708, this epoch 1.5182, total 1236.1431
(T) | Epoch=833, loss=7.6390, this epoch 1.4847, total 1237.6278
(T) | Epoch=834, loss=7.6361, this epoch 1.4809, total 1239.1087
(T) | Epoch=835, loss=7.6419, this epoch 1.4788, total 1240.5875
(T) | Epoch=836, loss=7.6333, this epoch 1.5233, total 1242.1108
(T) | Epoch=837, loss=7.6403, this epoch 1.4532, total 1243.5640
(T) | Epoch=838, loss=7.6808, this epoch 1.5588, total 1245.1227
(T) | Epoch=839, loss=7.6436, this epoch 1.4993, total 1246.6220
(T) | Epoch=840, loss=7.8465, this epoch 1.4475, total 1248.0695
(T) | Epoch=841, loss=7.6376, this epoch 1.4872, total 1249.5567
(T) | Epoch=842, loss=7.9338, this epoch 1.4540, total 1251.0108
(T) | Epoch=843, loss=7.6396, this epoch 1.4496, total 1252.4603
(T) | Epoch=844, loss=7.8279, this epoch 1.4754, total 1253.9358
(T) | Epoch=845, loss=7.6721, this epoch 1.4827, total 1255.4185
(T) | Epoch=846, loss=7.6277, this epoch 1.5493, total 1256.9677
(T) | Epoch=847, loss=7.6734, this epoch 1.4744, total 1258.4421
(T) | Epoch=848, loss=7.6208, this epoch 1.4638, total 1259.9059
(T) | Epoch=849, loss=7.6283, this epoch 1.4755, total 1261.3815
(T) | Epoch=850, loss=7.6647, this epoch 1.5397, total 1262.9212
(T) | Epoch=851, loss=7.7263, this epoch 1.4576, total 1264.3787
(T) | Epoch=852, loss=7.6009, this epoch 1.5175, total 1265.8962
(T) | Epoch=853, loss=7.6208, this epoch 1.5223, total 1267.4185
(T) | Epoch=854, loss=7.6169, this epoch 1.4609, total 1268.8794
(T) | Epoch=855, loss=7.5946, this epoch 1.4903, total 1270.3697
(T) | Epoch=856, loss=7.7374, this epoch 1.5114, total 1271.8811
(T) | Epoch=857, loss=7.6576, this epoch 1.4578, total 1273.3389
(T) | Epoch=858, loss=7.5710, this epoch 1.4657, total 1274.8046
(T) | Epoch=859, loss=7.5684, this epoch 1.4459, total 1276.2506
(T) | Epoch=860, loss=7.5653, this epoch 1.4426, total 1277.6932
(T) | Epoch=861, loss=7.6460, this epoch 1.4456, total 1279.1387
(T) | Epoch=862, loss=7.6017, this epoch 1.4617, total 1280.6004
(T) | Epoch=863, loss=7.5994, this epoch 1.4594, total 1282.0598
(T) | Epoch=864, loss=7.5516, this epoch 1.4640, total 1283.5239
(T) | Epoch=865, loss=7.6192, this epoch 1.4567, total 1284.9805
(T) | Epoch=866, loss=7.5535, this epoch 1.4996, total 1286.4801
(T) | Epoch=867, loss=7.6017, this epoch 1.4724, total 1287.9525
(T) | Epoch=868, loss=7.5214, this epoch 1.4889, total 1289.4414
(T) | Epoch=869, loss=7.6487, this epoch 1.4746, total 1290.9160
(T) | Epoch=870, loss=7.5530, this epoch 1.4916, total 1292.4076
(T) | Epoch=871, loss=7.5214, this epoch 1.5133, total 1293.9208
(T) | Epoch=872, loss=7.5198, this epoch 1.5749, total 1295.4957
(T) | Epoch=873, loss=7.5563, this epoch 1.4994, total 1296.9952
(T) | Epoch=874, loss=7.5525, this epoch 1.4595, total 1298.4547
(T) | Epoch=875, loss=7.5070, this epoch 1.5157, total 1299.9704
(T) | Epoch=876, loss=7.5476, this epoch 1.4588, total 1301.4292
(T) | Epoch=877, loss=7.5215, this epoch 1.4877, total 1302.9169
(T) | Epoch=878, loss=7.5331, this epoch 1.4644, total 1304.3812
(T) | Epoch=879, loss=7.5423, this epoch 1.4410, total 1305.8222
(T) | Epoch=880, loss=7.5014, this epoch 1.4538, total 1307.2760
(T) | Epoch=881, loss=7.4963, this epoch 1.4855, total 1308.7615
(T) | Epoch=882, loss=7.4871, this epoch 1.4862, total 1310.2477
(T) | Epoch=883, loss=7.5394, this epoch 1.4812, total 1311.7290
(T) | Epoch=884, loss=7.5239, this epoch 1.5275, total 1313.2565
(T) | Epoch=885, loss=7.6077, this epoch 1.5153, total 1314.7718
(T) | Epoch=886, loss=7.4911, this epoch 1.4815, total 1316.2532
(T) | Epoch=887, loss=7.4757, this epoch 1.4483, total 1317.7015
(T) | Epoch=888, loss=7.5696, this epoch 1.4498, total 1319.1513
(T) | Epoch=889, loss=7.4795, this epoch 1.5141, total 1320.6655
(T) | Epoch=890, loss=7.5284, this epoch 1.4600, total 1322.1255
(T) | Epoch=891, loss=7.4869, this epoch 1.4519, total 1323.5773
(T) | Epoch=892, loss=7.4715, this epoch 1.4367, total 1325.0141
(T) | Epoch=893, loss=7.5612, this epoch 1.4693, total 1326.4834
(T) | Epoch=894, loss=7.5040, this epoch 1.4294, total 1327.9127
(T) | Epoch=895, loss=7.4730, this epoch 1.4355, total 1329.3482
(T) | Epoch=896, loss=7.5167, this epoch 1.4581, total 1330.8063
(T) | Epoch=897, loss=7.4823, this epoch 1.4471, total 1332.2534
(T) | Epoch=898, loss=7.5450, this epoch 1.4772, total 1333.7306
(T) | Epoch=899, loss=7.5723, this epoch 1.4596, total 1335.1903
(T) | Epoch=900, loss=7.4532, this epoch 1.4707, total 1336.6609
(T) | Epoch=901, loss=7.5013, this epoch 1.4759, total 1338.1368
(T) | Epoch=902, loss=7.4777, this epoch 1.4678, total 1339.6046
(T) | Epoch=903, loss=8.0184, this epoch 1.4846, total 1341.0892
(T) | Epoch=904, loss=7.5532, this epoch 1.4685, total 1342.5578
(T) | Epoch=905, loss=7.4580, this epoch 1.4531, total 1344.0109
(T) | Epoch=906, loss=7.4644, this epoch 1.4880, total 1345.4988
(T) | Epoch=907, loss=7.4955, this epoch 1.4987, total 1346.9975
(T) | Epoch=908, loss=7.4946, this epoch 1.4599, total 1348.4574
(T) | Epoch=909, loss=7.4934, this epoch 1.4621, total 1349.9195
(T) | Epoch=910, loss=7.5578, this epoch 1.5047, total 1351.4243
(T) | Epoch=911, loss=7.5203, this epoch 1.4569, total 1352.8812
(T) | Epoch=912, loss=7.4988, this epoch 1.4372, total 1354.3184
(T) | Epoch=913, loss=7.4927, this epoch 1.5063, total 1355.8247
(T) | Epoch=914, loss=7.5270, this epoch 1.4641, total 1357.2888
(T) | Epoch=915, loss=7.4781, this epoch 1.4728, total 1358.7616
(T) | Epoch=916, loss=7.5513, this epoch 1.4539, total 1360.2155
(T) | Epoch=917, loss=7.4864, this epoch 1.5057, total 1361.7212
(T) | Epoch=918, loss=7.5257, this epoch 1.5324, total 1363.2536
(T) | Epoch=919, loss=7.4888, this epoch 1.4478, total 1364.7014
(T) | Epoch=920, loss=7.4680, this epoch 1.4644, total 1366.1658
(T) | Epoch=921, loss=7.5015, this epoch 1.4991, total 1367.6649
(T) | Epoch=922, loss=7.5331, this epoch 1.4804, total 1369.1453
(T) | Epoch=923, loss=7.6029, this epoch 1.5244, total 1370.6697
(T) | Epoch=924, loss=7.4840, this epoch 1.4882, total 1372.1579
(T) | Epoch=925, loss=7.4960, this epoch 1.4954, total 1373.6532
(T) | Epoch=926, loss=7.4580, this epoch 1.4937, total 1375.1469
(T) | Epoch=927, loss=7.4708, this epoch 1.4832, total 1376.6302
(T) | Epoch=928, loss=7.4672, this epoch 1.4929, total 1378.1230
(T) | Epoch=929, loss=7.4598, this epoch 1.5147, total 1379.6377
(T) | Epoch=930, loss=7.5423, this epoch 1.4892, total 1381.1269
(T) | Epoch=931, loss=7.4580, this epoch 1.4610, total 1382.5878
(T) | Epoch=932, loss=7.5290, this epoch 1.4922, total 1384.0800
(T) | Epoch=933, loss=7.5099, this epoch 1.4983, total 1385.5783
(T) | Epoch=934, loss=7.4667, this epoch 1.5299, total 1387.1082
(T) | Epoch=935, loss=7.4673, this epoch 1.5115, total 1388.6197
(T) | Epoch=936, loss=7.4829, this epoch 1.5422, total 1390.1619
(T) | Epoch=937, loss=7.5459, this epoch 1.4754, total 1391.6373
(T) | Epoch=938, loss=7.4652, this epoch 1.5240, total 1393.1613
(T) | Epoch=939, loss=7.4647, this epoch 1.5097, total 1394.6710
(T) | Epoch=940, loss=7.5075, this epoch 1.4664, total 1396.1374
(T) | Epoch=941, loss=7.4650, this epoch 1.4894, total 1397.6268
(T) | Epoch=942, loss=7.4524, this epoch 1.5002, total 1399.1271
(T) | Epoch=943, loss=7.5999, this epoch 1.4796, total 1400.6067
(T) | Epoch=944, loss=7.4609, this epoch 1.5341, total 1402.1408
(T) | Epoch=945, loss=7.4690, this epoch 1.4508, total 1403.5915
(T) | Epoch=946, loss=7.4562, this epoch 1.4521, total 1405.0437
(T) | Epoch=947, loss=7.5178, this epoch 1.4456, total 1406.4893
(T) | Epoch=948, loss=7.4710, this epoch 1.4675, total 1407.9568
(T) | Epoch=949, loss=7.4487, this epoch 1.5354, total 1409.4922
+++model saved ! 2016.pth
(T) | Epoch=950, loss=7.4660, this epoch 1.4429, total 1410.9352
(T) | Epoch=951, loss=7.4575, this epoch 1.4964, total 1412.4316
(T) | Epoch=952, loss=7.4778, this epoch 1.4325, total 1413.8641
(T) | Epoch=953, loss=7.4722, this epoch 1.4133, total 1415.2774
(T) | Epoch=954, loss=7.4648, this epoch 1.4816, total 1416.7590
(T) | Epoch=955, loss=7.4652, this epoch 1.4507, total 1418.2097
(T) | Epoch=956, loss=7.4660, this epoch 1.4313, total 1419.6409
(T) | Epoch=957, loss=7.4707, this epoch 1.4631, total 1421.1040
(T) | Epoch=958, loss=7.4581, this epoch 1.4275, total 1422.5315
(T) | Epoch=959, loss=7.4598, this epoch 1.4507, total 1423.9822
(T) | Epoch=960, loss=7.5022, this epoch 1.4882, total 1425.4705
(T) | Epoch=961, loss=7.4473, this epoch 1.4656, total 1426.9361
+++model saved ! 2016.pth
(T) | Epoch=962, loss=7.4643, this epoch 1.4862, total 1428.4223
(T) | Epoch=963, loss=7.4682, this epoch 1.5460, total 1429.9683
(T) | Epoch=964, loss=7.4684, this epoch 1.5549, total 1431.5232
(T) | Epoch=965, loss=7.5104, this epoch 1.4973, total 1433.0205
(T) | Epoch=966, loss=7.4625, this epoch 1.4762, total 1434.4968
(T) | Epoch=967, loss=7.4656, this epoch 1.4278, total 1435.9246
(T) | Epoch=968, loss=7.5318, this epoch 1.4750, total 1437.3995
(T) | Epoch=969, loss=7.4919, this epoch 1.4229, total 1438.8224
(T) | Epoch=970, loss=7.4457, this epoch 1.4491, total 1440.2715
+++model saved ! 2016.pth
(T) | Epoch=971, loss=7.4625, this epoch 1.4755, total 1441.7470
(T) | Epoch=972, loss=7.5440, this epoch 1.4624, total 1443.2094
(T) | Epoch=973, loss=8.0167, this epoch 1.4208, total 1444.6302
(T) | Epoch=974, loss=7.4560, this epoch 1.4677, total 1446.0979
(T) | Epoch=975, loss=7.4629, this epoch 1.4550, total 1447.5529
(T) | Epoch=976, loss=7.5489, this epoch 1.4290, total 1448.9819
(T) | Epoch=977, loss=7.4848, this epoch 1.4754, total 1450.4573
(T) | Epoch=978, loss=7.5312, this epoch 1.5350, total 1451.9923
(T) | Epoch=979, loss=7.6225, this epoch 1.5494, total 1453.5417
(T) | Epoch=980, loss=7.5117, this epoch 1.5482, total 1455.0899
(T) | Epoch=981, loss=7.6630, this epoch 1.5558, total 1456.6457
(T) | Epoch=982, loss=7.5768, this epoch 1.4591, total 1458.1048
(T) | Epoch=983, loss=7.5710, this epoch 1.4618, total 1459.5666
(T) | Epoch=984, loss=7.5170, this epoch 1.4908, total 1461.0574
(T) | Epoch=985, loss=7.5734, this epoch 1.4560, total 1462.5134
(T) | Epoch=986, loss=7.5337, this epoch 1.4936, total 1464.0070
(T) | Epoch=987, loss=7.5398, this epoch 1.4460, total 1465.4529
(T) | Epoch=988, loss=7.5815, this epoch 1.4562, total 1466.9092
(T) | Epoch=989, loss=7.5682, this epoch 1.4801, total 1468.3892
(T) | Epoch=990, loss=7.5196, this epoch 1.4359, total 1469.8251
(T) | Epoch=991, loss=7.4958, this epoch 1.4631, total 1471.2882
(T) | Epoch=992, loss=7.7848, this epoch 1.4294, total 1472.7176
(T) | Epoch=993, loss=7.4975, this epoch 1.4740, total 1474.1916
(T) | Epoch=994, loss=7.5405, this epoch 1.5206, total 1475.7122
(T) | Epoch=995, loss=7.5651, this epoch 1.5035, total 1477.2157
(T) | Epoch=996, loss=7.5036, this epoch 1.4475, total 1478.6633
(T) | Epoch=997, loss=7.4839, this epoch 1.4641, total 1480.1274
(T) | Epoch=998, loss=7.4669, this epoch 1.5210, total 1481.6484
(T) | Epoch=999, loss=7.5263, this epoch 1.4756, total 1483.1240
(T) | Epoch=1000, loss=7.4754, this epoch 1.4622, total 1484.5862
=== Final ===

==============================
LoRA FINE-TUNING
==============================
Random seed set to 0
Epoch: 0, loss: 33.4412, train_acc: 0.0000, train_recall: 0.0000, train_f1: 0.0000, val_acc: 0.000000, val_recall: 0.000000, val_f1: 0.000000
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed0.csv
Epoch: 1, loss: 30.9808, train_acc: 0.0000, train_recall: 0.0000, train_f1: 0.0000, val_acc: 0.000000, val_recall: 0.000000, val_f1: 0.000000
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed0.csv
Epoch: 2, loss: 26.9608, train_acc: 0.2056, train_recall: 0.2500, train_f1: 0.0853, val_acc: 0.217949, val_recall: 0.250000, val_f1: 0.089474
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed0.csv
Epoch: 3, loss: 60.2213, train_acc: 0.3944, train_recall: 0.2500, train_f1: 0.1414, val_acc: 0.371795, val_recall: 0.250000, val_f1: 0.135514
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed0.csv
Epoch: 4, loss: 118.4054, train_acc: 0.3778, train_recall: 0.2500, train_f1: 0.1371, val_acc: 0.397436, val_recall: 0.250000, val_f1: 0.142202
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed0.csv
Epoch: 5, loss: 155.9173, train_acc: 0.0222, train_recall: 0.2500, train_f1: 0.0109, val_acc: 0.012821, val_recall: 0.250000, val_f1: 0.006329
Epoch: 6, loss: 125.8680, train_acc: 0.3778, train_recall: 0.2500, train_f1: 0.1371, val_acc: 0.397436, val_recall: 0.250000, val_f1: 0.142202
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed0.csv
Epoch: 7, loss: 101.5830, train_acc: 0.3778, train_recall: 0.2500, train_f1: 0.1371, val_acc: 0.397436, val_recall: 0.250000, val_f1: 0.142202
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed0.csv
Epoch: 8, loss: 132.5692, train_acc: 0.3944, train_recall: 0.2500, train_f1: 0.1414, val_acc: 0.371795, val_recall: 0.250000, val_f1: 0.135514
Epoch: 9, loss: 143.1971, train_acc: 0.3944, train_recall: 0.2500, train_f1: 0.1414, val_acc: 0.371795, val_recall: 0.250000, val_f1: 0.135514
Epoch: 10, loss: 133.2408, train_acc: 0.3944, train_recall: 0.2500, train_f1: 0.1414, val_acc: 0.371795, val_recall: 0.250000, val_f1: 0.135514
Epoch: 11, loss: 108.7299, train_acc: 0.3944, train_recall: 0.2500, train_f1: 0.1414, val_acc: 0.371795, val_recall: 0.250000, val_f1: 0.135514
Epoch: 12, loss: 74.0624, train_acc: 0.3944, train_recall: 0.2500, train_f1: 0.1414, val_acc: 0.371795, val_recall: 0.250000, val_f1: 0.135514
Epoch: 13, loss: 81.9625, train_acc: 0.3778, train_recall: 0.2500, train_f1: 0.1371, val_acc: 0.397436, val_recall: 0.250000, val_f1: 0.142202
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed0.csv
Epoch: 14, loss: 86.6972, train_acc: 0.3778, train_recall: 0.2500, train_f1: 0.1371, val_acc: 0.397436, val_recall: 0.250000, val_f1: 0.142202
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed0.csv
Epoch: 15, loss: 78.2065, train_acc: 0.3778, train_recall: 0.2500, train_f1: 0.1371, val_acc: 0.397436, val_recall: 0.250000, val_f1: 0.142202
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed0.csv
Epoch: 16, loss: 59.3283, train_acc: 0.3778, train_recall: 0.2500, train_f1: 0.1371, val_acc: 0.397436, val_recall: 0.250000, val_f1: 0.142202
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed0.csv
Epoch: 17, loss: 96.0676, train_acc: 0.2056, train_recall: 0.2500, train_f1: 0.0853, val_acc: 0.217949, val_recall: 0.250000, val_f1: 0.089474
Epoch: 18, loss: 109.8125, train_acc: 0.2056, train_recall: 0.2500, train_f1: 0.0853, val_acc: 0.217949, val_recall: 0.250000, val_f1: 0.089474
Epoch: 19, loss: 102.7064, train_acc: 0.2056, train_recall: 0.2500, train_f1: 0.0853, val_acc: 0.217949, val_recall: 0.250000, val_f1: 0.089474
Epoch: 20, loss: 80.1277, train_acc: 0.2056, train_recall: 0.2500, train_f1: 0.0853, val_acc: 0.217949, val_recall: 0.250000, val_f1: 0.089474
Epoch: 21, loss: 45.4102, train_acc: 0.3611, train_recall: 0.3162, train_f1: 0.2241, val_acc: 0.423077, val_recall: 0.375761, val_f1: 0.265625
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed0.csv
Epoch: 22, loss: 60.9921, train_acc: 0.3944, train_recall: 0.2500, train_f1: 0.1414, val_acc: 0.371795, val_recall: 0.250000, val_f1: 0.135514
Epoch: 23, loss: 66.5141, train_acc: 0.3944, train_recall: 0.2500, train_f1: 0.1414, val_acc: 0.371795, val_recall: 0.250000, val_f1: 0.135514
Epoch: 24, loss: 63.4260, train_acc: 0.3944, train_recall: 0.2500, train_f1: 0.1414, val_acc: 0.371795, val_recall: 0.250000, val_f1: 0.135514
Epoch: 25, loss: 52.5844, train_acc: 0.4000, train_recall: 0.2537, train_f1: 0.1492, val_acc: 0.371795, val_recall: 0.250000, val_f1: 0.135514
Epoch: 26, loss: 74.1845, train_acc: 0.3778, train_recall: 0.2500, train_f1: 0.1371, val_acc: 0.397436, val_recall: 0.250000, val_f1: 0.142202
Epoch: 27, loss: 87.2273, train_acc: 0.3778, train_recall: 0.2500, train_f1: 0.1371, val_acc: 0.397436, val_recall: 0.250000, val_f1: 0.142202
Epoch: 28, loss: 90.6435, train_acc: 0.3778, train_recall: 0.2500, train_f1: 0.1371, val_acc: 0.397436, val_recall: 0.250000, val_f1: 0.142202
Epoch: 29, loss: 85.7399, train_acc: 0.3778, train_recall: 0.2500, train_f1: 0.1371, val_acc: 0.397436, val_recall: 0.250000, val_f1: 0.142202
Epoch: 30, loss: 73.7146, train_acc: 0.3778, train_recall: 0.2500, train_f1: 0.1371, val_acc: 0.397436, val_recall: 0.250000, val_f1: 0.142202
Epoch: 31, loss: 55.5603, train_acc: 0.3778, train_recall: 0.2500, train_f1: 0.1371, val_acc: 0.397436, val_recall: 0.250000, val_f1: 0.142202
Epoch: 32, loss: 57.8299, train_acc: 0.3944, train_recall: 0.2500, train_f1: 0.1414, val_acc: 0.371795, val_recall: 0.250000, val_f1: 0.135514
Epoch: 33, loss: 64.2687, train_acc: 0.3944, train_recall: 0.2500, train_f1: 0.1414, val_acc: 0.371795, val_recall: 0.250000, val_f1: 0.135514
Epoch: 34, loss: 63.1808, train_acc: 0.3944, train_recall: 0.2500, train_f1: 0.1414, val_acc: 0.371795, val_recall: 0.250000, val_f1: 0.135514
Epoch: 35, loss: 55.7498, train_acc: 0.4000, train_recall: 0.3125, train_f1: 0.2420, val_acc: 0.371795, val_recall: 0.250000, val_f1: 0.135514
Epoch: 36, loss: 42.9653, train_acc: 0.4000, train_recall: 0.3125, train_f1: 0.2259, val_acc: 0.371795, val_recall: 0.250000, val_f1: 0.135514
Epoch: 37, loss: 38.1719, train_acc: 0.1778, train_recall: 0.2720, train_f1: 0.1054, val_acc: 0.166667, val_recall: 0.191176, val_f1: 0.074713
Epoch: 38, loss: 47.1236, train_acc: 0.1111, train_recall: 0.1909, train_f1: 0.0669, val_acc: 0.089744, val_recall: 0.102941, val_f1: 0.051471
Epoch: 39, loss: 41.1435, train_acc: 0.2667, train_recall: 0.2723, train_f1: 0.1991, val_acc: 0.243590, val_recall: 0.186433, val_f1: 0.181222
Epoch: 40, loss: 44.7976, train_acc: 0.3778, train_recall: 0.3088, train_f1: 0.2195, val_acc: 0.397436, val_recall: 0.250000, val_f1: 0.142202
Epoch: 41, loss: 49.0834, train_acc: 0.3778, train_recall: 0.3088, train_f1: 0.2195, val_acc: 0.397436, val_recall: 0.250000, val_f1: 0.142202
Epoch: 42, loss: 46.6200, train_acc: 0.3833, train_recall: 0.3125, train_f1: 0.2377, val_acc: 0.397436, val_recall: 0.250000, val_f1: 0.142202
Epoch: 43, loss: 38.5140, train_acc: 0.3833, train_recall: 0.3125, train_f1: 0.2377, val_acc: 0.397436, val_recall: 0.250000, val_f1: 0.142202
Epoch: 44, loss: 28.5454, train_acc: 0.4222, train_recall: 0.3274, train_f1: 0.2783, val_acc: 0.397436, val_recall: 0.265573, val_f1: 0.179876
Epoch: 45, loss: 33.0702, train_acc: 0.4000, train_recall: 0.3125, train_f1: 0.2420, val_acc: 0.371795, val_recall: 0.250000, val_f1: 0.135514
Epoch: 46, loss: 30.7475, train_acc: 0.3889, train_recall: 0.3637, train_f1: 0.3294, val_acc: 0.410256, val_recall: 0.336714, val_f1: 0.251136
Epoch: 47, loss: 33.7124, train_acc: 0.2056, train_recall: 0.2500, train_f1: 0.0853, val_acc: 0.217949, val_recall: 0.250000, val_f1: 0.089474
Epoch: 48, loss: 29.5219, train_acc: 0.2056, train_recall: 0.2500, train_f1: 0.0853, val_acc: 0.217949, val_recall: 0.250000, val_f1: 0.089474
Epoch: 49, loss: 31.0213, train_acc: 0.3833, train_recall: 0.3125, train_f1: 0.2377, val_acc: 0.397436, val_recall: 0.250000, val_f1: 0.142202
Epoch: 50, loss: 31.6989, train_acc: 0.3833, train_recall: 0.3125, train_f1: 0.2377, val_acc: 0.397436, val_recall: 0.250000, val_f1: 0.142202
Epoch: 51, loss: 37.1582, train_acc: 0.4000, train_recall: 0.3125, train_f1: 0.2420, val_acc: 0.371795, val_recall: 0.250000, val_f1: 0.135514
Epoch: 52, loss: 40.0829, train_acc: 0.4000, train_recall: 0.3125, train_f1: 0.2420, val_acc: 0.371795, val_recall: 0.250000, val_f1: 0.135514
Epoch: 53, loss: 37.1672, train_acc: 0.4000, train_recall: 0.3125, train_f1: 0.2420, val_acc: 0.371795, val_recall: 0.250000, val_f1: 0.135514
Epoch: 54, loss: 33.5854, train_acc: 0.3833, train_recall: 0.3125, train_f1: 0.2377, val_acc: 0.397436, val_recall: 0.250000, val_f1: 0.142202
Epoch: 55, loss: 32.8373, train_acc: 0.3833, train_recall: 0.3125, train_f1: 0.2377, val_acc: 0.397436, val_recall: 0.250000, val_f1: 0.142202
Epoch: 56, loss: 28.8196, train_acc: 0.4222, train_recall: 0.3274, train_f1: 0.2783, val_acc: 0.397436, val_recall: 0.265573, val_f1: 0.179876
Epoch: 57, loss: 25.9532, train_acc: 0.4000, train_recall: 0.3709, train_f1: 0.3408, val_acc: 0.410256, val_recall: 0.336714, val_f1: 0.251136
Epoch: 58, loss: 29.0008, train_acc: 0.3722, train_recall: 0.3945, train_f1: 0.3316, val_acc: 0.371795, val_recall: 0.333491, val_f1: 0.230643
Epoch: 59, loss: 29.8326, train_acc: 0.3778, train_recall: 0.3858, train_f1: 0.3321, val_acc: 0.333333, val_recall: 0.289374, val_f1: 0.206712
Epoch: 60, loss: 28.5273, train_acc: 0.3722, train_recall: 0.3729, train_f1: 0.3103, val_acc: 0.333333, val_recall: 0.276091, val_f1: 0.204262
Epoch: 61, loss: 25.2897, train_acc: 0.3944, train_recall: 0.3737, train_f1: 0.3619, val_acc: 0.358974, val_recall: 0.293889, val_f1: 0.256265
Epoch: 62, loss: 30.0345, train_acc: 0.4278, train_recall: 0.3495, train_f1: 0.2929, val_acc: 0.410256, val_recall: 0.300203, val_f1: 0.221414
Epoch: 63, loss: 31.2300, train_acc: 0.4278, train_recall: 0.3495, train_f1: 0.2929, val_acc: 0.410256, val_recall: 0.300203, val_f1: 0.221414
Epoch: 64, loss: 27.5366, train_acc: 0.3944, train_recall: 0.3285, train_f1: 0.2412, val_acc: 0.410256, val_recall: 0.300203, val_f1: 0.225833
Epoch: 65, loss: 29.9227, train_acc: 0.3833, train_recall: 0.3279, train_f1: 0.2238, val_acc: 0.384615, val_recall: 0.268501, val_f1: 0.214021
Epoch: 66, loss: 31.2805, train_acc: 0.3778, train_recall: 0.3273, train_f1: 0.2249, val_acc: 0.384615, val_recall: 0.268501, val_f1: 0.215417
Epoch: 67, loss: 30.3786, train_acc: 0.3389, train_recall: 0.4097, train_f1: 0.2600, val_acc: 0.346154, val_recall: 0.304080, val_f1: 0.240345
Epoch: 68, loss: 29.5621, train_acc: 0.1833, train_recall: 0.3314, train_f1: 0.1160, val_acc: 0.192308, val_recall: 0.220588, val_f1: 0.089286
Epoch: 69, loss: 25.1018, train_acc: 0.3889, train_recall: 0.3575, train_f1: 0.2776, val_acc: 0.397436, val_recall: 0.322008, val_f1: 0.248168
Epoch: 70, loss: 27.6007, train_acc: 0.4056, train_recall: 0.3173, train_f1: 0.2690, val_acc: 0.397436, val_recall: 0.263348, val_f1: 0.207748
Epoch: 71, loss: 29.0712, train_acc: 0.4167, train_recall: 0.3321, train_f1: 0.2965, val_acc: 0.358974, val_recall: 0.227475, val_f1: 0.165736
Epoch: 72, loss: 28.0620, train_acc: 0.4111, train_recall: 0.3219, train_f1: 0.3077, val_acc: 0.397436, val_recall: 0.262236, val_f1: 0.214503
Epoch: 73, loss: 27.7708, train_acc: 0.4111, train_recall: 0.3219, train_f1: 0.3077, val_acc: 0.397436, val_recall: 0.262236, val_f1: 0.214503
Epoch: 74, loss: 27.6890, train_acc: 0.3833, train_recall: 0.3125, train_f1: 0.2377, val_acc: 0.384615, val_recall: 0.241935, val_f1: 0.138889
Epoch: 75, loss: 24.8230, train_acc: 0.4278, train_recall: 0.3822, train_f1: 0.3563, val_acc: 0.397436, val_recall: 0.322008, val_f1: 0.241289
Epoch: 76, loss: 27.9338, train_acc: 0.2222, train_recall: 0.3165, train_f1: 0.2057, val_acc: 0.217949, val_recall: 0.250000, val_f1: 0.089474
Epoch: 77, loss: 25.2366, train_acc: 0.4000, train_recall: 0.3711, train_f1: 0.3455, val_acc: 0.410256, val_recall: 0.336714, val_f1: 0.252877
Epoch: 78, loss: 24.9181, train_acc: 0.4333, train_recall: 0.3614, train_f1: 0.3770, val_acc: 0.358974, val_recall: 0.254040, val_f1: 0.235478
Epoch: 79, loss: 24.9404, train_acc: 0.4222, train_recall: 0.3502, train_f1: 0.3724, val_acc: 0.384615, val_recall: 0.271282, val_f1: 0.260854
Epoch: 80, loss: 24.9468, train_acc: 0.3833, train_recall: 0.3822, train_f1: 0.2912, val_acc: 0.371795, val_recall: 0.271004, val_f1: 0.272637
Epoch: 81, loss: 25.0619, train_acc: 0.3444, train_recall: 0.3730, train_f1: 0.2462, val_acc: 0.282051, val_recall: 0.210626, val_f1: 0.179779
Epoch: 82, loss: 24.6640, train_acc: 0.3500, train_recall: 0.4048, train_f1: 0.2719, val_acc: 0.371795, val_recall: 0.316937, val_f1: 0.248912
Epoch: 83, loss: 25.3306, train_acc: 0.3667, train_recall: 0.3724, train_f1: 0.3289, val_acc: 0.384615, val_recall: 0.331643, val_f1: 0.241471
Epoch: 84, loss: 24.4292, train_acc: 0.4278, train_recall: 0.3822, train_f1: 0.3563, val_acc: 0.397436, val_recall: 0.322008, val_f1: 0.241289
Epoch: 85, loss: 27.0028, train_acc: 0.3833, train_recall: 0.3156, train_f1: 0.2496, val_acc: 0.423077, val_recall: 0.279412, val_f1: 0.197491
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed0.csv
Epoch: 86, loss: 24.1400, train_acc: 0.4444, train_recall: 0.3991, train_f1: 0.4217, val_acc: 0.358974, val_recall: 0.287803, val_f1: 0.260797
Epoch: 87, loss: 25.1073, train_acc: 0.3722, train_recall: 0.3761, train_f1: 0.3370, val_acc: 0.384615, val_recall: 0.331643, val_f1: 0.239592
Epoch: 88, loss: 25.7000, train_acc: 0.3944, train_recall: 0.4031, train_f1: 0.3519, val_acc: 0.423077, val_recall: 0.375761, val_f1: 0.265625
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed0.csv
Epoch: 89, loss: 26.0527, train_acc: 0.4111, train_recall: 0.3231, train_f1: 0.2708, val_acc: 0.410256, val_recall: 0.293561, val_f1: 0.221783
Epoch: 90, loss: 30.0998, train_acc: 0.3833, train_recall: 0.3125, train_f1: 0.2377, val_acc: 0.397436, val_recall: 0.250000, val_f1: 0.142202
Epoch: 91, loss: 29.7476, train_acc: 0.3833, train_recall: 0.3125, train_f1: 0.2377, val_acc: 0.397436, val_recall: 0.250000, val_f1: 0.142202
Epoch: 92, loss: 29.4395, train_acc: 0.4056, train_recall: 0.3162, train_f1: 0.2498, val_acc: 0.371795, val_recall: 0.250000, val_f1: 0.135514
Epoch: 93, loss: 29.0607, train_acc: 0.4056, train_recall: 0.3162, train_f1: 0.2498, val_acc: 0.371795, val_recall: 0.250000, val_f1: 0.135514
Epoch: 94, loss: 23.7902, train_acc: 0.3944, train_recall: 0.3969, train_f1: 0.3415, val_acc: 0.346154, val_recall: 0.297438, val_f1: 0.214286
Epoch: 95, loss: 26.5592, train_acc: 0.3944, train_recall: 0.3969, train_f1: 0.3415, val_acc: 0.333333, val_recall: 0.289374, val_f1: 0.206712
Epoch: 96, loss: 26.1426, train_acc: 0.3944, train_recall: 0.3969, train_f1: 0.3415, val_acc: 0.346154, val_recall: 0.297438, val_f1: 0.216234
Epoch: 97, loss: 23.8390, train_acc: 0.4278, train_recall: 0.3889, train_f1: 0.3771, val_acc: 0.358974, val_recall: 0.287247, val_f1: 0.256206
Epoch: 98, loss: 28.5125, train_acc: 0.4000, train_recall: 0.3157, train_f1: 0.2265, val_acc: 0.397436, val_recall: 0.279412, val_f1: 0.193408
Epoch: 99, loss: 28.3009, train_acc: 0.3778, train_recall: 0.3768, train_f1: 0.2356, val_acc: 0.371795, val_recall: 0.274341, val_f1: 0.218889
Epoch: 100, loss: 27.0797, train_acc: 0.2056, train_recall: 0.3521, train_f1: 0.1390, val_acc: 0.205128, val_recall: 0.235294, val_f1: 0.094118
Epoch: 101, loss: 24.1432, train_acc: 0.3944, train_recall: 0.4269, train_f1: 0.3129, val_acc: 0.320513, val_recall: 0.261385, val_f1: 0.216732
Epoch: 102, loss: 24.4514, train_acc: 0.4222, train_recall: 0.3419, train_f1: 0.3370, val_acc: 0.371795, val_recall: 0.262105, val_f1: 0.242674
Epoch: 103, loss: 27.5824, train_acc: 0.4167, train_recall: 0.3298, train_f1: 0.2821, val_acc: 0.410256, val_recall: 0.300203, val_f1: 0.221414
Epoch: 104, loss: 25.6117, train_acc: 0.4278, train_recall: 0.3822, train_f1: 0.3563, val_acc: 0.397436, val_recall: 0.322008, val_f1: 0.241289
Epoch: 105, loss: 29.5582, train_acc: 0.2333, train_recall: 0.3272, train_f1: 0.2151, val_acc: 0.217949, val_recall: 0.250000, val_f1: 0.090426
Epoch: 106, loss: 31.5206, train_acc: 0.3944, train_recall: 0.3383, train_f1: 0.2981, val_acc: 0.410256, val_recall: 0.284630, val_f1: 0.218614
Epoch: 107, loss: 31.2692, train_acc: 0.3944, train_recall: 0.3383, train_f1: 0.2981, val_acc: 0.410256, val_recall: 0.284630, val_f1: 0.218614
Epoch: 108, loss: 26.4057, train_acc: 0.3889, train_recall: 0.3344, train_f1: 0.2406, val_acc: 0.346154, val_recall: 0.297438, val_f1: 0.214286
Epoch: 109, loss: 27.2519, train_acc: 0.4167, train_recall: 0.2935, train_f1: 0.2309, val_acc: 0.371795, val_recall: 0.280426, val_f1: 0.206863
Epoch: 110, loss: 28.0121, train_acc: 0.4111, train_recall: 0.2898, train_f1: 0.2232, val_acc: 0.371795, val_recall: 0.280426, val_f1: 0.206863
Epoch: 111, loss: 24.8873, train_acc: 0.3889, train_recall: 0.3992, train_f1: 0.3463, val_acc: 0.371795, val_recall: 0.333491, val_f1: 0.231061
Epoch: 112, loss: 27.5068, train_acc: 0.4000, train_recall: 0.3728, train_f1: 0.3354, val_acc: 0.397436, val_recall: 0.309772, val_f1: 0.238095
Epoch: 113, loss: 25.8042, train_acc: 0.4000, train_recall: 0.3728, train_f1: 0.3354, val_acc: 0.384615, val_recall: 0.301708, val_f1: 0.231157
Epoch: 114, loss: 26.0927, train_acc: 0.4333, train_recall: 0.3889, train_f1: 0.3620, val_acc: 0.397436, val_recall: 0.322008, val_f1: 0.241289
Epoch: 115, loss: 26.9984, train_acc: 0.4167, train_recall: 0.3781, train_f1: 0.3427, val_acc: 0.397436, val_recall: 0.322008, val_f1: 0.241289
Epoch: 116, loss: 26.3787, train_acc: 0.2944, train_recall: 0.3498, train_f1: 0.2928, val_acc: 0.346154, val_recall: 0.330122, val_f1: 0.217432
Epoch: 117, loss: 24.5054, train_acc: 0.4444, train_recall: 0.3674, train_f1: 0.3953, val_acc: 0.371795, val_recall: 0.263773, val_f1: 0.260253
Epoch: 118, loss: 29.8261, train_acc: 0.3833, train_recall: 0.3125, train_f1: 0.2377, val_acc: 0.384615, val_recall: 0.241935, val_f1: 0.138889
Epoch: 119, loss: 28.7033, train_acc: 0.4222, train_recall: 0.3371, train_f1: 0.2592, val_acc: 0.371795, val_recall: 0.233871, val_f1: 0.138095
Epoch: 120, loss: 31.8197, train_acc: 0.3944, train_recall: 0.3091, train_f1: 0.2100, val_acc: 0.371795, val_recall: 0.250000, val_f1: 0.138095
Epoch: 121, loss: 33.3537, train_acc: 0.3000, train_recall: 0.3081, train_f1: 0.1471, val_acc: 0.320513, val_recall: 0.215517, val_f1: 0.140449
Epoch: 122, loss: 30.8643, train_acc: 0.2667, train_recall: 0.2871, train_f1: 0.1448, val_acc: 0.294872, val_recall: 0.198276, val_f1: 0.141975
Epoch: 123, loss: 23.8851, train_acc: 0.3889, train_recall: 0.4232, train_f1: 0.3091, val_acc: 0.307692, val_recall: 0.253321, val_f1: 0.214286
Epoch: 124, loss: 26.6997, train_acc: 0.3944, train_recall: 0.3969, train_f1: 0.3415, val_acc: 0.346154, val_recall: 0.297438, val_f1: 0.214286
Epoch: 125, loss: 28.1065, train_acc: 0.3944, train_recall: 0.3504, train_f1: 0.2472, val_acc: 0.371795, val_recall: 0.333491, val_f1: 0.231061
Epoch: 126, loss: 26.7634, train_acc: 0.3944, train_recall: 0.3103, train_f1: 0.2341, val_acc: 0.384615, val_recall: 0.301708, val_f1: 0.231157
Epoch: 127, loss: 26.5616, train_acc: 0.4222, train_recall: 0.2972, train_f1: 0.2383, val_acc: 0.371795, val_recall: 0.280426, val_f1: 0.206863
Epoch: 128, loss: 26.6821, train_acc: 0.4222, train_recall: 0.3197, train_f1: 0.2556, val_acc: 0.397436, val_recall: 0.322008, val_f1: 0.241289
Epoch: 129, loss: 26.4285, train_acc: 0.3944, train_recall: 0.3504, train_f1: 0.2472, val_acc: 0.371795, val_recall: 0.333491, val_f1: 0.231061
Epoch: 130, loss: 29.4683, train_acc: 0.3778, train_recall: 0.2500, train_f1: 0.1377, val_acc: 0.397436, val_recall: 0.250000, val_f1: 0.142202
Epoch: 131, loss: 26.6121, train_acc: 0.4222, train_recall: 0.2782, train_f1: 0.1922, val_acc: 0.371795, val_recall: 0.233871, val_f1: 0.136792
Epoch: 132, loss: 30.8914, train_acc: 0.4222, train_recall: 0.2870, train_f1: 0.2083, val_acc: 0.410256, val_recall: 0.300203, val_f1: 0.221414
Epoch: 133, loss: 31.9628, train_acc: 0.4167, train_recall: 0.3223, train_f1: 0.2468, val_acc: 0.410256, val_recall: 0.336714, val_f1: 0.252082
Epoch: 134, loss: 35.2393, train_acc: 0.2111, train_recall: 0.2535, train_f1: 0.0932, val_acc: 0.217949, val_recall: 0.250000, val_f1: 0.089474
Epoch: 135, loss: 29.5936, train_acc: 0.4056, train_recall: 0.3185, train_f1: 0.2424, val_acc: 0.410256, val_recall: 0.342799, val_f1: 0.252741
Epoch: 136, loss: 26.5503, train_acc: 0.4333, train_recall: 0.3270, train_f1: 0.2705, val_acc: 0.397436, val_recall: 0.322008, val_f1: 0.241289
Epoch: 137, loss: 33.1845, train_acc: 0.3778, train_recall: 0.2500, train_f1: 0.1377, val_acc: 0.397436, val_recall: 0.250000, val_f1: 0.142202
Epoch: 138, loss: 35.8590, train_acc: 0.3778, train_recall: 0.2500, train_f1: 0.1377, val_acc: 0.397436, val_recall: 0.250000, val_f1: 0.142202
Epoch: 139, loss: 32.2057, train_acc: 0.3778, train_recall: 0.2500, train_f1: 0.1377, val_acc: 0.397436, val_recall: 0.250000, val_f1: 0.142202
Epoch: 140, loss: 25.9008, train_acc: 0.4222, train_recall: 0.3613, train_f1: 0.2950, val_acc: 0.358974, val_recall: 0.318786, val_f1: 0.226750
Epoch: 141, loss: 29.6609, train_acc: 0.4056, train_recall: 0.2862, train_f1: 0.2154, val_acc: 0.371795, val_recall: 0.280426, val_f1: 0.206863
Epoch: 142, loss: 29.4071, train_acc: 0.4056, train_recall: 0.3088, train_f1: 0.2364, val_acc: 0.397436, val_recall: 0.322008, val_f1: 0.241289
Epoch: 143, loss: 28.5286, train_acc: 0.2389, train_recall: 0.2650, train_f1: 0.1367, val_acc: 0.217949, val_recall: 0.243915, val_f1: 0.102582
Epoch: 144, loss: 24.6523, train_acc: 0.4389, train_recall: 0.3369, train_f1: 0.3172, val_acc: 0.358974, val_recall: 0.287803, val_f1: 0.260396
Epoch: 145, loss: 28.2300, train_acc: 0.4167, train_recall: 0.2746, train_f1: 0.1861, val_acc: 0.371795, val_recall: 0.233871, val_f1: 0.136792
Epoch: 146, loss: 26.4858, train_acc: 0.4167, train_recall: 0.2709, train_f1: 0.2337, val_acc: 0.333333, val_recall: 0.215795, val_f1: 0.187723
Epoch: 147, loss: 26.4002, train_acc: 0.4222, train_recall: 0.3334, train_f1: 0.3345, val_acc: 0.333333, val_recall: 0.215795, val_f1: 0.188025
Epoch: 148, loss: 27.6126, train_acc: 0.4278, train_recall: 0.3562, train_f1: 0.3373, val_acc: 0.397436, val_recall: 0.276565, val_f1: 0.216364
Epoch: 149, loss: 24.8731, train_acc: 0.4389, train_recall: 0.3975, train_f1: 0.3853, val_acc: 0.371795, val_recall: 0.293643, val_f1: 0.230600
Epoch: 150, loss: 25.9369, train_acc: 0.4222, train_recall: 0.3884, train_f1: 0.3604, val_acc: 0.410256, val_recall: 0.342799, val_f1: 0.255929
Epoch: 151, loss: 28.4205, train_acc: 0.2167, train_recall: 0.3160, train_f1: 0.1939, val_acc: 0.230769, val_recall: 0.258621, val_f1: 0.108065
Epoch: 152, loss: 27.7791, train_acc: 0.3611, train_recall: 0.3760, train_f1: 0.2413, val_acc: 0.333333, val_recall: 0.254564, val_f1: 0.204779
Epoch: 153, loss: 28.0226, train_acc: 0.1556, train_recall: 0.3545, train_f1: 0.1707, val_acc: 0.141026, val_recall: 0.360548, val_f1: 0.154477
Epoch: 154, loss: 26.9315, train_acc: 0.4278, train_recall: 0.3562, train_f1: 0.3373, val_acc: 0.384615, val_recall: 0.268501, val_f1: 0.212653
Epoch: 155, loss: 27.2578, train_acc: 0.4222, train_recall: 0.3402, train_f1: 0.2988, val_acc: 0.423077, val_recall: 0.286053, val_f1: 0.217060
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed0.csv
Epoch: 156, loss: 27.7942, train_acc: 0.4167, train_recall: 0.3235, train_f1: 0.2649, val_acc: 0.371795, val_recall: 0.250000, val_f1: 0.135514
Epoch: 157, loss: 26.5468, train_acc: 0.4278, train_recall: 0.3597, train_f1: 0.3390, val_acc: 0.371795, val_recall: 0.280426, val_f1: 0.206863
Epoch: 158, loss: 26.9959, train_acc: 0.3833, train_recall: 0.3368, train_f1: 0.2393, val_acc: 0.371795, val_recall: 0.333491, val_f1: 0.231061
Epoch: 159, loss: 28.8215, train_acc: 0.3889, train_recall: 0.3344, train_f1: 0.2406, val_acc: 0.346154, val_recall: 0.297438, val_f1: 0.214286
Epoch: 160, loss: 28.3976, train_acc: 0.4222, train_recall: 0.3441, train_f1: 0.2549, val_acc: 0.410256, val_recall: 0.337761, val_f1: 0.253205
Epoch: 161, loss: 25.4765, train_acc: 0.4500, train_recall: 0.3552, train_f1: 0.2984, val_acc: 0.397436, val_recall: 0.323055, val_f1: 0.247770
Epoch: 162, loss: 29.2989, train_acc: 0.4278, train_recall: 0.2907, train_f1: 0.2162, val_acc: 0.410256, val_recall: 0.300203, val_f1: 0.221414
Epoch: 163, loss: 30.5084, train_acc: 0.4056, train_recall: 0.2604, train_f1: 0.1632, val_acc: 0.397436, val_recall: 0.279412, val_f1: 0.190727
Epoch: 164, loss: 25.8198, train_acc: 0.4333, train_recall: 0.3303, train_f1: 0.2725, val_acc: 0.410256, val_recall: 0.336714, val_f1: 0.252082
Epoch: 165, loss: 27.7629, train_acc: 0.4167, train_recall: 0.3404, train_f1: 0.2519, val_acc: 0.397436, val_recall: 0.329696, val_f1: 0.245826
Epoch: 166, loss: 30.2843, train_acc: 0.3889, train_recall: 0.3344, train_f1: 0.2406, val_acc: 0.346154, val_recall: 0.297438, val_f1: 0.214286
Epoch: 167, loss: 30.2927, train_acc: 0.4000, train_recall: 0.3571, train_f1: 0.2509, val_acc: 0.371795, val_recall: 0.333491, val_f1: 0.231061
Epoch: 168, loss: 29.4003, train_acc: 0.3833, train_recall: 0.2568, train_f1: 0.1515, val_acc: 0.397436, val_recall: 0.256641, val_f1: 0.166503
Epoch: 169, loss: 24.2543, train_acc: 0.4556, train_recall: 0.3487, train_f1: 0.3131, val_acc: 0.423077, val_recall: 0.343666, val_f1: 0.292668
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed0.csv
Epoch: 170, loss: 29.7748, train_acc: 0.4056, train_recall: 0.3162, train_f1: 0.2498, val_acc: 0.371795, val_recall: 0.250000, val_f1: 0.135514
Epoch: 171, loss: 30.0693, train_acc: 0.4056, train_recall: 0.3162, train_f1: 0.2498, val_acc: 0.371795, val_recall: 0.250000, val_f1: 0.135514
Epoch: 172, loss: 25.2492, train_acc: 0.4556, train_recall: 0.4228, train_f1: 0.3930, val_acc: 0.448718, val_recall: 0.386917, val_f1: 0.281091
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed0.csv
Epoch: 173, loss: 25.8555, train_acc: 0.3833, train_recall: 0.3710, train_f1: 0.3296, val_acc: 0.397436, val_recall: 0.309772, val_f1: 0.239168
Epoch: 174, loss: 29.2298, train_acc: 0.3889, train_recall: 0.3193, train_f1: 0.2514, val_acc: 0.397436, val_recall: 0.256641, val_f1: 0.166503
Epoch: 175, loss: 26.1224, train_acc: 0.3944, train_recall: 0.3291, train_f1: 0.2735, val_acc: 0.397436, val_recall: 0.263283, val_f1: 0.184878
Epoch: 176, loss: 27.8736, train_acc: 0.2556, train_recall: 0.3345, train_f1: 0.2495, val_acc: 0.205128, val_recall: 0.235294, val_f1: 0.086022
Epoch: 177, loss: 32.3712, train_acc: 0.4111, train_recall: 0.3229, train_f1: 0.2638, val_acc: 0.397436, val_recall: 0.279412, val_f1: 0.190727
Epoch: 178, loss: 34.3686, train_acc: 0.4000, train_recall: 0.3125, train_f1: 0.2420, val_acc: 0.371795, val_recall: 0.250000, val_f1: 0.136792
Epoch: 179, loss: 30.8287, train_acc: 0.4222, train_recall: 0.3397, train_f1: 0.2817, val_acc: 0.410256, val_recall: 0.300203, val_f1: 0.224330
Epoch: 180, loss: 24.7326, train_acc: 0.3333, train_recall: 0.3816, train_f1: 0.2697, val_acc: 0.333333, val_recall: 0.278905, val_f1: 0.242063
Epoch: 181, loss: 28.7550, train_acc: 0.3389, train_recall: 0.3419, train_f1: 0.1668, val_acc: 0.320513, val_recall: 0.201613, val_f1: 0.126263
Epoch: 182, loss: 29.5605, train_acc: 0.3889, train_recall: 0.3901, train_f1: 0.3104, val_acc: 0.320513, val_recall: 0.274668, val_f1: 0.201449
Epoch: 183, loss: 32.1626, train_acc: 0.2333, train_recall: 0.3272, train_f1: 0.2151, val_acc: 0.217949, val_recall: 0.250000, val_f1: 0.090426
Epoch: 184, loss: 27.8577, train_acc: 0.3611, train_recall: 0.4121, train_f1: 0.2663, val_acc: 0.320513, val_recall: 0.268027, val_f1: 0.217803
Epoch: 185, loss: 24.1152, train_acc: 0.3722, train_recall: 0.4122, train_f1: 0.3002, val_acc: 0.294872, val_recall: 0.245256, val_f1: 0.210084
Epoch: 186, loss: 28.3028, train_acc: 0.4167, train_recall: 0.3523, train_f1: 0.3238, val_acc: 0.371795, val_recall: 0.280426, val_f1: 0.208361
Epoch: 187, loss: 30.1419, train_acc: 0.4167, train_recall: 0.3523, train_f1: 0.3238, val_acc: 0.371795, val_recall: 0.280426, val_f1: 0.208361
Epoch: 188, loss: 27.7201, train_acc: 0.4333, train_recall: 0.3889, train_f1: 0.3620, val_acc: 0.410256, val_recall: 0.336714, val_f1: 0.252877
Epoch: 189, loss: 27.5172, train_acc: 0.2333, train_recall: 0.3272, train_f1: 0.2151, val_acc: 0.217949, val_recall: 0.250000, val_f1: 0.089474
Epoch: 190, loss: 27.8781, train_acc: 0.3944, train_recall: 0.3383, train_f1: 0.2981, val_acc: 0.397436, val_recall: 0.276565, val_f1: 0.215000
Epoch: 191, loss: 28.2917, train_acc: 0.3889, train_recall: 0.3316, train_f1: 0.2882, val_acc: 0.410256, val_recall: 0.284630, val_f1: 0.218614
Epoch: 192, loss: 24.4173, train_acc: 0.4556, train_recall: 0.4010, train_f1: 0.4041, val_acc: 0.397436, val_recall: 0.321452, val_f1: 0.257395
Epoch: 193, loss: 26.5353, train_acc: 0.4389, train_recall: 0.3895, train_f1: 0.3712, val_acc: 0.397436, val_recall: 0.322008, val_f1: 0.241289
Epoch: 194, loss: 26.0884, train_acc: 0.4389, train_recall: 0.3895, train_f1: 0.3712, val_acc: 0.397436, val_recall: 0.322008, val_f1: 0.242945
Epoch: 195, loss: 23.9234, train_acc: 0.4444, train_recall: 0.4057, train_f1: 0.4217, val_acc: 0.384615, val_recall: 0.310574, val_f1: 0.278955
Epoch: 196, loss: 26.2209, train_acc: 0.4111, train_recall: 0.3863, train_f1: 0.3426, val_acc: 0.384615, val_recall: 0.308349, val_f1: 0.237093
Epoch: 197, loss: 25.3630, train_acc: 0.4278, train_recall: 0.4066, train_f1: 0.3560, val_acc: 0.410256, val_recall: 0.337761, val_f1: 0.255865
Epoch: 198, loss: 24.0099, train_acc: 0.4611, train_recall: 0.4304, train_f1: 0.3855, val_acc: 0.384615, val_recall: 0.337728, val_f1: 0.253188
Epoch: 199, loss: 26.4398, train_acc: 0.4333, train_recall: 0.3537, train_f1: 0.2725, val_acc: 0.410256, val_recall: 0.300203, val_f1: 0.230538
Epoch: 200, loss: 25.9988, train_acc: 0.4111, train_recall: 0.3203, train_f1: 0.2079, val_acc: 0.358974, val_recall: 0.241379, val_f1: 0.144330
Epoch: 201, loss: 26.8378, train_acc: 0.3167, train_recall: 0.3272, train_f1: 0.1571, val_acc: 0.294872, val_recall: 0.185484, val_f1: 0.127778
Epoch: 202, loss: 26.7680, train_acc: 0.4222, train_recall: 0.3371, train_f1: 0.2446, val_acc: 0.358974, val_recall: 0.225806, val_f1: 0.134615
Epoch: 203, loss: 24.0675, train_acc: 0.4333, train_recall: 0.4307, train_f1: 0.3962, val_acc: 0.358974, val_recall: 0.318786, val_f1: 0.228868
Epoch: 204, loss: 23.8261, train_acc: 0.4500, train_recall: 0.3750, train_f1: 0.3824, val_acc: 0.397436, val_recall: 0.295999, val_f1: 0.256699
Epoch: 205, loss: 23.6078, train_acc: 0.4556, train_recall: 0.3950, train_f1: 0.4067, val_acc: 0.423077, val_recall: 0.337581, val_f1: 0.292885
Epoch: 206, loss: 23.5401, train_acc: 0.4333, train_recall: 0.4277, train_f1: 0.3955, val_acc: 0.333333, val_recall: 0.282732, val_f1: 0.211403
Epoch: 207, loss: 23.5925, train_acc: 0.4333, train_recall: 0.4277, train_f1: 0.3955, val_acc: 0.320513, val_recall: 0.274668, val_f1: 0.203774
Epoch: 208, loss: 23.3698, train_acc: 0.4667, train_recall: 0.4152, train_f1: 0.4273, val_acc: 0.358974, val_recall: 0.289505, val_f1: 0.241701
Epoch: 209, loss: 23.6169, train_acc: 0.4722, train_recall: 0.4305, train_f1: 0.4146, val_acc: 0.435897, val_recall: 0.372211, val_f1: 0.278298
Epoch: 210, loss: 23.5121, train_acc: 0.4278, train_recall: 0.4271, train_f1: 0.3933, val_acc: 0.358974, val_recall: 0.318786, val_f1: 0.228868
Epoch: 211, loss: 23.9224, train_acc: 0.4500, train_recall: 0.4141, train_f1: 0.3966, val_acc: 0.384615, val_recall: 0.314991, val_f1: 0.242942
Epoch: 212, loss: 23.3287, train_acc: 0.4722, train_recall: 0.4341, train_f1: 0.4222, val_acc: 0.435897, val_recall: 0.378296, val_f1: 0.278889
Epoch: 213, loss: 24.1141, train_acc: 0.4722, train_recall: 0.4304, train_f1: 0.4102, val_acc: 0.448718, val_recall: 0.380832, val_f1: 0.283035
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed0.csv
Epoch: 214, loss: 23.1974, train_acc: 0.4056, train_recall: 0.4000, train_f1: 0.3357, val_acc: 0.346154, val_recall: 0.304080, val_f1: 0.222132
Epoch: 215, loss: 24.6873, train_acc: 0.4389, train_recall: 0.3975, train_f1: 0.3438, val_acc: 0.358974, val_recall: 0.285579, val_f1: 0.227396
Epoch: 216, loss: 24.8679, train_acc: 0.3278, train_recall: 0.3838, train_f1: 0.2520, val_acc: 0.294872, val_recall: 0.245256, val_f1: 0.214835
Epoch: 217, loss: 23.8171, train_acc: 0.4722, train_recall: 0.4304, train_f1: 0.4102, val_acc: 0.435897, val_recall: 0.372211, val_f1: 0.278298
Epoch: 218, loss: 23.3541, train_acc: 0.4611, train_recall: 0.4115, train_f1: 0.4213, val_acc: 0.371795, val_recall: 0.310852, val_f1: 0.241248
Epoch: 219, loss: 24.3800, train_acc: 0.4444, train_recall: 0.4042, train_f1: 0.3910, val_acc: 0.384615, val_recall: 0.308349, val_f1: 0.238636
Epoch: 220, loss: 24.7499, train_acc: 0.4333, train_recall: 0.4252, train_f1: 0.3913, val_acc: 0.397436, val_recall: 0.352434, val_f1: 0.251459
Epoch: 221, loss: 27.1428, train_acc: 0.4444, train_recall: 0.3575, train_f1: 0.3310, val_acc: 0.423077, val_recall: 0.314909, val_f1: 0.236922
Epoch: 222, loss: 25.8975, train_acc: 0.4444, train_recall: 0.2988, train_f1: 0.2417, val_acc: 0.423077, val_recall: 0.314353, val_f1: 0.251930
Epoch: 223, loss: 28.0210, train_acc: 0.4167, train_recall: 0.2746, train_f1: 0.1866, val_acc: 0.371795, val_recall: 0.233871, val_f1: 0.136792
Epoch: 224, loss: 27.6905, train_acc: 0.4111, train_recall: 0.3306, train_f1: 0.2466, val_acc: 0.397436, val_recall: 0.323055, val_f1: 0.244565
Epoch: 225, loss: 30.2653, train_acc: 0.2222, train_recall: 0.2610, train_f1: 0.1076, val_acc: 0.217949, val_recall: 0.250000, val_f1: 0.089474
Epoch: 226, loss: 25.6294, train_acc: 0.4000, train_recall: 0.3515, train_f1: 0.2748, val_acc: 0.410256, val_recall: 0.367140, val_f1: 0.259921
Epoch: 227, loss: 29.7512, train_acc: 0.4167, train_recall: 0.2647, train_f1: 0.1721, val_acc: 0.371795, val_recall: 0.250000, val_f1: 0.135514
Epoch: 228, loss: 30.9656, train_acc: 0.4167, train_recall: 0.2647, train_f1: 0.1715, val_acc: 0.384615, val_recall: 0.258065, val_f1: 0.152417
Epoch: 229, loss: 32.5611, train_acc: 0.4167, train_recall: 0.2746, train_f1: 0.1861, val_acc: 0.384615, val_recall: 0.241935, val_f1: 0.140187
Epoch: 230, loss: 34.2957, train_acc: 0.4222, train_recall: 0.3371, train_f1: 0.2866, val_acc: 0.384615, val_recall: 0.241935, val_f1: 0.140187
Epoch: 231, loss: 30.6837, train_acc: 0.4222, train_recall: 0.3356, train_f1: 0.3170, val_acc: 0.371795, val_recall: 0.236096, val_f1: 0.177646
Epoch: 232, loss: 33.8030, train_acc: 0.4222, train_recall: 0.3272, train_f1: 0.2721, val_acc: 0.371795, val_recall: 0.250000, val_f1: 0.135514
Epoch: 233, loss: 33.0171, train_acc: 0.4167, train_recall: 0.3235, train_f1: 0.2649, val_acc: 0.371795, val_recall: 0.250000, val_f1: 0.135514
Epoch: 234, loss: 27.5299, train_acc: 0.4556, train_recall: 0.3681, train_f1: 0.3515, val_acc: 0.410256, val_recall: 0.299647, val_f1: 0.234327
Epoch: 235, loss: 27.7411, train_acc: 0.4111, train_recall: 0.3863, train_f1: 0.3426, val_acc: 0.397436, val_recall: 0.316414, val_f1: 0.241685
Epoch: 236, loss: 30.1315, train_acc: 0.4278, train_recall: 0.4066, train_f1: 0.3569, val_acc: 0.410256, val_recall: 0.337761, val_f1: 0.253205
Epoch: 237, loss: 34.7617, train_acc: 0.2222, train_recall: 0.3199, train_f1: 0.2009, val_acc: 0.217949, val_recall: 0.250000, val_f1: 0.089474
Epoch: 238, loss: 31.2459, train_acc: 0.2778, train_recall: 0.3412, train_f1: 0.2720, val_acc: 0.243590, val_recall: 0.259488, val_f1: 0.131269
Epoch: 239, loss: 28.4499, train_acc: 0.4111, train_recall: 0.3863, train_f1: 0.3426, val_acc: 0.384615, val_recall: 0.308349, val_f1: 0.237093
Epoch: 240, loss: 26.5216, train_acc: 0.4333, train_recall: 0.3630, train_f1: 0.3312, val_acc: 0.384615, val_recall: 0.268501, val_f1: 0.214021
Epoch: 241, loss: 32.4731, train_acc: 0.4111, train_recall: 0.3199, train_f1: 0.2413, val_acc: 0.371795, val_recall: 0.250000, val_f1: 0.139423
Epoch: 242, loss: 36.1305, train_acc: 0.4056, train_recall: 0.3162, train_f1: 0.2337, val_acc: 0.371795, val_recall: 0.250000, val_f1: 0.139423
Epoch: 243, loss: 34.4928, train_acc: 0.3500, train_recall: 0.3399, train_f1: 0.1726, val_acc: 0.320513, val_recall: 0.215517, val_f1: 0.135870
Epoch: 244, loss: 33.3294, train_acc: 0.0500, train_recall: 0.2713, train_f1: 0.0520, val_acc: 0.051282, val_recall: 0.288032, val_f1: 0.073960
Epoch: 245, loss: 28.9880, train_acc: 0.4222, train_recall: 0.3371, train_f1: 0.2866, val_acc: 0.371795, val_recall: 0.233871, val_f1: 0.136792
Epoch: 246, loss: 28.6281, train_acc: 0.4222, train_recall: 0.3402, train_f1: 0.2985, val_acc: 0.397436, val_recall: 0.263283, val_f1: 0.192055
Epoch: 247, loss: 28.4634, train_acc: 0.4500, train_recall: 0.3056, train_f1: 0.2506, val_acc: 0.410256, val_recall: 0.299647, val_f1: 0.234327
Epoch: 248, loss: 27.7838, train_acc: 0.4333, train_recall: 0.3274, train_f1: 0.2804, val_acc: 0.397436, val_recall: 0.321452, val_f1: 0.253544
Epoch: 249, loss: 27.4895, train_acc: 0.4167, train_recall: 0.3295, train_f1: 0.2931, val_acc: 0.384615, val_recall: 0.310574, val_f1: 0.276030
Epoch: 250, loss: 29.4221, train_acc: 0.3944, train_recall: 0.3380, train_f1: 0.2439, val_acc: 0.358974, val_recall: 0.305503, val_f1: 0.221767
Epoch: 251, loss: 29.8204, train_acc: 0.3833, train_recall: 0.3307, train_f1: 0.2375, val_acc: 0.333333, val_recall: 0.289374, val_f1: 0.206712
Epoch: 252, loss: 27.6881, train_acc: 0.4333, train_recall: 0.3460, train_f1: 0.3152, val_acc: 0.397436, val_recall: 0.325836, val_f1: 0.290761
Epoch: 253, loss: 27.4426, train_acc: 0.4556, train_recall: 0.3424, train_f1: 0.3118, val_acc: 0.397436, val_recall: 0.321452, val_f1: 0.258814
Epoch: 254, loss: 28.1628, train_acc: 0.4500, train_recall: 0.3019, train_f1: 0.2832, val_acc: 0.410256, val_recall: 0.296310, val_f1: 0.282973
Epoch: 255, loss: 29.1755, train_acc: 0.4389, train_recall: 0.2821, train_f1: 0.2391, val_acc: 0.397436, val_recall: 0.261680, val_f1: 0.218803
Epoch: 256, loss: 29.1799, train_acc: 0.4389, train_recall: 0.2821, train_f1: 0.2391, val_acc: 0.397436, val_recall: 0.261680, val_f1: 0.218803
Epoch: 257, loss: 28.5337, train_acc: 0.4389, train_recall: 0.2821, train_f1: 0.2399, val_acc: 0.397436, val_recall: 0.261680, val_f1: 0.218803
Epoch: 258, loss: 27.2136, train_acc: 0.4722, train_recall: 0.3224, train_f1: 0.3093, val_acc: 0.423077, val_recall: 0.311016, val_f1: 0.300349
Epoch: 259, loss: 26.7272, train_acc: 0.4444, train_recall: 0.3657, train_f1: 0.3279, val_acc: 0.384615, val_recall: 0.329942, val_f1: 0.274522
Epoch: 260, loss: 28.8776, train_acc: 0.3722, train_recall: 0.3387, train_f1: 0.2345, val_acc: 0.371795, val_recall: 0.333491, val_f1: 0.230643
Epoch: 261, loss: 29.3053, train_acc: 0.4000, train_recall: 0.2770, train_f1: 0.1865, val_acc: 0.384615, val_recall: 0.248577, val_f1: 0.161792
Epoch: 262, loss: 26.4263, train_acc: 0.4667, train_recall: 0.3716, train_f1: 0.3220, val_acc: 0.435897, val_recall: 0.372211, val_f1: 0.274545
Epoch: 263, loss: 28.3398, train_acc: 0.4500, train_recall: 0.3344, train_f1: 0.2767, val_acc: 0.423077, val_recall: 0.339249, val_f1: 0.255357
Epoch: 264, loss: 27.3163, train_acc: 0.4500, train_recall: 0.3603, train_f1: 0.2912, val_acc: 0.435897, val_recall: 0.372211, val_f1: 0.271892
Epoch: 265, loss: 26.5961, train_acc: 0.4444, train_recall: 0.3670, train_f1: 0.3030, val_acc: 0.410256, val_recall: 0.351044, val_f1: 0.260543
Epoch: 266, loss: 27.1803, train_acc: 0.4444, train_recall: 0.3639, train_f1: 0.3015, val_acc: 0.384615, val_recall: 0.321632, val_f1: 0.242449
/home/ADS/cyang314/ucr_work/HINI_Baseline/GraphLoRA/model/GraphLoRA.py:218: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.
Consider using tensor.detach() first. (Triggered internally at /pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:836.)
  f.write(f'{pre_dataset} to {downstream_dataset}: seed: %d, epoch: %d, train_loss: %f, train_acc: %f, train_recall: %f, train_f1: %f, val_acc: %f, val_recall: %f, val_f1: %f\n' %
Epoch: 267, loss: 26.3674, train_acc: 0.4333, train_recall: 0.3673, train_f1: 0.3155, val_acc: 0.333333, val_recall: 0.308218, val_f1: 0.233943
Epoch: 268, loss: 26.7734, train_acc: 0.4444, train_recall: 0.3341, train_f1: 0.2776, val_acc: 0.410256, val_recall: 0.330629, val_f1: 0.251054
Epoch: 269, loss: 25.9764, train_acc: 0.4444, train_recall: 0.3128, train_f1: 0.2877, val_acc: 0.384615, val_recall: 0.287378, val_f1: 0.248857
Epoch: 270, loss: 26.3058, train_acc: 0.4389, train_recall: 0.2952, train_f1: 0.2252, val_acc: 0.384615, val_recall: 0.249133, val_f1: 0.177406
Epoch: 271, loss: 26.2660, train_acc: 0.4167, train_recall: 0.3553, train_f1: 0.2750, val_acc: 0.397436, val_recall: 0.352434, val_f1: 0.249413
Epoch: 272, loss: 26.5860, train_acc: 0.4500, train_recall: 0.3603, train_f1: 0.2912, val_acc: 0.435897, val_recall: 0.372211, val_f1: 0.271892
Epoch: 273, loss: 25.5970, train_acc: 0.4667, train_recall: 0.3680, train_f1: 0.3132, val_acc: 0.435897, val_recall: 0.372211, val_f1: 0.275417
Epoch: 274, loss: 26.6234, train_acc: 0.4333, train_recall: 0.2949, train_f1: 0.2248, val_acc: 0.371795, val_recall: 0.240512, val_f1: 0.159615
Epoch: 275, loss: 25.7460, train_acc: 0.4278, train_recall: 0.3682, train_f1: 0.2952, val_acc: 0.358974, val_recall: 0.318786, val_f1: 0.226750
Epoch: 276, loss: 25.4737, train_acc: 0.4278, train_recall: 0.3627, train_f1: 0.2899, val_acc: 0.397436, val_recall: 0.352434, val_f1: 0.251459
Epoch: 277, loss: 27.0680, train_acc: 0.4500, train_recall: 0.3085, train_f1: 0.2481, val_acc: 0.423077, val_recall: 0.314909, val_f1: 0.236922
Epoch: 278, loss: 25.8712, train_acc: 0.4667, train_recall: 0.3167, train_f1: 0.2738, val_acc: 0.410256, val_recall: 0.299647, val_f1: 0.237506
Epoch: 279, loss: 28.0131, train_acc: 0.4167, train_recall: 0.2746, train_f1: 0.1866, val_acc: 0.384615, val_recall: 0.241935, val_f1: 0.140187
Epoch: 280, loss: 27.5027, train_acc: 0.4278, train_recall: 0.2912, train_f1: 0.2226, val_acc: 0.423077, val_recall: 0.286053, val_f1: 0.219231
Epoch: 281, loss: 24.5705, train_acc: 0.4667, train_recall: 0.4481, train_f1: 0.4436, val_acc: 0.346154, val_recall: 0.316283, val_f1: 0.248166
Epoch: 282, loss: 25.9600, train_acc: 0.4611, train_recall: 0.4230, train_f1: 0.3939, val_acc: 0.448718, val_recall: 0.380832, val_f1: 0.279320
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed0.csv
Epoch: 283, loss: 26.1504, train_acc: 0.4611, train_recall: 0.4229, train_f1: 0.3903, val_acc: 0.448718, val_recall: 0.380832, val_f1: 0.280154
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed0.csv
Epoch: 284, loss: 24.4238, train_acc: 0.4556, train_recall: 0.4198, train_f1: 0.4013, val_acc: 0.435897, val_recall: 0.372211, val_f1: 0.274545
Epoch: 285, loss: 24.8458, train_acc: 0.4444, train_recall: 0.4042, train_f1: 0.3910, val_acc: 0.384615, val_recall: 0.308349, val_f1: 0.238636
Epoch: 286, loss: 25.6367, train_acc: 0.4389, train_recall: 0.3975, train_f1: 0.3853, val_acc: 0.384615, val_recall: 0.301708, val_f1: 0.235192
Epoch: 287, loss: 23.3638, train_acc: 0.4556, train_recall: 0.4142, train_f1: 0.4096, val_acc: 0.371795, val_recall: 0.300841, val_f1: 0.242268
Epoch: 288, loss: 24.7170, train_acc: 0.4611, train_recall: 0.4229, train_f1: 0.3750, val_acc: 0.435897, val_recall: 0.372211, val_f1: 0.278298
Epoch: 289, loss: 25.5528, train_acc: 0.3667, train_recall: 0.3824, train_f1: 0.2803, val_acc: 0.384615, val_recall: 0.343813, val_f1: 0.247676
Epoch: 290, loss: 24.6427, train_acc: 0.4111, train_recall: 0.3305, train_f1: 0.2586, val_acc: 0.371795, val_recall: 0.256085, val_f1: 0.170635
Epoch: 291, loss: 25.9363, train_acc: 0.3778, train_recall: 0.3088, train_f1: 0.1732, val_acc: 0.358974, val_recall: 0.225806, val_f1: 0.134615
Epoch: 292, loss: 24.7709, train_acc: 0.3389, train_recall: 0.3396, train_f1: 0.2215, val_acc: 0.294872, val_recall: 0.189933, val_f1: 0.180124
Epoch: 293, loss: 25.6287, train_acc: 0.3500, train_recall: 0.3410, train_f1: 0.2056, val_acc: 0.320513, val_recall: 0.215517, val_f1: 0.137363
Epoch: 294, loss: 24.2160, train_acc: 0.3611, train_recall: 0.3547, train_f1: 0.2362, val_acc: 0.320513, val_recall: 0.215517, val_f1: 0.137363
Epoch: 295, loss: 24.1733, train_acc: 0.3833, train_recall: 0.3864, train_f1: 0.2766, val_acc: 0.333333, val_recall: 0.289374, val_f1: 0.212032
Epoch: 296, loss: 25.6574, train_acc: 0.4056, train_recall: 0.4196, train_f1: 0.3519, val_acc: 0.371795, val_recall: 0.333491, val_f1: 0.233179
Epoch: 297, loss: 24.6939, train_acc: 0.4444, train_recall: 0.4042, train_f1: 0.3910, val_acc: 0.384615, val_recall: 0.308349, val_f1: 0.241279
Epoch: 298, loss: 23.6799, train_acc: 0.4778, train_recall: 0.3925, train_f1: 0.4192, val_acc: 0.397436, val_recall: 0.287656, val_f1: 0.291626
Epoch: 299, loss: 25.1863, train_acc: 0.4500, train_recall: 0.3490, train_f1: 0.3208, val_acc: 0.384615, val_recall: 0.264150, val_f1: 0.181667
epoch: 284, train_acc: 0.461111, val_acc: 0.448718, val_recall: 0.380832, val_f1: 0.280154
Running: year=2016 â†’ downstream_year=2017, seed=1
Random seed set to 42

==============================
PRE-TRAINING
==============================
create PreTrain instance...
pre-training...
(T) | Epoch=001, loss=7.8972, this epoch 1.5099, total 1.5099
+++model saved ! 2016.pth
(T) | Epoch=002, loss=7.8960, this epoch 1.4734, total 2.9833
+++model saved ! 2016.pth
(T) | Epoch=003, loss=7.8942, this epoch 1.5121, total 4.4954
+++model saved ! 2016.pth
(T) | Epoch=004, loss=7.8973, this epoch 1.5017, total 5.9971
(T) | Epoch=005, loss=7.8891, this epoch 1.5369, total 7.5340
+++model saved ! 2016.pth
(T) | Epoch=006, loss=7.8855, this epoch 1.4825, total 9.0165
+++model saved ! 2016.pth
(T) | Epoch=007, loss=7.8808, this epoch 1.4232, total 10.4396
+++model saved ! 2016.pth
(T) | Epoch=008, loss=7.8751, this epoch 1.5229, total 11.9626
+++model saved ! 2016.pth
(T) | Epoch=009, loss=7.8690, this epoch 1.5520, total 13.5145
+++model saved ! 2016.pth
(T) | Epoch=010, loss=7.8849, this epoch 1.4844, total 14.9989
(T) | Epoch=011, loss=7.8509, this epoch 1.4746, total 16.4735
+++model saved ! 2016.pth
(T) | Epoch=012, loss=7.8414, this epoch 1.4761, total 17.9496
+++model saved ! 2016.pth
(T) | Epoch=013, loss=7.8309, this epoch 1.4563, total 19.4059
+++model saved ! 2016.pth
(T) | Epoch=014, loss=7.8687, this epoch 1.4670, total 20.8728
(T) | Epoch=015, loss=7.8100, this epoch 1.5090, total 22.3818
+++model saved ! 2016.pth
(T) | Epoch=016, loss=8.9353, this epoch 1.4891, total 23.8710
(T) | Epoch=017, loss=7.7978, this epoch 1.4805, total 25.3515
+++model saved ! 2016.pth
(T) | Epoch=018, loss=7.8477, this epoch 1.5191, total 26.8706
(T) | Epoch=019, loss=8.3957, this epoch 1.4590, total 28.3296
(T) | Epoch=020, loss=7.8094, this epoch 1.4508, total 29.7804
(T) | Epoch=021, loss=8.1407, this epoch 1.5486, total 31.3290
(T) | Epoch=022, loss=8.0648, this epoch 1.5182, total 32.8472
(T) | Epoch=023, loss=7.8291, this epoch 1.5157, total 34.3629
(T) | Epoch=024, loss=7.8650, this epoch 1.5021, total 35.8650
(T) | Epoch=025, loss=7.9818, this epoch 1.4852, total 37.3501
(T) | Epoch=026, loss=7.8394, this epoch 1.4947, total 38.8449
(T) | Epoch=027, loss=7.9440, this epoch 1.4710, total 40.3159
(T) | Epoch=028, loss=7.9133, this epoch 1.4346, total 41.7505
(T) | Epoch=029, loss=7.8532, this epoch 1.5418, total 43.2923
(T) | Epoch=030, loss=7.8385, this epoch 1.5770, total 44.8693
(T) | Epoch=031, loss=7.8355, this epoch 1.5841, total 46.4533
(T) | Epoch=032, loss=7.8339, this epoch 1.6003, total 48.0536
(T) | Epoch=033, loss=7.8334, this epoch 1.5808, total 49.6344
(T) | Epoch=034, loss=7.8813, this epoch 1.5882, total 51.2226
(T) | Epoch=035, loss=7.8675, this epoch 1.5442, total 52.7668
(T) | Epoch=036, loss=7.8694, this epoch 1.5302, total 54.2970
(T) | Epoch=037, loss=7.8295, this epoch 1.5309, total 55.8278
(T) | Epoch=038, loss=7.8461, this epoch 1.5206, total 57.3485
(T) | Epoch=039, loss=7.8733, this epoch 1.4961, total 58.8445
(T) | Epoch=040, loss=7.9009, this epoch 1.4759, total 60.3204
(T) | Epoch=041, loss=7.8444, this epoch 1.5117, total 61.8321
(T) | Epoch=042, loss=7.8238, this epoch 1.4736, total 63.3057
(T) | Epoch=043, loss=7.8633, this epoch 1.4928, total 64.7985
(T) | Epoch=044, loss=7.8205, this epoch 1.4679, total 66.2664
(T) | Epoch=045, loss=7.8970, this epoch 1.5219, total 67.7883
(T) | Epoch=046, loss=7.8833, this epoch 1.4869, total 69.2752
(T) | Epoch=047, loss=7.8183, this epoch 1.4779, total 70.7531
(T) | Epoch=048, loss=7.8181, this epoch 1.5199, total 72.2731
(T) | Epoch=049, loss=7.8869, this epoch 1.4912, total 73.7642
(T) | Epoch=050, loss=7.8172, this epoch 1.4891, total 75.2534
(T) | Epoch=051, loss=7.8158, this epoch 1.4460, total 76.6994
(T) | Epoch=052, loss=7.8338, this epoch 1.5114, total 78.2107
(T) | Epoch=053, loss=7.8124, this epoch 1.4720, total 79.6827
(T) | Epoch=054, loss=7.8297, this epoch 1.4816, total 81.1643
(T) | Epoch=055, loss=7.8500, this epoch 1.4611, total 82.6254
(T) | Epoch=056, loss=7.8050, this epoch 1.5444, total 84.1698
(T) | Epoch=057, loss=7.8455, this epoch 1.5146, total 85.6844
(T) | Epoch=058, loss=7.7608, this epoch 1.4842, total 87.1686
+++model saved ! 2016.pth
(T) | Epoch=059, loss=7.7941, this epoch 1.5021, total 88.6707
(T) | Epoch=060, loss=7.7920, this epoch 1.4942, total 90.1649
(T) | Epoch=061, loss=7.8088, this epoch 1.4732, total 91.6381
(T) | Epoch=062, loss=7.7835, this epoch 1.4499, total 93.0880
(T) | Epoch=063, loss=7.8721, this epoch 1.4939, total 94.5819
(T) | Epoch=064, loss=7.8320, this epoch 1.4724, total 96.0544
(T) | Epoch=065, loss=7.8550, this epoch 1.4783, total 97.5327
(T) | Epoch=066, loss=7.7719, this epoch 1.5017, total 99.0344
(T) | Epoch=067, loss=7.7767, this epoch 1.5030, total 100.5374
(T) | Epoch=068, loss=7.8208, this epoch 1.4978, total 102.0352
(T) | Epoch=069, loss=7.7684, this epoch 1.4982, total 103.5334
(T) | Epoch=070, loss=7.7887, this epoch 1.5127, total 105.0461
(T) | Epoch=071, loss=7.8050, this epoch 1.4505, total 106.4966
(T) | Epoch=072, loss=7.7990, this epoch 1.4770, total 107.9736
(T) | Epoch=073, loss=7.7964, this epoch 1.4957, total 109.4693
(T) | Epoch=074, loss=7.8045, this epoch 1.5090, total 110.9783
(T) | Epoch=075, loss=7.7370, this epoch 1.5445, total 112.5228
+++model saved ! 2016.pth
(T) | Epoch=076, loss=7.7841, this epoch 1.5247, total 114.0475
(T) | Epoch=077, loss=7.7808, this epoch 1.4890, total 115.5365
(T) | Epoch=078, loss=7.7223, this epoch 1.4763, total 117.0127
+++model saved ! 2016.pth
(T) | Epoch=079, loss=7.7388, this epoch 1.4505, total 118.4633
(T) | Epoch=080, loss=7.7078, this epoch 1.5588, total 120.0221
+++model saved ! 2016.pth
(T) | Epoch=081, loss=7.7200, this epoch 1.5256, total 121.5477
(T) | Epoch=082, loss=7.7504, this epoch 1.5234, total 123.0711
(T) | Epoch=083, loss=7.7256, this epoch 1.4843, total 124.5554
(T) | Epoch=084, loss=7.7405, this epoch 1.5005, total 126.0559
(T) | Epoch=085, loss=7.9853, this epoch 1.4908, total 127.5467
(T) | Epoch=086, loss=7.7604, this epoch 1.4587, total 129.0055
(T) | Epoch=087, loss=7.6827, this epoch 1.4724, total 130.4779
+++model saved ! 2016.pth
(T) | Epoch=088, loss=7.6728, this epoch 1.4819, total 131.9598
+++model saved ! 2016.pth
(T) | Epoch=089, loss=7.7032, this epoch 1.4378, total 133.3976
(T) | Epoch=090, loss=7.6639, this epoch 1.4679, total 134.8656
+++model saved ! 2016.pth
(T) | Epoch=091, loss=7.7604, this epoch 1.4831, total 136.3487
(T) | Epoch=092, loss=7.7533, this epoch 1.5146, total 137.8633
(T) | Epoch=093, loss=7.6967, this epoch 1.4598, total 139.3231
(T) | Epoch=094, loss=7.6565, this epoch 1.4428, total 140.7659
+++model saved ! 2016.pth
(T) | Epoch=095, loss=7.7766, this epoch 1.5234, total 142.2893
(T) | Epoch=096, loss=7.7297, this epoch 1.4782, total 143.7675
(T) | Epoch=097, loss=7.6908, this epoch 1.5313, total 145.2988
(T) | Epoch=098, loss=7.7321, this epoch 1.5637, total 146.8625
(T) | Epoch=099, loss=7.6779, this epoch 1.4981, total 148.3606
(T) | Epoch=100, loss=7.6936, this epoch 1.4850, total 149.8456
(T) | Epoch=101, loss=7.7002, this epoch 1.4687, total 151.3143
(T) | Epoch=102, loss=7.6657, this epoch 1.5182, total 152.8325
(T) | Epoch=103, loss=7.6628, this epoch 1.4888, total 154.3213
(T) | Epoch=104, loss=7.6648, this epoch 1.5022, total 155.8234
(T) | Epoch=105, loss=7.8314, this epoch 1.4520, total 157.2754
(T) | Epoch=106, loss=7.6576, this epoch 1.4513, total 158.7267
(T) | Epoch=107, loss=7.9045, this epoch 1.5078, total 160.2346
(T) | Epoch=108, loss=7.6804, this epoch 1.4920, total 161.7266
(T) | Epoch=109, loss=7.6935, this epoch 1.5635, total 163.2901
(T) | Epoch=110, loss=7.6577, this epoch 1.5973, total 164.8874
(T) | Epoch=111, loss=7.6718, this epoch 1.5913, total 166.4787
(T) | Epoch=112, loss=8.1404, this epoch 1.4867, total 167.9654
(T) | Epoch=113, loss=7.6954, this epoch 1.4564, total 169.4218
(T) | Epoch=114, loss=7.7348, this epoch 1.4735, total 170.8953
(T) | Epoch=115, loss=7.6981, this epoch 1.5463, total 172.4416
(T) | Epoch=116, loss=7.9329, this epoch 1.5684, total 174.0101
(T) | Epoch=117, loss=7.9696, this epoch 1.4808, total 175.4909
(T) | Epoch=118, loss=7.7740, this epoch 1.4824, total 176.9733
(T) | Epoch=119, loss=7.7467, this epoch 1.5451, total 178.5185
(T) | Epoch=120, loss=7.8114, this epoch 1.5320, total 180.0505
(T) | Epoch=121, loss=7.7820, this epoch 1.5354, total 181.5859
(T) | Epoch=122, loss=7.8905, this epoch 1.5139, total 183.0998
(T) | Epoch=123, loss=7.8847, this epoch 1.4698, total 184.5696
(T) | Epoch=124, loss=7.7380, this epoch 1.4687, total 186.0383
(T) | Epoch=125, loss=7.7314, this epoch 1.4527, total 187.4910
(T) | Epoch=126, loss=7.7788, this epoch 1.4593, total 188.9503
(T) | Epoch=127, loss=7.7780, this epoch 1.4911, total 190.4414
(T) | Epoch=128, loss=7.7069, this epoch 1.4503, total 191.8917
(T) | Epoch=129, loss=7.6958, this epoch 1.4815, total 193.3732
(T) | Epoch=130, loss=7.6921, this epoch 1.4648, total 194.8380
(T) | Epoch=131, loss=7.7212, this epoch 1.4556, total 196.2936
(T) | Epoch=132, loss=7.6800, this epoch 1.4288, total 197.7224
(T) | Epoch=133, loss=7.6819, this epoch 1.4475, total 199.1699
(T) | Epoch=134, loss=7.7725, this epoch 1.4247, total 200.5946
(T) | Epoch=135, loss=8.0296, this epoch 1.4735, total 202.0682
(T) | Epoch=136, loss=7.7016, this epoch 1.4576, total 203.5257
(T) | Epoch=137, loss=7.7240, this epoch 1.4516, total 204.9774
(T) | Epoch=138, loss=7.6803, this epoch 1.4708, total 206.4482
(T) | Epoch=139, loss=7.6857, this epoch 1.4590, total 207.9072
(T) | Epoch=140, loss=7.7758, this epoch 1.4909, total 209.3981
(T) | Epoch=141, loss=7.6878, this epoch 1.4531, total 210.8512
(T) | Epoch=142, loss=7.6937, this epoch 1.5000, total 212.3512
(T) | Epoch=143, loss=7.6953, this epoch 1.4701, total 213.8213
(T) | Epoch=144, loss=7.6950, this epoch 1.4437, total 215.2650
(T) | Epoch=145, loss=7.6916, this epoch 1.4770, total 216.7420
(T) | Epoch=146, loss=7.8707, this epoch 1.4747, total 218.2167
(T) | Epoch=147, loss=7.9587, this epoch 1.4460, total 219.6627
(T) | Epoch=148, loss=7.7416, this epoch 1.4376, total 221.1004
(T) | Epoch=149, loss=7.6807, this epoch 1.4297, total 222.5301
(T) | Epoch=150, loss=7.6698, this epoch 1.4880, total 224.0181
(T) | Epoch=151, loss=7.7041, this epoch 1.4440, total 225.4621
(T) | Epoch=152, loss=7.6688, this epoch 1.4558, total 226.9180
(T) | Epoch=153, loss=7.6618, this epoch 1.4645, total 228.3825
(T) | Epoch=154, loss=7.7258, this epoch 1.5102, total 229.8926
(T) | Epoch=155, loss=7.6446, this epoch 1.4564, total 231.3490
+++model saved ! 2016.pth
(T) | Epoch=156, loss=7.6350, this epoch 1.4912, total 232.8403
+++model saved ! 2016.pth
(T) | Epoch=157, loss=7.6350, this epoch 1.4793, total 234.3196
+++model saved ! 2016.pth
(T) | Epoch=158, loss=7.6295, this epoch 1.4546, total 235.7741
+++model saved ! 2016.pth
(T) | Epoch=159, loss=7.9806, this epoch 1.4436, total 237.2178
(T) | Epoch=160, loss=7.6389, this epoch 1.4547, total 238.6724
(T) | Epoch=161, loss=7.7209, this epoch 1.5116, total 240.1840
(T) | Epoch=162, loss=7.7459, this epoch 1.4400, total 241.6240
(T) | Epoch=163, loss=7.7678, this epoch 1.4309, total 243.0549
(T) | Epoch=164, loss=7.7021, this epoch 1.4422, total 244.4971
(T) | Epoch=165, loss=7.7692, this epoch 1.4765, total 245.9736
(T) | Epoch=166, loss=7.9191, this epoch 1.4202, total 247.3938
(T) | Epoch=167, loss=7.7395, this epoch 1.4506, total 248.8444
(T) | Epoch=168, loss=7.8386, this epoch 1.4547, total 250.2991
(T) | Epoch=169, loss=7.7457, this epoch 1.4539, total 251.7531
(T) | Epoch=170, loss=7.8090, this epoch 1.4831, total 253.2361
(T) | Epoch=171, loss=7.8330, this epoch 1.5670, total 254.8031
(T) | Epoch=172, loss=7.7389, this epoch 1.5318, total 256.3349
(T) | Epoch=173, loss=7.7918, this epoch 1.4890, total 257.8239
(T) | Epoch=174, loss=7.7982, this epoch 1.5127, total 259.3366
(T) | Epoch=175, loss=7.7083, this epoch 1.4488, total 260.7854
(T) | Epoch=176, loss=7.7061, this epoch 1.4973, total 262.2827
(T) | Epoch=177, loss=7.6974, this epoch 1.4679, total 263.7506
(T) | Epoch=178, loss=7.7521, this epoch 1.5381, total 265.2887
(T) | Epoch=179, loss=7.7281, this epoch 1.4449, total 266.7336
(T) | Epoch=180, loss=7.7283, this epoch 1.5046, total 268.2383
(T) | Epoch=181, loss=7.7021, this epoch 1.4571, total 269.6953
(T) | Epoch=182, loss=7.6751, this epoch 1.4641, total 271.1595
(T) | Epoch=183, loss=7.7060, this epoch 1.4448, total 272.6042
(T) | Epoch=184, loss=7.6700, this epoch 1.4580, total 274.0622
(T) | Epoch=185, loss=7.7197, this epoch 1.4312, total 275.4934
(T) | Epoch=186, loss=7.6522, this epoch 1.4861, total 276.9795
(T) | Epoch=187, loss=7.7096, this epoch 1.4480, total 278.4275
(T) | Epoch=188, loss=7.6425, this epoch 1.4575, total 279.8851
(T) | Epoch=189, loss=7.6440, this epoch 1.4765, total 281.3615
(T) | Epoch=190, loss=7.6702, this epoch 1.4679, total 282.8294
(T) | Epoch=191, loss=7.6335, this epoch 1.4336, total 284.2631
(T) | Epoch=192, loss=7.6225, this epoch 1.4556, total 285.7186
+++model saved ! 2016.pth
(T) | Epoch=193, loss=7.6214, this epoch 1.4754, total 287.1940
+++model saved ! 2016.pth
(T) | Epoch=194, loss=7.6194, this epoch 1.4983, total 288.6923
+++model saved ! 2016.pth
(T) | Epoch=195, loss=7.6500, this epoch 1.4597, total 290.1520
(T) | Epoch=196, loss=7.6092, this epoch 1.4459, total 291.5979
+++model saved ! 2016.pth
(T) | Epoch=197, loss=7.5999, this epoch 1.4335, total 293.0314
+++model saved ! 2016.pth
(T) | Epoch=198, loss=7.6908, this epoch 1.4354, total 294.4669
(T) | Epoch=199, loss=7.6461, this epoch 1.4375, total 295.9044
(T) | Epoch=200, loss=7.6914, this epoch 1.4617, total 297.3661
(T) | Epoch=201, loss=7.7265, this epoch 1.4693, total 298.8354
(T) | Epoch=202, loss=7.6334, this epoch 1.4760, total 300.3114
(T) | Epoch=203, loss=7.6409, this epoch 1.4638, total 301.7752
(T) | Epoch=204, loss=7.6671, this epoch 1.4359, total 303.2110
(T) | Epoch=205, loss=7.6712, this epoch 1.4724, total 304.6834
(T) | Epoch=206, loss=7.6833, this epoch 1.5180, total 306.2014
(T) | Epoch=207, loss=7.7145, this epoch 1.4472, total 307.6486
(T) | Epoch=208, loss=7.6406, this epoch 1.4489, total 309.0975
(T) | Epoch=209, loss=7.6699, this epoch 1.4243, total 310.5218
(T) | Epoch=210, loss=7.6666, this epoch 1.4574, total 311.9793
(T) | Epoch=211, loss=7.6291, this epoch 1.4737, total 313.4530
(T) | Epoch=212, loss=7.7429, this epoch 1.4458, total 314.8988
(T) | Epoch=213, loss=7.6209, this epoch 1.4695, total 316.3683
(T) | Epoch=214, loss=7.6179, this epoch 1.4490, total 317.8173
(T) | Epoch=215, loss=7.6074, this epoch 1.4998, total 319.3171
(T) | Epoch=216, loss=7.7853, this epoch 1.4643, total 320.7814
(T) | Epoch=217, loss=7.6730, this epoch 1.5197, total 322.3011
(T) | Epoch=218, loss=7.6273, this epoch 1.4951, total 323.7963
(T) | Epoch=219, loss=7.6329, this epoch 1.4643, total 325.2606
(T) | Epoch=220, loss=7.6359, this epoch 1.4778, total 326.7383
(T) | Epoch=221, loss=7.7191, this epoch 1.4644, total 328.2027
(T) | Epoch=222, loss=7.6743, this epoch 1.4526, total 329.6554
(T) | Epoch=223, loss=7.6385, this epoch 1.4404, total 331.0958
(T) | Epoch=224, loss=7.6429, this epoch 1.5023, total 332.5981
(T) | Epoch=225, loss=7.6478, this epoch 1.4625, total 334.0606
(T) | Epoch=226, loss=7.6565, this epoch 1.4676, total 335.5282
(T) | Epoch=227, loss=7.7008, this epoch 1.4620, total 336.9901
(T) | Epoch=228, loss=7.6370, this epoch 1.4919, total 338.4821
(T) | Epoch=229, loss=7.6407, this epoch 1.4775, total 339.9596
(T) | Epoch=230, loss=7.6681, this epoch 1.4804, total 341.4400
(T) | Epoch=231, loss=7.6297, this epoch 1.5062, total 342.9462
(T) | Epoch=232, loss=7.6327, this epoch 1.4831, total 344.4293
(T) | Epoch=233, loss=7.6318, this epoch 1.4903, total 345.9196
(T) | Epoch=234, loss=7.6505, this epoch 1.4457, total 347.3653
(T) | Epoch=235, loss=7.6506, this epoch 1.4608, total 348.8261
(T) | Epoch=236, loss=7.6233, this epoch 1.4447, total 350.2708
(T) | Epoch=237, loss=7.6656, this epoch 1.4432, total 351.7139
(T) | Epoch=238, loss=7.6206, this epoch 1.4750, total 353.1889
(T) | Epoch=239, loss=7.6069, this epoch 1.4400, total 354.6289
(T) | Epoch=240, loss=7.6020, this epoch 1.4606, total 356.0895
(T) | Epoch=241, loss=7.6014, this epoch 1.4548, total 357.5443
(T) | Epoch=242, loss=7.5974, this epoch 1.4405, total 358.9849
+++model saved ! 2016.pth
(T) | Epoch=243, loss=7.7122, this epoch 1.4753, total 360.4602
(T) | Epoch=244, loss=7.5825, this epoch 1.4422, total 361.9024
+++model saved ! 2016.pth
(T) | Epoch=245, loss=7.6316, this epoch 1.4122, total 363.3146
(T) | Epoch=246, loss=7.6575, this epoch 1.4400, total 364.7547
(T) | Epoch=247, loss=7.6442, this epoch 1.4478, total 366.2024
(T) | Epoch=248, loss=7.6549, this epoch 1.4560, total 367.6585
(T) | Epoch=249, loss=7.6258, this epoch 1.4598, total 369.1183
(T) | Epoch=250, loss=7.6161, this epoch 1.4381, total 370.5564
(T) | Epoch=251, loss=7.6353, this epoch 1.4507, total 372.0070
(T) | Epoch=252, loss=7.6374, this epoch 1.5049, total 373.5119
(T) | Epoch=253, loss=7.6581, this epoch 1.5287, total 375.0406
(T) | Epoch=254, loss=7.6275, this epoch 1.5098, total 376.5505
(T) | Epoch=255, loss=7.6231, this epoch 1.4778, total 378.0283
(T) | Epoch=256, loss=7.6265, this epoch 1.5077, total 379.5360
(T) | Epoch=257, loss=7.6247, this epoch 1.4452, total 380.9813
(T) | Epoch=258, loss=7.6480, this epoch 1.4701, total 382.4514
(T) | Epoch=259, loss=7.6726, this epoch 1.4641, total 383.9155
(T) | Epoch=260, loss=7.7362, this epoch 1.5228, total 385.4383
(T) | Epoch=261, loss=7.6112, this epoch 1.4746, total 386.9129
(T) | Epoch=262, loss=7.5926, this epoch 1.5057, total 388.4186
(T) | Epoch=263, loss=7.6275, this epoch 1.5780, total 389.9966
(T) | Epoch=264, loss=7.5969, this epoch 1.5599, total 391.5565
(T) | Epoch=265, loss=7.5855, this epoch 1.4883, total 393.0447
(T) | Epoch=266, loss=7.5856, this epoch 1.4685, total 394.5133
(T) | Epoch=267, loss=7.5699, this epoch 1.4269, total 395.9402
+++model saved ! 2016.pth
(T) | Epoch=268, loss=7.6041, this epoch 1.4357, total 397.3759
(T) | Epoch=269, loss=7.5493, this epoch 1.4654, total 398.8413
+++model saved ! 2016.pth
(T) | Epoch=270, loss=7.5517, this epoch 1.4517, total 400.2930
(T) | Epoch=271, loss=7.5466, this epoch 1.4544, total 401.7474
+++model saved ! 2016.pth
(T) | Epoch=272, loss=8.0264, this epoch 1.4570, total 403.2043
(T) | Epoch=273, loss=7.6125, this epoch 1.4477, total 404.6521
(T) | Epoch=274, loss=7.6927, this epoch 1.4842, total 406.1362
(T) | Epoch=275, loss=7.7581, this epoch 1.4864, total 407.6227
(T) | Epoch=276, loss=7.6928, this epoch 1.4914, total 409.1141
(T) | Epoch=277, loss=7.7092, this epoch 1.4705, total 410.5845
(T) | Epoch=278, loss=7.7423, this epoch 1.4635, total 412.0480
(T) | Epoch=279, loss=7.7906, this epoch 1.4595, total 413.5075
(T) | Epoch=280, loss=7.9144, this epoch 1.4387, total 414.9461
(T) | Epoch=281, loss=7.8076, this epoch 1.5439, total 416.4900
(T) | Epoch=282, loss=7.8167, this epoch 1.4547, total 417.9447
(T) | Epoch=283, loss=7.7798, this epoch 1.5148, total 419.4595
(T) | Epoch=284, loss=7.8240, this epoch 1.4586, total 420.9181
(T) | Epoch=285, loss=7.7826, this epoch 1.4691, total 422.3872
(T) | Epoch=286, loss=7.9037, this epoch 1.4612, total 423.8484
(T) | Epoch=287, loss=7.9312, this epoch 1.4586, total 425.3069
(T) | Epoch=288, loss=7.9051, this epoch 1.4174, total 426.7243
(T) | Epoch=289, loss=7.9388, this epoch 1.5331, total 428.2574
(T) | Epoch=290, loss=7.8462, this epoch 1.4599, total 429.7173
(T) | Epoch=291, loss=7.7874, this epoch 1.5633, total 431.2806
(T) | Epoch=292, loss=7.7891, this epoch 1.4742, total 432.7547
(T) | Epoch=293, loss=7.9099, this epoch 1.5085, total 434.2633
(T) | Epoch=294, loss=7.7901, this epoch 1.4676, total 435.7308
(T) | Epoch=295, loss=7.7890, this epoch 1.4459, total 437.1767
(T) | Epoch=296, loss=7.7900, this epoch 1.4609, total 438.6376
(T) | Epoch=297, loss=7.8543, this epoch 1.4349, total 440.0725
(T) | Epoch=298, loss=8.0895, this epoch 1.4238, total 441.4964
(T) | Epoch=299, loss=7.7896, this epoch 1.4221, total 442.9184
(T) | Epoch=300, loss=7.9353, this epoch 1.4922, total 444.4106
(T) | Epoch=301, loss=7.7884, this epoch 1.4293, total 445.8398
(T) | Epoch=302, loss=7.9216, this epoch 1.5755, total 447.4153
(T) | Epoch=303, loss=7.7897, this epoch 1.5208, total 448.9362
(T) | Epoch=304, loss=7.7893, this epoch 1.5022, total 450.4384
(T) | Epoch=305, loss=7.7887, this epoch 1.5280, total 451.9664
(T) | Epoch=306, loss=7.9136, this epoch 1.5417, total 453.5081
(T) | Epoch=307, loss=7.9213, this epoch 1.4346, total 454.9426
(T) | Epoch=308, loss=7.7882, this epoch 1.5138, total 456.4564
(T) | Epoch=309, loss=7.8438, this epoch 1.4378, total 457.8943
(T) | Epoch=310, loss=7.7876, this epoch 1.4305, total 459.3248
(T) | Epoch=311, loss=7.7879, this epoch 1.4393, total 460.7641
(T) | Epoch=312, loss=7.7863, this epoch 1.4245, total 462.1886
(T) | Epoch=313, loss=7.7874, this epoch 1.4945, total 463.6831
(T) | Epoch=314, loss=7.8393, this epoch 1.4989, total 465.1821
(T) | Epoch=315, loss=7.8384, this epoch 1.4461, total 466.6282
(T) | Epoch=316, loss=7.7841, this epoch 1.4810, total 468.1092
(T) | Epoch=317, loss=7.8805, this epoch 1.4663, total 469.5755
(T) | Epoch=318, loss=7.7858, this epoch 1.4411, total 471.0165
(T) | Epoch=319, loss=7.7838, this epoch 1.4655, total 472.4820
(T) | Epoch=320, loss=7.7820, this epoch 1.5170, total 473.9990
(T) | Epoch=321, loss=7.8743, this epoch 1.4561, total 475.4551
(T) | Epoch=322, loss=7.8903, this epoch 1.4169, total 476.8720
(T) | Epoch=323, loss=7.8891, this epoch 1.4292, total 478.3013
(T) | Epoch=324, loss=7.7777, this epoch 1.4341, total 479.7354
(T) | Epoch=325, loss=7.8938, this epoch 1.4468, total 481.1822
(T) | Epoch=326, loss=7.7803, this epoch 1.4629, total 482.6451
(T) | Epoch=327, loss=7.7766, this epoch 1.4501, total 484.0952
(T) | Epoch=328, loss=7.7773, this epoch 1.4190, total 485.5142
(T) | Epoch=329, loss=7.7776, this epoch 1.4547, total 486.9689
(T) | Epoch=330, loss=7.8904, this epoch 1.4574, total 488.4264
(T) | Epoch=331, loss=7.8145, this epoch 1.4517, total 489.8781
(T) | Epoch=332, loss=7.7720, this epoch 1.4513, total 491.3294
(T) | Epoch=333, loss=7.8152, this epoch 1.5377, total 492.8672
(T) | Epoch=334, loss=7.7734, this epoch 1.5125, total 494.3797
(T) | Epoch=335, loss=7.7695, this epoch 1.5775, total 495.9571
(T) | Epoch=336, loss=7.7696, this epoch 1.5008, total 497.4579
(T) | Epoch=337, loss=7.8588, this epoch 1.4404, total 498.8983
(T) | Epoch=338, loss=7.8664, this epoch 1.4561, total 500.3544
(T) | Epoch=339, loss=7.8717, this epoch 1.4719, total 501.8262
(T) | Epoch=340, loss=7.8483, this epoch 1.5077, total 503.3339
(T) | Epoch=341, loss=7.7959, this epoch 1.4969, total 504.8309
(T) | Epoch=342, loss=7.7556, this epoch 1.5307, total 506.3616
(T) | Epoch=343, loss=7.9333, this epoch 1.4987, total 507.8603
(T) | Epoch=344, loss=7.8531, this epoch 1.5005, total 509.3607
(T) | Epoch=345, loss=7.9464, this epoch 1.5037, total 510.8644
(T) | Epoch=346, loss=7.7494, this epoch 1.4876, total 512.3520
(T) | Epoch=347, loss=7.8198, this epoch 1.5008, total 513.8528
(T) | Epoch=348, loss=7.8303, this epoch 1.5218, total 515.3746
(T) | Epoch=349, loss=7.8083, this epoch 1.4512, total 516.8258
(T) | Epoch=350, loss=7.7799, this epoch 1.4695, total 518.2953
(T) | Epoch=351, loss=7.7349, this epoch 1.4794, total 519.7747
(T) | Epoch=352, loss=7.8048, this epoch 1.3998, total 521.1745
(T) | Epoch=353, loss=7.7255, this epoch 1.4553, total 522.6299
(T) | Epoch=354, loss=7.7234, this epoch 1.4707, total 524.1006
(T) | Epoch=355, loss=7.7990, this epoch 1.4697, total 525.5703
(T) | Epoch=356, loss=7.7167, this epoch 1.4528, total 527.0231
(T) | Epoch=357, loss=7.7518, this epoch 1.4359, total 528.4590
(T) | Epoch=358, loss=7.7096, this epoch 1.5095, total 529.9685
(T) | Epoch=359, loss=7.7178, this epoch 1.4586, total 531.4271
(T) | Epoch=360, loss=7.7386, this epoch 1.4679, total 532.8950
(T) | Epoch=361, loss=7.6995, this epoch 1.4460, total 534.3410
(T) | Epoch=362, loss=7.7580, this epoch 1.4951, total 535.8360
(T) | Epoch=363, loss=7.7012, this epoch 1.4947, total 537.3308
(T) | Epoch=364, loss=7.7008, this epoch 1.4549, total 538.7857
(T) | Epoch=365, loss=7.7237, this epoch 1.5287, total 540.3144
(T) | Epoch=366, loss=7.6910, this epoch 1.4948, total 541.8092
(T) | Epoch=367, loss=7.7336, this epoch 1.4539, total 543.2630
(T) | Epoch=368, loss=7.6818, this epoch 1.4824, total 544.7455
(T) | Epoch=369, loss=7.8351, this epoch 1.4839, total 546.2293
(T) | Epoch=370, loss=7.7580, this epoch 1.4849, total 547.7143
(T) | Epoch=371, loss=7.6759, this epoch 1.4820, total 549.1963
(T) | Epoch=372, loss=7.6740, this epoch 1.4635, total 550.6598
(T) | Epoch=373, loss=7.6948, this epoch 1.4983, total 552.1581
(T) | Epoch=374, loss=7.6699, this epoch 1.4852, total 553.6433
(T) | Epoch=375, loss=7.6610, this epoch 1.4766, total 555.1199
(T) | Epoch=376, loss=7.8949, this epoch 1.4765, total 556.5964
(T) | Epoch=377, loss=7.6607, this epoch 1.4304, total 558.0268
(T) | Epoch=378, loss=7.6540, this epoch 1.5125, total 559.5393
(T) | Epoch=379, loss=7.6588, this epoch 1.4631, total 561.0023
(T) | Epoch=380, loss=7.6600, this epoch 1.4910, total 562.4933
(T) | Epoch=381, loss=7.6927, this epoch 1.4720, total 563.9652
(T) | Epoch=382, loss=7.6894, this epoch 1.4642, total 565.4294
(T) | Epoch=383, loss=7.6950, this epoch 1.4503, total 566.8797
(T) | Epoch=384, loss=7.6705, this epoch 1.4609, total 568.3407
(T) | Epoch=385, loss=7.6422, this epoch 1.4623, total 569.8029
(T) | Epoch=386, loss=7.6419, this epoch 1.4807, total 571.2837
(T) | Epoch=387, loss=7.6368, this epoch 1.5025, total 572.7861
(T) | Epoch=388, loss=7.6347, this epoch 1.5004, total 574.2866
(T) | Epoch=389, loss=7.6315, this epoch 1.4868, total 575.7733
(T) | Epoch=390, loss=7.6342, this epoch 1.5019, total 577.2753
(T) | Epoch=391, loss=7.6326, this epoch 1.4848, total 578.7600
(T) | Epoch=392, loss=7.6625, this epoch 1.5102, total 580.2703
(T) | Epoch=393, loss=7.6289, this epoch 1.4613, total 581.7316
(T) | Epoch=394, loss=7.6257, this epoch 1.4379, total 583.1696
(T) | Epoch=395, loss=7.6796, this epoch 1.4847, total 584.6542
(T) | Epoch=396, loss=7.6172, this epoch 1.5039, total 586.1581
(T) | Epoch=397, loss=7.6148, this epoch 1.4584, total 587.6165
(T) | Epoch=398, loss=7.6550, this epoch 1.4183, total 589.0348
(T) | Epoch=399, loss=7.6177, this epoch 1.4671, total 590.5019
(T) | Epoch=400, loss=7.6535, this epoch 1.4266, total 591.9284
(T) | Epoch=401, loss=7.6070, this epoch 1.4579, total 593.3863
(T) | Epoch=402, loss=7.6853, this epoch 1.4974, total 594.8837
(T) | Epoch=403, loss=7.7738, this epoch 1.5069, total 596.3906
(T) | Epoch=404, loss=7.6505, this epoch 1.4604, total 597.8510
(T) | Epoch=405, loss=7.6475, this epoch 1.4657, total 599.3167
(T) | Epoch=406, loss=7.6837, this epoch 1.4877, total 600.8044
(T) | Epoch=407, loss=7.6217, this epoch 1.4844, total 602.2888
(T) | Epoch=408, loss=7.6186, this epoch 1.4635, total 603.7523
(T) | Epoch=409, loss=7.6610, this epoch 1.4265, total 605.1788
(T) | Epoch=410, loss=7.6358, this epoch 1.4371, total 606.6160
(T) | Epoch=411, loss=7.6181, this epoch 1.5365, total 608.1525
(T) | Epoch=412, loss=7.6446, this epoch 1.4708, total 609.6233
(T) | Epoch=413, loss=7.6595, this epoch 1.4969, total 611.1202
(T) | Epoch=414, loss=7.6464, this epoch 1.4720, total 612.5922
(T) | Epoch=415, loss=7.6406, this epoch 1.5191, total 614.1113
(T) | Epoch=416, loss=7.6049, this epoch 1.4691, total 615.5804
(T) | Epoch=417, loss=7.6072, this epoch 1.5088, total 617.0892
(T) | Epoch=418, loss=7.6069, this epoch 1.5484, total 618.6375
(T) | Epoch=419, loss=7.6294, this epoch 1.4807, total 620.1183
(T) | Epoch=420, loss=7.5997, this epoch 1.4699, total 621.5881
(T) | Epoch=421, loss=7.6541, this epoch 1.4624, total 623.0505
(T) | Epoch=422, loss=7.6473, this epoch 1.4599, total 624.5105
(T) | Epoch=423, loss=7.6440, this epoch 1.4469, total 625.9574
(T) | Epoch=424, loss=7.6294, this epoch 1.4851, total 627.4425
(T) | Epoch=425, loss=7.5835, this epoch 1.4857, total 628.9282
(T) | Epoch=426, loss=7.6154, this epoch 1.4966, total 630.4248
(T) | Epoch=427, loss=7.5951, this epoch 1.4731, total 631.8979
(T) | Epoch=428, loss=7.6772, this epoch 1.4069, total 633.3047
(T) | Epoch=429, loss=7.5929, this epoch 1.4983, total 634.8030
(T) | Epoch=430, loss=7.5881, this epoch 1.5173, total 636.3203
(T) | Epoch=431, loss=7.5790, this epoch 1.4873, total 637.8076
(T) | Epoch=432, loss=7.6305, this epoch 1.5295, total 639.3372
(T) | Epoch=433, loss=7.5647, this epoch 1.5216, total 640.8588
(T) | Epoch=434, loss=7.5661, this epoch 1.4713, total 642.3301
(T) | Epoch=435, loss=8.8555, this epoch 1.4976, total 643.8277
(T) | Epoch=436, loss=7.5970, this epoch 1.4875, total 645.3152
(T) | Epoch=437, loss=7.6141, this epoch 1.4449, total 646.7600
(T) | Epoch=438, loss=7.6272, this epoch 1.5198, total 648.2799
(T) | Epoch=439, loss=7.6476, this epoch 1.4978, total 649.7776
(T) | Epoch=440, loss=7.6558, this epoch 1.4374, total 651.2150
(T) | Epoch=441, loss=7.6616, this epoch 1.5003, total 652.7153
(T) | Epoch=442, loss=7.7359, this epoch 1.5739, total 654.2892
(T) | Epoch=443, loss=7.6688, this epoch 1.4652, total 655.7544
(T) | Epoch=444, loss=7.7759, this epoch 1.4768, total 657.2312
(T) | Epoch=445, loss=7.6888, this epoch 1.4612, total 658.6923
(T) | Epoch=446, loss=7.6943, this epoch 1.4754, total 660.1678
(T) | Epoch=447, loss=7.6920, this epoch 1.4711, total 661.6389
(T) | Epoch=448, loss=7.7044, this epoch 1.4792, total 663.1181
(T) | Epoch=449, loss=7.7629, this epoch 1.4586, total 664.5767
(T) | Epoch=450, loss=7.7834, this epoch 1.5011, total 666.0779
(T) | Epoch=451, loss=7.7058, this epoch 1.4878, total 667.5657
(T) | Epoch=452, loss=7.7130, this epoch 1.5009, total 669.0666
(T) | Epoch=453, loss=7.7006, this epoch 1.4607, total 670.5273
(T) | Epoch=454, loss=7.8071, this epoch 1.4484, total 671.9757
(T) | Epoch=455, loss=7.7116, this epoch 1.4862, total 673.4619
(T) | Epoch=456, loss=7.6989, this epoch 1.4878, total 674.9497
(T) | Epoch=457, loss=7.7952, this epoch 1.4667, total 676.4163
(T) | Epoch=458, loss=7.7380, this epoch 1.4617, total 677.8781
(T) | Epoch=459, loss=7.6934, this epoch 1.4681, total 679.3462
(T) | Epoch=460, loss=7.7487, this epoch 1.4572, total 680.8034
(T) | Epoch=461, loss=7.6262, this epoch 1.4660, total 682.2694
(T) | Epoch=462, loss=7.6856, this epoch 1.4532, total 683.7225
(T) | Epoch=463, loss=7.7327, this epoch 1.4365, total 685.1591
(T) | Epoch=464, loss=7.7354, this epoch 1.4464, total 686.6054
(T) | Epoch=465, loss=7.6769, this epoch 1.4608, total 688.0662
(T) | Epoch=466, loss=7.6861, this epoch 1.4826, total 689.5488
(T) | Epoch=467, loss=7.6730, this epoch 1.4398, total 690.9886
(T) | Epoch=468, loss=7.7102, this epoch 1.4825, total 692.4711
(T) | Epoch=469, loss=7.7181, this epoch 1.5056, total 693.9766
(T) | Epoch=470, loss=7.6789, this epoch 1.4857, total 695.4623
(T) | Epoch=471, loss=7.6631, this epoch 1.4951, total 696.9574
(T) | Epoch=472, loss=7.6953, this epoch 1.5071, total 698.4645
(T) | Epoch=473, loss=7.6945, this epoch 1.5277, total 699.9922
(T) | Epoch=474, loss=7.6669, this epoch 1.4549, total 701.4471
(T) | Epoch=475, loss=7.7069, this epoch 1.4703, total 702.9173
(T) | Epoch=476, loss=7.6591, this epoch 1.4669, total 704.3843
(T) | Epoch=477, loss=7.8788, this epoch 1.5032, total 705.8875
(T) | Epoch=478, loss=7.6518, this epoch 1.5252, total 707.4127
(T) | Epoch=479, loss=7.6897, this epoch 1.4480, total 708.8607
(T) | Epoch=480, loss=7.6492, this epoch 1.4911, total 710.3517
(T) | Epoch=481, loss=7.6469, this epoch 1.4500, total 711.8017
(T) | Epoch=482, loss=7.6953, this epoch 1.4291, total 713.2308
(T) | Epoch=483, loss=7.6424, this epoch 1.5169, total 714.7477
(T) | Epoch=484, loss=7.6413, this epoch 1.5274, total 716.2751
(T) | Epoch=485, loss=7.6418, this epoch 1.5274, total 717.8025
(T) | Epoch=486, loss=7.6354, this epoch 1.4584, total 719.2609
(T) | Epoch=487, loss=7.6761, this epoch 1.4518, total 720.7127
(T) | Epoch=488, loss=7.6678, this epoch 1.4695, total 722.1821
(T) | Epoch=489, loss=7.6692, this epoch 1.4561, total 723.6382
(T) | Epoch=490, loss=7.6350, this epoch 1.5101, total 725.1483
(T) | Epoch=491, loss=7.7432, this epoch 1.4661, total 726.6144
(T) | Epoch=492, loss=7.6367, this epoch 1.5233, total 728.1377
(T) | Epoch=493, loss=7.6636, this epoch 1.4958, total 729.6334
(T) | Epoch=494, loss=7.6340, this epoch 1.4979, total 731.1314
(T) | Epoch=495, loss=7.7777, this epoch 1.5884, total 732.7197
(T) | Epoch=496, loss=7.6382, this epoch 1.5858, total 734.3055
(T) | Epoch=497, loss=7.6332, this epoch 1.5158, total 735.8214
(T) | Epoch=498, loss=7.6808, this epoch 1.4569, total 737.2782
(T) | Epoch=499, loss=7.6218, this epoch 1.4519, total 738.7302
(T) | Epoch=500, loss=7.6256, this epoch 1.5270, total 740.2572
(T) | Epoch=501, loss=7.6146, this epoch 1.5120, total 741.7692
(T) | Epoch=502, loss=7.6180, this epoch 1.4433, total 743.2125
(T) | Epoch=503, loss=7.6192, this epoch 1.4700, total 744.6825
(T) | Epoch=504, loss=7.6568, this epoch 1.4623, total 746.1448
(T) | Epoch=505, loss=7.6222, this epoch 1.4616, total 747.6065
(T) | Epoch=506, loss=7.6270, this epoch 1.5118, total 749.1182
(T) | Epoch=507, loss=7.7611, this epoch 1.4913, total 750.6095
(T) | Epoch=508, loss=7.6683, this epoch 1.4642, total 752.0738
(T) | Epoch=509, loss=7.6216, this epoch 1.4979, total 753.5717
(T) | Epoch=510, loss=7.6057, this epoch 1.4984, total 755.0701
(T) | Epoch=511, loss=7.5973, this epoch 1.5045, total 756.5745
(T) | Epoch=512, loss=7.6436, this epoch 1.4608, total 758.0353
(T) | Epoch=513, loss=7.6028, this epoch 1.4886, total 759.5239
(T) | Epoch=514, loss=7.6421, this epoch 1.4774, total 761.0013
(T) | Epoch=515, loss=7.6110, this epoch 1.4933, total 762.4946
(T) | Epoch=516, loss=7.5971, this epoch 1.5017, total 763.9964
(T) | Epoch=517, loss=7.5968, this epoch 1.5146, total 765.5110
(T) | Epoch=518, loss=7.5976, this epoch 1.4776, total 766.9885
(T) | Epoch=519, loss=7.6575, this epoch 1.4570, total 768.4455
(T) | Epoch=520, loss=7.5918, this epoch 1.4673, total 769.9128
(T) | Epoch=521, loss=7.8254, this epoch 1.5279, total 771.4407
(T) | Epoch=522, loss=7.5873, this epoch 1.5094, total 772.9501
(T) | Epoch=523, loss=7.6075, this epoch 1.5029, total 774.4530
(T) | Epoch=524, loss=7.5892, this epoch 1.5015, total 775.9545
(T) | Epoch=525, loss=7.5842, this epoch 1.5099, total 777.4644
(T) | Epoch=526, loss=7.5914, this epoch 1.5243, total 778.9886
(T) | Epoch=527, loss=7.5829, this epoch 1.4898, total 780.4784
(T) | Epoch=528, loss=7.6304, this epoch 1.4164, total 781.8948
(T) | Epoch=529, loss=7.6188, this epoch 1.4374, total 783.3323
(T) | Epoch=530, loss=7.5749, this epoch 1.4545, total 784.7868
(T) | Epoch=531, loss=7.5748, this epoch 1.4789, total 786.2657
(T) | Epoch=532, loss=7.5735, this epoch 1.4629, total 787.7286
(T) | Epoch=533, loss=7.5773, this epoch 1.4848, total 789.2134
(T) | Epoch=534, loss=7.5610, this epoch 1.4775, total 790.6909
(T) | Epoch=535, loss=7.5635, this epoch 1.4945, total 792.1854
(T) | Epoch=536, loss=7.5694, this epoch 1.5078, total 793.6932
(T) | Epoch=537, loss=7.5600, this epoch 1.5120, total 795.2052
(T) | Epoch=538, loss=7.5680, this epoch 1.4737, total 796.6790
(T) | Epoch=539, loss=7.6645, this epoch 1.4847, total 798.1636
(T) | Epoch=540, loss=7.5580, this epoch 1.4419, total 799.6055
(T) | Epoch=541, loss=7.6246, this epoch 1.4736, total 801.0791
(T) | Epoch=542, loss=7.6086, this epoch 1.4971, total 802.5762
(T) | Epoch=543, loss=7.5702, this epoch 1.4697, total 804.0459
(T) | Epoch=544, loss=7.5940, this epoch 1.4732, total 805.5191
(T) | Epoch=545, loss=7.6040, this epoch 1.4626, total 806.9816
(T) | Epoch=546, loss=7.6244, this epoch 1.5618, total 808.5435
(T) | Epoch=547, loss=7.5849, this epoch 1.5400, total 810.0835
(T) | Epoch=548, loss=7.6130, this epoch 1.5991, total 811.6826
(T) | Epoch=549, loss=7.5738, this epoch 1.5268, total 813.2094
(T) | Epoch=550, loss=7.5826, this epoch 1.5262, total 814.7357
(T) | Epoch=551, loss=7.6111, this epoch 1.5049, total 816.2406
(T) | Epoch=552, loss=7.5868, this epoch 1.5442, total 817.7848
(T) | Epoch=553, loss=7.5866, this epoch 1.4831, total 819.2679
(T) | Epoch=554, loss=7.5646, this epoch 1.4534, total 820.7213
(T) | Epoch=555, loss=7.5692, this epoch 1.5164, total 822.2377
(T) | Epoch=556, loss=7.6203, this epoch 1.5627, total 823.8004
(T) | Epoch=557, loss=7.5584, this epoch 1.5119, total 825.3123
(T) | Epoch=558, loss=7.5464, this epoch 1.5214, total 826.8337
+++model saved ! 2016.pth
(T) | Epoch=559, loss=7.5851, this epoch 1.4947, total 828.3285
(T) | Epoch=560, loss=7.6410, this epoch 1.5061, total 829.8346
(T) | Epoch=561, loss=7.6574, this epoch 1.4815, total 831.3160
(T) | Epoch=562, loss=7.6625, this epoch 1.4944, total 832.8104
(T) | Epoch=563, loss=7.5632, this epoch 1.5360, total 834.3464
(T) | Epoch=564, loss=7.5562, this epoch 1.5264, total 835.8728
(T) | Epoch=565, loss=7.5854, this epoch 1.5231, total 837.3959
(T) | Epoch=566, loss=7.6233, this epoch 1.4691, total 838.8651
(T) | Epoch=567, loss=7.6616, this epoch 1.4792, total 840.3443
(T) | Epoch=568, loss=7.6135, this epoch 1.4922, total 841.8365
(T) | Epoch=569, loss=7.5859, this epoch 1.5435, total 843.3800
(T) | Epoch=570, loss=7.6316, this epoch 1.4649, total 844.8449
(T) | Epoch=571, loss=7.6261, this epoch 1.5256, total 846.3705
(T) | Epoch=572, loss=7.5836, this epoch 1.5116, total 847.8821
(T) | Epoch=573, loss=7.5971, this epoch 1.4877, total 849.3698
(T) | Epoch=574, loss=7.5732, this epoch 1.5251, total 850.8949
(T) | Epoch=575, loss=7.6232, this epoch 1.4917, total 852.3866
(T) | Epoch=576, loss=7.5735, this epoch 1.5074, total 853.8939
(T) | Epoch=577, loss=7.6003, this epoch 1.4727, total 855.3666
(T) | Epoch=578, loss=7.5478, this epoch 1.4917, total 856.8583
(T) | Epoch=579, loss=7.5747, this epoch 1.5049, total 858.3632
(T) | Epoch=580, loss=7.5865, this epoch 1.4968, total 859.8600
(T) | Epoch=581, loss=7.5419, this epoch 1.5017, total 861.3617
+++model saved ! 2016.pth
(T) | Epoch=582, loss=7.5377, this epoch 1.5385, total 862.9002
+++model saved ! 2016.pth
(T) | Epoch=583, loss=7.5394, this epoch 1.5053, total 864.4054
(T) | Epoch=584, loss=7.5436, this epoch 1.6051, total 866.0106
(T) | Epoch=585, loss=7.6660, this epoch 1.5587, total 867.5692
(T) | Epoch=586, loss=7.6943, this epoch 1.5667, total 869.1359
(T) | Epoch=587, loss=7.5414, this epoch 1.5427, total 870.6786
(T) | Epoch=588, loss=7.5496, this epoch 1.5121, total 872.1907
(T) | Epoch=589, loss=7.6015, this epoch 1.5098, total 873.7006
(T) | Epoch=590, loss=7.5909, this epoch 1.5025, total 875.2031
(T) | Epoch=591, loss=7.5737, this epoch 1.4575, total 876.6606
(T) | Epoch=592, loss=7.5666, this epoch 1.4719, total 878.1325
(T) | Epoch=593, loss=7.6187, this epoch 1.4424, total 879.5749
(T) | Epoch=594, loss=7.5772, this epoch 1.5466, total 881.1215
(T) | Epoch=595, loss=7.5701, this epoch 1.5299, total 882.6514
(T) | Epoch=596, loss=7.5685, this epoch 1.4992, total 884.1506
(T) | Epoch=597, loss=7.7428, this epoch 1.5087, total 885.6593
(T) | Epoch=598, loss=7.6816, this epoch 1.4518, total 887.1112
(T) | Epoch=599, loss=7.5591, this epoch 1.5107, total 888.6218
(T) | Epoch=600, loss=7.5543, this epoch 1.5175, total 890.1393
(T) | Epoch=601, loss=7.5813, this epoch 1.5068, total 891.6461
(T) | Epoch=602, loss=7.5786, this epoch 1.5472, total 893.1933
(T) | Epoch=603, loss=7.5394, this epoch 1.4491, total 894.6424
(T) | Epoch=604, loss=7.5837, this epoch 1.4554, total 896.0978
(T) | Epoch=605, loss=7.6497, this epoch 1.5330, total 897.6308
(T) | Epoch=606, loss=7.6468, this epoch 1.3969, total 899.0278
(T) | Epoch=607, loss=7.5772, this epoch 1.4714, total 900.4992
(T) | Epoch=608, loss=7.5956, this epoch 1.4910, total 901.9902
(T) | Epoch=609, loss=7.5851, this epoch 1.5403, total 903.5305
(T) | Epoch=610, loss=7.5447, this epoch 1.5068, total 905.0374
(T) | Epoch=611, loss=7.5901, this epoch 1.5238, total 906.5611
(T) | Epoch=612, loss=7.5597, this epoch 1.4784, total 908.0395
(T) | Epoch=613, loss=7.5652, this epoch 1.5507, total 909.5902
(T) | Epoch=614, loss=7.6075, this epoch 1.4999, total 911.0900
(T) | Epoch=615, loss=7.5424, this epoch 1.5174, total 912.6075
(T) | Epoch=616, loss=7.5880, this epoch 1.5194, total 914.1268
(T) | Epoch=617, loss=7.5568, this epoch 1.5096, total 915.6364
(T) | Epoch=618, loss=7.9246, this epoch 1.4850, total 917.1214
(T) | Epoch=619, loss=7.5886, this epoch 1.4390, total 918.5604
(T) | Epoch=620, loss=7.5779, this epoch 1.4525, total 920.0129
(T) | Epoch=621, loss=7.6063, this epoch 1.5733, total 921.5862
(T) | Epoch=622, loss=7.7040, this epoch 1.4806, total 923.0668
(T) | Epoch=623, loss=7.6573, this epoch 1.4514, total 924.5182
(T) | Epoch=624, loss=7.6209, this epoch 1.4442, total 925.9625
(T) | Epoch=625, loss=7.7462, this epoch 1.4824, total 927.4449
(T) | Epoch=626, loss=7.7053, this epoch 1.4515, total 928.8964
(T) | Epoch=627, loss=7.7241, this epoch 1.4587, total 930.3551
(T) | Epoch=628, loss=7.6715, this epoch 1.4940, total 931.8491
(T) | Epoch=629, loss=7.7613, this epoch 1.4932, total 933.3422
(T) | Epoch=630, loss=7.6643, this epoch 1.5074, total 934.8497
(T) | Epoch=631, loss=7.6158, this epoch 1.5187, total 936.3684
(T) | Epoch=632, loss=7.6885, this epoch 1.4549, total 937.8233
(T) | Epoch=633, loss=7.7158, this epoch 1.4445, total 939.2678
(T) | Epoch=634, loss=7.6617, this epoch 1.4598, total 940.7276
(T) | Epoch=635, loss=7.6372, this epoch 1.4571, total 942.1847
(T) | Epoch=636, loss=7.5970, this epoch 1.4478, total 943.6325
(T) | Epoch=637, loss=7.5732, this epoch 1.4439, total 945.0764
(T) | Epoch=638, loss=7.5922, this epoch 1.4478, total 946.5242
(T) | Epoch=639, loss=7.5744, this epoch 1.4791, total 948.0032
(T) | Epoch=640, loss=7.5679, this epoch 1.4878, total 949.4911
(T) | Epoch=641, loss=7.5585, this epoch 1.4833, total 950.9744
(T) | Epoch=642, loss=7.5652, this epoch 1.5067, total 952.4810
(T) | Epoch=643, loss=7.5579, this epoch 1.4986, total 953.9796
(T) | Epoch=644, loss=7.5418, this epoch 1.5075, total 955.4871
(T) | Epoch=645, loss=7.5535, this epoch 1.4466, total 956.9337
(T) | Epoch=646, loss=7.6644, this epoch 1.4364, total 958.3700
(T) | Epoch=647, loss=7.5938, this epoch 1.4770, total 959.8470
(T) | Epoch=648, loss=7.5303, this epoch 1.4491, total 961.2961
+++model saved ! 2016.pth
(T) | Epoch=649, loss=7.5369, this epoch 1.4965, total 962.7926
(T) | Epoch=650, loss=7.5302, this epoch 1.4864, total 964.2790
+++model saved ! 2016.pth
(T) | Epoch=651, loss=7.7184, this epoch 1.4523, total 965.7314
(T) | Epoch=652, loss=7.5743, this epoch 1.4435, total 967.1749
(T) | Epoch=653, loss=7.5901, this epoch 1.4622, total 968.6371
(T) | Epoch=654, loss=7.5366, this epoch 1.5100, total 970.1470
(T) | Epoch=655, loss=7.5400, this epoch 1.4779, total 971.6250
(T) | Epoch=656, loss=7.5406, this epoch 1.4996, total 973.1246
(T) | Epoch=657, loss=7.5509, this epoch 1.4867, total 974.6113
(T) | Epoch=658, loss=7.5420, this epoch 1.5253, total 976.1365
(T) | Epoch=659, loss=7.5411, this epoch 1.5324, total 977.6689
(T) | Epoch=660, loss=7.5595, this epoch 1.5346, total 979.2036
(T) | Epoch=661, loss=7.5507, this epoch 1.4688, total 980.6723
(T) | Epoch=662, loss=7.5366, this epoch 1.5231, total 982.1955
(T) | Epoch=663, loss=7.5315, this epoch 1.5584, total 983.7538
(T) | Epoch=664, loss=7.5941, this epoch 1.5941, total 985.3480
(T) | Epoch=665, loss=7.5324, this epoch 1.5736, total 986.9215
(T) | Epoch=666, loss=7.5218, this epoch 1.5118, total 988.4333
+++model saved ! 2016.pth
(T) | Epoch=667, loss=7.5428, this epoch 1.5153, total 989.9487
(T) | Epoch=668, loss=7.5979, this epoch 1.4749, total 991.4236
(T) | Epoch=669, loss=7.5203, this epoch 1.4749, total 992.8985
+++model saved ! 2016.pth
(T) | Epoch=670, loss=7.5354, this epoch 1.4690, total 994.3675
(T) | Epoch=671, loss=7.5235, this epoch 1.5052, total 995.8727
(T) | Epoch=672, loss=7.5090, this epoch 1.4854, total 997.3581
+++model saved ! 2016.pth
(T) | Epoch=673, loss=7.6084, this epoch 1.4874, total 998.8455
(T) | Epoch=674, loss=7.6919, this epoch 1.5160, total 1000.3615
(T) | Epoch=675, loss=7.6284, this epoch 1.5668, total 1001.9283
(T) | Epoch=676, loss=7.5683, this epoch 1.4604, total 1003.3887
(T) | Epoch=677, loss=7.5386, this epoch 1.4927, total 1004.8814
(T) | Epoch=678, loss=7.5375, this epoch 1.5417, total 1006.4231
(T) | Epoch=679, loss=7.6165, this epoch 1.4856, total 1007.9087
(T) | Epoch=680, loss=7.5592, this epoch 1.4592, total 1009.3679
(T) | Epoch=681, loss=7.5891, this epoch 1.5162, total 1010.8842
(T) | Epoch=682, loss=7.6065, this epoch 1.4535, total 1012.3377
(T) | Epoch=683, loss=7.5375, this epoch 1.4576, total 1013.7953
(T) | Epoch=684, loss=7.5520, this epoch 1.4382, total 1015.2334
(T) | Epoch=685, loss=7.5973, this epoch 1.4337, total 1016.6671
(T) | Epoch=686, loss=7.5370, this epoch 1.4687, total 1018.1358
(T) | Epoch=687, loss=7.5928, this epoch 1.4831, total 1019.6189
(T) | Epoch=688, loss=7.5405, this epoch 1.4561, total 1021.0749
(T) | Epoch=689, loss=7.5507, this epoch 1.4666, total 1022.5415
(T) | Epoch=690, loss=7.5025, this epoch 1.4865, total 1024.0280
+++model saved ! 2016.pth
(T) | Epoch=691, loss=7.5100, this epoch 1.4733, total 1025.5013
(T) | Epoch=692, loss=7.5161, this epoch 1.4629, total 1026.9642
(T) | Epoch=693, loss=7.5075, this epoch 1.5001, total 1028.4643
(T) | Epoch=694, loss=7.5037, this epoch 1.4913, total 1029.9556
(T) | Epoch=695, loss=7.6855, this epoch 1.4684, total 1031.4241
(T) | Epoch=696, loss=7.6191, this epoch 1.4429, total 1032.8670
(T) | Epoch=697, loss=7.4859, this epoch 1.4535, total 1034.3205
+++model saved ! 2016.pth
(T) | Epoch=698, loss=7.4998, this epoch 1.4608, total 1035.7814
(T) | Epoch=699, loss=7.6113, this epoch 1.5106, total 1037.2919
(T) | Epoch=700, loss=7.5673, this epoch 1.4901, total 1038.7820
(T) | Epoch=701, loss=7.6014, this epoch 1.4765, total 1040.2586
(T) | Epoch=702, loss=7.5841, this epoch 1.4860, total 1041.7445
(T) | Epoch=703, loss=7.5623, this epoch 1.4724, total 1043.2169
(T) | Epoch=704, loss=7.5112, this epoch 1.4994, total 1044.7163
(T) | Epoch=705, loss=7.5196, this epoch 1.5669, total 1046.2832
(T) | Epoch=706, loss=7.5050, this epoch 1.4798, total 1047.7630
(T) | Epoch=707, loss=7.5091, this epoch 1.4713, total 1049.2343
(T) | Epoch=708, loss=7.5170, this epoch 1.4040, total 1050.6383
(T) | Epoch=709, loss=7.4916, this epoch 1.4541, total 1052.0924
(T) | Epoch=710, loss=7.4894, this epoch 1.4418, total 1053.5342
(T) | Epoch=711, loss=7.4918, this epoch 1.4815, total 1055.0157
(T) | Epoch=712, loss=7.4801, this epoch 1.4672, total 1056.4829
+++model saved ! 2016.pth
(T) | Epoch=713, loss=7.4883, this epoch 1.4659, total 1057.9488
(T) | Epoch=714, loss=7.5451, this epoch 1.4637, total 1059.4125
(T) | Epoch=715, loss=7.5167, this epoch 1.4330, total 1060.8455
(T) | Epoch=716, loss=7.7756, this epoch 1.4616, total 1062.3070
(T) | Epoch=717, loss=7.5639, this epoch 1.4824, total 1063.7895
(T) | Epoch=718, loss=7.5330, this epoch 1.4603, total 1065.2498
(T) | Epoch=719, loss=7.4973, this epoch 1.4821, total 1066.7318
(T) | Epoch=720, loss=7.5384, this epoch 1.4526, total 1068.1844
(T) | Epoch=721, loss=7.5110, this epoch 1.4742, total 1069.6586
(T) | Epoch=722, loss=7.5924, this epoch 1.4616, total 1071.1202
(T) | Epoch=723, loss=7.6101, this epoch 1.4462, total 1072.5664
(T) | Epoch=724, loss=7.6637, this epoch 1.4807, total 1074.0470
(T) | Epoch=725, loss=7.5691, this epoch 1.4925, total 1075.5395
(T) | Epoch=726, loss=7.5562, this epoch 1.4676, total 1077.0071
(T) | Epoch=727, loss=7.5286, this epoch 1.4711, total 1078.4782
(T) | Epoch=728, loss=7.6278, this epoch 1.4624, total 1079.9406
(T) | Epoch=729, loss=7.4906, this epoch 1.4925, total 1081.4331
(T) | Epoch=730, loss=7.5124, this epoch 1.4525, total 1082.8856
(T) | Epoch=731, loss=7.5713, this epoch 1.4606, total 1084.3462
(T) | Epoch=732, loss=7.5772, this epoch 1.4662, total 1085.8124
(T) | Epoch=733, loss=7.4962, this epoch 1.4589, total 1087.2712
(T) | Epoch=734, loss=7.6442, this epoch 1.4592, total 1088.7304
(T) | Epoch=735, loss=7.5288, this epoch 1.4550, total 1090.1854
(T) | Epoch=736, loss=7.6173, this epoch 1.4439, total 1091.6293
(T) | Epoch=737, loss=7.4920, this epoch 1.4487, total 1093.0780
(T) | Epoch=738, loss=7.4910, this epoch 1.4365, total 1094.5146
(T) | Epoch=739, loss=7.4957, this epoch 1.4445, total 1095.9591
(T) | Epoch=740, loss=7.5523, this epoch 1.4884, total 1097.4475
(T) | Epoch=741, loss=7.4833, this epoch 1.4876, total 1098.9351
(T) | Epoch=742, loss=7.4919, this epoch 1.4626, total 1100.3977
(T) | Epoch=743, loss=7.5959, this epoch 1.4718, total 1101.8694
(T) | Epoch=744, loss=7.5345, this epoch 1.4841, total 1103.3536
(T) | Epoch=745, loss=7.5529, this epoch 1.4593, total 1104.8129
(T) | Epoch=746, loss=7.6533, this epoch 1.4677, total 1106.2806
(T) | Epoch=747, loss=7.5002, this epoch 1.4660, total 1107.7466
(T) | Epoch=748, loss=7.6444, this epoch 1.4857, total 1109.2323
(T) | Epoch=749, loss=7.5042, this epoch 1.4812, total 1110.7135
(T) | Epoch=750, loss=7.4901, this epoch 1.5009, total 1112.2144
(T) | Epoch=751, loss=7.4888, this epoch 1.5405, total 1113.7549
(T) | Epoch=752, loss=7.4750, this epoch 1.4943, total 1115.2491
+++model saved ! 2016.pth
(T) | Epoch=753, loss=7.5470, this epoch 1.4968, total 1116.7459
(T) | Epoch=754, loss=7.5540, this epoch 1.4872, total 1118.2330
(T) | Epoch=755, loss=7.4729, this epoch 1.4983, total 1119.7314
+++model saved ! 2016.pth
(T) | Epoch=756, loss=7.5052, this epoch 1.5492, total 1121.2805
(T) | Epoch=757, loss=7.4819, this epoch 1.5290, total 1122.8095
(T) | Epoch=758, loss=7.6917, this epoch 1.5256, total 1124.3351
(T) | Epoch=759, loss=7.5317, this epoch 1.5138, total 1125.8489
(T) | Epoch=760, loss=7.4822, this epoch 1.4878, total 1127.3367
(T) | Epoch=761, loss=7.4894, this epoch 1.4698, total 1128.8065
(T) | Epoch=762, loss=7.5406, this epoch 1.4798, total 1130.2864
(T) | Epoch=763, loss=7.5323, this epoch 1.4919, total 1131.7783
(T) | Epoch=764, loss=7.6193, this epoch 1.4951, total 1133.2734
(T) | Epoch=765, loss=7.5374, this epoch 1.4966, total 1134.7700
(T) | Epoch=766, loss=7.5751, this epoch 1.4954, total 1136.2654
(T) | Epoch=767, loss=7.5834, this epoch 1.4894, total 1137.7549
(T) | Epoch=768, loss=7.7935, this epoch 1.5112, total 1139.2661
(T) | Epoch=769, loss=7.5117, this epoch 1.5272, total 1140.7933
(T) | Epoch=770, loss=7.5153, this epoch 1.4993, total 1142.2926
(T) | Epoch=771, loss=7.5309, this epoch 1.5402, total 1143.8328
(T) | Epoch=772, loss=7.6173, this epoch 1.5075, total 1145.3403
(T) | Epoch=773, loss=7.5414, this epoch 1.5478, total 1146.8881
(T) | Epoch=774, loss=7.5881, this epoch 1.5386, total 1148.4267
(T) | Epoch=775, loss=7.4857, this epoch 1.5282, total 1149.9549
(T) | Epoch=776, loss=7.4799, this epoch 1.4876, total 1151.4426
(T) | Epoch=777, loss=7.5660, this epoch 1.4927, total 1152.9352
(T) | Epoch=778, loss=7.4705, this epoch 1.4774, total 1154.4126
+++model saved ! 2016.pth
(T) | Epoch=779, loss=7.5744, this epoch 1.4821, total 1155.8947
(T) | Epoch=780, loss=7.4698, this epoch 1.4562, total 1157.3509
+++model saved ! 2016.pth
(T) | Epoch=781, loss=7.5405, this epoch 1.4647, total 1158.8156
(T) | Epoch=782, loss=7.4797, this epoch 1.4372, total 1160.2528
(T) | Epoch=783, loss=7.6766, this epoch 1.4443, total 1161.6971
(T) | Epoch=784, loss=7.4664, this epoch 1.4793, total 1163.1764
+++model saved ! 2016.pth
(T) | Epoch=785, loss=7.5509, this epoch 1.4569, total 1164.6333
(T) | Epoch=786, loss=7.4781, this epoch 1.5067, total 1166.1400
(T) | Epoch=787, loss=7.4693, this epoch 1.4011, total 1167.5411
(T) | Epoch=788, loss=7.4842, this epoch 1.4411, total 1168.9821
(T) | Epoch=789, loss=7.5137, this epoch 1.4548, total 1170.4369
(T) | Epoch=790, loss=7.5145, this epoch 1.4643, total 1171.9012
(T) | Epoch=791, loss=7.6304, this epoch 1.4970, total 1173.3983
(T) | Epoch=792, loss=7.5807, this epoch 1.5181, total 1174.9164
(T) | Epoch=793, loss=7.5676, this epoch 1.5350, total 1176.4514
(T) | Epoch=794, loss=7.5561, this epoch 1.4600, total 1177.9114
(T) | Epoch=795, loss=7.4869, this epoch 1.4448, total 1179.3562
(T) | Epoch=796, loss=7.4931, this epoch 1.5079, total 1180.8641
(T) | Epoch=797, loss=7.5260, this epoch 1.4948, total 1182.3589
(T) | Epoch=798, loss=7.4712, this epoch 1.4969, total 1183.8559
(T) | Epoch=799, loss=7.5479, this epoch 1.5542, total 1185.4101
(T) | Epoch=800, loss=7.4716, this epoch 1.5302, total 1186.9403
(T) | Epoch=801, loss=7.5732, this epoch 1.5433, total 1188.4836
(T) | Epoch=802, loss=7.4689, this epoch 1.5182, total 1190.0018
(T) | Epoch=803, loss=7.4664, this epoch 1.4256, total 1191.4274
+++model saved ! 2016.pth
(T) | Epoch=804, loss=7.4937, this epoch 1.4811, total 1192.9085
(T) | Epoch=805, loss=7.4592, this epoch 1.4375, total 1194.3461
+++model saved ! 2016.pth
(T) | Epoch=806, loss=7.4727, this epoch 1.4818, total 1195.8278
(T) | Epoch=807, loss=7.4544, this epoch 1.4841, total 1197.3119
+++model saved ! 2016.pth
(T) | Epoch=808, loss=7.4552, this epoch 1.5015, total 1198.8134
(T) | Epoch=809, loss=7.4606, this epoch 1.4573, total 1200.2708
(T) | Epoch=810, loss=7.5576, this epoch 1.5241, total 1201.7949
(T) | Epoch=811, loss=7.4504, this epoch 1.4547, total 1203.2496
+++model saved ! 2016.pth
(T) | Epoch=812, loss=7.4696, this epoch 1.4952, total 1204.7448
(T) | Epoch=813, loss=7.4539, this epoch 1.4485, total 1206.1933
(T) | Epoch=814, loss=7.5601, this epoch 1.4517, total 1207.6450
(T) | Epoch=815, loss=7.4713, this epoch 1.5291, total 1209.1741
(T) | Epoch=816, loss=7.4558, this epoch 1.5013, total 1210.6754
(T) | Epoch=817, loss=7.4746, this epoch 1.4737, total 1212.1491
(T) | Epoch=818, loss=7.4876, this epoch 1.4964, total 1213.6455
(T) | Epoch=819, loss=7.4837, this epoch 1.4683, total 1215.1138
(T) | Epoch=820, loss=7.4662, this epoch 1.4635, total 1216.5773
(T) | Epoch=821, loss=7.4507, this epoch 1.5485, total 1218.1258
(T) | Epoch=822, loss=7.5482, this epoch 1.5386, total 1219.6643
(T) | Epoch=823, loss=8.8821, this epoch 1.4548, total 1221.1191
(T) | Epoch=824, loss=7.4677, this epoch 1.4965, total 1222.6156
(T) | Epoch=825, loss=7.5743, this epoch 1.5214, total 1224.1371
(T) | Epoch=826, loss=7.6496, this epoch 1.4871, total 1225.6241
(T) | Epoch=827, loss=7.6390, this epoch 1.4816, total 1227.1057
(T) | Epoch=828, loss=7.5075, this epoch 1.5492, total 1228.6549
(T) | Epoch=829, loss=7.8442, this epoch 1.5368, total 1230.1916
(T) | Epoch=830, loss=7.8455, this epoch 1.5107, total 1231.7024
(T) | Epoch=831, loss=7.6267, this epoch 1.5172, total 1233.2195
(T) | Epoch=832, loss=7.8708, this epoch 1.4514, total 1234.6710
(T) | Epoch=833, loss=7.6390, this epoch 1.4691, total 1236.1401
(T) | Epoch=834, loss=7.6361, this epoch 1.4614, total 1237.6015
(T) | Epoch=835, loss=7.6419, this epoch 1.4559, total 1239.0574
(T) | Epoch=836, loss=7.6333, this epoch 1.4786, total 1240.5360
(T) | Epoch=837, loss=7.6403, this epoch 1.4683, total 1242.0044
(T) | Epoch=838, loss=7.6808, this epoch 1.5236, total 1243.5280
(T) | Epoch=839, loss=7.6436, this epoch 1.4653, total 1244.9932
(T) | Epoch=840, loss=7.8465, this epoch 1.5339, total 1246.5272
(T) | Epoch=841, loss=7.6376, this epoch 1.4817, total 1248.0089
(T) | Epoch=842, loss=7.9338, this epoch 1.4390, total 1249.4479
(T) | Epoch=843, loss=7.6396, this epoch 1.4383, total 1250.8862
(T) | Epoch=844, loss=7.8279, this epoch 1.4906, total 1252.3768
(T) | Epoch=845, loss=7.6721, this epoch 1.4555, total 1253.8323
(T) | Epoch=846, loss=7.6277, this epoch 1.4754, total 1255.3077
(T) | Epoch=847, loss=7.6734, this epoch 1.4723, total 1256.7800
(T) | Epoch=848, loss=7.6208, this epoch 1.4990, total 1258.2790
(T) | Epoch=849, loss=7.6283, this epoch 1.5044, total 1259.7833
(T) | Epoch=850, loss=7.6647, this epoch 1.4376, total 1261.2209
(T) | Epoch=851, loss=7.7263, this epoch 1.5351, total 1262.7560
(T) | Epoch=852, loss=7.6009, this epoch 1.5630, total 1264.3190
(T) | Epoch=853, loss=7.6208, this epoch 1.5667, total 1265.8857
(T) | Epoch=854, loss=7.6169, this epoch 1.5785, total 1267.4642
(T) | Epoch=855, loss=7.5946, this epoch 1.5257, total 1268.9899
(T) | Epoch=856, loss=7.7374, this epoch 1.5628, total 1270.5527
(T) | Epoch=857, loss=7.6576, this epoch 1.5519, total 1272.1046
(T) | Epoch=858, loss=7.5710, this epoch 1.4970, total 1273.6015
(T) | Epoch=859, loss=7.5684, this epoch 1.5689, total 1275.1704
(T) | Epoch=860, loss=7.5653, this epoch 1.5577, total 1276.7282
(T) | Epoch=861, loss=7.6460, this epoch 1.5323, total 1278.2604
(T) | Epoch=862, loss=7.6017, this epoch 1.4617, total 1279.7222
(T) | Epoch=863, loss=7.5994, this epoch 1.4767, total 1281.1988
(T) | Epoch=864, loss=7.5516, this epoch 1.5585, total 1282.7573
(T) | Epoch=865, loss=7.6192, this epoch 1.5484, total 1284.3057
(T) | Epoch=866, loss=7.5535, this epoch 1.4881, total 1285.7937
(T) | Epoch=867, loss=7.6017, this epoch 1.4938, total 1287.2876
(T) | Epoch=868, loss=7.5214, this epoch 1.4813, total 1288.7689
(T) | Epoch=869, loss=7.6487, this epoch 1.5073, total 1290.2761
(T) | Epoch=870, loss=7.5530, this epoch 1.4855, total 1291.7616
(T) | Epoch=871, loss=7.5214, this epoch 1.4959, total 1293.2576
(T) | Epoch=872, loss=7.5198, this epoch 1.4583, total 1294.7159
(T) | Epoch=873, loss=7.5563, this epoch 1.5175, total 1296.2334
(T) | Epoch=874, loss=7.5525, this epoch 1.4725, total 1297.7060
(T) | Epoch=875, loss=7.5070, this epoch 1.5320, total 1299.2380
(T) | Epoch=876, loss=7.5476, this epoch 1.4753, total 1300.7133
(T) | Epoch=877, loss=7.5215, this epoch 1.4860, total 1302.1993
(T) | Epoch=878, loss=7.5331, this epoch 1.5559, total 1303.7552
(T) | Epoch=879, loss=7.5423, this epoch 1.5304, total 1305.2855
(T) | Epoch=880, loss=7.5014, this epoch 1.5006, total 1306.7861
(T) | Epoch=881, loss=7.4963, this epoch 1.4515, total 1308.2376
(T) | Epoch=882, loss=7.4871, this epoch 1.4814, total 1309.7191
(T) | Epoch=883, loss=7.5394, this epoch 1.4374, total 1311.1565
(T) | Epoch=884, loss=7.5239, this epoch 1.4769, total 1312.6334
(T) | Epoch=885, loss=7.6077, this epoch 1.4936, total 1314.1270
(T) | Epoch=886, loss=7.4911, this epoch 1.4648, total 1315.5918
(T) | Epoch=887, loss=7.4757, this epoch 1.5031, total 1317.0949
(T) | Epoch=888, loss=7.5696, this epoch 1.4642, total 1318.5591
(T) | Epoch=889, loss=7.4795, this epoch 1.5187, total 1320.0778
(T) | Epoch=890, loss=7.5284, this epoch 1.5132, total 1321.5910
(T) | Epoch=891, loss=7.4869, this epoch 1.4998, total 1323.0907
(T) | Epoch=892, loss=7.4715, this epoch 1.4593, total 1324.5501
(T) | Epoch=893, loss=7.5612, this epoch 1.4611, total 1326.0112
(T) | Epoch=894, loss=7.5040, this epoch 1.4961, total 1327.5073
(T) | Epoch=895, loss=7.4730, this epoch 1.4989, total 1329.0062
(T) | Epoch=896, loss=7.5167, this epoch 1.4686, total 1330.4748
(T) | Epoch=897, loss=7.4823, this epoch 1.4747, total 1331.9494
(T) | Epoch=898, loss=7.5450, this epoch 1.4658, total 1333.4152
(T) | Epoch=899, loss=7.5723, this epoch 1.4489, total 1334.8642
(T) | Epoch=900, loss=7.4532, this epoch 1.5010, total 1336.3652
(T) | Epoch=901, loss=7.5013, this epoch 1.4720, total 1337.8372
(T) | Epoch=902, loss=7.4777, this epoch 1.4607, total 1339.2979
(T) | Epoch=903, loss=8.0184, this epoch 1.5431, total 1340.8410
(T) | Epoch=904, loss=7.5532, this epoch 1.5144, total 1342.3554
(T) | Epoch=905, loss=7.4580, this epoch 1.4947, total 1343.8501
(T) | Epoch=906, loss=7.4644, this epoch 1.4667, total 1345.3169
(T) | Epoch=907, loss=7.4955, this epoch 1.4675, total 1346.7844
(T) | Epoch=908, loss=7.4946, this epoch 1.5215, total 1348.3059
(T) | Epoch=909, loss=7.4934, this epoch 1.5084, total 1349.8143
(T) | Epoch=910, loss=7.5578, this epoch 1.4686, total 1351.2829
(T) | Epoch=911, loss=7.5203, this epoch 1.5211, total 1352.8040
(T) | Epoch=912, loss=7.4988, this epoch 1.4623, total 1354.2662
(T) | Epoch=913, loss=7.4927, this epoch 1.4539, total 1355.7202
(T) | Epoch=914, loss=7.5270, this epoch 1.4711, total 1357.1913
(T) | Epoch=915, loss=7.4781, this epoch 1.4754, total 1358.6667
(T) | Epoch=916, loss=7.5513, this epoch 1.4655, total 1360.1322
(T) | Epoch=917, loss=7.4864, this epoch 1.5125, total 1361.6446
(T) | Epoch=918, loss=7.5257, this epoch 1.5120, total 1363.1567
(T) | Epoch=919, loss=7.4888, this epoch 1.4912, total 1364.6478
(T) | Epoch=920, loss=7.4680, this epoch 1.5084, total 1366.1562
(T) | Epoch=921, loss=7.5015, this epoch 1.4839, total 1367.6401
(T) | Epoch=922, loss=7.5331, this epoch 1.5121, total 1369.1522
(T) | Epoch=923, loss=7.6029, this epoch 1.4273, total 1370.5795
(T) | Epoch=924, loss=7.4840, this epoch 1.4446, total 1372.0241
(T) | Epoch=925, loss=7.4960, this epoch 1.4841, total 1373.5081
(T) | Epoch=926, loss=7.4580, this epoch 1.5180, total 1375.0261
(T) | Epoch=927, loss=7.4708, this epoch 1.5234, total 1376.5495
(T) | Epoch=928, loss=7.4672, this epoch 1.5541, total 1378.1036
(T) | Epoch=929, loss=7.4598, this epoch 1.5106, total 1379.6142
(T) | Epoch=930, loss=7.5423, this epoch 1.4884, total 1381.1026
(T) | Epoch=931, loss=7.4580, this epoch 1.4612, total 1382.5638
(T) | Epoch=932, loss=7.5290, this epoch 1.4638, total 1384.0276
(T) | Epoch=933, loss=7.5099, this epoch 1.5081, total 1385.5357
(T) | Epoch=934, loss=7.4667, this epoch 1.5221, total 1387.0578
(T) | Epoch=935, loss=7.4673, this epoch 1.5015, total 1388.5593
(T) | Epoch=936, loss=7.4829, this epoch 1.4591, total 1390.0184
(T) | Epoch=937, loss=7.5459, this epoch 1.5289, total 1391.5472
(T) | Epoch=938, loss=7.4652, this epoch 1.5298, total 1393.0771
(T) | Epoch=939, loss=7.4647, this epoch 1.4287, total 1394.5058
(T) | Epoch=940, loss=7.5075, this epoch 1.5012, total 1396.0070
(T) | Epoch=941, loss=7.4650, this epoch 1.5507, total 1397.5577
(T) | Epoch=942, loss=7.4524, this epoch 1.5299, total 1399.0876
(T) | Epoch=943, loss=7.5999, this epoch 1.5157, total 1400.6034
(T) | Epoch=944, loss=7.4609, this epoch 1.5358, total 1402.1392
(T) | Epoch=945, loss=7.4690, this epoch 1.5502, total 1403.6893
(T) | Epoch=946, loss=7.4562, this epoch 1.5100, total 1405.1994
(T) | Epoch=947, loss=7.5178, this epoch 1.5100, total 1406.7094
(T) | Epoch=948, loss=7.4710, this epoch 1.4919, total 1408.2013
(T) | Epoch=949, loss=7.4487, this epoch 1.5378, total 1409.7391
+++model saved ! 2016.pth
(T) | Epoch=950, loss=7.4660, this epoch 1.4923, total 1411.2314
(T) | Epoch=951, loss=7.4575, this epoch 1.4761, total 1412.7076
(T) | Epoch=952, loss=7.4778, this epoch 1.5391, total 1414.2467
(T) | Epoch=953, loss=7.4722, this epoch 1.4754, total 1415.7220
(T) | Epoch=954, loss=7.4648, this epoch 1.4746, total 1417.1966
(T) | Epoch=955, loss=7.4652, this epoch 1.5040, total 1418.7006
(T) | Epoch=956, loss=7.4660, this epoch 1.4765, total 1420.1771
(T) | Epoch=957, loss=7.4707, this epoch 1.5082, total 1421.6854
(T) | Epoch=958, loss=7.4581, this epoch 1.5357, total 1423.2210
(T) | Epoch=959, loss=7.4598, this epoch 1.5188, total 1424.7399
(T) | Epoch=960, loss=7.5022, this epoch 1.4903, total 1426.2302
(T) | Epoch=961, loss=7.4473, this epoch 1.4496, total 1427.6798
+++model saved ! 2016.pth
(T) | Epoch=962, loss=7.4643, this epoch 1.5016, total 1429.1814
(T) | Epoch=963, loss=7.4682, this epoch 1.5145, total 1430.6959
(T) | Epoch=964, loss=7.4684, this epoch 1.4668, total 1432.1627
(T) | Epoch=965, loss=7.5104, this epoch 1.4831, total 1433.6458
(T) | Epoch=966, loss=7.4625, this epoch 1.5033, total 1435.1490
(T) | Epoch=967, loss=7.4656, this epoch 1.4688, total 1436.6178
(T) | Epoch=968, loss=7.5318, this epoch 1.4534, total 1438.0712
(T) | Epoch=969, loss=7.4919, this epoch 1.5013, total 1439.5726
(T) | Epoch=970, loss=7.4457, this epoch 1.4918, total 1441.0643
+++model saved ! 2016.pth
(T) | Epoch=971, loss=7.4625, this epoch 1.5057, total 1442.5700
(T) | Epoch=972, loss=7.5440, this epoch 1.4676, total 1444.0376
(T) | Epoch=973, loss=8.0167, this epoch 1.5614, total 1445.5990
(T) | Epoch=974, loss=7.4560, this epoch 1.5966, total 1447.1956
(T) | Epoch=975, loss=7.4629, this epoch 1.4652, total 1448.6609
(T) | Epoch=976, loss=7.5489, this epoch 1.4659, total 1450.1268
(T) | Epoch=977, loss=7.4848, this epoch 1.4595, total 1451.5862
(T) | Epoch=978, loss=7.5312, this epoch 1.4453, total 1453.0315
(T) | Epoch=979, loss=7.6225, this epoch 1.4496, total 1454.4811
(T) | Epoch=980, loss=7.5117, this epoch 1.4548, total 1455.9360
(T) | Epoch=981, loss=7.6630, this epoch 1.4762, total 1457.4122
(T) | Epoch=982, loss=7.5768, this epoch 1.4812, total 1458.8934
(T) | Epoch=983, loss=7.5710, this epoch 1.4941, total 1460.3875
(T) | Epoch=984, loss=7.5170, this epoch 1.5130, total 1461.9004
(T) | Epoch=985, loss=7.5734, this epoch 1.4857, total 1463.3861
(T) | Epoch=986, loss=7.5337, this epoch 1.4479, total 1464.8340
(T) | Epoch=987, loss=7.5398, this epoch 1.4899, total 1466.3239
(T) | Epoch=988, loss=7.5815, this epoch 1.5004, total 1467.8243
(T) | Epoch=989, loss=7.5682, this epoch 1.4319, total 1469.2562
(T) | Epoch=990, loss=7.5196, this epoch 1.4641, total 1470.7203
(T) | Epoch=991, loss=7.4958, this epoch 1.4660, total 1472.1863
(T) | Epoch=992, loss=7.7848, this epoch 1.4873, total 1473.6736
(T) | Epoch=993, loss=7.4975, this epoch 1.4751, total 1475.1487
(T) | Epoch=994, loss=7.5405, this epoch 1.4730, total 1476.6217
(T) | Epoch=995, loss=7.5651, this epoch 1.4882, total 1478.1100
(T) | Epoch=996, loss=7.5036, this epoch 1.4540, total 1479.5640
(T) | Epoch=997, loss=7.4839, this epoch 1.4829, total 1481.0469
(T) | Epoch=998, loss=7.4669, this epoch 1.4707, total 1482.5176
(T) | Epoch=999, loss=7.5263, this epoch 1.4472, total 1483.9648
(T) | Epoch=1000, loss=7.4754, this epoch 1.5072, total 1485.4720
=== Final ===

==============================
LoRA FINE-TUNING
==============================
Random seed set to 1
Epoch: 0, loss: 39.7156, train_acc: 0.0000, train_recall: 0.0000, train_f1: 0.0000, val_acc: 0.000000, val_recall: 0.000000, val_f1: 0.000000
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed1.csv
Epoch: 1, loss: 35.2203, train_acc: 0.0000, train_recall: 0.0000, train_f1: 0.0000, val_acc: 0.000000, val_recall: 0.000000, val_f1: 0.000000
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed1.csv
Epoch: 2, loss: 30.5880, train_acc: 0.3833, train_recall: 0.2500, train_f1: 0.1386, val_acc: 0.397436, val_recall: 0.250000, val_f1: 0.142202
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed1.csv
Epoch: 3, loss: 65.2363, train_acc: 0.2056, train_recall: 0.2500, train_f1: 0.0853, val_acc: 0.217949, val_recall: 0.250000, val_f1: 0.089474
Epoch: 4, loss: 100.9905, train_acc: 0.1167, train_recall: 0.3107, train_f1: 0.0835, val_acc: 0.089744, val_recall: 0.301724, val_f1: 0.069699
Epoch: 5, loss: 136.2606, train_acc: 0.3889, train_recall: 0.2500, train_f1: 0.1400, val_acc: 0.371795, val_recall: 0.250000, val_f1: 0.135514
Epoch: 6, loss: 133.1425, train_acc: 0.3889, train_recall: 0.2500, train_f1: 0.1400, val_acc: 0.371795, val_recall: 0.250000, val_f1: 0.135514
Epoch: 7, loss: 108.7159, train_acc: 0.3889, train_recall: 0.2500, train_f1: 0.1400, val_acc: 0.371795, val_recall: 0.250000, val_f1: 0.135514
Epoch: 8, loss: 72.2404, train_acc: 0.3889, train_recall: 0.2500, train_f1: 0.1400, val_acc: 0.371795, val_recall: 0.250000, val_f1: 0.135514
Epoch: 9, loss: 53.7895, train_acc: 0.3833, train_recall: 0.2500, train_f1: 0.1386, val_acc: 0.397436, val_recall: 0.250000, val_f1: 0.142202
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed1.csv
Epoch: 10, loss: 69.2541, train_acc: 0.3833, train_recall: 0.2500, train_f1: 0.1386, val_acc: 0.397436, val_recall: 0.250000, val_f1: 0.142202
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed1.csv
Epoch: 11, loss: 69.6493, train_acc: 0.3833, train_recall: 0.2500, train_f1: 0.1386, val_acc: 0.397436, val_recall: 0.250000, val_f1: 0.142202
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed1.csv
Epoch: 12, loss: 58.9664, train_acc: 0.3833, train_recall: 0.2500, train_f1: 0.1386, val_acc: 0.397436, val_recall: 0.250000, val_f1: 0.142202
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed1.csv
Epoch: 13, loss: 73.1934, train_acc: 0.1111, train_recall: 0.1081, train_f1: 0.0851, val_acc: 0.102564, val_recall: 0.094118, val_f1: 0.084211
Epoch: 14, loss: 85.5962, train_acc: 0.2056, train_recall: 0.2500, train_f1: 0.0853, val_acc: 0.217949, val_recall: 0.250000, val_f1: 0.089474
Epoch: 15, loss: 79.3873, train_acc: 0.2056, train_recall: 0.2500, train_f1: 0.0853, val_acc: 0.217949, val_recall: 0.250000, val_f1: 0.089474
Epoch: 16, loss: 59.4724, train_acc: 0.2056, train_recall: 0.2500, train_f1: 0.0853, val_acc: 0.217949, val_recall: 0.250000, val_f1: 0.089474
Epoch: 17, loss: 40.3102, train_acc: 0.3889, train_recall: 0.2500, train_f1: 0.1400, val_acc: 0.371795, val_recall: 0.250000, val_f1: 0.135514
Epoch: 18, loss: 46.0774, train_acc: 0.3889, train_recall: 0.2500, train_f1: 0.1400, val_acc: 0.371795, val_recall: 0.250000, val_f1: 0.135514
Epoch: 19, loss: 43.8638, train_acc: 0.3889, train_recall: 0.2500, train_f1: 0.1400, val_acc: 0.371795, val_recall: 0.250000, val_f1: 0.135514
Epoch: 20, loss: 44.3647, train_acc: 0.3833, train_recall: 0.2500, train_f1: 0.1386, val_acc: 0.397436, val_recall: 0.250000, val_f1: 0.142202
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed1.csv
Epoch: 21, loss: 49.8099, train_acc: 0.3833, train_recall: 0.2500, train_f1: 0.1386, val_acc: 0.397436, val_recall: 0.250000, val_f1: 0.142202
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed1.csv
Epoch: 22, loss: 47.8342, train_acc: 0.3833, train_recall: 0.2500, train_f1: 0.1386, val_acc: 0.397436, val_recall: 0.250000, val_f1: 0.142202
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed1.csv
Epoch: 23, loss: 40.6255, train_acc: 0.3722, train_recall: 0.2399, train_f1: 0.1856, val_acc: 0.371795, val_recall: 0.248331, val_f1: 0.170153
Epoch: 24, loss: 40.8398, train_acc: 0.3889, train_recall: 0.2500, train_f1: 0.1400, val_acc: 0.371795, val_recall: 0.250000, val_f1: 0.135514
Epoch: 25, loss: 37.4794, train_acc: 0.3833, train_recall: 0.2500, train_f1: 0.1386, val_acc: 0.397436, val_recall: 0.250000, val_f1: 0.142202
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed1.csv
Epoch: 26, loss: 32.8910, train_acc: 0.3889, train_recall: 0.2502, train_f1: 0.1576, val_acc: 0.358974, val_recall: 0.241379, val_f1: 0.132075
Epoch: 27, loss: 30.2981, train_acc: 0.3833, train_recall: 0.2500, train_f1: 0.1386, val_acc: 0.397436, val_recall: 0.250000, val_f1: 0.142202
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed1.csv
Epoch: 28, loss: 28.8825, train_acc: 0.3111, train_recall: 0.2734, train_f1: 0.2102, val_acc: 0.269231, val_recall: 0.235801, val_f1: 0.171774
Epoch: 29, loss: 34.6992, train_acc: 0.1389, train_recall: 0.1084, train_f1: 0.1075, val_acc: 0.089744, val_recall: 0.078600, val_f1: 0.069794
Epoch: 30, loss: 34.0834, train_acc: 0.2500, train_recall: 0.2435, train_f1: 0.1577, val_acc: 0.192308, val_recall: 0.196247, val_f1: 0.113991
Epoch: 31, loss: 28.2788, train_acc: 0.4000, train_recall: 0.3208, train_f1: 0.2415, val_acc: 0.397436, val_recall: 0.315923, val_f1: 0.238984
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed1.csv
Epoch: 32, loss: 27.7222, train_acc: 0.3889, train_recall: 0.2526, train_f1: 0.2076, val_acc: 0.384615, val_recall: 0.245273, val_f1: 0.196364
Epoch: 33, loss: 32.0241, train_acc: 0.3833, train_recall: 0.2500, train_f1: 0.1386, val_acc: 0.397436, val_recall: 0.250000, val_f1: 0.142202
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed1.csv
Epoch: 34, loss: 31.2248, train_acc: 0.3722, train_recall: 0.2401, train_f1: 0.1946, val_acc: 0.397436, val_recall: 0.263904, val_f1: 0.201389
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed1.csv
Epoch: 35, loss: 32.8505, train_acc: 0.3778, train_recall: 0.2430, train_f1: 0.1486, val_acc: 0.358974, val_recall: 0.241379, val_f1: 0.132075
Epoch: 36, loss: 30.6502, train_acc: 0.3889, train_recall: 0.2526, train_f1: 0.2076, val_acc: 0.384615, val_recall: 0.245273, val_f1: 0.196364
Epoch: 37, loss: 29.5837, train_acc: 0.3722, train_recall: 0.2419, train_f1: 0.1953, val_acc: 0.397436, val_recall: 0.253337, val_f1: 0.201674
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed1.csv
Epoch: 38, loss: 28.0710, train_acc: 0.3611, train_recall: 0.2327, train_f1: 0.1754, val_acc: 0.371795, val_recall: 0.248331, val_f1: 0.170153
Epoch: 39, loss: 28.0437, train_acc: 0.2556, train_recall: 0.2638, train_f1: 0.1567, val_acc: 0.256410, val_recall: 0.260911, val_f1: 0.148810
Epoch: 40, loss: 27.5771, train_acc: 0.2611, train_recall: 0.2674, train_f1: 0.1612, val_acc: 0.269231, val_recall: 0.268975, val_f1: 0.160129
Epoch: 41, loss: 26.0414, train_acc: 0.2667, train_recall: 0.1723, train_f1: 0.1763, val_acc: 0.333333, val_recall: 0.220801, val_f1: 0.199486
Epoch: 42, loss: 27.8975, train_acc: 0.2889, train_recall: 0.1883, train_f1: 0.1458, val_acc: 0.307692, val_recall: 0.193548, val_f1: 0.137931
Epoch: 43, loss: 28.2772, train_acc: 0.3722, train_recall: 0.2401, train_f1: 0.1946, val_acc: 0.410256, val_recall: 0.272525, val_f1: 0.206687
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed1.csv
Epoch: 44, loss: 28.2149, train_acc: 0.3722, train_recall: 0.2410, train_f1: 0.2101, val_acc: 0.358974, val_recall: 0.234149, val_f1: 0.202649
Epoch: 45, loss: 28.0928, train_acc: 0.3778, train_recall: 0.2455, train_f1: 0.1994, val_acc: 0.397436, val_recall: 0.253337, val_f1: 0.201674
Epoch: 46, loss: 28.5622, train_acc: 0.3889, train_recall: 0.2500, train_f1: 0.1400, val_acc: 0.371795, val_recall: 0.250000, val_f1: 0.135514
Epoch: 47, loss: 25.0624, train_acc: 0.3944, train_recall: 0.3174, train_f1: 0.2507, val_acc: 0.397436, val_recall: 0.315923, val_f1: 0.240292
Epoch: 48, loss: 29.7577, train_acc: 0.2500, train_recall: 0.2696, train_f1: 0.1466, val_acc: 0.230769, val_recall: 0.244782, val_f1: 0.124668
Epoch: 49, loss: 31.4220, train_acc: 0.3833, train_recall: 0.2500, train_f1: 0.1386, val_acc: 0.397436, val_recall: 0.250000, val_f1: 0.142202
Epoch: 50, loss: 31.3759, train_acc: 0.3833, train_recall: 0.2500, train_f1: 0.1386, val_acc: 0.397436, val_recall: 0.250000, val_f1: 0.142202
Epoch: 51, loss: 25.9324, train_acc: 0.3833, train_recall: 0.2500, train_f1: 0.1386, val_acc: 0.397436, val_recall: 0.250000, val_f1: 0.142202
Epoch: 52, loss: 31.4374, train_acc: 0.3944, train_recall: 0.3125, train_f1: 0.2406, val_acc: 0.371795, val_recall: 0.250000, val_f1: 0.135514
Epoch: 53, loss: 33.4100, train_acc: 0.4000, train_recall: 0.3798, train_f1: 0.3392, val_acc: 0.384615, val_recall: 0.307302, val_f1: 0.234675
Epoch: 54, loss: 36.3010, train_acc: 0.2778, train_recall: 0.3458, train_f1: 0.2505, val_acc: 0.230769, val_recall: 0.246450, val_f1: 0.127709
Epoch: 55, loss: 36.1917, train_acc: 0.3889, train_recall: 0.3089, train_f1: 0.2224, val_acc: 0.358974, val_recall: 0.241379, val_f1: 0.132075
Epoch: 56, loss: 32.8463, train_acc: 0.4000, train_recall: 0.3734, train_f1: 0.3206, val_acc: 0.397436, val_recall: 0.315923, val_f1: 0.243295
Epoch: 57, loss: 31.0091, train_acc: 0.2111, train_recall: 0.3125, train_f1: 0.1694, val_acc: 0.217949, val_recall: 0.250000, val_f1: 0.090426
Epoch: 58, loss: 26.7209, train_acc: 0.3778, train_recall: 0.3679, train_f1: 0.2657, val_acc: 0.384615, val_recall: 0.295066, val_f1: 0.243911
Epoch: 59, loss: 30.8568, train_acc: 0.3667, train_recall: 0.2980, train_f1: 0.1652, val_acc: 0.358974, val_recall: 0.225806, val_f1: 0.137255
Epoch: 60, loss: 29.9127, train_acc: 0.3611, train_recall: 0.2944, train_f1: 0.1617, val_acc: 0.358974, val_recall: 0.225806, val_f1: 0.138614
Epoch: 61, loss: 26.8642, train_acc: 0.3667, train_recall: 0.2946, train_f1: 0.1633, val_acc: 0.358974, val_recall: 0.241379, val_f1: 0.141414
Epoch: 62, loss: 27.3581, train_acc: 0.3889, train_recall: 0.3089, train_f1: 0.2224, val_acc: 0.358974, val_recall: 0.241379, val_f1: 0.132075
Epoch: 63, loss: 25.2868, train_acc: 0.4000, train_recall: 0.3793, train_f1: 0.3234, val_acc: 0.423077, val_recall: 0.319260, val_f1: 0.250154
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed1.csv
Epoch: 64, loss: 26.1144, train_acc: 0.2333, train_recall: 0.2917, train_f1: 0.1750, val_acc: 0.179487, val_recall: 0.187627, val_f1: 0.109987
Epoch: 65, loss: 27.0022, train_acc: 0.2556, train_recall: 0.3060, train_f1: 0.2422, val_acc: 0.179487, val_recall: 0.187627, val_f1: 0.103846
Epoch: 66, loss: 25.3060, train_acc: 0.3944, train_recall: 0.3762, train_f1: 0.3206, val_acc: 0.384615, val_recall: 0.307302, val_f1: 0.234675
Epoch: 67, loss: 25.7962, train_acc: 0.3889, train_recall: 0.3125, train_f1: 0.2230, val_acc: 0.397436, val_recall: 0.250000, val_f1: 0.143519
Epoch: 68, loss: 26.2718, train_acc: 0.4056, train_recall: 0.3802, train_f1: 0.3253, val_acc: 0.384615, val_recall: 0.307302, val_f1: 0.236015
Epoch: 69, loss: 25.2891, train_acc: 0.4056, train_recall: 0.3833, train_f1: 0.3272, val_acc: 0.384615, val_recall: 0.307302, val_f1: 0.236015
Epoch: 70, loss: 26.0515, train_acc: 0.4056, train_recall: 0.3860, train_f1: 0.3279, val_acc: 0.423077, val_recall: 0.319260, val_f1: 0.250154
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed1.csv
Epoch: 71, loss: 25.9905, train_acc: 0.4000, train_recall: 0.3824, train_f1: 0.3244, val_acc: 0.423077, val_recall: 0.319260, val_f1: 0.250154
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed1.csv
Epoch: 72, loss: 25.4342, train_acc: 0.3944, train_recall: 0.3762, train_f1: 0.3206, val_acc: 0.384615, val_recall: 0.307302, val_f1: 0.234675
Epoch: 73, loss: 25.5375, train_acc: 0.4056, train_recall: 0.3833, train_f1: 0.3267, val_acc: 0.384615, val_recall: 0.307302, val_f1: 0.236015
Epoch: 74, loss: 24.3639, train_acc: 0.4278, train_recall: 0.3998, train_f1: 0.3572, val_acc: 0.423077, val_recall: 0.320928, val_f1: 0.294248
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed1.csv
Epoch: 75, loss: 25.0398, train_acc: 0.3889, train_recall: 0.3125, train_f1: 0.2117, val_acc: 0.397436, val_recall: 0.250000, val_f1: 0.144860
Epoch: 76, loss: 26.0262, train_acc: 0.3889, train_recall: 0.3726, train_f1: 0.2850, val_acc: 0.384615, val_recall: 0.307302, val_f1: 0.240638
Epoch: 77, loss: 26.8393, train_acc: 0.3944, train_recall: 0.3762, train_f1: 0.3206, val_acc: 0.384615, val_recall: 0.307302, val_f1: 0.234675
Epoch: 78, loss: 25.7426, train_acc: 0.3778, train_recall: 0.3655, train_f1: 0.2751, val_acc: 0.384615, val_recall: 0.307302, val_f1: 0.241066
Epoch: 79, loss: 24.0170, train_acc: 0.4056, train_recall: 0.3860, train_f1: 0.3329, val_acc: 0.410256, val_recall: 0.311195, val_f1: 0.243194
Epoch: 80, loss: 24.9621, train_acc: 0.3944, train_recall: 0.3161, train_f1: 0.2306, val_acc: 0.397436, val_recall: 0.250000, val_f1: 0.143519
Epoch: 81, loss: 28.5140, train_acc: 0.3944, train_recall: 0.3125, train_f1: 0.2406, val_acc: 0.358974, val_recall: 0.241379, val_f1: 0.132075
Epoch: 82, loss: 28.2083, train_acc: 0.3944, train_recall: 0.3125, train_f1: 0.2406, val_acc: 0.371795, val_recall: 0.250000, val_f1: 0.135514
Epoch: 83, loss: 27.1386, train_acc: 0.2111, train_recall: 0.3125, train_f1: 0.1856, val_acc: 0.217949, val_recall: 0.250000, val_f1: 0.089474
Epoch: 84, loss: 26.8797, train_acc: 0.3889, train_recall: 0.3125, train_f1: 0.2391, val_acc: 0.397436, val_recall: 0.250000, val_f1: 0.143519
Epoch: 85, loss: 25.7134, train_acc: 0.3944, train_recall: 0.3161, train_f1: 0.2467, val_acc: 0.397436, val_recall: 0.250000, val_f1: 0.143519
Epoch: 86, loss: 29.0367, train_acc: 0.3944, train_recall: 0.3125, train_f1: 0.2406, val_acc: 0.371795, val_recall: 0.250000, val_f1: 0.135514
Epoch: 87, loss: 29.5605, train_acc: 0.3944, train_recall: 0.3125, train_f1: 0.2406, val_acc: 0.358974, val_recall: 0.241379, val_f1: 0.132075
Epoch: 88, loss: 30.2988, train_acc: 0.2111, train_recall: 0.3125, train_f1: 0.1856, val_acc: 0.217949, val_recall: 0.250000, val_f1: 0.090426
Epoch: 89, loss: 26.0759, train_acc: 0.2111, train_recall: 0.3125, train_f1: 0.1694, val_acc: 0.217949, val_recall: 0.250000, val_f1: 0.090426
Epoch: 90, loss: 26.2090, train_acc: 0.4000, train_recall: 0.3187, train_f1: 0.2966, val_acc: 0.384615, val_recall: 0.245273, val_f1: 0.197931
Epoch: 91, loss: 30.3147, train_acc: 0.3889, train_recall: 0.3089, train_f1: 0.2224, val_acc: 0.358974, val_recall: 0.241379, val_f1: 0.132075
Epoch: 92, loss: 30.3060, train_acc: 0.3833, train_recall: 0.3077, train_f1: 0.2436, val_acc: 0.346154, val_recall: 0.221079, val_f1: 0.191250
Epoch: 93, loss: 31.5932, train_acc: 0.3944, train_recall: 0.3151, train_f1: 0.2926, val_acc: 0.397436, val_recall: 0.253337, val_f1: 0.203270
Epoch: 94, loss: 35.2814, train_acc: 0.3944, train_recall: 0.3125, train_f1: 0.2406, val_acc: 0.358974, val_recall: 0.241379, val_f1: 0.132075
Epoch: 95, loss: 34.7038, train_acc: 0.3944, train_recall: 0.3125, train_f1: 0.2406, val_acc: 0.358974, val_recall: 0.241379, val_f1: 0.132075
Epoch: 96, loss: 30.6904, train_acc: 0.4000, train_recall: 0.3187, train_f1: 0.3123, val_acc: 0.384615, val_recall: 0.245273, val_f1: 0.197931
Epoch: 97, loss: 28.8531, train_acc: 0.4000, train_recall: 0.3187, train_f1: 0.3126, val_acc: 0.384615, val_recall: 0.245273, val_f1: 0.197931
Epoch: 98, loss: 28.7883, train_acc: 0.3944, train_recall: 0.3125, train_f1: 0.2406, val_acc: 0.358974, val_recall: 0.241379, val_f1: 0.132075
Epoch: 99, loss: 24.2747, train_acc: 0.4111, train_recall: 0.3258, train_f1: 0.3058, val_acc: 0.397436, val_recall: 0.253893, val_f1: 0.208162
Epoch: 100, loss: 28.1848, train_acc: 0.2111, train_recall: 0.3125, train_f1: 0.1694, val_acc: 0.217949, val_recall: 0.250000, val_f1: 0.090426
Epoch: 101, loss: 28.3621, train_acc: 0.1611, train_recall: 0.4158, train_f1: 0.1254, val_acc: 0.128205, val_recall: 0.147059, val_f1: 0.106383
Epoch: 102, loss: 25.5253, train_acc: 0.4000, train_recall: 0.3824, train_f1: 0.3403, val_acc: 0.423077, val_recall: 0.319260, val_f1: 0.250154
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed1.csv
Epoch: 103, loss: 25.0003, train_acc: 0.4111, train_recall: 0.3251, train_f1: 0.3300, val_acc: 0.358974, val_recall: 0.231368, val_f1: 0.199365
Epoch: 104, loss: 26.0330, train_acc: 0.4056, train_recall: 0.2626, train_f1: 0.2292, val_acc: 0.358974, val_recall: 0.231368, val_f1: 0.199365
Epoch: 105, loss: 28.2125, train_acc: 0.3889, train_recall: 0.2536, train_f1: 0.1462, val_acc: 0.397436, val_recall: 0.250000, val_f1: 0.142202
Epoch: 106, loss: 26.5202, train_acc: 0.3889, train_recall: 0.2500, train_f1: 0.1400, val_acc: 0.371795, val_recall: 0.250000, val_f1: 0.135514
Epoch: 107, loss: 25.9697, train_acc: 0.4000, train_recall: 0.3235, train_f1: 0.2479, val_acc: 0.423077, val_recall: 0.319260, val_f1: 0.247309
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed1.csv
Epoch: 108, loss: 26.1285, train_acc: 0.4278, train_recall: 0.3566, train_f1: 0.3139, val_acc: 0.358974, val_recall: 0.280050, val_f1: 0.240050
Epoch: 109, loss: 26.2359, train_acc: 0.3944, train_recall: 0.3173, train_f1: 0.2381, val_acc: 0.384615, val_recall: 0.307302, val_f1: 0.231976
Epoch: 110, loss: 25.9764, train_acc: 0.4111, train_recall: 0.3280, train_f1: 0.2482, val_acc: 0.384615, val_recall: 0.307302, val_f1: 0.233170
Epoch: 111, loss: 26.4411, train_acc: 0.4000, train_recall: 0.3204, train_f1: 0.2459, val_acc: 0.423077, val_recall: 0.319260, val_f1: 0.247309
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed1.csv
Epoch: 112, loss: 25.0747, train_acc: 0.4000, train_recall: 0.3227, train_f1: 0.2985, val_acc: 0.371795, val_recall: 0.288670, val_f1: 0.259149
Epoch: 113, loss: 25.7312, train_acc: 0.4000, train_recall: 0.3208, train_f1: 0.2411, val_acc: 0.384615, val_recall: 0.307302, val_f1: 0.233170
Epoch: 114, loss: 24.9799, train_acc: 0.4222, train_recall: 0.3465, train_f1: 0.3154, val_acc: 0.371795, val_recall: 0.288670, val_f1: 0.257713
Epoch: 115, loss: 26.4770, train_acc: 0.3889, train_recall: 0.3100, train_f1: 0.2327, val_acc: 0.423077, val_recall: 0.319260, val_f1: 0.247309
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed1.csv
Epoch: 116, loss: 24.7478, train_acc: 0.4111, train_recall: 0.3457, train_f1: 0.3042, val_acc: 0.358974, val_recall: 0.280050, val_f1: 0.240050
Epoch: 117, loss: 25.4363, train_acc: 0.4000, train_recall: 0.3145, train_f1: 0.2375, val_acc: 0.384615, val_recall: 0.307302, val_f1: 0.233170
Epoch: 118, loss: 25.3113, train_acc: 0.3556, train_recall: 0.3165, train_f1: 0.2224, val_acc: 0.358974, val_recall: 0.292220, val_f1: 0.218039
Epoch: 119, loss: 25.6047, train_acc: 0.4056, train_recall: 0.3860, train_f1: 0.3429, val_acc: 0.423077, val_recall: 0.319260, val_f1: 0.246242
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed1.csv
Epoch: 120, loss: 24.7910, train_acc: 0.4167, train_recall: 0.4000, train_f1: 0.3521, val_acc: 0.384615, val_recall: 0.307302, val_f1: 0.230091
Epoch: 121, loss: 26.1827, train_acc: 0.2056, train_recall: 0.3057, train_f1: 0.1841, val_acc: 0.217949, val_recall: 0.250000, val_f1: 0.089474
Epoch: 122, loss: 27.0297, train_acc: 0.3944, train_recall: 0.3125, train_f1: 0.2406, val_acc: 0.358974, val_recall: 0.241379, val_f1: 0.132075
Epoch: 123, loss: 25.8362, train_acc: 0.4111, train_recall: 0.3267, train_f1: 0.2783, val_acc: 0.410256, val_recall: 0.258621, val_f1: 0.161526
Epoch: 124, loss: 25.6979, train_acc: 0.4000, train_recall: 0.3179, train_f1: 0.3240, val_acc: 0.346154, val_recall: 0.222747, val_f1: 0.193050
Epoch: 125, loss: 26.8924, train_acc: 0.3944, train_recall: 0.3160, train_f1: 0.2247, val_acc: 0.410256, val_recall: 0.258621, val_f1: 0.161526
Epoch: 126, loss: 27.1798, train_acc: 0.2111, train_recall: 0.3714, train_f1: 0.1294, val_acc: 0.115385, val_recall: 0.077586, val_f1: 0.076271
Epoch: 127, loss: 24.4563, train_acc: 0.4000, train_recall: 0.3702, train_f1: 0.3338, val_acc: 0.384615, val_recall: 0.301217, val_f1: 0.230682
Epoch: 128, loss: 28.8708, train_acc: 0.2111, train_recall: 0.3125, train_f1: 0.1856, val_acc: 0.217949, val_recall: 0.250000, val_f1: 0.090426
Epoch: 129, loss: 27.1495, train_acc: 0.4000, train_recall: 0.3981, train_f1: 0.3451, val_acc: 0.371795, val_recall: 0.287002, val_f1: 0.219529
Epoch: 130, loss: 28.2475, train_acc: 0.3889, train_recall: 0.3125, train_f1: 0.2397, val_acc: 0.397436, val_recall: 0.250000, val_f1: 0.143519
Epoch: 131, loss: 24.2489, train_acc: 0.4222, train_recall: 0.3959, train_f1: 0.4147, val_acc: 0.371795, val_recall: 0.288670, val_f1: 0.260977
Epoch: 132, loss: 27.2624, train_acc: 0.3889, train_recall: 0.2500, train_f1: 0.1400, val_acc: 0.358974, val_recall: 0.241379, val_f1: 0.132075
Epoch: 133, loss: 26.4846, train_acc: 0.4056, train_recall: 0.3403, train_f1: 0.2502, val_acc: 0.384615, val_recall: 0.307302, val_f1: 0.229384
Epoch: 134, loss: 25.1211, train_acc: 0.4278, train_recall: 0.3565, train_f1: 0.3162, val_acc: 0.371795, val_recall: 0.288670, val_f1: 0.254712
Epoch: 135, loss: 25.3937, train_acc: 0.4278, train_recall: 0.3346, train_f1: 0.3103, val_acc: 0.423077, val_recall: 0.320928, val_f1: 0.283977
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed1.csv
Epoch: 136, loss: 27.5203, train_acc: 0.3889, train_recall: 0.2500, train_f1: 0.1400, val_acc: 0.371795, val_recall: 0.250000, val_f1: 0.135514
Epoch: 137, loss: 25.5277, train_acc: 0.4056, train_recall: 0.3244, train_f1: 0.2451, val_acc: 0.384615, val_recall: 0.307302, val_f1: 0.233170
Epoch: 138, loss: 26.0394, train_acc: 0.4167, train_recall: 0.3405, train_f1: 0.2684, val_acc: 0.423077, val_recall: 0.319816, val_f1: 0.258766
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed1.csv
Epoch: 139, loss: 26.1536, train_acc: 0.4167, train_recall: 0.4088, train_f1: 0.3696, val_acc: 0.371795, val_recall: 0.287002, val_f1: 0.219529
Epoch: 140, loss: 24.9328, train_acc: 0.4278, train_recall: 0.3994, train_f1: 0.4175, val_acc: 0.371795, val_recall: 0.288670, val_f1: 0.262658
Epoch: 141, loss: 24.7713, train_acc: 0.4500, train_recall: 0.4136, train_f1: 0.4370, val_acc: 0.410256, val_recall: 0.315089, val_f1: 0.302678
Epoch: 142, loss: 26.2363, train_acc: 0.4111, train_recall: 0.3267, train_f1: 0.2737, val_acc: 0.410256, val_recall: 0.258621, val_f1: 0.161526
Epoch: 143, loss: 24.9317, train_acc: 0.4111, train_recall: 0.3837, train_f1: 0.3438, val_acc: 0.384615, val_recall: 0.307302, val_f1: 0.233170
Epoch: 144, loss: 24.3940, train_acc: 0.4278, train_recall: 0.4090, train_f1: 0.4165, val_acc: 0.371795, val_recall: 0.288670, val_f1: 0.257713
Epoch: 145, loss: 25.0767, train_acc: 0.4000, train_recall: 0.3981, train_f1: 0.3451, val_acc: 0.371795, val_recall: 0.287002, val_f1: 0.219529
Epoch: 146, loss: 24.4538, train_acc: 0.4333, train_recall: 0.4191, train_f1: 0.4153, val_acc: 0.371795, val_recall: 0.288114, val_f1: 0.249588
Epoch: 147, loss: 25.1054, train_acc: 0.4167, train_recall: 0.3905, train_f1: 0.3489, val_acc: 0.384615, val_recall: 0.307302, val_f1: 0.236015
Epoch: 148, loss: 24.8583, train_acc: 0.4111, train_recall: 0.4028, train_f1: 0.3509, val_acc: 0.371795, val_recall: 0.298682, val_f1: 0.224868
Epoch: 149, loss: 23.4559, train_acc: 0.4556, train_recall: 0.4269, train_f1: 0.4243, val_acc: 0.371795, val_recall: 0.288670, val_f1: 0.261644
Epoch: 150, loss: 26.0713, train_acc: 0.3722, train_recall: 0.3015, train_f1: 0.1888, val_acc: 0.371795, val_recall: 0.234427, val_f1: 0.151282
Epoch: 151, loss: 24.2138, train_acc: 0.3944, train_recall: 0.3153, train_f1: 0.2431, val_acc: 0.358974, val_recall: 0.227475, val_f1: 0.169682
Epoch: 152, loss: 25.8767, train_acc: 0.3944, train_recall: 0.3698, train_f1: 0.2739, val_acc: 0.384615, val_recall: 0.307302, val_f1: 0.240638
Epoch: 153, loss: 29.2624, train_acc: 0.2111, train_recall: 0.3125, train_f1: 0.1694, val_acc: 0.217949, val_recall: 0.250000, val_f1: 0.090426
Epoch: 154, loss: 25.9244, train_acc: 0.4056, train_recall: 0.3833, train_f1: 0.2838, val_acc: 0.384615, val_recall: 0.307302, val_f1: 0.240638
Epoch: 155, loss: 24.6952, train_acc: 0.3833, train_recall: 0.3054, train_f1: 0.1756, val_acc: 0.358974, val_recall: 0.241379, val_f1: 0.135922
Epoch: 156, loss: 27.4414, train_acc: 0.3833, train_recall: 0.3089, train_f1: 0.2102, val_acc: 0.397436, val_recall: 0.250000, val_f1: 0.143519
Epoch: 157, loss: 27.9972, train_acc: 0.3833, train_recall: 0.3089, train_f1: 0.2102, val_acc: 0.397436, val_recall: 0.250000, val_f1: 0.143519
Epoch: 158, loss: 26.8771, train_acc: 0.3056, train_recall: 0.3647, train_f1: 0.2728, val_acc: 0.358974, val_recall: 0.332068, val_f1: 0.224319
Epoch: 159, loss: 23.8250, train_acc: 0.3556, train_recall: 0.5290, train_f1: 0.3326, val_acc: 0.282051, val_recall: 0.232219, val_f1: 0.244844
Epoch: 160, loss: 26.1612, train_acc: 0.3944, train_recall: 0.3125, train_f1: 0.2406, val_acc: 0.358974, val_recall: 0.241379, val_f1: 0.132075
Epoch: 161, loss: 26.1503, train_acc: 0.4333, train_recall: 0.3406, train_f1: 0.3226, val_acc: 0.397436, val_recall: 0.251669, val_f1: 0.180888
Epoch: 162, loss: 25.5659, train_acc: 0.4222, train_recall: 0.3330, train_f1: 0.3270, val_acc: 0.346154, val_recall: 0.219410, val_f1: 0.164477
Epoch: 163, loss: 25.0142, train_acc: 0.4111, train_recall: 0.3260, train_f1: 0.3175, val_acc: 0.346154, val_recall: 0.219410, val_f1: 0.162543
Epoch: 164, loss: 24.0454, train_acc: 0.4500, train_recall: 0.4137, train_f1: 0.4360, val_acc: 0.384615, val_recall: 0.297291, val_f1: 0.279233
Epoch: 165, loss: 25.4227, train_acc: 0.4000, train_recall: 0.3981, train_f1: 0.3451, val_acc: 0.371795, val_recall: 0.287002, val_f1: 0.221744
Epoch: 166, loss: 26.5562, train_acc: 0.3778, train_recall: 0.3993, train_f1: 0.3345, val_acc: 0.410256, val_recall: 0.351044, val_f1: 0.256390
Epoch: 167, loss: 25.2618, train_acc: 0.4167, train_recall: 0.3899, train_f1: 0.3672, val_acc: 0.435897, val_recall: 0.327881, val_f1: 0.268453
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed1.csv
Epoch: 168, loss: 26.4377, train_acc: 0.3944, train_recall: 0.3125, train_f1: 0.2406, val_acc: 0.358974, val_recall: 0.241379, val_f1: 0.132075
Epoch: 169, loss: 26.0567, train_acc: 0.3944, train_recall: 0.3125, train_f1: 0.2406, val_acc: 0.358974, val_recall: 0.241379, val_f1: 0.132075
Epoch: 170, loss: 26.7629, train_acc: 0.4000, train_recall: 0.3196, train_f1: 0.2541, val_acc: 0.410256, val_recall: 0.258621, val_f1: 0.161526
Epoch: 171, loss: 25.6235, train_acc: 0.4111, train_recall: 0.3959, train_f1: 0.3486, val_acc: 0.423077, val_recall: 0.319816, val_f1: 0.261465
Epoch: 172, loss: 25.6510, train_acc: 0.4278, train_recall: 0.4249, train_f1: 0.4135, val_acc: 0.423077, val_recall: 0.347494, val_f1: 0.298480
Epoch: 173, loss: 25.2076, train_acc: 0.4000, train_recall: 0.3798, train_f1: 0.3392, val_acc: 0.384615, val_recall: 0.307302, val_f1: 0.234675
Epoch: 174, loss: 24.5683, train_acc: 0.3944, train_recall: 0.3762, train_f1: 0.2733, val_acc: 0.320513, val_recall: 0.264199, val_f1: 0.218498
Epoch: 175, loss: 25.7199, train_acc: 0.2056, train_recall: 0.3693, train_f1: 0.1449, val_acc: 0.141026, val_recall: 0.089266, val_f1: 0.094792
Epoch: 176, loss: 25.7752, train_acc: 0.2167, train_recall: 0.3760, train_f1: 0.1745, val_acc: 0.128205, val_recall: 0.082314, val_f1: 0.101974
Epoch: 177, loss: 25.3265, train_acc: 0.4278, train_recall: 0.3344, train_f1: 0.3004, val_acc: 0.320513, val_recall: 0.214405, val_f1: 0.143557
Epoch: 178, loss: 25.1901, train_acc: 0.4222, train_recall: 0.3935, train_f1: 0.3744, val_acc: 0.435897, val_recall: 0.327881, val_f1: 0.268453
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed1.csv
Epoch: 179, loss: 24.5970, train_acc: 0.4278, train_recall: 0.4090, train_f1: 0.4165, val_acc: 0.371795, val_recall: 0.288670, val_f1: 0.257713
Epoch: 180, loss: 26.2902, train_acc: 0.4111, train_recall: 0.4028, train_f1: 0.3513, val_acc: 0.384615, val_recall: 0.307302, val_f1: 0.229384
Epoch: 181, loss: 25.5681, train_acc: 0.4000, train_recall: 0.3798, train_f1: 0.3392, val_acc: 0.384615, val_recall: 0.307302, val_f1: 0.231976
Epoch: 182, loss: 24.4731, train_acc: 0.4444, train_recall: 0.3513, train_f1: 0.3336, val_acc: 0.371795, val_recall: 0.288670, val_f1: 0.264632
Epoch: 183, loss: 26.6651, train_acc: 0.3944, train_recall: 0.2571, train_f1: 0.1543, val_acc: 0.410256, val_recall: 0.258621, val_f1: 0.160185
Epoch: 184, loss: 25.6968, train_acc: 0.4111, train_recall: 0.2661, train_f1: 0.2328, val_acc: 0.358974, val_recall: 0.231368, val_f1: 0.201387
Epoch: 185, loss: 25.2420, train_acc: 0.4222, train_recall: 0.3314, train_f1: 0.2858, val_acc: 0.423077, val_recall: 0.319816, val_f1: 0.260927
Epoch: 186, loss: 24.5743, train_acc: 0.4333, train_recall: 0.3602, train_f1: 0.3197, val_acc: 0.371795, val_recall: 0.288670, val_f1: 0.256683
Epoch: 187, loss: 25.4664, train_acc: 0.4111, train_recall: 0.3439, train_f1: 0.2533, val_acc: 0.384615, val_recall: 0.307302, val_f1: 0.229384
Epoch: 188, loss: 25.3579, train_acc: 0.4167, train_recall: 0.3475, train_f1: 0.2564, val_acc: 0.384615, val_recall: 0.307302, val_f1: 0.230091
Epoch: 189, loss: 24.1986, train_acc: 0.4611, train_recall: 0.3780, train_f1: 0.3447, val_acc: 0.358974, val_recall: 0.280050, val_f1: 0.245808
Epoch: 190, loss: 25.0571, train_acc: 0.4056, train_recall: 0.3796, train_f1: 0.3556, val_acc: 0.423077, val_recall: 0.319816, val_f1: 0.260927
Epoch: 191, loss: 24.1795, train_acc: 0.4278, train_recall: 0.3455, train_f1: 0.3689, val_acc: 0.346154, val_recall: 0.222747, val_f1: 0.196271
Epoch: 192, loss: 24.8927, train_acc: 0.4056, train_recall: 0.3293, train_f1: 0.2873, val_acc: 0.358974, val_recall: 0.241379, val_f1: 0.135922
Epoch: 193, loss: 25.4018, train_acc: 0.3778, train_recall: 0.3993, train_f1: 0.3345, val_acc: 0.410256, val_recall: 0.351044, val_f1: 0.254033
Epoch: 194, loss: 25.5773, train_acc: 0.4000, train_recall: 0.3981, train_f1: 0.3451, val_acc: 0.371795, val_recall: 0.287002, val_f1: 0.219529
Epoch: 195, loss: 24.4313, train_acc: 0.4056, train_recall: 0.3953, train_f1: 0.3558, val_acc: 0.384615, val_recall: 0.295066, val_f1: 0.226190
Epoch: 196, loss: 26.4873, train_acc: 0.3944, train_recall: 0.3125, train_f1: 0.2406, val_acc: 0.358974, val_recall: 0.241379, val_f1: 0.132075
Epoch: 197, loss: 25.8473, train_acc: 0.4056, train_recall: 0.3356, train_f1: 0.2924, val_acc: 0.346154, val_recall: 0.232759, val_f1: 0.132353
Epoch: 198, loss: 28.0945, train_acc: 0.2111, train_recall: 0.3125, train_f1: 0.1856, val_acc: 0.217949, val_recall: 0.250000, val_f1: 0.090426
Epoch: 199, loss: 25.9448, train_acc: 0.2056, train_recall: 0.4385, train_f1: 0.1820, val_acc: 0.141026, val_recall: 0.135199, val_f1: 0.130978
Epoch: 200, loss: 27.5180, train_acc: 0.1889, train_recall: 0.3649, train_f1: 0.1401, val_acc: 0.128205, val_recall: 0.080645, val_f1: 0.078125
Epoch: 201, loss: 25.5378, train_acc: 0.4111, train_recall: 0.3267, train_f1: 0.2737, val_acc: 0.410256, val_recall: 0.258621, val_f1: 0.161526
Epoch: 202, loss: 29.2552, train_acc: 0.3944, train_recall: 0.3125, train_f1: 0.2406, val_acc: 0.358974, val_recall: 0.241379, val_f1: 0.132075
Epoch: 203, loss: 31.0139, train_acc: 0.3944, train_recall: 0.3125, train_f1: 0.2406, val_acc: 0.358974, val_recall: 0.241379, val_f1: 0.132075
Epoch: 204, loss: 27.9338, train_acc: 0.4056, train_recall: 0.3356, train_f1: 0.2924, val_acc: 0.346154, val_recall: 0.232759, val_f1: 0.132353
Epoch: 205, loss: 33.0678, train_acc: 0.2056, train_recall: 0.2500, train_f1: 0.0853, val_acc: 0.217949, val_recall: 0.250000, val_f1: 0.089474
Epoch: 206, loss: 30.9930, train_acc: 0.2056, train_recall: 0.2500, train_f1: 0.0853, val_acc: 0.217949, val_recall: 0.250000, val_f1: 0.089474
Epoch: 207, loss: 34.7585, train_acc: 0.3833, train_recall: 0.2500, train_f1: 0.1386, val_acc: 0.397436, val_recall: 0.250000, val_f1: 0.142202
Epoch: 208, loss: 37.6677, train_acc: 0.3833, train_recall: 0.2500, train_f1: 0.1386, val_acc: 0.397436, val_recall: 0.250000, val_f1: 0.142202
Epoch: 209, loss: 35.9144, train_acc: 0.3833, train_recall: 0.2500, train_f1: 0.1386, val_acc: 0.397436, val_recall: 0.250000, val_f1: 0.142202
Epoch: 210, loss: 29.6280, train_acc: 0.4167, train_recall: 0.2714, train_f1: 0.1860, val_acc: 0.410256, val_recall: 0.258621, val_f1: 0.160185
Epoch: 211, loss: 37.8155, train_acc: 0.3889, train_recall: 0.2500, train_f1: 0.1400, val_acc: 0.371795, val_recall: 0.250000, val_f1: 0.135514
Epoch: 212, loss: 42.8493, train_acc: 0.3889, train_recall: 0.2500, train_f1: 0.1400, val_acc: 0.371795, val_recall: 0.250000, val_f1: 0.135514
Epoch: 213, loss: 42.9538, train_acc: 0.3889, train_recall: 0.2500, train_f1: 0.1400, val_acc: 0.371795, val_recall: 0.250000, val_f1: 0.135514
Epoch: 214, loss: 38.6803, train_acc: 0.3889, train_recall: 0.2500, train_f1: 0.1400, val_acc: 0.371795, val_recall: 0.250000, val_f1: 0.135514
Epoch: 215, loss: 30.3997, train_acc: 0.4056, train_recall: 0.3770, train_f1: 0.3386, val_acc: 0.384615, val_recall: 0.307302, val_f1: 0.233170
Epoch: 216, loss: 34.0885, train_acc: 0.2111, train_recall: 0.3125, train_f1: 0.1856, val_acc: 0.217949, val_recall: 0.250000, val_f1: 0.089474
Epoch: 217, loss: 34.6824, train_acc: 0.2167, train_recall: 0.3130, train_f1: 0.1984, val_acc: 0.217949, val_recall: 0.250000, val_f1: 0.089474
Epoch: 218, loss: 37.4493, train_acc: 0.3889, train_recall: 0.3125, train_f1: 0.2397, val_acc: 0.397436, val_recall: 0.250000, val_f1: 0.142202
Epoch: 219, loss: 40.9978, train_acc: 0.3889, train_recall: 0.3125, train_f1: 0.2391, val_acc: 0.397436, val_recall: 0.250000, val_f1: 0.142202
Epoch: 220, loss: 39.9231, train_acc: 0.3889, train_recall: 0.3125, train_f1: 0.2391, val_acc: 0.397436, val_recall: 0.250000, val_f1: 0.143519
Epoch: 221, loss: 34.6182, train_acc: 0.3778, train_recall: 0.3241, train_f1: 0.2861, val_acc: 0.346154, val_recall: 0.217742, val_f1: 0.131068
Epoch: 222, loss: 28.9194, train_acc: 0.4000, train_recall: 0.3981, train_f1: 0.3305, val_acc: 0.371795, val_recall: 0.287002, val_f1: 0.221744
Epoch: 223, loss: 29.7490, train_acc: 0.2556, train_recall: 0.4892, train_f1: 0.2225, val_acc: 0.179487, val_recall: 0.193712, val_f1: 0.157462
Epoch: 224, loss: 33.5700, train_acc: 0.4000, train_recall: 0.3798, train_f1: 0.3392, val_acc: 0.384615, val_recall: 0.307302, val_f1: 0.236015
Epoch: 225, loss: 37.2461, train_acc: 0.4056, train_recall: 0.3770, train_f1: 0.3386, val_acc: 0.384615, val_recall: 0.307302, val_f1: 0.236015
Epoch: 226, loss: 37.5090, train_acc: 0.4111, train_recall: 0.3837, train_f1: 0.3438, val_acc: 0.384615, val_recall: 0.307302, val_f1: 0.236015
Epoch: 227, loss: 35.5023, train_acc: 0.4167, train_recall: 0.3905, train_f1: 0.3484, val_acc: 0.384615, val_recall: 0.307302, val_f1: 0.236015
Epoch: 228, loss: 32.3374, train_acc: 0.4111, train_recall: 0.3933, train_f1: 0.3488, val_acc: 0.384615, val_recall: 0.307302, val_f1: 0.234675
Epoch: 229, loss: 30.3215, train_acc: 0.2611, train_recall: 0.3351, train_f1: 0.2520, val_acc: 0.217949, val_recall: 0.237830, val_f1: 0.114236
Epoch: 230, loss: 27.3289, train_acc: 0.4111, train_recall: 0.3959, train_f1: 0.3490, val_acc: 0.410256, val_recall: 0.311195, val_f1: 0.243194
Epoch: 231, loss: 28.5090, train_acc: 0.4000, train_recall: 0.3793, train_f1: 0.3389, val_acc: 0.423077, val_recall: 0.319260, val_f1: 0.250154
Epoch: 232, loss: 30.3935, train_acc: 0.3944, train_recall: 0.3161, train_f1: 0.2467, val_acc: 0.397436, val_recall: 0.250000, val_f1: 0.143519
Epoch: 233, loss: 27.8456, train_acc: 0.3944, train_recall: 0.3161, train_f1: 0.2467, val_acc: 0.397436, val_recall: 0.250000, val_f1: 0.143519
Epoch: 234, loss: 30.8551, train_acc: 0.3944, train_recall: 0.3125, train_f1: 0.2406, val_acc: 0.358974, val_recall: 0.241379, val_f1: 0.132075
Epoch: 235, loss: 33.4353, train_acc: 0.3944, train_recall: 0.3125, train_f1: 0.2406, val_acc: 0.358974, val_recall: 0.241379, val_f1: 0.132075
Epoch: 236, loss: 32.3267, train_acc: 0.3944, train_recall: 0.3125, train_f1: 0.2245, val_acc: 0.358974, val_recall: 0.241379, val_f1: 0.132075
Epoch: 237, loss: 27.7735, train_acc: 0.3722, train_recall: 0.2982, train_f1: 0.1646, val_acc: 0.294872, val_recall: 0.198276, val_f1: 0.118557
Epoch: 238, loss: 31.2497, train_acc: 0.1389, train_recall: 0.3888, train_f1: 0.1338, val_acc: 0.115385, val_recall: 0.367647, val_f1: 0.116729
Epoch: 239, loss: 30.3981, train_acc: 0.4000, train_recall: 0.3824, train_f1: 0.3403, val_acc: 0.423077, val_recall: 0.319260, val_f1: 0.250154
Epoch: 240, loss: 32.8766, train_acc: 0.4000, train_recall: 0.3824, train_f1: 0.3403, val_acc: 0.423077, val_recall: 0.319260, val_f1: 0.250154
Epoch: 241, loss: 33.3705, train_acc: 0.4111, train_recall: 0.3959, train_f1: 0.3486, val_acc: 0.410256, val_recall: 0.311195, val_f1: 0.240495
Epoch: 242, loss: 32.1267, train_acc: 0.4056, train_recall: 0.4017, train_f1: 0.3492, val_acc: 0.371795, val_recall: 0.287002, val_f1: 0.219529
Epoch: 243, loss: 29.5726, train_acc: 0.4056, train_recall: 0.4048, train_f1: 0.3493, val_acc: 0.384615, val_recall: 0.301708, val_f1: 0.230322
Epoch: 244, loss: 25.9723, train_acc: 0.4222, train_recall: 0.3658, train_f1: 0.2999, val_acc: 0.423077, val_recall: 0.353579, val_f1: 0.287985
Epoch: 245, loss: 29.0355, train_acc: 0.3889, train_recall: 0.2500, train_f1: 0.1400, val_acc: 0.358974, val_recall: 0.241379, val_f1: 0.132075
Epoch: 246, loss: 31.8537, train_acc: 0.3889, train_recall: 0.2500, train_f1: 0.1400, val_acc: 0.371795, val_recall: 0.250000, val_f1: 0.135514
Epoch: 247, loss: 30.6745, train_acc: 0.3889, train_recall: 0.2500, train_f1: 0.1400, val_acc: 0.371795, val_recall: 0.250000, val_f1: 0.135514
Epoch: 248, loss: 25.9115, train_acc: 0.4278, train_recall: 0.2769, train_f1: 0.2414, val_acc: 0.397436, val_recall: 0.257230, val_f1: 0.224850
Epoch: 249, loss: 28.9962, train_acc: 0.3889, train_recall: 0.2693, train_f1: 0.1882, val_acc: 0.358974, val_recall: 0.225806, val_f1: 0.134615
Epoch: 250, loss: 29.4884, train_acc: 0.3833, train_recall: 0.2657, train_f1: 0.1840, val_acc: 0.358974, val_recall: 0.225806, val_f1: 0.134615
Epoch: 251, loss: 31.5710, train_acc: 0.2056, train_recall: 0.2500, train_f1: 0.0853, val_acc: 0.217949, val_recall: 0.250000, val_f1: 0.089474
Epoch: 252, loss: 29.1458, train_acc: 0.3667, train_recall: 0.3363, train_f1: 0.2307, val_acc: 0.410256, val_recall: 0.351044, val_f1: 0.253571
Epoch: 253, loss: 27.5904, train_acc: 0.4167, train_recall: 0.2867, train_f1: 0.2568, val_acc: 0.346154, val_recall: 0.218854, val_f1: 0.157235
Epoch: 254, loss: 27.5054, train_acc: 0.4333, train_recall: 0.2806, train_f1: 0.2446, val_acc: 0.410256, val_recall: 0.265851, val_f1: 0.230980
Epoch: 255, loss: 31.2145, train_acc: 0.3889, train_recall: 0.2500, train_f1: 0.1400, val_acc: 0.371795, val_recall: 0.250000, val_f1: 0.135514
Epoch: 256, loss: 31.0361, train_acc: 0.3889, train_recall: 0.2500, train_f1: 0.1400, val_acc: 0.371795, val_recall: 0.250000, val_f1: 0.135514
Epoch: 257, loss: 29.7277, train_acc: 0.4167, train_recall: 0.3295, train_f1: 0.3215, val_acc: 0.371795, val_recall: 0.236096, val_f1: 0.179348
Epoch: 258, loss: 31.2344, train_acc: 0.4222, train_recall: 0.3335, train_f1: 0.3129, val_acc: 0.397436, val_recall: 0.251669, val_f1: 0.179474
/home/ADS/cyang314/ucr_work/HINI_Baseline/GraphLoRA/model/GraphLoRA.py:218: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.
Consider using tensor.detach() first. (Triggered internally at /pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:836.)
  f.write(f'{pre_dataset} to {downstream_dataset}: seed: %d, epoch: %d, train_loss: %f, train_acc: %f, train_recall: %f, train_f1: %f, val_acc: %f, val_recall: %f, val_f1: %f\n' %
Epoch: 259, loss: 29.3135, train_acc: 0.4222, train_recall: 0.3332, train_f1: 0.3225, val_acc: 0.371795, val_recall: 0.236096, val_f1: 0.179348
Epoch: 260, loss: 26.9335, train_acc: 0.4333, train_recall: 0.3395, train_f1: 0.3422, val_acc: 0.410256, val_recall: 0.265851, val_f1: 0.230980
Epoch: 261, loss: 27.6662, train_acc: 0.3944, train_recall: 0.3125, train_f1: 0.2406, val_acc: 0.358974, val_recall: 0.241379, val_f1: 0.132075
Epoch: 262, loss: 25.2733, train_acc: 0.4222, train_recall: 0.4036, train_f1: 0.3548, val_acc: 0.384615, val_recall: 0.307302, val_f1: 0.230091
Epoch: 263, loss: 27.6347, train_acc: 0.2056, train_recall: 0.3057, train_f1: 0.1841, val_acc: 0.217949, val_recall: 0.250000, val_f1: 0.089474
Epoch: 264, loss: 25.2291, train_acc: 0.4278, train_recall: 0.4250, train_f1: 0.4059, val_acc: 0.410256, val_recall: 0.338873, val_f1: 0.284222
Epoch: 265, loss: 25.6234, train_acc: 0.4222, train_recall: 0.3492, train_f1: 0.3570, val_acc: 0.358974, val_recall: 0.227475, val_f1: 0.169682
Epoch: 266, loss: 24.7014, train_acc: 0.4222, train_recall: 0.3331, train_f1: 0.3097, val_acc: 0.358974, val_recall: 0.227475, val_f1: 0.169203
Epoch: 267, loss: 26.9703, train_acc: 0.3944, train_recall: 0.3125, train_f1: 0.2245, val_acc: 0.358974, val_recall: 0.241379, val_f1: 0.132075
Epoch: 268, loss: 26.6577, train_acc: 0.3944, train_recall: 0.3125, train_f1: 0.2245, val_acc: 0.358974, val_recall: 0.241379, val_f1: 0.132075
Epoch: 269, loss: 27.3794, train_acc: 0.1611, train_recall: 0.3396, train_f1: 0.1398, val_acc: 0.051282, val_recall: 0.033927, val_f1: 0.047909
Epoch: 270, loss: 26.8189, train_acc: 0.3944, train_recall: 0.3161, train_f1: 0.2467, val_acc: 0.397436, val_recall: 0.250000, val_f1: 0.143519
Epoch: 271, loss: 25.2711, train_acc: 0.4167, train_recall: 0.3995, train_f1: 0.3627, val_acc: 0.423077, val_recall: 0.319816, val_f1: 0.258766
Epoch: 272, loss: 25.2686, train_acc: 0.4389, train_recall: 0.4226, train_f1: 0.4238, val_acc: 0.371795, val_recall: 0.288670, val_f1: 0.257309
Epoch: 273, loss: 25.9774, train_acc: 0.4111, train_recall: 0.4028, train_f1: 0.3513, val_acc: 0.384615, val_recall: 0.307302, val_f1: 0.229384
Epoch: 274, loss: 26.0389, train_acc: 0.4111, train_recall: 0.3280, train_f1: 0.2472, val_acc: 0.384615, val_recall: 0.307302, val_f1: 0.231976
Epoch: 275, loss: 25.4835, train_acc: 0.4000, train_recall: 0.3145, train_f1: 0.2386, val_acc: 0.384615, val_recall: 0.307302, val_f1: 0.233170
Epoch: 276, loss: 26.5945, train_acc: 0.3944, train_recall: 0.2571, train_f1: 0.1543, val_acc: 0.397436, val_recall: 0.250000, val_f1: 0.143519
Epoch: 277, loss: 25.6839, train_acc: 0.4167, train_recall: 0.3499, train_f1: 0.2734, val_acc: 0.397436, val_recall: 0.303687, val_f1: 0.245180
Epoch: 278, loss: 25.7841, train_acc: 0.4278, train_recall: 0.3547, train_f1: 0.2678, val_acc: 0.384615, val_recall: 0.307302, val_f1: 0.230091
Epoch: 279, loss: 26.6842, train_acc: 0.4222, train_recall: 0.3511, train_f1: 0.2599, val_acc: 0.384615, val_recall: 0.307302, val_f1: 0.230091
Epoch: 280, loss: 26.1858, train_acc: 0.4222, train_recall: 0.3511, train_f1: 0.2599, val_acc: 0.371795, val_recall: 0.298682, val_f1: 0.223287
Epoch: 281, loss: 25.3141, train_acc: 0.4500, train_recall: 0.3709, train_f1: 0.3345, val_acc: 0.358974, val_recall: 0.280050, val_f1: 0.245808
Epoch: 282, loss: 26.2706, train_acc: 0.4000, train_recall: 0.3329, train_f1: 0.2513, val_acc: 0.384615, val_recall: 0.295066, val_f1: 0.226190
Epoch: 283, loss: 25.1771, train_acc: 0.4389, train_recall: 0.3000, train_f1: 0.2923, val_acc: 0.346154, val_recall: 0.222747, val_f1: 0.198189
Epoch: 284, loss: 25.5547, train_acc: 0.4444, train_recall: 0.3036, train_f1: 0.2958, val_acc: 0.346154, val_recall: 0.222747, val_f1: 0.200000
Epoch: 285, loss: 25.7799, train_acc: 0.4111, train_recall: 0.3527, train_f1: 0.2597, val_acc: 0.410256, val_recall: 0.331120, val_f1: 0.250815
Epoch: 286, loss: 25.8214, train_acc: 0.4167, train_recall: 0.3594, train_f1: 0.2638, val_acc: 0.423077, val_recall: 0.345825, val_f1: 0.260571
Epoch: 287, loss: 25.0522, train_acc: 0.4389, train_recall: 0.3000, train_f1: 0.2923, val_acc: 0.333333, val_recall: 0.214127, val_f1: 0.191277
Epoch: 288, loss: 25.3512, train_acc: 0.4333, train_recall: 0.2964, train_f1: 0.2892, val_acc: 0.358974, val_recall: 0.231368, val_f1: 0.206696
Epoch: 289, loss: 25.9766, train_acc: 0.3944, train_recall: 0.2728, train_f1: 0.2053, val_acc: 0.358974, val_recall: 0.225806, val_f1: 0.134615
Epoch: 290, loss: 25.0664, train_acc: 0.4333, train_recall: 0.3697, train_f1: 0.3143, val_acc: 0.410256, val_recall: 0.338873, val_f1: 0.281447
Epoch: 291, loss: 25.4753, train_acc: 0.4222, train_recall: 0.3479, train_f1: 0.2634, val_acc: 0.371795, val_recall: 0.298682, val_f1: 0.223287
Epoch: 292, loss: 24.9465, train_acc: 0.4500, train_recall: 0.3708, train_f1: 0.3373, val_acc: 0.384615, val_recall: 0.303376, val_f1: 0.268937
Epoch: 293, loss: 25.1418, train_acc: 0.4222, train_recall: 0.3566, train_f1: 0.2794, val_acc: 0.423077, val_recall: 0.345825, val_f1: 0.260571
Epoch: 294, loss: 24.6818, train_acc: 0.4500, train_recall: 0.3804, train_f1: 0.3318, val_acc: 0.410256, val_recall: 0.338873, val_f1: 0.281447
Epoch: 295, loss: 24.6092, train_acc: 0.4611, train_recall: 0.3779, train_f1: 0.3488, val_acc: 0.384615, val_recall: 0.297291, val_f1: 0.271587
Epoch: 296, loss: 24.4326, train_acc: 0.4667, train_recall: 0.4404, train_f1: 0.4497, val_acc: 0.384615, val_recall: 0.297291, val_f1: 0.271587
Epoch: 297, loss: 24.3764, train_acc: 0.4500, train_recall: 0.4330, train_f1: 0.4295, val_acc: 0.423077, val_recall: 0.340853, val_f1: 0.294611
Epoch: 298, loss: 24.4103, train_acc: 0.4611, train_recall: 0.4465, train_f1: 0.4381, val_acc: 0.423077, val_recall: 0.347494, val_f1: 0.296669
Epoch: 299, loss: 23.9918, train_acc: 0.4500, train_recall: 0.4297, train_f1: 0.4340, val_acc: 0.384615, val_recall: 0.297291, val_f1: 0.271587
epoch: 179, train_acc: 0.422222, val_acc: 0.435897, val_recall: 0.327881, val_f1: 0.268453
Running: year=2016 â†’ downstream_year=2017, seed=2
Random seed set to 42

==============================
PRE-TRAINING
==============================
create PreTrain instance...
pre-training...
(T) | Epoch=001, loss=7.8972, this epoch 1.5366, total 1.5366
+++model saved ! 2016.pth
(T) | Epoch=002, loss=7.8960, this epoch 1.5130, total 3.0496
+++model saved ! 2016.pth
(T) | Epoch=003, loss=7.8942, this epoch 1.4567, total 4.5064
+++model saved ! 2016.pth
(T) | Epoch=004, loss=7.8973, this epoch 1.4729, total 5.9793
(T) | Epoch=005, loss=7.8891, this epoch 1.4722, total 7.4514
+++model saved ! 2016.pth
(T) | Epoch=006, loss=7.8855, this epoch 1.4773, total 8.9288
+++model saved ! 2016.pth
(T) | Epoch=007, loss=7.8808, this epoch 1.4954, total 10.4242
+++model saved ! 2016.pth
(T) | Epoch=008, loss=7.8751, this epoch 1.5187, total 11.9429
+++model saved ! 2016.pth
(T) | Epoch=009, loss=7.8690, this epoch 1.4759, total 13.4188
+++model saved ! 2016.pth
(T) | Epoch=010, loss=7.8849, this epoch 1.4809, total 14.8998
(T) | Epoch=011, loss=7.8509, this epoch 1.4742, total 16.3740
+++model saved ! 2016.pth
(T) | Epoch=012, loss=7.8414, this epoch 1.4229, total 17.7969
+++model saved ! 2016.pth
(T) | Epoch=013, loss=7.8309, this epoch 1.4982, total 19.2952
+++model saved ! 2016.pth
(T) | Epoch=014, loss=7.8687, this epoch 1.5238, total 20.8190
(T) | Epoch=015, loss=7.8100, this epoch 1.4826, total 22.3016
+++model saved ! 2016.pth
(T) | Epoch=016, loss=8.9353, this epoch 1.4619, total 23.7635
(T) | Epoch=017, loss=7.7978, this epoch 1.5124, total 25.2759
+++model saved ! 2016.pth
(T) | Epoch=018, loss=7.8477, this epoch 1.5057, total 26.7816
(T) | Epoch=019, loss=8.3957, this epoch 1.4906, total 28.2722
(T) | Epoch=020, loss=7.8094, this epoch 1.4671, total 29.7393
(T) | Epoch=021, loss=8.1407, this epoch 1.4656, total 31.2049
(T) | Epoch=022, loss=8.0648, this epoch 1.5156, total 32.7206
(T) | Epoch=023, loss=7.8291, this epoch 1.4853, total 34.2059
(T) | Epoch=024, loss=7.8650, this epoch 1.5087, total 35.7146
(T) | Epoch=025, loss=7.9818, this epoch 1.4870, total 37.2016
(T) | Epoch=026, loss=7.8394, this epoch 1.4829, total 38.6844
(T) | Epoch=027, loss=7.9440, this epoch 1.4633, total 40.1477
(T) | Epoch=028, loss=7.9133, this epoch 1.4748, total 41.6225
(T) | Epoch=029, loss=7.8532, this epoch 1.5166, total 43.1391
(T) | Epoch=030, loss=7.8385, this epoch 1.4642, total 44.6034
(T) | Epoch=031, loss=7.8355, this epoch 1.4460, total 46.0493
(T) | Epoch=032, loss=7.8339, this epoch 1.4670, total 47.5164
(T) | Epoch=033, loss=7.8334, this epoch 1.4788, total 48.9951
(T) | Epoch=034, loss=7.8813, this epoch 1.5228, total 50.5179
(T) | Epoch=035, loss=7.8675, this epoch 1.4838, total 52.0018
(T) | Epoch=036, loss=7.8694, this epoch 1.4782, total 53.4800
(T) | Epoch=037, loss=7.8295, this epoch 1.4620, total 54.9420
(T) | Epoch=038, loss=7.8461, this epoch 1.4864, total 56.4284
(T) | Epoch=039, loss=7.8733, this epoch 1.4387, total 57.8671
(T) | Epoch=040, loss=7.9009, this epoch 1.4386, total 59.3057
(T) | Epoch=041, loss=7.8444, this epoch 1.4727, total 60.7784
(T) | Epoch=042, loss=7.8238, this epoch 1.5057, total 62.2841
(T) | Epoch=043, loss=7.8633, this epoch 1.4939, total 63.7780
(T) | Epoch=044, loss=7.8205, this epoch 1.4453, total 65.2233
(T) | Epoch=045, loss=7.8970, this epoch 1.4331, total 66.6564
(T) | Epoch=046, loss=7.8833, this epoch 1.5096, total 68.1661
(T) | Epoch=047, loss=7.8183, this epoch 1.4668, total 69.6329
(T) | Epoch=048, loss=7.8181, this epoch 1.4469, total 71.0798
(T) | Epoch=049, loss=7.8869, this epoch 1.4850, total 72.5648
(T) | Epoch=050, loss=7.8172, this epoch 1.4854, total 74.0502
(T) | Epoch=051, loss=7.8158, this epoch 1.4448, total 75.4951
(T) | Epoch=052, loss=7.8338, this epoch 1.4919, total 76.9869
(T) | Epoch=053, loss=7.8124, this epoch 1.4846, total 78.4715
(T) | Epoch=054, loss=7.8297, this epoch 1.4801, total 79.9516
(T) | Epoch=055, loss=7.8500, this epoch 1.4723, total 81.4239
(T) | Epoch=056, loss=7.8050, this epoch 1.4883, total 82.9122
(T) | Epoch=057, loss=7.8455, this epoch 1.4629, total 84.3751
(T) | Epoch=058, loss=7.7608, this epoch 1.4568, total 85.8319
+++model saved ! 2016.pth
(T) | Epoch=059, loss=7.7941, this epoch 1.4727, total 87.3045
(T) | Epoch=060, loss=7.7920, this epoch 1.4854, total 88.7899
(T) | Epoch=061, loss=7.8088, this epoch 1.4696, total 90.2595
(T) | Epoch=062, loss=7.7835, this epoch 1.4880, total 91.7475
(T) | Epoch=063, loss=7.8721, this epoch 1.5037, total 93.2512
(T) | Epoch=064, loss=7.8320, this epoch 1.4763, total 94.7275
(T) | Epoch=065, loss=7.8550, this epoch 1.4635, total 96.1910
(T) | Epoch=066, loss=7.7719, this epoch 1.4486, total 97.6396
(T) | Epoch=067, loss=7.7767, this epoch 1.4256, total 99.0651
(T) | Epoch=068, loss=7.8208, this epoch 1.4865, total 100.5516
(T) | Epoch=069, loss=7.7684, this epoch 1.4483, total 102.0000
(T) | Epoch=070, loss=7.7887, this epoch 1.5017, total 103.5017
(T) | Epoch=071, loss=7.8050, this epoch 1.4416, total 104.9433
(T) | Epoch=072, loss=7.7990, this epoch 1.4814, total 106.4247
(T) | Epoch=073, loss=7.7964, this epoch 1.4531, total 107.8778
(T) | Epoch=074, loss=7.8045, this epoch 1.4473, total 109.3250
(T) | Epoch=075, loss=7.7370, this epoch 1.4358, total 110.7608
+++model saved ! 2016.pth
(T) | Epoch=076, loss=7.7841, this epoch 1.4997, total 112.2606
(T) | Epoch=077, loss=7.7808, this epoch 1.4831, total 113.7436
(T) | Epoch=078, loss=7.7223, this epoch 1.4687, total 115.2124
+++model saved ! 2016.pth
(T) | Epoch=079, loss=7.7388, this epoch 1.4388, total 116.6511
(T) | Epoch=080, loss=7.7078, this epoch 1.4909, total 118.1420
+++model saved ! 2016.pth
(T) | Epoch=081, loss=7.7200, this epoch 1.4744, total 119.6164
(T) | Epoch=082, loss=7.7504, this epoch 1.4535, total 121.0699
(T) | Epoch=083, loss=7.7256, this epoch 1.4969, total 122.5668
(T) | Epoch=084, loss=7.7405, this epoch 1.4631, total 124.0299
(T) | Epoch=085, loss=7.9853, this epoch 1.4441, total 125.4741
(T) | Epoch=086, loss=7.7604, this epoch 1.4874, total 126.9614
(T) | Epoch=087, loss=7.6827, this epoch 1.4744, total 128.4359
+++model saved ! 2016.pth
(T) | Epoch=088, loss=7.6728, this epoch 1.5238, total 129.9596
+++model saved ! 2016.pth
(T) | Epoch=089, loss=7.7032, this epoch 1.4471, total 131.4067
(T) | Epoch=090, loss=7.6639, this epoch 1.4597, total 132.8664
+++model saved ! 2016.pth
(T) | Epoch=091, loss=7.7604, this epoch 1.4725, total 134.3389
(T) | Epoch=092, loss=7.7533, this epoch 1.4537, total 135.7926
(T) | Epoch=093, loss=7.6967, this epoch 1.4496, total 137.2422
(T) | Epoch=094, loss=7.6565, this epoch 1.4658, total 138.7080
+++model saved ! 2016.pth
(T) | Epoch=095, loss=7.7766, this epoch 1.4674, total 140.1754
(T) | Epoch=096, loss=7.7297, this epoch 1.4571, total 141.6325
(T) | Epoch=097, loss=7.6908, this epoch 1.4668, total 143.0993
(T) | Epoch=098, loss=7.7321, this epoch 1.4539, total 144.5532
(T) | Epoch=099, loss=7.6779, this epoch 1.4515, total 146.0046
(T) | Epoch=100, loss=7.6936, this epoch 1.4635, total 147.4681
(T) | Epoch=101, loss=7.7002, this epoch 1.4679, total 148.9361
(T) | Epoch=102, loss=7.6657, this epoch 1.4962, total 150.4323
(T) | Epoch=103, loss=7.6628, this epoch 1.4858, total 151.9181
(T) | Epoch=104, loss=7.6648, this epoch 1.4574, total 153.3754
(T) | Epoch=105, loss=7.8314, this epoch 1.4523, total 154.8278
(T) | Epoch=106, loss=7.6576, this epoch 1.4394, total 156.2672
(T) | Epoch=107, loss=7.9045, this epoch 1.4903, total 157.7575
(T) | Epoch=108, loss=7.6804, this epoch 1.4483, total 159.2058
(T) | Epoch=109, loss=7.6935, this epoch 1.4747, total 160.6805
(T) | Epoch=110, loss=7.6577, this epoch 1.4674, total 162.1479
(T) | Epoch=111, loss=7.6718, this epoch 1.4654, total 163.6132
(T) | Epoch=112, loss=8.1404, this epoch 1.4717, total 165.0849
(T) | Epoch=113, loss=7.6954, this epoch 1.4601, total 166.5450
(T) | Epoch=114, loss=7.7348, this epoch 1.5057, total 168.0508
(T) | Epoch=115, loss=7.6981, this epoch 1.4565, total 169.5073
(T) | Epoch=116, loss=7.9329, this epoch 1.4760, total 170.9832
(T) | Epoch=117, loss=7.9696, this epoch 1.5029, total 172.4861
(T) | Epoch=118, loss=7.7740, this epoch 1.4617, total 173.9479
(T) | Epoch=119, loss=7.7467, this epoch 1.5055, total 175.4533
(T) | Epoch=120, loss=7.8114, this epoch 1.4524, total 176.9057
(T) | Epoch=121, loss=7.7820, this epoch 1.5047, total 178.4104
(T) | Epoch=122, loss=7.8905, this epoch 1.4676, total 179.8779
(T) | Epoch=123, loss=7.8847, this epoch 1.4667, total 181.3447
(T) | Epoch=124, loss=7.7380, this epoch 1.4616, total 182.8063
(T) | Epoch=125, loss=7.7314, this epoch 1.4826, total 184.2888
(T) | Epoch=126, loss=7.7788, this epoch 1.4533, total 185.7422
(T) | Epoch=127, loss=7.7780, this epoch 1.5439, total 187.2861
(T) | Epoch=128, loss=7.7069, this epoch 1.4611, total 188.7472
(T) | Epoch=129, loss=7.6958, this epoch 1.4749, total 190.2220
(T) | Epoch=130, loss=7.6921, this epoch 1.4817, total 191.7037
(T) | Epoch=131, loss=7.7212, this epoch 1.4407, total 193.1444
(T) | Epoch=132, loss=7.6800, this epoch 1.4684, total 194.6128
(T) | Epoch=133, loss=7.6819, this epoch 1.4453, total 196.0581
(T) | Epoch=134, loss=7.7725, this epoch 1.4986, total 197.5567
(T) | Epoch=135, loss=8.0296, this epoch 1.4683, total 199.0250
(T) | Epoch=136, loss=7.7016, this epoch 1.4439, total 200.4689
(T) | Epoch=137, loss=7.7240, this epoch 1.4728, total 201.9417
(T) | Epoch=138, loss=7.6803, this epoch 1.5223, total 203.4641
(T) | Epoch=139, loss=7.6857, this epoch 1.4618, total 204.9258
(T) | Epoch=140, loss=7.7758, this epoch 1.5624, total 206.4883
(T) | Epoch=141, loss=7.6878, this epoch 1.5206, total 208.0089
(T) | Epoch=142, loss=7.6937, this epoch 1.4644, total 209.4733
(T) | Epoch=143, loss=7.6953, this epoch 1.5091, total 210.9824
(T) | Epoch=144, loss=7.6950, this epoch 1.5111, total 212.4935
(T) | Epoch=145, loss=7.6916, this epoch 1.4857, total 213.9792
(T) | Epoch=146, loss=7.8707, this epoch 1.4794, total 215.4586
(T) | Epoch=147, loss=7.9587, this epoch 1.4749, total 216.9335
(T) | Epoch=148, loss=7.7416, this epoch 1.4577, total 218.3912
(T) | Epoch=149, loss=7.6807, this epoch 1.4500, total 219.8412
(T) | Epoch=150, loss=7.6698, this epoch 1.5281, total 221.3693
(T) | Epoch=151, loss=7.7041, this epoch 1.5007, total 222.8701
(T) | Epoch=152, loss=7.6688, this epoch 1.5045, total 224.3746
(T) | Epoch=153, loss=7.6618, this epoch 1.4868, total 225.8614
(T) | Epoch=154, loss=7.7258, this epoch 1.4697, total 227.3311
(T) | Epoch=155, loss=7.6446, this epoch 1.4724, total 228.8035
+++model saved ! 2016.pth
(T) | Epoch=156, loss=7.6350, this epoch 1.4847, total 230.2882
+++model saved ! 2016.pth
(T) | Epoch=157, loss=7.6350, this epoch 1.4738, total 231.7621
+++model saved ! 2016.pth
(T) | Epoch=158, loss=7.6295, this epoch 1.5165, total 233.2786
+++model saved ! 2016.pth
(T) | Epoch=159, loss=7.9806, this epoch 1.5231, total 234.8017
(T) | Epoch=160, loss=7.6389, this epoch 1.5193, total 236.3210
(T) | Epoch=161, loss=7.7209, this epoch 1.5058, total 237.8268
(T) | Epoch=162, loss=7.7459, this epoch 1.4853, total 239.3121
(T) | Epoch=163, loss=7.7678, this epoch 1.5143, total 240.8264
(T) | Epoch=164, loss=7.7021, this epoch 1.5038, total 242.3303
(T) | Epoch=165, loss=7.7692, this epoch 1.4758, total 243.8061
(T) | Epoch=166, loss=7.9191, this epoch 1.5506, total 245.3567
(T) | Epoch=167, loss=7.7395, this epoch 1.4802, total 246.8369
(T) | Epoch=168, loss=7.8386, this epoch 1.5016, total 248.3385
(T) | Epoch=169, loss=7.7457, this epoch 1.5390, total 249.8776
(T) | Epoch=170, loss=7.8090, this epoch 1.4520, total 251.3295
(T) | Epoch=171, loss=7.8330, this epoch 1.4573, total 252.7868
(T) | Epoch=172, loss=7.7389, this epoch 1.4273, total 254.2142
(T) | Epoch=173, loss=7.7918, this epoch 1.4951, total 255.7093
(T) | Epoch=174, loss=7.7982, this epoch 1.4791, total 257.1884
(T) | Epoch=175, loss=7.7083, this epoch 1.5266, total 258.7150
(T) | Epoch=176, loss=7.7061, this epoch 1.5175, total 260.2326
(T) | Epoch=177, loss=7.6974, this epoch 1.4958, total 261.7284
(T) | Epoch=178, loss=7.7521, this epoch 1.4531, total 263.1814
(T) | Epoch=179, loss=7.7281, this epoch 1.4327, total 264.6141
(T) | Epoch=180, loss=7.7283, this epoch 1.4797, total 266.0938
(T) | Epoch=181, loss=7.7021, this epoch 1.4710, total 267.5648
(T) | Epoch=182, loss=7.6751, this epoch 1.4793, total 269.0441
(T) | Epoch=183, loss=7.7060, this epoch 1.4839, total 270.5280
(T) | Epoch=184, loss=7.6700, this epoch 1.4935, total 272.0215
(T) | Epoch=185, loss=7.7197, this epoch 1.5007, total 273.5223
(T) | Epoch=186, loss=7.6522, this epoch 1.5226, total 275.0449
(T) | Epoch=187, loss=7.7096, this epoch 1.4930, total 276.5378
(T) | Epoch=188, loss=7.6425, this epoch 1.4656, total 278.0035
(T) | Epoch=189, loss=7.6440, this epoch 1.4501, total 279.4536
(T) | Epoch=190, loss=7.6702, this epoch 1.4629, total 280.9164
(T) | Epoch=191, loss=7.6335, this epoch 1.4426, total 282.3591
(T) | Epoch=192, loss=7.6225, this epoch 1.4454, total 283.8044
+++model saved ! 2016.pth
(T) | Epoch=193, loss=7.6214, this epoch 1.4755, total 285.2799
+++model saved ! 2016.pth
(T) | Epoch=194, loss=7.6194, this epoch 1.4707, total 286.7506
+++model saved ! 2016.pth
(T) | Epoch=195, loss=7.6500, this epoch 1.4591, total 288.2097
(T) | Epoch=196, loss=7.6092, this epoch 1.4661, total 289.6758
+++model saved ! 2016.pth
(T) | Epoch=197, loss=7.5999, this epoch 1.5064, total 291.1822
+++model saved ! 2016.pth
(T) | Epoch=198, loss=7.6908, this epoch 1.4946, total 292.6768
(T) | Epoch=199, loss=7.6461, this epoch 1.4840, total 294.1608
(T) | Epoch=200, loss=7.6914, this epoch 1.4693, total 295.6302
(T) | Epoch=201, loss=7.7265, this epoch 1.4809, total 297.1111
(T) | Epoch=202, loss=7.6334, this epoch 1.4827, total 298.5938
(T) | Epoch=203, loss=7.6409, this epoch 1.4434, total 300.0372
(T) | Epoch=204, loss=7.6671, this epoch 1.4901, total 301.5273
(T) | Epoch=205, loss=7.6712, this epoch 1.4897, total 303.0170
(T) | Epoch=206, loss=7.6833, this epoch 1.5096, total 304.5266
(T) | Epoch=207, loss=7.7145, this epoch 1.5157, total 306.0423
(T) | Epoch=208, loss=7.6406, this epoch 1.5319, total 307.5741
(T) | Epoch=209, loss=7.6699, this epoch 1.5119, total 309.0861
(T) | Epoch=210, loss=7.6666, this epoch 1.5454, total 310.6315
(T) | Epoch=211, loss=7.6291, this epoch 1.4879, total 312.1193
(T) | Epoch=212, loss=7.7429, this epoch 1.4648, total 313.5842
(T) | Epoch=213, loss=7.6209, this epoch 1.4802, total 315.0644
(T) | Epoch=214, loss=7.6179, this epoch 1.4760, total 316.5404
(T) | Epoch=215, loss=7.6074, this epoch 1.4893, total 318.0296
(T) | Epoch=216, loss=7.7853, this epoch 1.5056, total 319.5352
(T) | Epoch=217, loss=7.6730, this epoch 1.4794, total 321.0146
(T) | Epoch=218, loss=7.6273, this epoch 1.4547, total 322.4693
(T) | Epoch=219, loss=7.6329, this epoch 1.5399, total 324.0091
(T) | Epoch=220, loss=7.6359, this epoch 1.5112, total 325.5203
(T) | Epoch=221, loss=7.7191, this epoch 1.4695, total 326.9898
(T) | Epoch=222, loss=7.6743, this epoch 1.4657, total 328.4555
(T) | Epoch=223, loss=7.6385, this epoch 1.4640, total 329.9195
(T) | Epoch=224, loss=7.6429, this epoch 1.4748, total 331.3943
(T) | Epoch=225, loss=7.6478, this epoch 1.4730, total 332.8673
(T) | Epoch=226, loss=7.6565, this epoch 1.4717, total 334.3390
(T) | Epoch=227, loss=7.7008, this epoch 1.4578, total 335.7968
(T) | Epoch=228, loss=7.6370, this epoch 1.4740, total 337.2708
(T) | Epoch=229, loss=7.6407, this epoch 1.5048, total 338.7757
(T) | Epoch=230, loss=7.6681, this epoch 1.4825, total 340.2582
(T) | Epoch=231, loss=7.6297, this epoch 1.4883, total 341.7465
(T) | Epoch=232, loss=7.6327, this epoch 1.4463, total 343.1928
(T) | Epoch=233, loss=7.6318, this epoch 1.4310, total 344.6238
(T) | Epoch=234, loss=7.6505, this epoch 1.4061, total 346.0300
(T) | Epoch=235, loss=7.6506, this epoch 1.4701, total 347.5001
(T) | Epoch=236, loss=7.6233, this epoch 1.4808, total 348.9809
(T) | Epoch=237, loss=7.6656, this epoch 1.4627, total 350.4436
(T) | Epoch=238, loss=7.6206, this epoch 1.4576, total 351.9012
(T) | Epoch=239, loss=7.6069, this epoch 1.4414, total 353.3426
(T) | Epoch=240, loss=7.6020, this epoch 1.4343, total 354.7769
(T) | Epoch=241, loss=7.6014, this epoch 1.4793, total 356.2562
(T) | Epoch=242, loss=7.5974, this epoch 1.4500, total 357.7062
+++model saved ! 2016.pth
(T) | Epoch=243, loss=7.7122, this epoch 1.5037, total 359.2099
(T) | Epoch=244, loss=7.5825, this epoch 1.4984, total 360.7083
+++model saved ! 2016.pth
(T) | Epoch=245, loss=7.6316, this epoch 1.4727, total 362.1810
(T) | Epoch=246, loss=7.6575, this epoch 1.4738, total 363.6548
(T) | Epoch=247, loss=7.6442, this epoch 1.4699, total 365.1247
(T) | Epoch=248, loss=7.6549, this epoch 1.4545, total 366.5793
(T) | Epoch=249, loss=7.6258, this epoch 1.4726, total 368.0519
(T) | Epoch=250, loss=7.6161, this epoch 1.4411, total 369.4930
(T) | Epoch=251, loss=7.6353, this epoch 1.4711, total 370.9641
(T) | Epoch=252, loss=7.6374, this epoch 1.4607, total 372.4248
(T) | Epoch=253, loss=7.6581, this epoch 1.5341, total 373.9588
(T) | Epoch=254, loss=7.6275, this epoch 1.4487, total 375.4076
(T) | Epoch=255, loss=7.6231, this epoch 1.4997, total 376.9073
(T) | Epoch=256, loss=7.6265, this epoch 1.5061, total 378.4133
(T) | Epoch=257, loss=7.6247, this epoch 1.5022, total 379.9155
(T) | Epoch=258, loss=7.6480, this epoch 1.5304, total 381.4459
(T) | Epoch=259, loss=7.6726, this epoch 1.5056, total 382.9515
(T) | Epoch=260, loss=7.7362, this epoch 1.5306, total 384.4821
(T) | Epoch=261, loss=7.6112, this epoch 1.5057, total 385.9879
(T) | Epoch=262, loss=7.5926, this epoch 1.5279, total 387.5158
(T) | Epoch=263, loss=7.6275, this epoch 1.4885, total 389.0043
(T) | Epoch=264, loss=7.5969, this epoch 1.4839, total 390.4882
(T) | Epoch=265, loss=7.5855, this epoch 1.4425, total 391.9307
(T) | Epoch=266, loss=7.5856, this epoch 1.4665, total 393.3972
(T) | Epoch=267, loss=7.5699, this epoch 1.4780, total 394.8752
+++model saved ! 2016.pth
(T) | Epoch=268, loss=7.6041, this epoch 1.5136, total 396.3888
(T) | Epoch=269, loss=7.5493, this epoch 1.5335, total 397.9223
+++model saved ! 2016.pth
(T) | Epoch=270, loss=7.5517, this epoch 1.5164, total 399.4388
(T) | Epoch=271, loss=7.5466, this epoch 1.5476, total 400.9864
+++model saved ! 2016.pth
(T) | Epoch=272, loss=8.0264, this epoch 1.5301, total 402.5165
(T) | Epoch=273, loss=7.6125, this epoch 1.4407, total 403.9571
(T) | Epoch=274, loss=7.6927, this epoch 1.4693, total 405.4264
(T) | Epoch=275, loss=7.7581, this epoch 1.4934, total 406.9198
(T) | Epoch=276, loss=7.6928, this epoch 1.5044, total 408.4243
(T) | Epoch=277, loss=7.7092, this epoch 1.4779, total 409.9021
(T) | Epoch=278, loss=7.7423, this epoch 1.5300, total 411.4322
(T) | Epoch=279, loss=7.7906, this epoch 1.5323, total 412.9644
(T) | Epoch=280, loss=7.9144, this epoch 1.5376, total 414.5021
(T) | Epoch=281, loss=7.8076, this epoch 1.5016, total 416.0037
(T) | Epoch=282, loss=7.8167, this epoch 1.5408, total 417.5445
(T) | Epoch=283, loss=7.7798, this epoch 1.5294, total 419.0738
(T) | Epoch=284, loss=7.8240, this epoch 1.4881, total 420.5619
(T) | Epoch=285, loss=7.7826, this epoch 1.4653, total 422.0272
(T) | Epoch=286, loss=7.9037, this epoch 1.4835, total 423.5108
(T) | Epoch=287, loss=7.9312, this epoch 1.5363, total 425.0471
(T) | Epoch=288, loss=7.9051, this epoch 1.5451, total 426.5922
(T) | Epoch=289, loss=7.9388, this epoch 1.5229, total 428.1152
(T) | Epoch=290, loss=7.8462, this epoch 1.5379, total 429.6531
(T) | Epoch=291, loss=7.7874, this epoch 1.4869, total 431.1399
(T) | Epoch=292, loss=7.7891, this epoch 1.4518, total 432.5917
(T) | Epoch=293, loss=7.9099, this epoch 1.4627, total 434.0544
(T) | Epoch=294, loss=7.7901, this epoch 1.5017, total 435.5561
(T) | Epoch=295, loss=7.7890, this epoch 1.4665, total 437.0226
(T) | Epoch=296, loss=7.7900, this epoch 1.4608, total 438.4834
(T) | Epoch=297, loss=7.8543, this epoch 1.4529, total 439.9363
(T) | Epoch=298, loss=8.0895, this epoch 1.4806, total 441.4169
(T) | Epoch=299, loss=7.7896, this epoch 1.5018, total 442.9186
(T) | Epoch=300, loss=7.9353, this epoch 1.5093, total 444.4279
(T) | Epoch=301, loss=7.7884, this epoch 1.5017, total 445.9296
(T) | Epoch=302, loss=7.9216, this epoch 1.4963, total 447.4259
(T) | Epoch=303, loss=7.7897, this epoch 1.4804, total 448.9063
(T) | Epoch=304, loss=7.7893, this epoch 1.5100, total 450.4163
(T) | Epoch=305, loss=7.7887, this epoch 1.4929, total 451.9092
(T) | Epoch=306, loss=7.9136, this epoch 1.5335, total 453.4427
(T) | Epoch=307, loss=7.9213, this epoch 1.4598, total 454.9025
(T) | Epoch=308, loss=7.7882, this epoch 1.4437, total 456.3463
(T) | Epoch=309, loss=7.8438, this epoch 1.4316, total 457.7779
(T) | Epoch=310, loss=7.7876, this epoch 1.4702, total 459.2481
(T) | Epoch=311, loss=7.7879, this epoch 1.4906, total 460.7387
(T) | Epoch=312, loss=7.7863, this epoch 1.4898, total 462.2285
(T) | Epoch=313, loss=7.7874, this epoch 1.4563, total 463.6848
(T) | Epoch=314, loss=7.8393, this epoch 1.4512, total 465.1360
(T) | Epoch=315, loss=7.8384, this epoch 1.4687, total 466.6047
(T) | Epoch=316, loss=7.7841, this epoch 1.4266, total 468.0313
(T) | Epoch=317, loss=7.8805, this epoch 1.4869, total 469.5182
(T) | Epoch=318, loss=7.7858, this epoch 1.4753, total 470.9935
(T) | Epoch=319, loss=7.7838, this epoch 1.4693, total 472.4629
(T) | Epoch=320, loss=7.7820, this epoch 1.4743, total 473.9372
(T) | Epoch=321, loss=7.8743, this epoch 1.4493, total 475.3865
(T) | Epoch=322, loss=7.8903, this epoch 1.4446, total 476.8311
(T) | Epoch=323, loss=7.8891, this epoch 1.4333, total 478.2644
(T) | Epoch=324, loss=7.7777, this epoch 1.4527, total 479.7171
(T) | Epoch=325, loss=7.8938, this epoch 1.4683, total 481.1854
(T) | Epoch=326, loss=7.7803, this epoch 1.4595, total 482.6449
(T) | Epoch=327, loss=7.7766, this epoch 1.4527, total 484.0976
(T) | Epoch=328, loss=7.7773, this epoch 1.4659, total 485.5635
(T) | Epoch=329, loss=7.7776, this epoch 1.4922, total 487.0556
(T) | Epoch=330, loss=7.8904, this epoch 1.4948, total 488.5504
(T) | Epoch=331, loss=7.8145, this epoch 1.4836, total 490.0340
(T) | Epoch=332, loss=7.7720, this epoch 1.5243, total 491.5583
(T) | Epoch=333, loss=7.8152, this epoch 1.4956, total 493.0539
(T) | Epoch=334, loss=7.7734, this epoch 1.4823, total 494.5362
(T) | Epoch=335, loss=7.7695, this epoch 1.4627, total 495.9989
(T) | Epoch=336, loss=7.7696, this epoch 1.5120, total 497.5109
(T) | Epoch=337, loss=7.8588, this epoch 1.5280, total 499.0389
(T) | Epoch=338, loss=7.8664, this epoch 1.4591, total 500.4980
(T) | Epoch=339, loss=7.8717, this epoch 1.5098, total 502.0078
(T) | Epoch=340, loss=7.8483, this epoch 1.5291, total 503.5369
(T) | Epoch=341, loss=7.7959, this epoch 1.4934, total 505.0303
(T) | Epoch=342, loss=7.7556, this epoch 1.5187, total 506.5490
(T) | Epoch=343, loss=7.9333, this epoch 1.4900, total 508.0390
(T) | Epoch=344, loss=7.8531, this epoch 1.5134, total 509.5524
(T) | Epoch=345, loss=7.9464, this epoch 1.4589, total 511.0113
(T) | Epoch=346, loss=7.7494, this epoch 1.4458, total 512.4571
(T) | Epoch=347, loss=7.8198, this epoch 1.5017, total 513.9588
(T) | Epoch=348, loss=7.8303, this epoch 1.4702, total 515.4290
(T) | Epoch=349, loss=7.8083, this epoch 1.4252, total 516.8542
(T) | Epoch=350, loss=7.7799, this epoch 1.5022, total 518.3564
(T) | Epoch=351, loss=7.7349, this epoch 1.4913, total 519.8477
(T) | Epoch=352, loss=7.8048, this epoch 1.4956, total 521.3433
(T) | Epoch=353, loss=7.7255, this epoch 1.4522, total 522.7955
(T) | Epoch=354, loss=7.7234, this epoch 1.4510, total 524.2464
(T) | Epoch=355, loss=7.7990, this epoch 1.4628, total 525.7093
(T) | Epoch=356, loss=7.7167, this epoch 1.4668, total 527.1761
(T) | Epoch=357, loss=7.7518, this epoch 1.5038, total 528.6799
(T) | Epoch=358, loss=7.7096, this epoch 1.4656, total 530.1456
(T) | Epoch=359, loss=7.7178, this epoch 1.4643, total 531.6099
(T) | Epoch=360, loss=7.7386, this epoch 1.4727, total 533.0826
(T) | Epoch=361, loss=7.6995, this epoch 1.4307, total 534.5133
(T) | Epoch=362, loss=7.7580, this epoch 1.4753, total 535.9886
(T) | Epoch=363, loss=7.7012, this epoch 1.4347, total 537.4233
(T) | Epoch=364, loss=7.7008, this epoch 1.4323, total 538.8556
(T) | Epoch=365, loss=7.7237, this epoch 1.4897, total 540.3453
(T) | Epoch=366, loss=7.6910, this epoch 1.4540, total 541.7992
(T) | Epoch=367, loss=7.7336, this epoch 1.4624, total 543.2616
(T) | Epoch=368, loss=7.6818, this epoch 1.4405, total 544.7021
(T) | Epoch=369, loss=7.8351, this epoch 1.4996, total 546.2017
(T) | Epoch=370, loss=7.7580, this epoch 1.5892, total 547.7909
(T) | Epoch=371, loss=7.6759, this epoch 1.4707, total 549.2616
(T) | Epoch=372, loss=7.6740, this epoch 1.5322, total 550.7938
(T) | Epoch=373, loss=7.6948, this epoch 1.5210, total 552.3148
(T) | Epoch=374, loss=7.6699, this epoch 1.5508, total 553.8656
(T) | Epoch=375, loss=7.6610, this epoch 1.4732, total 555.3388
(T) | Epoch=376, loss=7.8949, this epoch 1.4619, total 556.8007
(T) | Epoch=377, loss=7.6607, this epoch 1.4872, total 558.2879
(T) | Epoch=378, loss=7.6540, this epoch 1.4477, total 559.7356
(T) | Epoch=379, loss=7.6588, this epoch 1.5351, total 561.2707
(T) | Epoch=380, loss=7.6600, this epoch 1.5501, total 562.8208
(T) | Epoch=381, loss=7.6927, this epoch 1.5473, total 564.3681
(T) | Epoch=382, loss=7.6894, this epoch 1.5509, total 565.9190
(T) | Epoch=383, loss=7.6950, this epoch 1.4812, total 567.4003
(T) | Epoch=384, loss=7.6705, this epoch 1.4352, total 568.8355
(T) | Epoch=385, loss=7.6422, this epoch 1.4744, total 570.3098
(T) | Epoch=386, loss=7.6419, this epoch 1.4803, total 571.7901
(T) | Epoch=387, loss=7.6368, this epoch 1.5474, total 573.3375
(T) | Epoch=388, loss=7.6347, this epoch 1.5685, total 574.9059
(T) | Epoch=389, loss=7.6315, this epoch 1.5727, total 576.4787
(T) | Epoch=390, loss=7.6342, this epoch 1.4551, total 577.9338
(T) | Epoch=391, loss=7.6326, this epoch 1.5223, total 579.4561
(T) | Epoch=392, loss=7.6625, this epoch 1.4426, total 580.8987
(T) | Epoch=393, loss=7.6289, this epoch 1.4742, total 582.3729
(T) | Epoch=394, loss=7.6257, this epoch 1.4509, total 583.8238
(T) | Epoch=395, loss=7.6796, this epoch 1.4611, total 585.2850
(T) | Epoch=396, loss=7.6172, this epoch 1.4953, total 586.7803
(T) | Epoch=397, loss=7.6148, this epoch 1.4518, total 588.2321
(T) | Epoch=398, loss=7.6550, this epoch 1.4837, total 589.7158
(T) | Epoch=399, loss=7.6177, this epoch 1.4522, total 591.1680
(T) | Epoch=400, loss=7.6535, this epoch 1.4599, total 592.6279
(T) | Epoch=401, loss=7.6070, this epoch 1.4951, total 594.1230
(T) | Epoch=402, loss=7.6853, this epoch 1.4869, total 595.6099
(T) | Epoch=403, loss=7.7738, this epoch 1.5468, total 597.1567
(T) | Epoch=404, loss=7.6505, this epoch 1.5557, total 598.7124
(T) | Epoch=405, loss=7.6475, this epoch 1.4811, total 600.1934
(T) | Epoch=406, loss=7.6837, this epoch 1.5125, total 601.7059
(T) | Epoch=407, loss=7.6217, this epoch 1.4597, total 603.1656
(T) | Epoch=408, loss=7.6186, this epoch 1.5004, total 604.6660
(T) | Epoch=409, loss=7.6610, this epoch 1.4540, total 606.1200
(T) | Epoch=410, loss=7.6358, this epoch 1.4537, total 607.5737
(T) | Epoch=411, loss=7.6181, this epoch 1.4684, total 609.0421
(T) | Epoch=412, loss=7.6446, this epoch 1.4888, total 610.5309
(T) | Epoch=413, loss=7.6595, this epoch 1.4863, total 612.0172
(T) | Epoch=414, loss=7.6464, this epoch 1.4938, total 613.5110
(T) | Epoch=415, loss=7.6406, this epoch 1.4311, total 614.9420
(T) | Epoch=416, loss=7.6049, this epoch 1.4942, total 616.4362
(T) | Epoch=417, loss=7.6072, this epoch 1.4873, total 617.9236
(T) | Epoch=418, loss=7.6069, this epoch 1.5034, total 619.4270
(T) | Epoch=419, loss=7.6294, this epoch 1.4987, total 620.9257
(T) | Epoch=420, loss=7.5997, this epoch 1.4945, total 622.4201
(T) | Epoch=421, loss=7.6541, this epoch 1.4706, total 623.8907
(T) | Epoch=422, loss=7.6473, this epoch 1.4976, total 625.3883
(T) | Epoch=423, loss=7.6440, this epoch 1.4887, total 626.8771
(T) | Epoch=424, loss=7.6294, this epoch 1.4748, total 628.3518
(T) | Epoch=425, loss=7.5835, this epoch 1.4650, total 629.8169
(T) | Epoch=426, loss=7.6154, this epoch 1.4675, total 631.2843
(T) | Epoch=427, loss=7.5951, this epoch 1.5040, total 632.7883
(T) | Epoch=428, loss=7.6772, this epoch 1.4840, total 634.2723
(T) | Epoch=429, loss=7.5929, this epoch 1.5215, total 635.7938
(T) | Epoch=430, loss=7.5881, this epoch 1.5218, total 637.3156
(T) | Epoch=431, loss=7.5790, this epoch 1.5268, total 638.8423
(T) | Epoch=432, loss=7.6305, this epoch 1.4782, total 640.3205
(T) | Epoch=433, loss=7.5647, this epoch 1.5138, total 641.8342
(T) | Epoch=434, loss=7.5661, this epoch 1.5039, total 643.3382
(T) | Epoch=435, loss=8.8555, this epoch 1.4627, total 644.8009
(T) | Epoch=436, loss=7.5970, this epoch 1.5201, total 646.3210
(T) | Epoch=437, loss=7.6141, this epoch 1.5031, total 647.8241
(T) | Epoch=438, loss=7.6272, this epoch 1.4657, total 649.2898
(T) | Epoch=439, loss=7.6476, this epoch 1.5219, total 650.8117
(T) | Epoch=440, loss=7.6558, this epoch 1.5015, total 652.3132
(T) | Epoch=441, loss=7.6616, this epoch 1.4164, total 653.7296
(T) | Epoch=442, loss=7.7359, this epoch 1.4197, total 655.1493
(T) | Epoch=443, loss=7.6688, this epoch 1.4677, total 656.6170
(T) | Epoch=444, loss=7.7759, this epoch 1.5242, total 658.1412
(T) | Epoch=445, loss=7.6888, this epoch 1.4935, total 659.6347
(T) | Epoch=446, loss=7.6943, this epoch 1.5020, total 661.1368
(T) | Epoch=447, loss=7.6920, this epoch 1.4453, total 662.5821
(T) | Epoch=448, loss=7.7044, this epoch 1.4780, total 664.0601
(T) | Epoch=449, loss=7.7629, this epoch 1.5092, total 665.5693
(T) | Epoch=450, loss=7.7834, this epoch 1.5035, total 667.0727
(T) | Epoch=451, loss=7.7058, this epoch 1.4714, total 668.5441
(T) | Epoch=452, loss=7.7130, this epoch 1.5290, total 670.0731
(T) | Epoch=453, loss=7.7006, this epoch 1.5077, total 671.5808
(T) | Epoch=454, loss=7.8071, this epoch 1.4843, total 673.0651
(T) | Epoch=455, loss=7.7116, this epoch 1.4955, total 674.5605
(T) | Epoch=456, loss=7.6989, this epoch 1.4484, total 676.0089
(T) | Epoch=457, loss=7.7952, this epoch 1.4790, total 677.4879
(T) | Epoch=458, loss=7.7380, this epoch 1.5233, total 679.0112
(T) | Epoch=459, loss=7.6934, this epoch 1.4847, total 680.4959
(T) | Epoch=460, loss=7.7487, this epoch 1.4492, total 681.9450
(T) | Epoch=461, loss=7.6262, this epoch 1.4722, total 683.4172
(T) | Epoch=462, loss=7.6856, this epoch 1.4677, total 684.8849
(T) | Epoch=463, loss=7.7327, this epoch 1.4799, total 686.3648
(T) | Epoch=464, loss=7.7354, this epoch 1.5383, total 687.9031
(T) | Epoch=465, loss=7.6769, this epoch 1.4610, total 689.3641
(T) | Epoch=466, loss=7.6861, this epoch 1.4824, total 690.8464
(T) | Epoch=467, loss=7.6730, this epoch 1.4718, total 692.3183
(T) | Epoch=468, loss=7.7102, this epoch 1.4900, total 693.8083
(T) | Epoch=469, loss=7.7181, this epoch 1.5229, total 695.3311
(T) | Epoch=470, loss=7.6789, this epoch 1.4613, total 696.7924
(T) | Epoch=471, loss=7.6631, this epoch 1.4449, total 698.2373
(T) | Epoch=472, loss=7.6953, this epoch 1.4776, total 699.7149
(T) | Epoch=473, loss=7.6945, this epoch 1.5671, total 701.2820
(T) | Epoch=474, loss=7.6669, this epoch 1.4984, total 702.7804
(T) | Epoch=475, loss=7.7069, this epoch 1.4684, total 704.2487
(T) | Epoch=476, loss=7.6591, this epoch 1.4696, total 705.7183
(T) | Epoch=477, loss=7.8788, this epoch 1.4644, total 707.1827
(T) | Epoch=478, loss=7.6518, this epoch 1.5091, total 708.6918
(T) | Epoch=479, loss=7.6897, this epoch 1.5108, total 710.2026
(T) | Epoch=480, loss=7.6492, this epoch 1.5455, total 711.7481
(T) | Epoch=481, loss=7.6469, this epoch 1.5019, total 713.2500
(T) | Epoch=482, loss=7.6953, this epoch 1.4878, total 714.7379
(T) | Epoch=483, loss=7.6424, this epoch 1.4622, total 716.2000
(T) | Epoch=484, loss=7.6413, this epoch 1.5188, total 717.7188
(T) | Epoch=485, loss=7.6418, this epoch 1.4617, total 719.1805
(T) | Epoch=486, loss=7.6354, this epoch 1.5344, total 720.7149
(T) | Epoch=487, loss=7.6761, this epoch 1.4388, total 722.1537
(T) | Epoch=488, loss=7.6678, this epoch 1.5241, total 723.6778
(T) | Epoch=489, loss=7.6692, this epoch 1.4925, total 725.1703
(T) | Epoch=490, loss=7.6350, this epoch 1.4977, total 726.6679
(T) | Epoch=491, loss=7.7432, this epoch 1.4379, total 728.1059
(T) | Epoch=492, loss=7.6367, this epoch 1.5278, total 729.6337
(T) | Epoch=493, loss=7.6636, this epoch 1.4524, total 731.0861
(T) | Epoch=494, loss=7.6340, this epoch 1.4548, total 732.5409
(T) | Epoch=495, loss=7.7777, this epoch 1.4578, total 733.9987
(T) | Epoch=496, loss=7.6382, this epoch 1.4814, total 735.4801
(T) | Epoch=497, loss=7.6332, this epoch 1.4457, total 736.9257
(T) | Epoch=498, loss=7.6808, this epoch 1.4998, total 738.4255
(T) | Epoch=499, loss=7.6218, this epoch 1.4917, total 739.9172
(T) | Epoch=500, loss=7.6256, this epoch 1.4868, total 741.4040
(T) | Epoch=501, loss=7.6146, this epoch 1.4953, total 742.8993
(T) | Epoch=502, loss=7.6180, this epoch 1.4979, total 744.3971
(T) | Epoch=503, loss=7.6192, this epoch 1.5200, total 745.9172
(T) | Epoch=504, loss=7.6568, this epoch 1.5158, total 747.4329
(T) | Epoch=505, loss=7.6222, this epoch 1.4669, total 748.8999
(T) | Epoch=506, loss=7.6270, this epoch 1.4952, total 750.3951
(T) | Epoch=507, loss=7.7611, this epoch 1.5262, total 751.9212
(T) | Epoch=508, loss=7.6683, this epoch 1.4894, total 753.4107
(T) | Epoch=509, loss=7.6216, this epoch 1.4745, total 754.8852
(T) | Epoch=510, loss=7.6057, this epoch 1.4557, total 756.3408
(T) | Epoch=511, loss=7.5973, this epoch 1.5061, total 757.8469
(T) | Epoch=512, loss=7.6436, this epoch 1.4706, total 759.3175
(T) | Epoch=513, loss=7.6028, this epoch 1.4586, total 760.7761
(T) | Epoch=514, loss=7.6421, this epoch 1.4453, total 762.2214
(T) | Epoch=515, loss=7.6110, this epoch 1.4815, total 763.7029
(T) | Epoch=516, loss=7.5971, this epoch 1.4827, total 765.1857
(T) | Epoch=517, loss=7.5968, this epoch 1.4561, total 766.6417
(T) | Epoch=518, loss=7.5976, this epoch 1.4885, total 768.1302
(T) | Epoch=519, loss=7.6575, this epoch 1.4579, total 769.5881
(T) | Epoch=520, loss=7.5918, this epoch 1.4730, total 771.0611
(T) | Epoch=521, loss=7.8254, this epoch 1.4426, total 772.5037
(T) | Epoch=522, loss=7.5873, this epoch 1.5482, total 774.0519
(T) | Epoch=523, loss=7.6075, this epoch 1.4774, total 775.5293
(T) | Epoch=524, loss=7.5892, this epoch 1.4269, total 776.9563
(T) | Epoch=525, loss=7.5842, this epoch 1.4926, total 778.4488
(T) | Epoch=526, loss=7.5914, this epoch 1.4581, total 779.9069
(T) | Epoch=527, loss=7.5829, this epoch 1.4799, total 781.3869
(T) | Epoch=528, loss=7.6304, this epoch 1.5121, total 782.8989
(T) | Epoch=529, loss=7.6188, this epoch 1.4381, total 784.3370
(T) | Epoch=530, loss=7.5749, this epoch 1.4728, total 785.8098
(T) | Epoch=531, loss=7.5748, this epoch 1.5153, total 787.3251
(T) | Epoch=532, loss=7.5735, this epoch 1.5026, total 788.8277
(T) | Epoch=533, loss=7.5773, this epoch 1.4966, total 790.3243
(T) | Epoch=534, loss=7.5610, this epoch 1.4637, total 791.7880
(T) | Epoch=535, loss=7.5635, this epoch 1.5109, total 793.2989
(T) | Epoch=536, loss=7.5694, this epoch 1.4874, total 794.7863
(T) | Epoch=537, loss=7.5600, this epoch 1.4643, total 796.2506
(T) | Epoch=538, loss=7.5680, this epoch 1.4997, total 797.7504
(T) | Epoch=539, loss=7.6645, this epoch 1.4693, total 799.2197
(T) | Epoch=540, loss=7.5580, this epoch 1.4828, total 800.7025
(T) | Epoch=541, loss=7.6246, this epoch 1.4808, total 802.1833
(T) | Epoch=542, loss=7.6086, this epoch 1.4521, total 803.6354
(T) | Epoch=543, loss=7.5702, this epoch 1.4665, total 805.1019
(T) | Epoch=544, loss=7.5940, this epoch 1.4598, total 806.5617
(T) | Epoch=545, loss=7.6040, this epoch 1.4643, total 808.0260
(T) | Epoch=546, loss=7.6244, this epoch 1.4997, total 809.5257
(T) | Epoch=547, loss=7.5849, this epoch 1.4896, total 811.0153
(T) | Epoch=548, loss=7.6130, this epoch 1.4803, total 812.4956
(T) | Epoch=549, loss=7.5738, this epoch 1.4511, total 813.9467
(T) | Epoch=550, loss=7.5826, this epoch 1.4586, total 815.4052
(T) | Epoch=551, loss=7.6111, this epoch 1.4301, total 816.8354
(T) | Epoch=552, loss=7.5868, this epoch 1.4832, total 818.3185
(T) | Epoch=553, loss=7.5866, this epoch 1.4845, total 819.8030
(T) | Epoch=554, loss=7.5646, this epoch 1.4746, total 821.2776
(T) | Epoch=555, loss=7.5692, this epoch 1.5347, total 822.8122
(T) | Epoch=556, loss=7.6203, this epoch 1.4321, total 824.2443
(T) | Epoch=557, loss=7.5584, this epoch 1.4646, total 825.7089
(T) | Epoch=558, loss=7.5464, this epoch 1.4828, total 827.1917
+++model saved ! 2016.pth
(T) | Epoch=559, loss=7.5851, this epoch 1.4640, total 828.6558
(T) | Epoch=560, loss=7.6410, this epoch 1.4302, total 830.0859
(T) | Epoch=561, loss=7.6574, this epoch 1.4324, total 831.5183
(T) | Epoch=562, loss=7.6625, this epoch 1.4533, total 832.9717
(T) | Epoch=563, loss=7.5632, this epoch 1.4421, total 834.4138
(T) | Epoch=564, loss=7.5562, this epoch 1.4231, total 835.8369
(T) | Epoch=565, loss=7.5854, this epoch 1.4554, total 837.2922
(T) | Epoch=566, loss=7.6233, this epoch 1.4920, total 838.7842
(T) | Epoch=567, loss=7.6616, this epoch 1.5084, total 840.2926
(T) | Epoch=568, loss=7.6135, this epoch 1.5181, total 841.8107
(T) | Epoch=569, loss=7.5859, this epoch 1.5234, total 843.3340
(T) | Epoch=570, loss=7.6316, this epoch 1.4756, total 844.8096
(T) | Epoch=571, loss=7.6261, this epoch 1.5220, total 846.3317
(T) | Epoch=572, loss=7.5836, this epoch 1.5056, total 847.8373
(T) | Epoch=573, loss=7.5971, this epoch 1.4559, total 849.2932
(T) | Epoch=574, loss=7.5732, this epoch 1.4214, total 850.7146
(T) | Epoch=575, loss=7.6232, this epoch 1.5535, total 852.2681
(T) | Epoch=576, loss=7.5735, this epoch 1.5223, total 853.7904
(T) | Epoch=577, loss=7.6003, this epoch 1.4753, total 855.2657
(T) | Epoch=578, loss=7.5478, this epoch 1.4790, total 856.7447
(T) | Epoch=579, loss=7.5747, this epoch 1.4286, total 858.1734
(T) | Epoch=580, loss=7.5865, this epoch 1.4465, total 859.6198
(T) | Epoch=581, loss=7.5419, this epoch 1.4532, total 861.0730
+++model saved ! 2016.pth
(T) | Epoch=582, loss=7.5377, this epoch 1.4742, total 862.5472
+++model saved ! 2016.pth
(T) | Epoch=583, loss=7.5394, this epoch 1.4643, total 864.0115
(T) | Epoch=584, loss=7.5436, this epoch 1.5299, total 865.5414
(T) | Epoch=585, loss=7.6660, this epoch 1.4797, total 867.0211
(T) | Epoch=586, loss=7.6943, this epoch 1.4839, total 868.5049
(T) | Epoch=587, loss=7.5414, this epoch 1.4537, total 869.9587
(T) | Epoch=588, loss=7.5496, this epoch 1.4872, total 871.4458
(T) | Epoch=589, loss=7.6015, this epoch 1.4706, total 872.9164
(T) | Epoch=590, loss=7.5909, this epoch 1.4336, total 874.3500
(T) | Epoch=591, loss=7.5737, this epoch 1.4852, total 875.8352
(T) | Epoch=592, loss=7.5666, this epoch 1.4759, total 877.3111
(T) | Epoch=593, loss=7.6187, this epoch 1.4887, total 878.7999
(T) | Epoch=594, loss=7.5772, this epoch 1.5504, total 880.3503
(T) | Epoch=595, loss=7.5701, this epoch 1.5521, total 881.9023
(T) | Epoch=596, loss=7.5685, this epoch 1.5389, total 883.4412
(T) | Epoch=597, loss=7.7428, this epoch 1.4619, total 884.9031
(T) | Epoch=598, loss=7.6816, this epoch 1.5073, total 886.4105
(T) | Epoch=599, loss=7.5591, this epoch 1.4786, total 887.8891
(T) | Epoch=600, loss=7.5543, this epoch 1.4650, total 889.3541
(T) | Epoch=601, loss=7.5813, this epoch 1.4748, total 890.8289
(T) | Epoch=602, loss=7.5786, this epoch 1.4576, total 892.2865
(T) | Epoch=603, loss=7.5394, this epoch 1.4813, total 893.7678
(T) | Epoch=604, loss=7.5837, this epoch 1.4476, total 895.2154
(T) | Epoch=605, loss=7.6497, this epoch 1.5055, total 896.7210
(T) | Epoch=606, loss=7.6468, this epoch 1.4579, total 898.1789
(T) | Epoch=607, loss=7.5772, this epoch 1.4314, total 899.6103
(T) | Epoch=608, loss=7.5956, this epoch 1.4916, total 901.1019
(T) | Epoch=609, loss=7.5851, this epoch 1.4783, total 902.5802
(T) | Epoch=610, loss=7.5447, this epoch 1.4815, total 904.0617
(T) | Epoch=611, loss=7.5901, this epoch 1.4581, total 905.5198
(T) | Epoch=612, loss=7.5597, this epoch 1.5034, total 907.0232
(T) | Epoch=613, loss=7.5652, this epoch 1.4592, total 908.4824
(T) | Epoch=614, loss=7.6075, this epoch 1.4907, total 909.9730
(T) | Epoch=615, loss=7.5424, this epoch 1.4486, total 911.4217
(T) | Epoch=616, loss=7.5880, this epoch 1.4877, total 912.9094
(T) | Epoch=617, loss=7.5568, this epoch 1.4798, total 914.3892
(T) | Epoch=618, loss=7.9246, this epoch 1.4935, total 915.8827
(T) | Epoch=619, loss=7.5886, this epoch 1.5000, total 917.3826
(T) | Epoch=620, loss=7.5779, this epoch 1.5354, total 918.9180
(T) | Epoch=621, loss=7.6063, this epoch 1.5234, total 920.4414
(T) | Epoch=622, loss=7.7040, this epoch 1.5176, total 921.9590
(T) | Epoch=623, loss=7.6573, this epoch 1.4734, total 923.4323
(T) | Epoch=624, loss=7.6209, this epoch 1.4634, total 924.8957
(T) | Epoch=625, loss=7.7462, this epoch 1.4401, total 926.3358
(T) | Epoch=626, loss=7.7053, this epoch 1.4467, total 927.7825
(T) | Epoch=627, loss=7.7241, this epoch 1.4794, total 929.2619
(T) | Epoch=628, loss=7.6715, this epoch 1.4467, total 930.7086
(T) | Epoch=629, loss=7.7613, this epoch 1.5048, total 932.2134
(T) | Epoch=630, loss=7.6643, this epoch 1.5205, total 933.7339
(T) | Epoch=631, loss=7.6158, this epoch 1.4638, total 935.1976
(T) | Epoch=632, loss=7.6885, this epoch 1.4477, total 936.6453
(T) | Epoch=633, loss=7.7158, this epoch 1.5168, total 938.1621
(T) | Epoch=634, loss=7.6617, this epoch 1.4742, total 939.6363
(T) | Epoch=635, loss=7.6372, this epoch 1.4902, total 941.1265
(T) | Epoch=636, loss=7.5970, this epoch 1.4223, total 942.5488
(T) | Epoch=637, loss=7.5732, this epoch 1.4646, total 944.0134
(T) | Epoch=638, loss=7.5922, this epoch 1.4342, total 945.4476
(T) | Epoch=639, loss=7.5744, this epoch 1.4774, total 946.9249
(T) | Epoch=640, loss=7.5679, this epoch 1.4574, total 948.3823
(T) | Epoch=641, loss=7.5585, this epoch 1.4343, total 949.8166
(T) | Epoch=642, loss=7.5652, this epoch 1.4527, total 951.2693
(T) | Epoch=643, loss=7.5579, this epoch 1.4857, total 952.7550
(T) | Epoch=644, loss=7.5418, this epoch 1.4668, total 954.2218
(T) | Epoch=645, loss=7.5535, this epoch 1.4573, total 955.6791
(T) | Epoch=646, loss=7.6644, this epoch 1.4540, total 957.1331
(T) | Epoch=647, loss=7.5938, this epoch 1.4292, total 958.5623
(T) | Epoch=648, loss=7.5303, this epoch 1.4857, total 960.0480
+++model saved ! 2016.pth
(T) | Epoch=649, loss=7.5369, this epoch 1.5094, total 961.5575
(T) | Epoch=650, loss=7.5302, this epoch 1.4986, total 963.0561
+++model saved ! 2016.pth
(T) | Epoch=651, loss=7.7184, this epoch 1.4911, total 964.5471
(T) | Epoch=652, loss=7.5743, this epoch 1.4625, total 966.0096
(T) | Epoch=653, loss=7.5901, this epoch 1.4718, total 967.4815
(T) | Epoch=654, loss=7.5366, this epoch 1.4329, total 968.9144
(T) | Epoch=655, loss=7.5400, this epoch 1.4990, total 970.4134
(T) | Epoch=656, loss=7.5406, this epoch 1.4317, total 971.8451
(T) | Epoch=657, loss=7.5509, this epoch 1.4306, total 973.2757
(T) | Epoch=658, loss=7.5420, this epoch 1.4376, total 974.7132
(T) | Epoch=659, loss=7.5411, this epoch 1.4839, total 976.1971
(T) | Epoch=660, loss=7.5595, this epoch 1.4250, total 977.6222
(T) | Epoch=661, loss=7.5507, this epoch 1.4400, total 979.0622
(T) | Epoch=662, loss=7.5366, this epoch 1.4438, total 980.5060
(T) | Epoch=663, loss=7.5315, this epoch 1.4865, total 981.9925
(T) | Epoch=664, loss=7.5941, this epoch 1.4581, total 983.4506
(T) | Epoch=665, loss=7.5324, this epoch 1.4639, total 984.9145
(T) | Epoch=666, loss=7.5218, this epoch 1.4766, total 986.3911
+++model saved ! 2016.pth
(T) | Epoch=667, loss=7.5428, this epoch 1.4940, total 987.8852
(T) | Epoch=668, loss=7.5979, this epoch 1.4776, total 989.3627
(T) | Epoch=669, loss=7.5203, this epoch 1.4976, total 990.8604
+++model saved ! 2016.pth
(T) | Epoch=670, loss=7.5354, this epoch 1.4895, total 992.3499
(T) | Epoch=671, loss=7.5235, this epoch 1.5135, total 993.8634
(T) | Epoch=672, loss=7.5090, this epoch 1.4791, total 995.3424
+++model saved ! 2016.pth
(T) | Epoch=673, loss=7.6084, this epoch 1.4548, total 996.7972
(T) | Epoch=674, loss=7.6919, this epoch 1.4587, total 998.2559
(T) | Epoch=675, loss=7.6284, this epoch 1.4682, total 999.7240
(T) | Epoch=676, loss=7.5683, this epoch 1.4826, total 1001.2066
(T) | Epoch=677, loss=7.5386, this epoch 1.4864, total 1002.6930
(T) | Epoch=678, loss=7.5375, this epoch 1.5197, total 1004.2127
(T) | Epoch=679, loss=7.6165, this epoch 1.4572, total 1005.6699
(T) | Epoch=680, loss=7.5592, this epoch 1.5017, total 1007.1716
(T) | Epoch=681, loss=7.5891, this epoch 1.5487, total 1008.7204
(T) | Epoch=682, loss=7.6065, this epoch 1.4834, total 1010.2037
(T) | Epoch=683, loss=7.5375, this epoch 1.4822, total 1011.6860
(T) | Epoch=684, loss=7.5520, this epoch 1.4713, total 1013.1573
(T) | Epoch=685, loss=7.5973, this epoch 1.4798, total 1014.6371
(T) | Epoch=686, loss=7.5370, this epoch 1.4765, total 1016.1136
(T) | Epoch=687, loss=7.5928, this epoch 1.4841, total 1017.5977
(T) | Epoch=688, loss=7.5405, this epoch 1.4585, total 1019.0562
(T) | Epoch=689, loss=7.5507, this epoch 1.4539, total 1020.5101
(T) | Epoch=690, loss=7.5025, this epoch 1.4816, total 1021.9916
+++model saved ! 2016.pth
(T) | Epoch=691, loss=7.5100, this epoch 1.5001, total 1023.4917
(T) | Epoch=692, loss=7.5161, this epoch 1.5238, total 1025.0155
(T) | Epoch=693, loss=7.5075, this epoch 1.4674, total 1026.4829
(T) | Epoch=694, loss=7.5037, this epoch 1.4706, total 1027.9535
(T) | Epoch=695, loss=7.6855, this epoch 1.4517, total 1029.4052
(T) | Epoch=696, loss=7.6191, this epoch 1.4877, total 1030.8929
(T) | Epoch=697, loss=7.4859, this epoch 1.4770, total 1032.3698
+++model saved ! 2016.pth
(T) | Epoch=698, loss=7.4998, this epoch 1.5251, total 1033.8949
(T) | Epoch=699, loss=7.6113, this epoch 1.4843, total 1035.3792
(T) | Epoch=700, loss=7.5673, this epoch 1.4654, total 1036.8446
(T) | Epoch=701, loss=7.6014, this epoch 1.4585, total 1038.3032
(T) | Epoch=702, loss=7.5841, this epoch 1.4519, total 1039.7551
(T) | Epoch=703, loss=7.5623, this epoch 1.4317, total 1041.1867
(T) | Epoch=704, loss=7.5112, this epoch 1.4854, total 1042.6721
(T) | Epoch=705, loss=7.5196, this epoch 1.4647, total 1044.1368
(T) | Epoch=706, loss=7.5050, this epoch 1.4653, total 1045.6021
(T) | Epoch=707, loss=7.5091, this epoch 1.5048, total 1047.1070
(T) | Epoch=708, loss=7.5170, this epoch 1.5025, total 1048.6095
(T) | Epoch=709, loss=7.4916, this epoch 1.4698, total 1050.0793
(T) | Epoch=710, loss=7.4894, this epoch 1.4697, total 1051.5490
(T) | Epoch=711, loss=7.4918, this epoch 1.4589, total 1053.0080
(T) | Epoch=712, loss=7.4801, this epoch 1.4426, total 1054.4505
+++model saved ! 2016.pth
(T) | Epoch=713, loss=7.4883, this epoch 1.4408, total 1055.8913
(T) | Epoch=714, loss=7.5451, this epoch 1.4334, total 1057.3247
(T) | Epoch=715, loss=7.5167, this epoch 1.4178, total 1058.7425
(T) | Epoch=716, loss=7.7756, this epoch 1.4413, total 1060.1837
(T) | Epoch=717, loss=7.5639, this epoch 1.4162, total 1061.5999
(T) | Epoch=718, loss=7.5330, this epoch 1.4647, total 1063.0646
(T) | Epoch=719, loss=7.4973, this epoch 1.4544, total 1064.5190
(T) | Epoch=720, loss=7.5384, this epoch 1.4669, total 1065.9859
(T) | Epoch=721, loss=7.5110, this epoch 1.4707, total 1067.4566
(T) | Epoch=722, loss=7.5924, this epoch 1.4461, total 1068.9027
(T) | Epoch=723, loss=7.6101, this epoch 1.4747, total 1070.3774
(T) | Epoch=724, loss=7.6637, this epoch 1.4827, total 1071.8601
(T) | Epoch=725, loss=7.5691, this epoch 1.4480, total 1073.3082
(T) | Epoch=726, loss=7.5562, this epoch 1.4674, total 1074.7755
(T) | Epoch=727, loss=7.5286, this epoch 1.4671, total 1076.2426
(T) | Epoch=728, loss=7.6278, this epoch 1.4792, total 1077.7218
(T) | Epoch=729, loss=7.4906, this epoch 1.4531, total 1079.1749
(T) | Epoch=730, loss=7.5124, this epoch 1.4698, total 1080.6448
(T) | Epoch=731, loss=7.5713, this epoch 1.4659, total 1082.1106
(T) | Epoch=732, loss=7.5772, this epoch 1.4924, total 1083.6030
(T) | Epoch=733, loss=7.4962, this epoch 1.4566, total 1085.0596
(T) | Epoch=734, loss=7.6442, this epoch 1.5019, total 1086.5615
(T) | Epoch=735, loss=7.5288, this epoch 1.4517, total 1088.0132
(T) | Epoch=736, loss=7.6173, this epoch 1.4928, total 1089.5060
(T) | Epoch=737, loss=7.4920, this epoch 1.4970, total 1091.0030
(T) | Epoch=738, loss=7.4910, this epoch 1.4516, total 1092.4545
(T) | Epoch=739, loss=7.4957, this epoch 1.4901, total 1093.9446
(T) | Epoch=740, loss=7.5523, this epoch 1.4749, total 1095.4195
(T) | Epoch=741, loss=7.4833, this epoch 1.5191, total 1096.9386
(T) | Epoch=742, loss=7.4919, this epoch 1.4910, total 1098.4296
(T) | Epoch=743, loss=7.5959, this epoch 1.4921, total 1099.9218
(T) | Epoch=744, loss=7.5345, this epoch 1.4822, total 1101.4039
(T) | Epoch=745, loss=7.5529, this epoch 1.4718, total 1102.8757
(T) | Epoch=746, loss=7.6533, this epoch 1.4382, total 1104.3140
(T) | Epoch=747, loss=7.5002, this epoch 1.4367, total 1105.7507
(T) | Epoch=748, loss=7.6444, this epoch 1.4363, total 1107.1870
(T) | Epoch=749, loss=7.5042, this epoch 1.4800, total 1108.6669
(T) | Epoch=750, loss=7.4901, this epoch 1.4699, total 1110.1368
(T) | Epoch=751, loss=7.4888, this epoch 1.4654, total 1111.6023
(T) | Epoch=752, loss=7.4750, this epoch 1.4754, total 1113.0776
+++model saved ! 2016.pth
(T) | Epoch=753, loss=7.5470, this epoch 1.4690, total 1114.5466
(T) | Epoch=754, loss=7.5540, this epoch 1.4601, total 1116.0068
(T) | Epoch=755, loss=7.4729, this epoch 1.4725, total 1117.4793
+++model saved ! 2016.pth
(T) | Epoch=756, loss=7.5052, this epoch 1.4494, total 1118.9287
(T) | Epoch=757, loss=7.4819, this epoch 1.4715, total 1120.4002
(T) | Epoch=758, loss=7.6917, this epoch 1.4255, total 1121.8257
(T) | Epoch=759, loss=7.5317, this epoch 1.4444, total 1123.2701
(T) | Epoch=760, loss=7.4822, this epoch 1.4231, total 1124.6932
(T) | Epoch=761, loss=7.4894, this epoch 1.4483, total 1126.1415
(T) | Epoch=762, loss=7.5406, this epoch 1.4418, total 1127.5833
(T) | Epoch=763, loss=7.5323, this epoch 1.4969, total 1129.0802
(T) | Epoch=764, loss=7.6193, this epoch 1.4769, total 1130.5571
(T) | Epoch=765, loss=7.5374, this epoch 1.4978, total 1132.0548
(T) | Epoch=766, loss=7.5751, this epoch 1.4671, total 1133.5220
(T) | Epoch=767, loss=7.5834, this epoch 1.4779, total 1134.9999
(T) | Epoch=768, loss=7.7935, this epoch 1.4868, total 1136.4867
(T) | Epoch=769, loss=7.5117, this epoch 1.5129, total 1137.9996
(T) | Epoch=770, loss=7.5153, this epoch 1.5069, total 1139.5064
(T) | Epoch=771, loss=7.5309, this epoch 1.4575, total 1140.9639
(T) | Epoch=772, loss=7.6173, this epoch 1.4506, total 1142.4145
(T) | Epoch=773, loss=7.5414, this epoch 1.4617, total 1143.8762
(T) | Epoch=774, loss=7.5881, this epoch 1.4423, total 1145.3185
(T) | Epoch=775, loss=7.4857, this epoch 1.4848, total 1146.8033
(T) | Epoch=776, loss=7.4799, this epoch 1.4816, total 1148.2848
(T) | Epoch=777, loss=7.5660, this epoch 1.4626, total 1149.7474
(T) | Epoch=778, loss=7.4705, this epoch 1.4816, total 1151.2290
+++model saved ! 2016.pth
(T) | Epoch=779, loss=7.5744, this epoch 1.4764, total 1152.7054
(T) | Epoch=780, loss=7.4698, this epoch 1.4800, total 1154.1854
+++model saved ! 2016.pth
(T) | Epoch=781, loss=7.5405, this epoch 1.5080, total 1155.6934
(T) | Epoch=782, loss=7.4797, this epoch 1.5209, total 1157.2143
(T) | Epoch=783, loss=7.6766, this epoch 1.4691, total 1158.6834
(T) | Epoch=784, loss=7.4664, this epoch 1.4262, total 1160.1095
+++model saved ! 2016.pth
(T) | Epoch=785, loss=7.5509, this epoch 1.4402, total 1161.5497
(T) | Epoch=786, loss=7.4781, this epoch 1.4408, total 1162.9905
(T) | Epoch=787, loss=7.4693, this epoch 1.4484, total 1164.4390
(T) | Epoch=788, loss=7.4842, this epoch 1.4832, total 1165.9222
(T) | Epoch=789, loss=7.5137, this epoch 1.4744, total 1167.3966
(T) | Epoch=790, loss=7.5145, this epoch 1.4927, total 1168.8893
(T) | Epoch=791, loss=7.6304, this epoch 1.4470, total 1170.3363
(T) | Epoch=792, loss=7.5807, this epoch 1.4880, total 1171.8244
(T) | Epoch=793, loss=7.5676, this epoch 1.4765, total 1173.3009
(T) | Epoch=794, loss=7.5561, this epoch 1.4595, total 1174.7603
(T) | Epoch=795, loss=7.4869, this epoch 1.4775, total 1176.2378
(T) | Epoch=796, loss=7.4931, this epoch 1.5103, total 1177.7481
(T) | Epoch=797, loss=7.5260, this epoch 1.4742, total 1179.2223
(T) | Epoch=798, loss=7.4712, this epoch 1.4474, total 1180.6697
(T) | Epoch=799, loss=7.5479, this epoch 1.4468, total 1182.1165
(T) | Epoch=800, loss=7.4716, this epoch 1.4618, total 1183.5783
(T) | Epoch=801, loss=7.5732, this epoch 1.4403, total 1185.0186
(T) | Epoch=802, loss=7.4689, this epoch 1.4337, total 1186.4523
(T) | Epoch=803, loss=7.4664, this epoch 1.4407, total 1187.8929
+++model saved ! 2016.pth
(T) | Epoch=804, loss=7.4937, this epoch 1.4649, total 1189.3578
(T) | Epoch=805, loss=7.4592, this epoch 1.4801, total 1190.8380
+++model saved ! 2016.pth
(T) | Epoch=806, loss=7.4727, this epoch 1.4714, total 1192.3094
(T) | Epoch=807, loss=7.4544, this epoch 1.4589, total 1193.7683
+++model saved ! 2016.pth
(T) | Epoch=808, loss=7.4552, this epoch 1.5211, total 1195.2895
(T) | Epoch=809, loss=7.4606, this epoch 1.4647, total 1196.7541
(T) | Epoch=810, loss=7.5576, this epoch 1.4915, total 1198.2456
(T) | Epoch=811, loss=7.4504, this epoch 1.4927, total 1199.7383
+++model saved ! 2016.pth
(T) | Epoch=812, loss=7.4696, this epoch 1.5070, total 1201.2453
(T) | Epoch=813, loss=7.4539, this epoch 1.4733, total 1202.7186
(T) | Epoch=814, loss=7.5601, this epoch 1.4983, total 1204.2169
(T) | Epoch=815, loss=7.4713, this epoch 1.4780, total 1205.6949
(T) | Epoch=816, loss=7.4558, this epoch 1.4535, total 1207.1484
(T) | Epoch=817, loss=7.4746, this epoch 1.5497, total 1208.6981
(T) | Epoch=818, loss=7.4876, this epoch 1.4757, total 1210.1739
(T) | Epoch=819, loss=7.4837, this epoch 1.4623, total 1211.6362
(T) | Epoch=820, loss=7.4662, this epoch 1.4746, total 1213.1108
(T) | Epoch=821, loss=7.4507, this epoch 1.4495, total 1214.5602
(T) | Epoch=822, loss=7.5482, this epoch 1.5099, total 1216.0702
(T) | Epoch=823, loss=8.8821, this epoch 1.5040, total 1217.5741
(T) | Epoch=824, loss=7.4677, this epoch 1.4515, total 1219.0256
(T) | Epoch=825, loss=7.5743, this epoch 1.4566, total 1220.4823
(T) | Epoch=826, loss=7.6496, this epoch 1.4660, total 1221.9483
(T) | Epoch=827, loss=7.6390, this epoch 1.4424, total 1223.3907
(T) | Epoch=828, loss=7.5075, this epoch 1.4371, total 1224.8278
(T) | Epoch=829, loss=7.8442, this epoch 1.4996, total 1226.3274
(T) | Epoch=830, loss=7.8455, this epoch 1.4596, total 1227.7870
(T) | Epoch=831, loss=7.6267, this epoch 1.4634, total 1229.2505
(T) | Epoch=832, loss=7.8708, this epoch 1.4577, total 1230.7081
(T) | Epoch=833, loss=7.6390, this epoch 1.4630, total 1232.1711
(T) | Epoch=834, loss=7.6361, this epoch 1.4958, total 1233.6670
(T) | Epoch=835, loss=7.6419, this epoch 1.4471, total 1235.1141
(T) | Epoch=836, loss=7.6333, this epoch 1.4721, total 1236.5862
(T) | Epoch=837, loss=7.6403, this epoch 1.5072, total 1238.0934
(T) | Epoch=838, loss=7.6808, this epoch 1.5291, total 1239.6226
(T) | Epoch=839, loss=7.6436, this epoch 1.4749, total 1241.0974
(T) | Epoch=840, loss=7.8465, this epoch 1.4934, total 1242.5909
(T) | Epoch=841, loss=7.6376, this epoch 1.4838, total 1244.0746
(T) | Epoch=842, loss=7.9338, this epoch 1.4558, total 1245.5304
(T) | Epoch=843, loss=7.6396, this epoch 1.4705, total 1247.0009
(T) | Epoch=844, loss=7.8279, this epoch 1.4381, total 1248.4390
(T) | Epoch=845, loss=7.6721, this epoch 1.4406, total 1249.8796
(T) | Epoch=846, loss=7.6277, this epoch 1.4510, total 1251.3307
(T) | Epoch=847, loss=7.6734, this epoch 1.4510, total 1252.7817
(T) | Epoch=848, loss=7.6208, this epoch 1.5152, total 1254.2968
(T) | Epoch=849, loss=7.6283, this epoch 1.4563, total 1255.7532
(T) | Epoch=850, loss=7.6647, this epoch 1.4941, total 1257.2473
(T) | Epoch=851, loss=7.7263, this epoch 1.4700, total 1258.7173
(T) | Epoch=852, loss=7.6009, this epoch 1.4818, total 1260.1990
(T) | Epoch=853, loss=7.6208, this epoch 1.4983, total 1261.6973
(T) | Epoch=854, loss=7.6169, this epoch 1.4280, total 1263.1253
(T) | Epoch=855, loss=7.5946, this epoch 1.4843, total 1264.6096
(T) | Epoch=856, loss=7.7374, this epoch 1.4618, total 1266.0714
(T) | Epoch=857, loss=7.6576, this epoch 1.4598, total 1267.5312
(T) | Epoch=858, loss=7.5710, this epoch 1.4720, total 1269.0032
(T) | Epoch=859, loss=7.5684, this epoch 1.4298, total 1270.4330
(T) | Epoch=860, loss=7.5653, this epoch 1.4483, total 1271.8813
(T) | Epoch=861, loss=7.6460, this epoch 1.4425, total 1273.3238
(T) | Epoch=862, loss=7.6017, this epoch 1.4567, total 1274.7805
(T) | Epoch=863, loss=7.5994, this epoch 1.4382, total 1276.2187
(T) | Epoch=864, loss=7.5516, this epoch 1.4365, total 1277.6552
(T) | Epoch=865, loss=7.6192, this epoch 1.4196, total 1279.0748
(T) | Epoch=866, loss=7.5535, this epoch 1.4623, total 1280.5371
(T) | Epoch=867, loss=7.6017, this epoch 1.4808, total 1282.0179
(T) | Epoch=868, loss=7.5214, this epoch 1.4718, total 1283.4897
(T) | Epoch=869, loss=7.6487, this epoch 1.4780, total 1284.9677
(T) | Epoch=870, loss=7.5530, this epoch 1.4636, total 1286.4313
(T) | Epoch=871, loss=7.5214, this epoch 1.4396, total 1287.8708
(T) | Epoch=872, loss=7.5198, this epoch 1.4347, total 1289.3056
(T) | Epoch=873, loss=7.5563, this epoch 1.4168, total 1290.7224
(T) | Epoch=874, loss=7.5525, this epoch 1.4375, total 1292.1599
(T) | Epoch=875, loss=7.5070, this epoch 1.5188, total 1293.6788
(T) | Epoch=876, loss=7.5476, this epoch 1.4639, total 1295.1427
(T) | Epoch=877, loss=7.5215, this epoch 1.4449, total 1296.5876
(T) | Epoch=878, loss=7.5331, this epoch 1.4191, total 1298.0068
(T) | Epoch=879, loss=7.5423, this epoch 1.4643, total 1299.4711
(T) | Epoch=880, loss=7.5014, this epoch 1.4876, total 1300.9587
(T) | Epoch=881, loss=7.4963, this epoch 1.4507, total 1302.4093
(T) | Epoch=882, loss=7.4871, this epoch 1.4863, total 1303.8956
(T) | Epoch=883, loss=7.5394, this epoch 1.4576, total 1305.3532
(T) | Epoch=884, loss=7.5239, this epoch 1.4634, total 1306.8166
(T) | Epoch=885, loss=7.6077, this epoch 1.4975, total 1308.3141
(T) | Epoch=886, loss=7.4911, this epoch 1.4629, total 1309.7770
(T) | Epoch=887, loss=7.4757, this epoch 1.4971, total 1311.2741
(T) | Epoch=888, loss=7.5696, this epoch 1.4383, total 1312.7124
(T) | Epoch=889, loss=7.4795, this epoch 1.4710, total 1314.1834
(T) | Epoch=890, loss=7.5284, this epoch 1.4669, total 1315.6504
(T) | Epoch=891, loss=7.4869, this epoch 1.4435, total 1317.0938
(T) | Epoch=892, loss=7.4715, this epoch 1.4900, total 1318.5838
(T) | Epoch=893, loss=7.5612, this epoch 1.4850, total 1320.0688
(T) | Epoch=894, loss=7.5040, this epoch 1.4659, total 1321.5347
(T) | Epoch=895, loss=7.4730, this epoch 1.4555, total 1322.9903
(T) | Epoch=896, loss=7.5167, this epoch 1.4328, total 1324.4230
(T) | Epoch=897, loss=7.4823, this epoch 1.4561, total 1325.8791
(T) | Epoch=898, loss=7.5450, this epoch 1.4559, total 1327.3350
(T) | Epoch=899, loss=7.5723, this epoch 1.4368, total 1328.7717
(T) | Epoch=900, loss=7.4532, this epoch 1.3985, total 1330.1703
(T) | Epoch=901, loss=7.5013, this epoch 1.4211, total 1331.5914
(T) | Epoch=902, loss=7.4777, this epoch 1.4381, total 1333.0296
(T) | Epoch=903, loss=8.0184, this epoch 1.4676, total 1334.4971
(T) | Epoch=904, loss=7.5532, this epoch 1.4815, total 1335.9787
(T) | Epoch=905, loss=7.4580, this epoch 1.4963, total 1337.4749
(T) | Epoch=906, loss=7.4644, this epoch 1.4463, total 1338.9213
(T) | Epoch=907, loss=7.4955, this epoch 1.5196, total 1340.4409
(T) | Epoch=908, loss=7.4946, this epoch 1.4682, total 1341.9091
(T) | Epoch=909, loss=7.4934, this epoch 1.4628, total 1343.3719
(T) | Epoch=910, loss=7.5578, this epoch 1.4651, total 1344.8370
(T) | Epoch=911, loss=7.5203, this epoch 1.4446, total 1346.2816
(T) | Epoch=912, loss=7.4988, this epoch 1.4767, total 1347.7583
(T) | Epoch=913, loss=7.4927, this epoch 1.4614, total 1349.2197
(T) | Epoch=914, loss=7.5270, this epoch 1.4407, total 1350.6604
(T) | Epoch=915, loss=7.4781, this epoch 1.4712, total 1352.1316
(T) | Epoch=916, loss=7.5513, this epoch 1.4879, total 1353.6194
(T) | Epoch=917, loss=7.4864, this epoch 1.4983, total 1355.1177
(T) | Epoch=918, loss=7.5257, this epoch 1.4327, total 1356.5504
(T) | Epoch=919, loss=7.4888, this epoch 1.4438, total 1357.9942
(T) | Epoch=920, loss=7.4680, this epoch 1.5050, total 1359.4992
(T) | Epoch=921, loss=7.5015, this epoch 1.4525, total 1360.9517
(T) | Epoch=922, loss=7.5331, this epoch 1.4878, total 1362.4395
(T) | Epoch=923, loss=7.6029, this epoch 1.4987, total 1363.9382
(T) | Epoch=924, loss=7.4840, this epoch 1.4634, total 1365.4016
(T) | Epoch=925, loss=7.4960, this epoch 1.4478, total 1366.8494
(T) | Epoch=926, loss=7.4580, this epoch 1.4896, total 1368.3390
(T) | Epoch=927, loss=7.4708, this epoch 1.4323, total 1369.7712
(T) | Epoch=928, loss=7.4672, this epoch 1.4361, total 1371.2073
(T) | Epoch=929, loss=7.4598, this epoch 1.4837, total 1372.6910
(T) | Epoch=930, loss=7.5423, this epoch 1.4475, total 1374.1385
(T) | Epoch=931, loss=7.4580, this epoch 1.4514, total 1375.5899
(T) | Epoch=932, loss=7.5290, this epoch 1.4606, total 1377.0505
(T) | Epoch=933, loss=7.5099, this epoch 1.4557, total 1378.5062
(T) | Epoch=934, loss=7.4667, this epoch 1.4313, total 1379.9375
(T) | Epoch=935, loss=7.4673, this epoch 1.4282, total 1381.3657
(T) | Epoch=936, loss=7.4829, this epoch 1.4777, total 1382.8434
(T) | Epoch=937, loss=7.5459, this epoch 1.4321, total 1384.2755
(T) | Epoch=938, loss=7.4652, this epoch 1.4980, total 1385.7735
(T) | Epoch=939, loss=7.4647, this epoch 1.4279, total 1387.2014
(T) | Epoch=940, loss=7.5075, this epoch 1.4406, total 1388.6420
(T) | Epoch=941, loss=7.4650, this epoch 1.4528, total 1390.0948
(T) | Epoch=942, loss=7.4524, this epoch 1.4671, total 1391.5618
(T) | Epoch=943, loss=7.5999, this epoch 1.4986, total 1393.0605
(T) | Epoch=944, loss=7.4609, this epoch 1.4303, total 1394.4908
(T) | Epoch=945, loss=7.4690, this epoch 1.4349, total 1395.9256
(T) | Epoch=946, loss=7.4562, this epoch 1.4831, total 1397.4088
(T) | Epoch=947, loss=7.5178, this epoch 1.4852, total 1398.8940
(T) | Epoch=948, loss=7.4710, this epoch 1.4698, total 1400.3638
(T) | Epoch=949, loss=7.4487, this epoch 1.4630, total 1401.8269
+++model saved ! 2016.pth
(T) | Epoch=950, loss=7.4660, this epoch 1.4915, total 1403.3184
(T) | Epoch=951, loss=7.4575, this epoch 1.4585, total 1404.7769
(T) | Epoch=952, loss=7.4778, this epoch 1.5056, total 1406.2825
(T) | Epoch=953, loss=7.4722, this epoch 1.4626, total 1407.7451
(T) | Epoch=954, loss=7.4648, this epoch 1.4549, total 1409.2000
(T) | Epoch=955, loss=7.4652, this epoch 1.5193, total 1410.7193
(T) | Epoch=956, loss=7.4660, this epoch 1.5052, total 1412.2245
(T) | Epoch=957, loss=7.4707, this epoch 1.4859, total 1413.7105
(T) | Epoch=958, loss=7.4581, this epoch 1.4569, total 1415.1673
(T) | Epoch=959, loss=7.4598, this epoch 1.4683, total 1416.6356
(T) | Epoch=960, loss=7.5022, this epoch 1.4584, total 1418.0940
(T) | Epoch=961, loss=7.4473, this epoch 1.5001, total 1419.5941
+++model saved ! 2016.pth
(T) | Epoch=962, loss=7.4643, this epoch 1.5010, total 1421.0951
(T) | Epoch=963, loss=7.4682, this epoch 1.4912, total 1422.5863
(T) | Epoch=964, loss=7.4684, this epoch 1.4975, total 1424.0838
(T) | Epoch=965, loss=7.5104, this epoch 1.4548, total 1425.5386
(T) | Epoch=966, loss=7.4625, this epoch 1.4402, total 1426.9789
(T) | Epoch=967, loss=7.4656, this epoch 1.4450, total 1428.4238
(T) | Epoch=968, loss=7.5318, this epoch 1.5226, total 1429.9464
(T) | Epoch=969, loss=7.4919, this epoch 1.4728, total 1431.4192
(T) | Epoch=970, loss=7.4457, this epoch 1.4545, total 1432.8738
+++model saved ! 2016.pth
(T) | Epoch=971, loss=7.4625, this epoch 1.4728, total 1434.3465
(T) | Epoch=972, loss=7.5440, this epoch 1.4362, total 1435.7827
(T) | Epoch=973, loss=8.0167, this epoch 1.4164, total 1437.1990
(T) | Epoch=974, loss=7.4560, this epoch 1.4360, total 1438.6350
(T) | Epoch=975, loss=7.4629, this epoch 1.4828, total 1440.1179
(T) | Epoch=976, loss=7.5489, this epoch 1.4989, total 1441.6168
(T) | Epoch=977, loss=7.4848, this epoch 1.4431, total 1443.0599
(T) | Epoch=978, loss=7.5312, this epoch 1.4253, total 1444.4852
(T) | Epoch=979, loss=7.6225, this epoch 1.4507, total 1445.9359
(T) | Epoch=980, loss=7.5117, this epoch 1.4417, total 1447.3776
(T) | Epoch=981, loss=7.6630, this epoch 1.4450, total 1448.8226
(T) | Epoch=982, loss=7.5768, this epoch 1.4327, total 1450.2553
(T) | Epoch=983, loss=7.5710, this epoch 1.4262, total 1451.6815
(T) | Epoch=984, loss=7.5170, this epoch 1.4550, total 1453.1365
(T) | Epoch=985, loss=7.5734, this epoch 1.4221, total 1454.5587
(T) | Epoch=986, loss=7.5337, this epoch 1.4703, total 1456.0290
(T) | Epoch=987, loss=7.5398, this epoch 1.4712, total 1457.5002
(T) | Epoch=988, loss=7.5815, this epoch 1.4516, total 1458.9518
(T) | Epoch=989, loss=7.5682, this epoch 1.4523, total 1460.4041
(T) | Epoch=990, loss=7.5196, this epoch 1.4855, total 1461.8895
(T) | Epoch=991, loss=7.4958, this epoch 1.4873, total 1463.3768
(T) | Epoch=992, loss=7.7848, this epoch 1.4590, total 1464.8358
(T) | Epoch=993, loss=7.4975, this epoch 1.4866, total 1466.3224
(T) | Epoch=994, loss=7.5405, this epoch 1.4775, total 1467.8000
(T) | Epoch=995, loss=7.5651, this epoch 1.4901, total 1469.2901
(T) | Epoch=996, loss=7.5036, this epoch 1.5183, total 1470.8084
(T) | Epoch=997, loss=7.4839, this epoch 1.4856, total 1472.2940
(T) | Epoch=998, loss=7.4669, this epoch 1.5562, total 1473.8502
(T) | Epoch=999, loss=7.5263, this epoch 1.4870, total 1475.3372
(T) | Epoch=1000, loss=7.4754, this epoch 1.4706, total 1476.8078
=== Final ===

==============================
LoRA FINE-TUNING
==============================
Random seed set to 2
Epoch: 0, loss: 35.3051, train_acc: 0.0000, train_recall: 0.0000, train_f1: 0.0000, val_acc: 0.000000, val_recall: 0.000000, val_f1: 0.000000
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed2.csv
Epoch: 1, loss: 32.1584, train_acc: 0.0000, train_recall: 0.0000, train_f1: 0.0000, val_acc: 0.000000, val_recall: 0.000000, val_f1: 0.000000
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed2.csv
Epoch: 2, loss: 28.2828, train_acc: 0.3944, train_recall: 0.3333, train_f1: 0.2656, val_acc: 0.371795, val_recall: 0.250000, val_f1: 0.135514
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed2.csv
Epoch: 3, loss: 45.6821, train_acc: 0.2333, train_recall: 0.3333, train_f1: 0.2182, val_acc: 0.166667, val_recall: 0.250000, val_f1: 0.072222
Epoch: 4, loss: 78.7733, train_acc: 0.3722, train_recall: 0.3333, train_f1: 0.2597, val_acc: 0.435897, val_recall: 0.250000, val_f1: 0.153153
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed2.csv
Epoch: 5, loss: 102.1099, train_acc: 0.0167, train_recall: 0.2500, train_f1: 0.0082, val_acc: 0.025641, val_recall: 0.250000, val_f1: 0.012500
Epoch: 6, loss: 83.9052, train_acc: 0.3667, train_recall: 0.2500, train_f1: 0.1341, val_acc: 0.435897, val_recall: 0.250000, val_f1: 0.151786
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed2.csv
Epoch: 7, loss: 70.2221, train_acc: 0.3667, train_recall: 0.2500, train_f1: 0.1341, val_acc: 0.435897, val_recall: 0.250000, val_f1: 0.151786
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed2.csv
Epoch: 8, loss: 57.6187, train_acc: 0.0111, train_recall: 0.0061, train_f1: 0.0118, val_acc: 0.000000, val_recall: 0.000000, val_f1: 0.000000
Epoch: 9, loss: 57.1656, train_acc: 0.3889, train_recall: 0.2500, train_f1: 0.1400, val_acc: 0.371795, val_recall: 0.250000, val_f1: 0.135514
Epoch: 10, loss: 62.3547, train_acc: 0.3889, train_recall: 0.2500, train_f1: 0.1400, val_acc: 0.371795, val_recall: 0.250000, val_f1: 0.135514
Epoch: 11, loss: 56.4008, train_acc: 0.3889, train_recall: 0.2500, train_f1: 0.1400, val_acc: 0.371795, val_recall: 0.250000, val_f1: 0.135514
Epoch: 12, loss: 41.8398, train_acc: 0.3889, train_recall: 0.2000, train_f1: 0.1124, val_acc: 0.346154, val_recall: 0.186207, val_f1: 0.102857
Epoch: 13, loss: 43.9057, train_acc: 0.0111, train_recall: 0.0061, train_f1: 0.0118, val_acc: 0.000000, val_recall: 0.000000, val_f1: 0.000000
Epoch: 14, loss: 42.3109, train_acc: 0.3000, train_recall: 0.2184, train_f1: 0.1532, val_acc: 0.410256, val_recall: 0.259050, val_f1: 0.191228
Epoch: 15, loss: 58.1225, train_acc: 0.2278, train_recall: 0.2500, train_f1: 0.0928, val_acc: 0.166667, val_recall: 0.250000, val_f1: 0.071429
Epoch: 16, loss: 55.8522, train_acc: 0.2389, train_recall: 0.2576, train_f1: 0.1083, val_acc: 0.166667, val_recall: 0.250000, val_f1: 0.071429
Epoch: 17, loss: 58.8919, train_acc: 0.3667, train_recall: 0.2500, train_f1: 0.1347, val_acc: 0.435897, val_recall: 0.250000, val_f1: 0.153153
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed2.csv
Epoch: 18, loss: 58.5590, train_acc: 0.3667, train_recall: 0.2500, train_f1: 0.1341, val_acc: 0.435897, val_recall: 0.250000, val_f1: 0.151786
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed2.csv
Epoch: 19, loss: 50.0208, train_acc: 0.3667, train_recall: 0.2500, train_f1: 0.1341, val_acc: 0.435897, val_recall: 0.250000, val_f1: 0.151786
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed2.csv
Epoch: 20, loss: 34.3579, train_acc: 0.3667, train_recall: 0.2500, train_f1: 0.1347, val_acc: 0.435897, val_recall: 0.250000, val_f1: 0.153153
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed2.csv
Epoch: 21, loss: 40.9447, train_acc: 0.4056, train_recall: 0.2683, train_f1: 0.1752, val_acc: 0.294872, val_recall: 0.198276, val_f1: 0.116162
Epoch: 22, loss: 50.0669, train_acc: 0.4056, train_recall: 0.2683, train_f1: 0.1752, val_acc: 0.294872, val_recall: 0.198276, val_f1: 0.116162
Epoch: 23, loss: 51.0337, train_acc: 0.4056, train_recall: 0.2683, train_f1: 0.1752, val_acc: 0.294872, val_recall: 0.198276, val_f1: 0.117347
Epoch: 24, loss: 55.7218, train_acc: 0.2278, train_recall: 0.2500, train_f1: 0.0928, val_acc: 0.166667, val_recall: 0.250000, val_f1: 0.071429
Epoch: 25, loss: 54.3550, train_acc: 0.2278, train_recall: 0.2500, train_f1: 0.0928, val_acc: 0.166667, val_recall: 0.250000, val_f1: 0.071429
Epoch: 26, loss: 45.0859, train_acc: 0.3667, train_recall: 0.2509, train_f1: 0.1791, val_acc: 0.256410, val_recall: 0.193634, val_f1: 0.133333
Epoch: 27, loss: 41.3503, train_acc: 0.4056, train_recall: 0.2683, train_f1: 0.1752, val_acc: 0.294872, val_recall: 0.198276, val_f1: 0.117347
Epoch: 28, loss: 32.0864, train_acc: 0.3667, train_recall: 0.2509, train_f1: 0.1795, val_acc: 0.243590, val_recall: 0.174403, val_f1: 0.116758
Epoch: 29, loss: 34.8452, train_acc: 0.3667, train_recall: 0.2500, train_f1: 0.1341, val_acc: 0.435897, val_recall: 0.250000, val_f1: 0.151786
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed2.csv
Epoch: 30, loss: 41.9800, train_acc: 0.3667, train_recall: 0.2500, train_f1: 0.1341, val_acc: 0.435897, val_recall: 0.250000, val_f1: 0.151786
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed2.csv
Epoch: 31, loss: 41.8745, train_acc: 0.3667, train_recall: 0.2500, train_f1: 0.1341, val_acc: 0.435897, val_recall: 0.250000, val_f1: 0.151786
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed2.csv
Epoch: 32, loss: 39.7850, train_acc: 0.2278, train_recall: 0.2500, train_f1: 0.0932, val_acc: 0.166667, val_recall: 0.250000, val_f1: 0.072222
Epoch: 33, loss: 34.7670, train_acc: 0.2444, train_recall: 0.2544, train_f1: 0.1295, val_acc: 0.205128, val_recall: 0.272059, val_f1: 0.113971
Epoch: 34, loss: 33.9859, train_acc: 0.3667, train_recall: 0.2500, train_f1: 0.1341, val_acc: 0.435897, val_recall: 0.250000, val_f1: 0.151786
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed2.csv
Epoch: 35, loss: 30.8523, train_acc: 0.4278, train_recall: 0.2789, train_f1: 0.2259, val_acc: 0.282051, val_recall: 0.184584, val_f1: 0.139216
Epoch: 36, loss: 35.5490, train_acc: 0.4222, train_recall: 0.2727, train_f1: 0.1853, val_acc: 0.320513, val_recall: 0.212982, val_f1: 0.139971
Epoch: 37, loss: 34.9165, train_acc: 0.4222, train_recall: 0.2727, train_f1: 0.1853, val_acc: 0.333333, val_recall: 0.220335, val_f1: 0.152231
Epoch: 38, loss: 34.3992, train_acc: 0.3667, train_recall: 0.2500, train_f1: 0.1341, val_acc: 0.435897, val_recall: 0.250000, val_f1: 0.151786
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed2.csv
Epoch: 39, loss: 33.2265, train_acc: 0.3667, train_recall: 0.2500, train_f1: 0.1341, val_acc: 0.435897, val_recall: 0.250000, val_f1: 0.151786
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed2.csv
Epoch: 40, loss: 30.0360, train_acc: 0.4278, train_recall: 0.3561, train_f1: 0.3108, val_acc: 0.333333, val_recall: 0.220335, val_f1: 0.152231
Epoch: 41, loss: 26.7692, train_acc: 0.4278, train_recall: 0.3582, train_f1: 0.3447, val_acc: 0.294872, val_recall: 0.193205, val_f1: 0.144928
Epoch: 42, loss: 31.4113, train_acc: 0.2333, train_recall: 0.3333, train_f1: 0.2182, val_acc: 0.166667, val_recall: 0.250000, val_f1: 0.071429
Epoch: 43, loss: 30.9717, train_acc: 0.3667, train_recall: 0.4035, train_f1: 0.3525, val_acc: 0.448718, val_recall: 0.399887, val_f1: 0.273083
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed2.csv
Epoch: 44, loss: 36.4848, train_acc: 0.3722, train_recall: 0.3333, train_f1: 0.2597, val_acc: 0.435897, val_recall: 0.250000, val_f1: 0.153153
Epoch: 45, loss: 36.1806, train_acc: 0.3722, train_recall: 0.3333, train_f1: 0.2597, val_acc: 0.435897, val_recall: 0.250000, val_f1: 0.153153
Epoch: 46, loss: 30.4102, train_acc: 0.3722, train_recall: 0.3333, train_f1: 0.2597, val_acc: 0.435897, val_recall: 0.250000, val_f1: 0.153153
Epoch: 47, loss: 30.7255, train_acc: 0.3944, train_recall: 0.3333, train_f1: 0.2656, val_acc: 0.333333, val_recall: 0.224138, val_f1: 0.125000
Epoch: 48, loss: 33.0713, train_acc: 0.4111, train_recall: 0.3516, train_f1: 0.3008, val_acc: 0.307692, val_recall: 0.206897, val_f1: 0.120000
Epoch: 49, loss: 31.3827, train_acc: 0.3833, train_recall: 0.3944, train_f1: 0.3570, val_acc: 0.282051, val_recall: 0.295756, val_f1: 0.186275
Epoch: 50, loss: 33.6755, train_acc: 0.2389, train_recall: 0.3369, train_f1: 0.2256, val_acc: 0.166667, val_recall: 0.250000, val_f1: 0.073034
Epoch: 51, loss: 27.8379, train_acc: 0.3833, train_recall: 0.3944, train_f1: 0.3570, val_acc: 0.282051, val_recall: 0.295756, val_f1: 0.186275
Epoch: 52, loss: 25.6732, train_acc: 0.4333, train_recall: 0.3622, train_f1: 0.3513, val_acc: 0.282051, val_recall: 0.184584, val_f1: 0.140816
Epoch: 53, loss: 32.1904, train_acc: 0.3722, train_recall: 0.3333, train_f1: 0.2597, val_acc: 0.435897, val_recall: 0.250000, val_f1: 0.153153
Epoch: 54, loss: 33.4824, train_acc: 0.3722, train_recall: 0.3333, train_f1: 0.2597, val_acc: 0.435897, val_recall: 0.250000, val_f1: 0.154545
Epoch: 55, loss: 29.4662, train_acc: 0.3500, train_recall: 0.3180, train_f1: 0.1779, val_acc: 0.410256, val_recall: 0.235294, val_f1: 0.150943
Epoch: 56, loss: 32.1962, train_acc: 0.3944, train_recall: 0.3333, train_f1: 0.1869, val_acc: 0.320513, val_recall: 0.215517, val_f1: 0.123762
Epoch: 57, loss: 33.0521, train_acc: 0.3944, train_recall: 0.3333, train_f1: 0.1869, val_acc: 0.307692, val_recall: 0.206897, val_f1: 0.120000
Epoch: 58, loss: 28.9986, train_acc: 0.3722, train_recall: 0.3569, train_f1: 0.2569, val_acc: 0.256410, val_recall: 0.236074, val_f1: 0.169028
Epoch: 59, loss: 34.8611, train_acc: 0.2333, train_recall: 0.3333, train_f1: 0.2182, val_acc: 0.166667, val_recall: 0.250000, val_f1: 0.072222
Epoch: 60, loss: 34.3831, train_acc: 0.2333, train_recall: 0.3333, train_f1: 0.2182, val_acc: 0.166667, val_recall: 0.250000, val_f1: 0.072222
Epoch: 61, loss: 29.6776, train_acc: 0.3833, train_recall: 0.3455, train_f1: 0.2841, val_acc: 0.435897, val_recall: 0.250000, val_f1: 0.154545
Epoch: 62, loss: 29.7623, train_acc: 0.3722, train_recall: 0.3333, train_f1: 0.2597, val_acc: 0.435897, val_recall: 0.250000, val_f1: 0.153153
Epoch: 63, loss: 24.7460, train_acc: 0.4333, train_recall: 0.3705, train_f1: 0.3876, val_acc: 0.294872, val_recall: 0.184331, val_f1: 0.166807
Epoch: 64, loss: 27.8712, train_acc: 0.4222, train_recall: 0.3640, train_f1: 0.3258, val_acc: 0.282051, val_recall: 0.189655, val_f1: 0.112245
Epoch: 65, loss: 27.8305, train_acc: 0.3833, train_recall: 0.3944, train_f1: 0.3570, val_acc: 0.269231, val_recall: 0.276525, val_f1: 0.176957
Epoch: 66, loss: 27.7065, train_acc: 0.4000, train_recall: 0.4177, train_f1: 0.3693, val_acc: 0.307692, val_recall: 0.344828, val_f1: 0.204119
Epoch: 67, loss: 24.6483, train_acc: 0.4444, train_recall: 0.4222, train_f1: 0.4469, val_acc: 0.307692, val_recall: 0.286706, val_f1: 0.234278
Epoch: 68, loss: 28.0993, train_acc: 0.3778, train_recall: 0.3369, train_f1: 0.2673, val_acc: 0.435897, val_recall: 0.250000, val_f1: 0.154545
Epoch: 69, loss: 28.1976, train_acc: 0.3778, train_recall: 0.3369, train_f1: 0.2673, val_acc: 0.435897, val_recall: 0.250000, val_f1: 0.154545
Epoch: 70, loss: 28.0780, train_acc: 0.4333, train_recall: 0.3618, train_f1: 0.3472, val_acc: 0.294872, val_recall: 0.193205, val_f1: 0.144928
Epoch: 71, loss: 27.7077, train_acc: 0.4278, train_recall: 0.3561, train_f1: 0.3108, val_acc: 0.333333, val_recall: 0.220335, val_f1: 0.153061
Epoch: 72, loss: 24.2216, train_acc: 0.4333, train_recall: 0.4151, train_f1: 0.4406, val_acc: 0.307692, val_recall: 0.285438, val_f1: 0.245126
Epoch: 73, loss: 26.6269, train_acc: 0.2944, train_recall: 0.3611, train_f1: 0.3044, val_acc: 0.192308, val_recall: 0.240950, val_f1: 0.115847
Epoch: 74, loss: 25.5683, train_acc: 0.4000, train_recall: 0.4029, train_f1: 0.3718, val_acc: 0.435897, val_recall: 0.345023, val_f1: 0.261049
Epoch: 75, loss: 24.7763, train_acc: 0.4056, train_recall: 0.3862, train_f1: 0.4219, val_acc: 0.256410, val_recall: 0.198373, val_f1: 0.186407
Epoch: 76, loss: 27.4894, train_acc: 0.4389, train_recall: 0.3683, train_f1: 0.3353, val_acc: 0.320513, val_recall: 0.212982, val_f1: 0.142477
Epoch: 77, loss: 27.5904, train_acc: 0.4222, train_recall: 0.3640, train_f1: 0.3258, val_acc: 0.282051, val_recall: 0.189655, val_f1: 0.112245
Epoch: 78, loss: 28.2846, train_acc: 0.2389, train_recall: 0.3369, train_f1: 0.2256, val_acc: 0.166667, val_recall: 0.250000, val_f1: 0.073864
Epoch: 79, loss: 24.2808, train_acc: 0.4333, train_recall: 0.4266, train_f1: 0.3591, val_acc: 0.307692, val_recall: 0.321072, val_f1: 0.240146
Epoch: 80, loss: 26.4473, train_acc: 0.3667, train_recall: 0.3293, train_f1: 0.2641, val_acc: 0.435897, val_recall: 0.250000, val_f1: 0.155963
Epoch: 81, loss: 26.9051, train_acc: 0.4000, train_recall: 0.3395, train_f1: 0.2414, val_acc: 0.256410, val_recall: 0.169878, val_f1: 0.121739
Epoch: 82, loss: 27.4343, train_acc: 0.4000, train_recall: 0.3395, train_f1: 0.2414, val_acc: 0.256410, val_recall: 0.169878, val_f1: 0.121739
Epoch: 83, loss: 28.3184, train_acc: 0.3500, train_recall: 0.3180, train_f1: 0.1861, val_acc: 0.410256, val_recall: 0.235294, val_f1: 0.150943
Epoch: 84, loss: 26.0250, train_acc: 0.3944, train_recall: 0.3392, train_f1: 0.2649, val_acc: 0.282051, val_recall: 0.176978, val_f1: 0.161765
Epoch: 85, loss: 24.6116, train_acc: 0.4111, train_recall: 0.3517, train_f1: 0.2660, val_acc: 0.256410, val_recall: 0.169878, val_f1: 0.122222
Epoch: 86, loss: 24.6761, train_acc: 0.3889, train_recall: 0.4112, train_f1: 0.3739, val_acc: 0.474359, val_recall: 0.414593, val_f1: 0.293919
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed2.csv
Epoch: 87, loss: 25.5203, train_acc: 0.3722, train_recall: 0.4070, train_f1: 0.3603, val_acc: 0.448718, val_recall: 0.399887, val_f1: 0.277356
Epoch: 88, loss: 26.4122, train_acc: 0.3833, train_recall: 0.3455, train_f1: 0.2841, val_acc: 0.435897, val_recall: 0.250000, val_f1: 0.155963
Epoch: 89, loss: 24.2271, train_acc: 0.4389, train_recall: 0.3706, train_f1: 0.3716, val_acc: 0.282051, val_recall: 0.184584, val_f1: 0.141454
Epoch: 90, loss: 24.7478, train_acc: 0.4333, train_recall: 0.3647, train_f1: 0.3334, val_acc: 0.320513, val_recall: 0.211714, val_f1: 0.151864
Epoch: 91, loss: 24.4928, train_acc: 0.3611, train_recall: 0.3833, train_f1: 0.3510, val_acc: 0.435897, val_recall: 0.356900, val_f1: 0.260501
Epoch: 92, loss: 25.8301, train_acc: 0.3556, train_recall: 0.3913, train_f1: 0.3456, val_acc: 0.423077, val_recall: 0.349548, val_f1: 0.253639
Epoch: 93, loss: 25.2748, train_acc: 0.3889, train_recall: 0.3932, train_f1: 0.3594, val_acc: 0.435897, val_recall: 0.345023, val_f1: 0.258204
Epoch: 94, loss: 23.5167, train_acc: 0.4444, train_recall: 0.4222, train_f1: 0.4466, val_acc: 0.307692, val_recall: 0.286706, val_f1: 0.236977
Epoch: 95, loss: 26.3720, train_acc: 0.4333, train_recall: 0.3645, train_f1: 0.3285, val_acc: 0.320513, val_recall: 0.212982, val_f1: 0.142477
Epoch: 96, loss: 25.1123, train_acc: 0.4389, train_recall: 0.3683, train_f1: 0.3353, val_acc: 0.333333, val_recall: 0.220335, val_f1: 0.154847
Epoch: 97, loss: 27.7736, train_acc: 0.3722, train_recall: 0.3333, train_f1: 0.2597, val_acc: 0.435897, val_recall: 0.250000, val_f1: 0.155963
Epoch: 98, loss: 26.8772, train_acc: 0.4000, train_recall: 0.4008, train_f1: 0.3669, val_acc: 0.448718, val_recall: 0.352376, val_f1: 0.265085
Epoch: 99, loss: 29.7655, train_acc: 0.2278, train_recall: 0.3272, train_f1: 0.1551, val_acc: 0.153846, val_recall: 0.230769, val_f1: 0.068966
Epoch: 100, loss: 27.9352, train_acc: 0.3500, train_recall: 0.4704, train_f1: 0.2644, val_acc: 0.217949, val_recall: 0.273873, val_f1: 0.148077
Epoch: 101, loss: 28.3608, train_acc: 0.3778, train_recall: 0.4035, train_f1: 0.2101, val_acc: 0.282051, val_recall: 0.187120, val_f1: 0.133410
Epoch: 102, loss: 28.9604, train_acc: 0.4278, train_recall: 0.3561, train_f1: 0.3108, val_acc: 0.320513, val_recall: 0.212982, val_f1: 0.141162
Epoch: 103, loss: 29.4486, train_acc: 0.3778, train_recall: 0.3369, train_f1: 0.2673, val_acc: 0.435897, val_recall: 0.250000, val_f1: 0.155963
Epoch: 104, loss: 29.5202, train_acc: 0.3778, train_recall: 0.3369, train_f1: 0.2673, val_acc: 0.435897, val_recall: 0.250000, val_f1: 0.155963
Epoch: 105, loss: 31.1384, train_acc: 0.4278, train_recall: 0.3561, train_f1: 0.3108, val_acc: 0.320513, val_recall: 0.212982, val_f1: 0.141162
Epoch: 106, loss: 30.8589, train_acc: 0.4222, train_recall: 0.3523, train_f1: 0.3040, val_acc: 0.320513, val_recall: 0.212982, val_f1: 0.141162
Epoch: 107, loss: 26.0983, train_acc: 0.4333, train_recall: 0.3620, train_f1: 0.3492, val_acc: 0.282051, val_recall: 0.184584, val_f1: 0.140568
Epoch: 108, loss: 26.8879, train_acc: 0.3778, train_recall: 0.3417, train_f1: 0.2820, val_acc: 0.435897, val_recall: 0.250000, val_f1: 0.157407
Epoch: 109, loss: 30.8508, train_acc: 0.2389, train_recall: 0.3371, train_f1: 0.2261, val_acc: 0.179487, val_recall: 0.257353, val_f1: 0.088149
Epoch: 110, loss: 31.3617, train_acc: 0.2389, train_recall: 0.3371, train_f1: 0.2261, val_acc: 0.179487, val_recall: 0.257353, val_f1: 0.088149
Epoch: 111, loss: 28.2249, train_acc: 0.3889, train_recall: 0.3909, train_f1: 0.3595, val_acc: 0.448718, val_recall: 0.352376, val_f1: 0.266284
Epoch: 112, loss: 25.5442, train_acc: 0.3722, train_recall: 0.3400, train_f1: 0.2341, val_acc: 0.423077, val_recall: 0.242647, val_f1: 0.158654
Epoch: 113, loss: 26.9043, train_acc: 0.4056, train_recall: 0.4721, train_f1: 0.3209, val_acc: 0.282051, val_recall: 0.272000, val_f1: 0.218931
Epoch: 114, loss: 29.3203, train_acc: 0.3722, train_recall: 0.4114, train_f1: 0.2244, val_acc: 0.230769, val_recall: 0.155172, val_f1: 0.097826
Epoch: 115, loss: 29.0619, train_acc: 0.4278, train_recall: 0.4154, train_f1: 0.3798, val_acc: 0.307692, val_recall: 0.291777, val_f1: 0.200501
Epoch: 116, loss: 28.4225, train_acc: 0.4000, train_recall: 0.4177, train_f1: 0.3692, val_acc: 0.294872, val_recall: 0.325597, val_f1: 0.198718
Epoch: 117, loss: 26.6552, train_acc: 0.4000, train_recall: 0.4177, train_f1: 0.3693, val_acc: 0.320513, val_recall: 0.352181, val_f1: 0.222865
Epoch: 118, loss: 24.6239, train_acc: 0.4000, train_recall: 0.4029, train_f1: 0.3718, val_acc: 0.448718, val_recall: 0.352376, val_f1: 0.266932
Epoch: 119, loss: 28.1271, train_acc: 0.3778, train_recall: 0.3369, train_f1: 0.2673, val_acc: 0.435897, val_recall: 0.250000, val_f1: 0.155963
Epoch: 120, loss: 26.6276, train_acc: 0.3778, train_recall: 0.3369, train_f1: 0.2673, val_acc: 0.435897, val_recall: 0.250000, val_f1: 0.155963
Epoch: 121, loss: 28.6933, train_acc: 0.4278, train_recall: 0.3561, train_f1: 0.3108, val_acc: 0.320513, val_recall: 0.212982, val_f1: 0.142477
Epoch: 122, loss: 30.1468, train_acc: 0.4056, train_recall: 0.3455, train_f1: 0.2900, val_acc: 0.320513, val_recall: 0.215517, val_f1: 0.121359
Epoch: 123, loss: 26.6012, train_acc: 0.4444, train_recall: 0.4198, train_f1: 0.4142, val_acc: 0.333333, val_recall: 0.306483, val_f1: 0.238320
Epoch: 124, loss: 26.0421, train_acc: 0.4500, train_recall: 0.4449, train_f1: 0.4470, val_acc: 0.307692, val_recall: 0.321072, val_f1: 0.229312
Epoch: 125, loss: 25.2435, train_acc: 0.4000, train_recall: 0.4029, train_f1: 0.3714, val_acc: 0.435897, val_recall: 0.345023, val_f1: 0.260025
Epoch: 126, loss: 27.3614, train_acc: 0.3778, train_recall: 0.3415, train_f1: 0.2876, val_acc: 0.435897, val_recall: 0.250000, val_f1: 0.157407
Epoch: 127, loss: 24.4359, train_acc: 0.4167, train_recall: 0.3598, train_f1: 0.3794, val_acc: 0.294872, val_recall: 0.183063, val_f1: 0.167019
Epoch: 128, loss: 27.2169, train_acc: 0.4389, train_recall: 0.3683, train_f1: 0.3353, val_acc: 0.320513, val_recall: 0.212982, val_f1: 0.142477
Epoch: 129, loss: 26.6987, train_acc: 0.4556, train_recall: 0.4270, train_f1: 0.4207, val_acc: 0.333333, val_recall: 0.306483, val_f1: 0.238320
Epoch: 130, loss: 25.9776, train_acc: 0.4056, train_recall: 0.4215, train_f1: 0.3775, val_acc: 0.307692, val_recall: 0.332950, val_f1: 0.214286
Epoch: 131, loss: 26.6342, train_acc: 0.2944, train_recall: 0.3611, train_f1: 0.3044, val_acc: 0.192308, val_recall: 0.240950, val_f1: 0.115847
Epoch: 132, loss: 25.9103, train_acc: 0.4056, train_recall: 0.4067, train_f1: 0.3753, val_acc: 0.448718, val_recall: 0.352376, val_f1: 0.268088
Epoch: 133, loss: 26.2866, train_acc: 0.3278, train_recall: 0.3870, train_f1: 0.1907, val_acc: 0.423077, val_recall: 0.242647, val_f1: 0.161765
Epoch: 134, loss: 25.5852, train_acc: 0.4056, train_recall: 0.4721, train_f1: 0.3207, val_acc: 0.294872, val_recall: 0.279353, val_f1: 0.235568
Epoch: 135, loss: 26.4511, train_acc: 0.4056, train_recall: 0.4744, train_f1: 0.3177, val_acc: 0.282051, val_recall: 0.272000, val_f1: 0.221380
Epoch: 136, loss: 25.1120, train_acc: 0.3944, train_recall: 0.4700, train_f1: 0.3160, val_acc: 0.294872, val_recall: 0.279353, val_f1: 0.231960
Epoch: 137, loss: 24.1876, train_acc: 0.4611, train_recall: 0.4455, train_f1: 0.4624, val_acc: 0.333333, val_recall: 0.335778, val_f1: 0.260368
Epoch: 138, loss: 24.8671, train_acc: 0.4222, train_recall: 0.4273, train_f1: 0.3884, val_acc: 0.474359, val_recall: 0.402715, val_f1: 0.291446
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed2.csv
Epoch: 139, loss: 24.7605, train_acc: 0.4000, train_recall: 0.4029, train_f1: 0.3718, val_acc: 0.448718, val_recall: 0.352376, val_f1: 0.263281
Epoch: 140, loss: 23.6946, train_acc: 0.4444, train_recall: 0.4222, train_f1: 0.4467, val_acc: 0.320513, val_recall: 0.294059, val_f1: 0.247364
Epoch: 141, loss: 25.3109, train_acc: 0.4333, train_recall: 0.3773, train_f1: 0.3676, val_acc: 0.307692, val_recall: 0.213703, val_f1: 0.170222
Epoch: 142, loss: 23.7936, train_acc: 0.4500, train_recall: 0.4258, train_f1: 0.4501, val_acc: 0.320513, val_recall: 0.294059, val_f1: 0.247364
Epoch: 143, loss: 24.7788, train_acc: 0.3944, train_recall: 0.3968, train_f1: 0.3675, val_acc: 0.448718, val_recall: 0.352376, val_f1: 0.261517
Epoch: 144, loss: 24.7780, train_acc: 0.4222, train_recall: 0.4273, train_f1: 0.3884, val_acc: 0.474359, val_recall: 0.402715, val_f1: 0.287579
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed2.csv
Epoch: 145, loss: 24.1605, train_acc: 0.4611, train_recall: 0.4455, train_f1: 0.4624, val_acc: 0.333333, val_recall: 0.335778, val_f1: 0.256398
Epoch: 146, loss: 24.5073, train_acc: 0.4333, train_recall: 0.3354, train_f1: 0.2873, val_acc: 0.346154, val_recall: 0.313836, val_f1: 0.246731
Epoch: 147, loss: 23.9111, train_acc: 0.4500, train_recall: 0.3460, train_f1: 0.3290, val_acc: 0.320513, val_recall: 0.294059, val_f1: 0.249088
Epoch: 148, loss: 25.9070, train_acc: 0.3778, train_recall: 0.2622, train_f1: 0.1585, val_acc: 0.435897, val_recall: 0.250000, val_f1: 0.154545
Epoch: 149, loss: 23.6882, train_acc: 0.4556, train_recall: 0.3622, train_f1: 0.3368, val_acc: 0.333333, val_recall: 0.335778, val_f1: 0.254416
Epoch: 150, loss: 25.1554, train_acc: 0.4278, train_recall: 0.4181, train_f1: 0.3860, val_acc: 0.307692, val_recall: 0.291777, val_f1: 0.196172
Epoch: 151, loss: 25.2369, train_acc: 0.4278, train_recall: 0.4181, train_f1: 0.3860, val_acc: 0.307692, val_recall: 0.291777, val_f1: 0.196172
Epoch: 152, loss: 23.7539, train_acc: 0.4389, train_recall: 0.4354, train_f1: 0.3965, val_acc: 0.346154, val_recall: 0.348202, val_f1: 0.234503
Epoch: 153, loss: 26.1107, train_acc: 0.3833, train_recall: 0.3455, train_f1: 0.2841, val_acc: 0.435897, val_recall: 0.250000, val_f1: 0.154545
Epoch: 154, loss: 24.9092, train_acc: 0.3778, train_recall: 0.3533, train_f1: 0.3165, val_acc: 0.371795, val_recall: 0.225113, val_f1: 0.159447
Epoch: 155, loss: 25.3549, train_acc: 0.4333, train_recall: 0.4316, train_f1: 0.3879, val_acc: 0.333333, val_recall: 0.340849, val_f1: 0.218750
Epoch: 156, loss: 26.9612, train_acc: 0.4278, train_recall: 0.4154, train_f1: 0.3795, val_acc: 0.307692, val_recall: 0.291777, val_f1: 0.196172
Epoch: 157, loss: 26.9591, train_acc: 0.4389, train_recall: 0.4225, train_f1: 0.3855, val_acc: 0.307692, val_recall: 0.291777, val_f1: 0.196172
Epoch: 158, loss: 25.5272, train_acc: 0.4333, train_recall: 0.4316, train_f1: 0.3879, val_acc: 0.333333, val_recall: 0.340849, val_f1: 0.218750
Epoch: 159, loss: 23.0219, train_acc: 0.4556, train_recall: 0.4395, train_f1: 0.4592, val_acc: 0.346154, val_recall: 0.343131, val_f1: 0.268973
Epoch: 160, loss: 25.9077, train_acc: 0.3722, train_recall: 0.3333, train_f1: 0.2597, val_acc: 0.435897, val_recall: 0.250000, val_f1: 0.153153
Epoch: 161, loss: 24.0043, train_acc: 0.4389, train_recall: 0.3658, train_f1: 0.3543, val_acc: 0.294872, val_recall: 0.191937, val_f1: 0.151124
Epoch: 162, loss: 25.2380, train_acc: 0.4278, train_recall: 0.3561, train_f1: 0.3108, val_acc: 0.333333, val_recall: 0.220335, val_f1: 0.154847
Epoch: 163, loss: 22.9814, train_acc: 0.4667, train_recall: 0.4491, train_f1: 0.4662, val_acc: 0.333333, val_recall: 0.335778, val_f1: 0.259443
Epoch: 164, loss: 23.1971, train_acc: 0.4222, train_recall: 0.4249, train_f1: 0.3885, val_acc: 0.487179, val_recall: 0.410068, val_f1: 0.300813
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed2.csv
Epoch: 165, loss: 23.1176, train_acc: 0.4056, train_recall: 0.4041, train_f1: 0.3816, val_acc: 0.448718, val_recall: 0.352376, val_f1: 0.268088
Epoch: 166, loss: 24.1553, train_acc: 0.4389, train_recall: 0.3683, train_f1: 0.3109, val_acc: 0.333333, val_recall: 0.220335, val_f1: 0.157018
Epoch: 167, loss: 24.9616, train_acc: 0.2333, train_recall: 0.4057, train_f1: 0.1482, val_acc: 0.141026, val_recall: 0.211538, val_f1: 0.068750
Epoch: 168, loss: 22.8219, train_acc: 0.4222, train_recall: 0.4904, train_f1: 0.3353, val_acc: 0.307692, val_recall: 0.286706, val_f1: 0.246343
Epoch: 169, loss: 23.8691, train_acc: 0.3944, train_recall: 0.3522, train_f1: 0.3096, val_acc: 0.435897, val_recall: 0.250000, val_f1: 0.157407
Epoch: 170, loss: 24.9378, train_acc: 0.4278, train_recall: 0.3561, train_f1: 0.3108, val_acc: 0.333333, val_recall: 0.220335, val_f1: 0.155808
Epoch: 171, loss: 23.1647, train_acc: 0.4500, train_recall: 0.3780, train_f1: 0.3789, val_acc: 0.294872, val_recall: 0.191937, val_f1: 0.152144
Epoch: 172, loss: 23.5351, train_acc: 0.4222, train_recall: 0.4273, train_f1: 0.3894, val_acc: 0.474359, val_recall: 0.402715, val_f1: 0.291446
Epoch: 173, loss: 23.2869, train_acc: 0.4222, train_recall: 0.4273, train_f1: 0.3898, val_acc: 0.474359, val_recall: 0.402715, val_f1: 0.291446
Epoch: 174, loss: 22.9370, train_acc: 0.4722, train_recall: 0.4403, train_f1: 0.4663, val_acc: 0.320513, val_recall: 0.294059, val_f1: 0.250367
Epoch: 175, loss: 24.0208, train_acc: 0.4389, train_recall: 0.3683, train_f1: 0.3353, val_acc: 0.333333, val_recall: 0.220335, val_f1: 0.155808
Epoch: 176, loss: 23.4715, train_acc: 0.4444, train_recall: 0.4413, train_f1: 0.4444, val_acc: 0.294872, val_recall: 0.312451, val_f1: 0.217204
Epoch: 177, loss: 23.9455, train_acc: 0.3778, train_recall: 0.3531, train_f1: 0.3224, val_acc: 0.371795, val_recall: 0.225113, val_f1: 0.160833
Epoch: 178, loss: 23.0222, train_acc: 0.4667, train_recall: 0.4531, train_f1: 0.4575, val_acc: 0.307692, val_recall: 0.321072, val_f1: 0.224330
Epoch: 179, loss: 23.2453, train_acc: 0.4444, train_recall: 0.4263, train_f1: 0.3945, val_acc: 0.320513, val_recall: 0.299130, val_f1: 0.214787
Epoch: 180, loss: 22.8830, train_acc: 0.4778, train_recall: 0.4542, train_f1: 0.4762, val_acc: 0.333333, val_recall: 0.334510, val_f1: 0.262401
Epoch: 181, loss: 23.1247, train_acc: 0.4222, train_recall: 0.4224, train_f1: 0.3958, val_acc: 0.461538, val_recall: 0.371606, val_f1: 0.277245
Epoch: 182, loss: 23.1225, train_acc: 0.4500, train_recall: 0.4321, train_f1: 0.4168, val_acc: 0.282051, val_recall: 0.272000, val_f1: 0.206585
Epoch: 183, loss: 22.9644, train_acc: 0.4611, train_recall: 0.4493, train_f1: 0.4277, val_acc: 0.307692, val_recall: 0.321072, val_f1: 0.229893
Epoch: 184, loss: 23.0708, train_acc: 0.3944, train_recall: 0.3968, train_f1: 0.3473, val_acc: 0.448718, val_recall: 0.352376, val_f1: 0.269935
Epoch: 185, loss: 22.7850, train_acc: 0.4222, train_recall: 0.4954, train_f1: 0.3377, val_acc: 0.333333, val_recall: 0.335778, val_f1: 0.270084
Epoch: 186, loss: 22.9986, train_acc: 0.4111, train_recall: 0.4782, train_f1: 0.3255, val_acc: 0.294872, val_recall: 0.279353, val_f1: 0.233526
Epoch: 187, loss: 22.5001, train_acc: 0.4722, train_recall: 0.4499, train_f1: 0.4436, val_acc: 0.333333, val_recall: 0.335778, val_f1: 0.261334
Epoch: 188, loss: 22.8579, train_acc: 0.4778, train_recall: 0.4544, train_f1: 0.4785, val_acc: 0.333333, val_recall: 0.334510, val_f1: 0.266865
Epoch: 189, loss: 22.7874, train_acc: 0.4667, train_recall: 0.4365, train_f1: 0.4606, val_acc: 0.307692, val_recall: 0.286706, val_f1: 0.237618
Epoch: 190, loss: 22.6791, train_acc: 0.4667, train_recall: 0.4491, train_f1: 0.4662, val_acc: 0.333333, val_recall: 0.335778, val_f1: 0.259443
Epoch: 191, loss: 22.6564, train_acc: 0.4833, train_recall: 0.4575, train_f1: 0.4786, val_acc: 0.346154, val_recall: 0.343131, val_f1: 0.274458
Epoch: 192, loss: 22.8020, train_acc: 0.4722, train_recall: 0.4403, train_f1: 0.4663, val_acc: 0.307692, val_recall: 0.286706, val_f1: 0.239734
Epoch: 193, loss: 23.1445, train_acc: 0.4278, train_recall: 0.4310, train_f1: 0.3916, val_acc: 0.474359, val_recall: 0.402715, val_f1: 0.291446
Epoch: 194, loss: 23.0255, train_acc: 0.4222, train_recall: 0.4249, train_f1: 0.3880, val_acc: 0.487179, val_recall: 0.410068, val_f1: 0.298780
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed2.csv
Epoch: 195, loss: 22.9387, train_acc: 0.4556, train_recall: 0.4270, train_f1: 0.4211, val_acc: 0.346154, val_recall: 0.313836, val_f1: 0.250871
Epoch: 196, loss: 22.6981, train_acc: 0.4611, train_recall: 0.4432, train_f1: 0.4637, val_acc: 0.333333, val_recall: 0.335778, val_f1: 0.259443
Epoch: 197, loss: 22.4099, train_acc: 0.4833, train_recall: 0.4601, train_f1: 0.4785, val_acc: 0.333333, val_recall: 0.335778, val_f1: 0.257323
Epoch: 198, loss: 22.6666, train_acc: 0.4667, train_recall: 0.4365, train_f1: 0.4372, val_acc: 0.307692, val_recall: 0.286706, val_f1: 0.240621
Epoch: 199, loss: 22.9832, train_acc: 0.4278, train_recall: 0.4287, train_f1: 0.3690, val_acc: 0.487179, val_recall: 0.410068, val_f1: 0.302134
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed2.csv
Epoch: 200, loss: 23.3662, train_acc: 0.3944, train_recall: 0.4866, train_f1: 0.2993, val_acc: 0.294872, val_recall: 0.313719, val_f1: 0.217244
Epoch: 201, loss: 23.9013, train_acc: 0.3944, train_recall: 0.4194, train_f1: 0.2415, val_acc: 0.282051, val_recall: 0.185852, val_f1: 0.142857
Epoch: 202, loss: 23.8762, train_acc: 0.3556, train_recall: 0.3933, train_f1: 0.2477, val_acc: 0.269231, val_recall: 0.167089, val_f1: 0.160247
Epoch: 203, loss: 23.4218, train_acc: 0.4056, train_recall: 0.4275, train_f1: 0.2624, val_acc: 0.256410, val_recall: 0.167343, val_f1: 0.138562
Epoch: 204, loss: 22.8107, train_acc: 0.4278, train_recall: 0.3697, train_f1: 0.3697, val_acc: 0.282051, val_recall: 0.187588, val_f1: 0.186179
Epoch: 205, loss: 24.3006, train_acc: 0.4000, train_recall: 0.4177, train_f1: 0.3451, val_acc: 0.294872, val_recall: 0.325597, val_f1: 0.197049
Epoch: 206, loss: 25.0084, train_acc: 0.4222, train_recall: 0.3765, train_f1: 0.3454, val_acc: 0.282051, val_recall: 0.200265, val_f1: 0.133908
Epoch: 207, loss: 22.8255, train_acc: 0.4556, train_recall: 0.4436, train_f1: 0.3815, val_acc: 0.346154, val_recall: 0.348202, val_f1: 0.238450
Epoch: 208, loss: 25.3629, train_acc: 0.3833, train_recall: 0.3455, train_f1: 0.2841, val_acc: 0.435897, val_recall: 0.250000, val_f1: 0.157407
Epoch: 209, loss: 24.5610, train_acc: 0.4056, train_recall: 0.4230, train_f1: 0.3529, val_acc: 0.448718, val_recall: 0.388009, val_f1: 0.274916
Epoch: 210, loss: 23.2945, train_acc: 0.4111, train_recall: 0.4243, train_f1: 0.3612, val_acc: 0.448718, val_recall: 0.388009, val_f1: 0.277021
Epoch: 211, loss: 22.8812, train_acc: 0.4000, train_recall: 0.4485, train_f1: 0.3047, val_acc: 0.282051, val_recall: 0.238902, val_f1: 0.214167
Epoch: 212, loss: 23.1032, train_acc: 0.3944, train_recall: 0.4229, train_f1: 0.2842, val_acc: 0.294872, val_recall: 0.184331, val_f1: 0.176945
Epoch: 213, loss: 24.0825, train_acc: 0.3833, train_recall: 0.3453, train_f1: 0.2652, val_acc: 0.435897, val_recall: 0.250000, val_f1: 0.160377
Epoch: 214, loss: 23.6779, train_acc: 0.3944, train_recall: 0.4866, train_f1: 0.2993, val_acc: 0.282051, val_recall: 0.306366, val_f1: 0.205503
Epoch: 215, loss: 23.7401, train_acc: 0.4556, train_recall: 0.4459, train_f1: 0.4013, val_acc: 0.333333, val_recall: 0.340849, val_f1: 0.221188
Epoch: 216, loss: 23.4061, train_acc: 0.4444, train_recall: 0.4263, train_f1: 0.3945, val_acc: 0.320513, val_recall: 0.299130, val_f1: 0.214787
Epoch: 217, loss: 24.9210, train_acc: 0.3889, train_recall: 0.3491, train_f1: 0.2917, val_acc: 0.435897, val_recall: 0.250000, val_f1: 0.157407
Epoch: 218, loss: 23.6976, train_acc: 0.4111, train_recall: 0.4243, train_f1: 0.3842, val_acc: 0.448718, val_recall: 0.388009, val_f1: 0.272478
Epoch: 219, loss: 23.8983, train_acc: 0.4056, train_recall: 0.4215, train_f1: 0.3776, val_acc: 0.320513, val_recall: 0.352181, val_f1: 0.218405
Epoch: 220, loss: 24.4560, train_acc: 0.4389, train_recall: 0.3683, train_f1: 0.3353, val_acc: 0.320513, val_recall: 0.212982, val_f1: 0.142477
Epoch: 221, loss: 23.3864, train_acc: 0.4389, train_recall: 0.3745, train_f1: 0.3916, val_acc: 0.294872, val_recall: 0.183063, val_f1: 0.165696
Epoch: 222, loss: 23.2898, train_acc: 0.4333, train_recall: 0.3710, train_f1: 0.3887, val_acc: 0.294872, val_recall: 0.183063, val_f1: 0.165938
Epoch: 223, loss: 23.7407, train_acc: 0.4556, train_recall: 0.4385, train_f1: 0.4034, val_acc: 0.320513, val_recall: 0.299130, val_f1: 0.212572
Epoch: 224, loss: 23.8762, train_acc: 0.4056, train_recall: 0.4215, train_f1: 0.3776, val_acc: 0.320513, val_recall: 0.352181, val_f1: 0.218405
Epoch: 225, loss: 23.3134, train_acc: 0.4611, train_recall: 0.4494, train_f1: 0.4684, val_acc: 0.333333, val_recall: 0.325636, val_f1: 0.252153
Epoch: 226, loss: 23.7884, train_acc: 0.3889, train_recall: 0.3491, train_f1: 0.2917, val_acc: 0.435897, val_recall: 0.250000, val_f1: 0.155963
Epoch: 227, loss: 24.5541, train_acc: 0.4444, train_recall: 0.3971, train_f1: 0.3953, val_acc: 0.333333, val_recall: 0.274653, val_f1: 0.217536
Epoch: 228, loss: 24.1735, train_acc: 0.4500, train_recall: 0.4425, train_f1: 0.4032, val_acc: 0.346154, val_recall: 0.348202, val_f1: 0.234503
Epoch: 229, loss: 24.1262, train_acc: 0.4500, train_recall: 0.4449, train_f1: 0.4470, val_acc: 0.307692, val_recall: 0.321072, val_f1: 0.224330
Epoch: 230, loss: 24.7314, train_acc: 0.3889, train_recall: 0.3491, train_f1: 0.2917, val_acc: 0.435897, val_recall: 0.250000, val_f1: 0.154545
Epoch: 231, loss: 25.1603, train_acc: 0.4333, train_recall: 0.3620, train_f1: 0.3495, val_acc: 0.282051, val_recall: 0.184584, val_f1: 0.139717
Epoch: 232, loss: 24.6426, train_acc: 0.4389, train_recall: 0.3690, train_f1: 0.3715, val_acc: 0.307692, val_recall: 0.191684, val_f1: 0.171569
Epoch: 233, loss: 25.7357, train_acc: 0.3778, train_recall: 0.3369, train_f1: 0.2673, val_acc: 0.435897, val_recall: 0.250000, val_f1: 0.154545
Epoch: 234, loss: 23.1330, train_acc: 0.4556, train_recall: 0.4012, train_f1: 0.4391, val_acc: 0.256410, val_recall: 0.186027, val_f1: 0.178235
Epoch: 235, loss: 23.7011, train_acc: 0.4000, train_recall: 0.4901, train_f1: 0.3028, val_acc: 0.294872, val_recall: 0.313719, val_f1: 0.219788
Epoch: 236, loss: 24.8964, train_acc: 0.3944, train_recall: 0.4866, train_f1: 0.2993, val_acc: 0.294872, val_recall: 0.313719, val_f1: 0.219788
Epoch: 237, loss: 22.9647, train_acc: 0.4667, train_recall: 0.4365, train_f1: 0.4372, val_acc: 0.307692, val_recall: 0.286706, val_f1: 0.240621
Epoch: 238, loss: 24.3370, train_acc: 0.4167, train_recall: 0.3550, train_f1: 0.3609, val_acc: 0.294872, val_recall: 0.183063, val_f1: 0.165938
Epoch: 239, loss: 25.5044, train_acc: 0.4278, train_recall: 0.3561, train_f1: 0.3108, val_acc: 0.333333, val_recall: 0.220335, val_f1: 0.153932
Epoch: 240, loss: 23.8886, train_acc: 0.4500, train_recall: 0.3812, train_f1: 0.3962, val_acc: 0.307692, val_recall: 0.191684, val_f1: 0.172742
Epoch: 241, loss: 24.6877, train_acc: 0.3889, train_recall: 0.3491, train_f1: 0.2917, val_acc: 0.435897, val_recall: 0.250000, val_f1: 0.157407
Epoch: 242, loss: 24.4111, train_acc: 0.4444, train_recall: 0.4411, train_f1: 0.4170, val_acc: 0.307692, val_recall: 0.321072, val_f1: 0.229312
Epoch: 243, loss: 23.6966, train_acc: 0.4444, train_recall: 0.4390, train_f1: 0.3755, val_acc: 0.346154, val_recall: 0.348202, val_f1: 0.240597
Epoch: 244, loss: 23.6508, train_acc: 0.4611, train_recall: 0.4305, train_f1: 0.4004, val_acc: 0.358974, val_recall: 0.322457, val_f1: 0.260453
Epoch: 245, loss: 23.2783, train_acc: 0.4222, train_recall: 0.3634, train_f1: 0.3580, val_acc: 0.294872, val_recall: 0.183063, val_f1: 0.168129
Epoch: 246, loss: 22.8832, train_acc: 0.4167, train_recall: 0.4819, train_f1: 0.3336, val_acc: 0.307692, val_recall: 0.286706, val_f1: 0.248127
Epoch: 247, loss: 22.8513, train_acc: 0.4778, train_recall: 0.4537, train_f1: 0.4491, val_acc: 0.333333, val_recall: 0.335778, val_f1: 0.262637
Epoch: 248, loss: 22.8181, train_acc: 0.4778, train_recall: 0.4563, train_f1: 0.4734, val_acc: 0.333333, val_recall: 0.335778, val_f1: 0.259443
Epoch: 249, loss: 22.9309, train_acc: 0.4778, train_recall: 0.4567, train_f1: 0.4771, val_acc: 0.333333, val_recall: 0.335778, val_f1: 0.261751
Epoch: 250, loss: 22.7822, train_acc: 0.4833, train_recall: 0.4575, train_f1: 0.4782, val_acc: 0.333333, val_recall: 0.335778, val_f1: 0.258557
Epoch: 251, loss: 22.7151, train_acc: 0.4889, train_recall: 0.4563, train_f1: 0.4811, val_acc: 0.320513, val_recall: 0.294059, val_f1: 0.247364
Epoch: 252, loss: 23.0423, train_acc: 0.4722, train_recall: 0.4508, train_f1: 0.4751, val_acc: 0.333333, val_recall: 0.334510, val_f1: 0.263511
Epoch: 253, loss: 23.1313, train_acc: 0.4556, train_recall: 0.4461, train_f1: 0.4064, val_acc: 0.346154, val_recall: 0.348202, val_f1: 0.234503
Epoch: 254, loss: 22.9483, train_acc: 0.4667, train_recall: 0.4539, train_f1: 0.4279, val_acc: 0.333333, val_recall: 0.339581, val_f1: 0.229546
Epoch: 255, loss: 23.1485, train_acc: 0.4333, train_recall: 0.4325, train_f1: 0.3960, val_acc: 0.487179, val_recall: 0.410068, val_f1: 0.293603
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed2.csv
Epoch: 256, loss: 22.9187, train_acc: 0.4389, train_recall: 0.4361, train_f1: 0.4035, val_acc: 0.487179, val_recall: 0.410068, val_f1: 0.293603
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed2.csv
Epoch: 257, loss: 23.1673, train_acc: 0.4556, train_recall: 0.4436, train_f1: 0.4057, val_acc: 0.346154, val_recall: 0.348202, val_f1: 0.235474
Epoch: 258, loss: 22.9092, train_acc: 0.4556, train_recall: 0.4461, train_f1: 0.4064, val_acc: 0.346154, val_recall: 0.348202, val_f1: 0.236941
Epoch: 259, loss: 23.0044, train_acc: 0.4333, train_recall: 0.4325, train_f1: 0.3960, val_acc: 0.487179, val_recall: 0.410068, val_f1: 0.296797
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed2.csv
Epoch: 260, loss: 23.0454, train_acc: 0.4333, train_recall: 0.4325, train_f1: 0.3960, val_acc: 0.487179, val_recall: 0.410068, val_f1: 0.296797
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed2.csv
Epoch: 261, loss: 22.5964, train_acc: 0.4167, train_recall: 0.5011, train_f1: 0.3183, val_acc: 0.294872, val_recall: 0.313719, val_f1: 0.219788
/home/ADS/cyang314/ucr_work/HINI_Baseline/GraphLoRA/model/GraphLoRA.py:218: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.
Consider using tensor.detach() first. (Triggered internally at /pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:836.)
  f.write(f'{pre_dataset} to {downstream_dataset}: seed: %d, epoch: %d, train_loss: %f, train_acc: %f, train_recall: %f, train_f1: %f, val_acc: %f, val_recall: %f, val_f1: %f\n' %
Epoch: 262, loss: 23.0985, train_acc: 0.4167, train_recall: 0.4933, train_f1: 0.3117, val_acc: 0.282051, val_recall: 0.273268, val_f1: 0.207143
Epoch: 263, loss: 22.7386, train_acc: 0.3778, train_recall: 0.4740, train_f1: 0.2965, val_acc: 0.461538, val_recall: 0.395362, val_f1: 0.300813
Epoch: 264, loss: 23.6566, train_acc: 0.4278, train_recall: 0.4290, train_f1: 0.3882, val_acc: 0.487179, val_recall: 0.410068, val_f1: 0.298214
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed2.csv
Epoch: 265, loss: 23.4198, train_acc: 0.4056, train_recall: 0.4215, train_f1: 0.3776, val_acc: 0.320513, val_recall: 0.352181, val_f1: 0.220594
Epoch: 266, loss: 25.8529, train_acc: 0.3944, train_recall: 0.3333, train_f1: 0.2656, val_acc: 0.320513, val_recall: 0.215517, val_f1: 0.121359
Epoch: 267, loss: 24.9364, train_acc: 0.4278, train_recall: 0.3561, train_f1: 0.3108, val_acc: 0.333333, val_recall: 0.220335, val_f1: 0.153932
Epoch: 268, loss: 26.6872, train_acc: 0.3722, train_recall: 0.3333, train_f1: 0.2597, val_acc: 0.435897, val_recall: 0.250000, val_f1: 0.153153
Epoch: 269, loss: 26.5023, train_acc: 0.3833, train_recall: 0.3455, train_f1: 0.2841, val_acc: 0.435897, val_recall: 0.250000, val_f1: 0.154545
Epoch: 270, loss: 25.6646, train_acc: 0.2444, train_recall: 0.3407, train_f1: 0.2335, val_acc: 0.179487, val_recall: 0.257353, val_f1: 0.087319
Epoch: 271, loss: 23.0340, train_acc: 0.4667, train_recall: 0.4556, train_f1: 0.4582, val_acc: 0.307692, val_recall: 0.321072, val_f1: 0.226215
Epoch: 272, loss: 24.4825, train_acc: 0.4389, train_recall: 0.3656, train_f1: 0.3521, val_acc: 0.294872, val_recall: 0.193205, val_f1: 0.146062
Epoch: 273, loss: 24.9403, train_acc: 0.3889, train_recall: 0.3489, train_f1: 0.2974, val_acc: 0.435897, val_recall: 0.250000, val_f1: 0.154545
Epoch: 274, loss: 23.7342, train_acc: 0.4444, train_recall: 0.3777, train_f1: 0.3936, val_acc: 0.294872, val_recall: 0.183063, val_f1: 0.164281
Epoch: 275, loss: 24.6158, train_acc: 0.4389, train_recall: 0.3683, train_f1: 0.3353, val_acc: 0.333333, val_recall: 0.220335, val_f1: 0.153932
Epoch: 276, loss: 25.9451, train_acc: 0.2444, train_recall: 0.3407, train_f1: 0.2335, val_acc: 0.179487, val_recall: 0.257353, val_f1: 0.087319
Epoch: 277, loss: 23.3505, train_acc: 0.4833, train_recall: 0.4598, train_f1: 0.4774, val_acc: 0.333333, val_recall: 0.335778, val_f1: 0.259443
Epoch: 278, loss: 23.2334, train_acc: 0.4500, train_recall: 0.3780, train_f1: 0.3789, val_acc: 0.294872, val_recall: 0.191937, val_f1: 0.152144
Epoch: 279, loss: 25.8069, train_acc: 0.3833, train_recall: 0.3453, train_f1: 0.2901, val_acc: 0.435897, val_recall: 0.250000, val_f1: 0.155963
Epoch: 280, loss: 24.3858, train_acc: 0.3889, train_recall: 0.3487, train_f1: 0.3027, val_acc: 0.435897, val_recall: 0.250000, val_f1: 0.155963
Epoch: 281, loss: 24.0874, train_acc: 0.4278, train_recall: 0.4990, train_f1: 0.3416, val_acc: 0.320513, val_recall: 0.329693, val_f1: 0.254725
Epoch: 282, loss: 25.5851, train_acc: 0.3889, train_recall: 0.4828, train_f1: 0.2908, val_acc: 0.282051, val_recall: 0.306366, val_f1: 0.205503
Epoch: 283, loss: 25.8192, train_acc: 0.4000, train_recall: 0.4177, train_f1: 0.3451, val_acc: 0.294872, val_recall: 0.325597, val_f1: 0.201322
Epoch: 284, loss: 24.4011, train_acc: 0.4667, train_recall: 0.4442, train_f1: 0.4094, val_acc: 0.371795, val_recall: 0.362908, val_f1: 0.278242
Epoch: 285, loss: 22.6257, train_acc: 0.4778, train_recall: 0.4537, train_f1: 0.4495, val_acc: 0.346154, val_recall: 0.343131, val_f1: 0.275521
Epoch: 286, loss: 26.0331, train_acc: 0.3889, train_recall: 0.3491, train_f1: 0.2917, val_acc: 0.435897, val_recall: 0.250000, val_f1: 0.154545
Epoch: 287, loss: 24.9870, train_acc: 0.3889, train_recall: 0.3514, train_f1: 0.3008, val_acc: 0.435897, val_recall: 0.250000, val_f1: 0.155963
Epoch: 288, loss: 24.2642, train_acc: 0.4056, train_recall: 0.4872, train_f1: 0.3272, val_acc: 0.307692, val_recall: 0.321072, val_f1: 0.243206
Epoch: 289, loss: 25.6098, train_acc: 0.4556, train_recall: 0.4436, train_f1: 0.4057, val_acc: 0.333333, val_recall: 0.340849, val_f1: 0.224962
Epoch: 290, loss: 26.4789, train_acc: 0.4611, train_recall: 0.4471, train_f1: 0.4093, val_acc: 0.346154, val_recall: 0.349469, val_f1: 0.230330
Epoch: 291, loss: 25.1553, train_acc: 0.4444, train_recall: 0.4415, train_f1: 0.4002, val_acc: 0.346154, val_recall: 0.358812, val_f1: 0.237787
Epoch: 292, loss: 23.5911, train_acc: 0.4556, train_recall: 0.3622, train_f1: 0.3365, val_acc: 0.333333, val_recall: 0.335778, val_f1: 0.258557
Epoch: 293, loss: 24.9487, train_acc: 0.3833, train_recall: 0.2658, train_f1: 0.1661, val_acc: 0.435897, val_recall: 0.250000, val_f1: 0.153153
Epoch: 294, loss: 24.1846, train_acc: 0.4444, train_recall: 0.2947, train_f1: 0.2530, val_acc: 0.294872, val_recall: 0.191937, val_f1: 0.150143
Epoch: 295, loss: 24.9787, train_acc: 0.4444, train_recall: 0.2944, train_f1: 0.2508, val_acc: 0.294872, val_recall: 0.193205, val_f1: 0.146062
Epoch: 296, loss: 23.2009, train_acc: 0.4778, train_recall: 0.3691, train_f1: 0.3503, val_acc: 0.320513, val_recall: 0.294059, val_f1: 0.249346
Epoch: 297, loss: 23.8764, train_acc: 0.4167, train_recall: 0.3439, train_f1: 0.2626, val_acc: 0.474359, val_recall: 0.402715, val_f1: 0.286444
Epoch: 298, loss: 24.7252, train_acc: 0.3000, train_recall: 0.3647, train_f1: 0.3116, val_acc: 0.192308, val_recall: 0.240950, val_f1: 0.113185
Epoch: 299, loss: 23.0573, train_acc: 0.4778, train_recall: 0.4567, train_f1: 0.4775, val_acc: 0.320513, val_recall: 0.327157, val_f1: 0.251134
epoch: 265, train_acc: 0.427778, val_acc: 0.487179, val_recall: 0.410068, val_f1: 0.298214
Running: year=2016 â†’ downstream_year=2017, seed=3
Random seed set to 42

==============================
PRE-TRAINING
==============================
create PreTrain instance...
pre-training...
(T) | Epoch=001, loss=7.8972, this epoch 1.4890, total 1.4890
+++model saved ! 2016.pth
(T) | Epoch=002, loss=7.8960, this epoch 1.5169, total 3.0059
+++model saved ! 2016.pth
(T) | Epoch=003, loss=7.8942, this epoch 1.5921, total 4.5980
+++model saved ! 2016.pth
(T) | Epoch=004, loss=7.8973, this epoch 1.5174, total 6.1154
(T) | Epoch=005, loss=7.8891, this epoch 1.5547, total 7.6700
+++model saved ! 2016.pth
(T) | Epoch=006, loss=7.8855, this epoch 1.5250, total 9.1950
+++model saved ! 2016.pth
(T) | Epoch=007, loss=7.8808, this epoch 1.4749, total 10.6699
+++model saved ! 2016.pth
(T) | Epoch=008, loss=7.8751, this epoch 1.4332, total 12.1031
+++model saved ! 2016.pth
(T) | Epoch=009, loss=7.8690, this epoch 1.4877, total 13.5908
+++model saved ! 2016.pth
(T) | Epoch=010, loss=7.8849, this epoch 1.4303, total 15.0211
(T) | Epoch=011, loss=7.8509, this epoch 1.4830, total 16.5041
+++model saved ! 2016.pth
(T) | Epoch=012, loss=7.8414, this epoch 1.4943, total 17.9984
+++model saved ! 2016.pth
(T) | Epoch=013, loss=7.8309, this epoch 1.5043, total 19.5027
+++model saved ! 2016.pth
(T) | Epoch=014, loss=7.8687, this epoch 1.4522, total 20.9550
(T) | Epoch=015, loss=7.8100, this epoch 1.4498, total 22.4047
+++model saved ! 2016.pth
(T) | Epoch=016, loss=8.9353, this epoch 1.4552, total 23.8600
(T) | Epoch=017, loss=7.7978, this epoch 1.4674, total 25.3274
+++model saved ! 2016.pth
(T) | Epoch=018, loss=7.8477, this epoch 1.4960, total 26.8233
(T) | Epoch=019, loss=8.3957, this epoch 1.4703, total 28.2937
(T) | Epoch=020, loss=7.8094, this epoch 1.4451, total 29.7388
(T) | Epoch=021, loss=8.1407, this epoch 1.4542, total 31.1929
(T) | Epoch=022, loss=8.0648, this epoch 1.4845, total 32.6774
(T) | Epoch=023, loss=7.8291, this epoch 1.4795, total 34.1569
(T) | Epoch=024, loss=7.8650, this epoch 1.5413, total 35.6982
(T) | Epoch=025, loss=7.9818, this epoch 1.4939, total 37.1921
(T) | Epoch=026, loss=7.8394, this epoch 1.5046, total 38.6967
(T) | Epoch=027, loss=7.9440, this epoch 1.4584, total 40.1551
(T) | Epoch=028, loss=7.9133, this epoch 1.5195, total 41.6747
(T) | Epoch=029, loss=7.8532, this epoch 1.4514, total 43.1260
(T) | Epoch=030, loss=7.8385, this epoch 1.4916, total 44.6177
(T) | Epoch=031, loss=7.8355, this epoch 1.4813, total 46.0990
(T) | Epoch=032, loss=7.8339, this epoch 1.4589, total 47.5579
(T) | Epoch=033, loss=7.8334, this epoch 1.4884, total 49.0462
(T) | Epoch=034, loss=7.8813, this epoch 1.4978, total 50.5440
(T) | Epoch=035, loss=7.8675, this epoch 1.5011, total 52.0451
(T) | Epoch=036, loss=7.8694, this epoch 1.4754, total 53.5205
(T) | Epoch=037, loss=7.8295, this epoch 1.5307, total 55.0512
(T) | Epoch=038, loss=7.8461, this epoch 1.5168, total 56.5680
(T) | Epoch=039, loss=7.8733, this epoch 1.4974, total 58.0654
(T) | Epoch=040, loss=7.9009, this epoch 1.5564, total 59.6218
(T) | Epoch=041, loss=7.8444, this epoch 1.4531, total 61.0749
(T) | Epoch=042, loss=7.8238, this epoch 1.5067, total 62.5816
(T) | Epoch=043, loss=7.8633, this epoch 1.5211, total 64.1028
(T) | Epoch=044, loss=7.8205, this epoch 1.4567, total 65.5594
(T) | Epoch=045, loss=7.8970, this epoch 1.4985, total 67.0580
(T) | Epoch=046, loss=7.8833, this epoch 1.4672, total 68.5251
(T) | Epoch=047, loss=7.8183, this epoch 1.5267, total 70.0519
(T) | Epoch=048, loss=7.8181, this epoch 1.5008, total 71.5527
(T) | Epoch=049, loss=7.8869, this epoch 1.4176, total 72.9703
(T) | Epoch=050, loss=7.8172, this epoch 1.4450, total 74.4153
(T) | Epoch=051, loss=7.8158, this epoch 1.4373, total 75.8526
(T) | Epoch=052, loss=7.8338, this epoch 1.4371, total 77.2897
(T) | Epoch=053, loss=7.8124, this epoch 1.4341, total 78.7238
(T) | Epoch=054, loss=7.8297, this epoch 1.4280, total 80.1517
(T) | Epoch=055, loss=7.8500, this epoch 1.4457, total 81.5974
(T) | Epoch=056, loss=7.8050, this epoch 1.4397, total 83.0371
(T) | Epoch=057, loss=7.8455, this epoch 1.5037, total 84.5408
(T) | Epoch=058, loss=7.7608, this epoch 1.4818, total 86.0226
+++model saved ! 2016.pth
(T) | Epoch=059, loss=7.7941, this epoch 1.4573, total 87.4799
(T) | Epoch=060, loss=7.7920, this epoch 1.4666, total 88.9465
(T) | Epoch=061, loss=7.8088, this epoch 1.4684, total 90.4149
(T) | Epoch=062, loss=7.7835, this epoch 1.5116, total 91.9265
(T) | Epoch=063, loss=7.8721, this epoch 1.5003, total 93.4268
(T) | Epoch=064, loss=7.8320, this epoch 1.4576, total 94.8844
(T) | Epoch=065, loss=7.8550, this epoch 1.4370, total 96.3214
(T) | Epoch=066, loss=7.7719, this epoch 1.4480, total 97.7695
(T) | Epoch=067, loss=7.7767, this epoch 1.4563, total 99.2258
(T) | Epoch=068, loss=7.8208, this epoch 1.4406, total 100.6664
(T) | Epoch=069, loss=7.7684, this epoch 1.4575, total 102.1239
(T) | Epoch=070, loss=7.7887, this epoch 1.4602, total 103.5841
(T) | Epoch=071, loss=7.8050, this epoch 1.4591, total 105.0432
(T) | Epoch=072, loss=7.7990, this epoch 1.5121, total 106.5553
(T) | Epoch=073, loss=7.7964, this epoch 1.4789, total 108.0342
(T) | Epoch=074, loss=7.8045, this epoch 1.5092, total 109.5434
(T) | Epoch=075, loss=7.7370, this epoch 1.5062, total 111.0496
+++model saved ! 2016.pth
(T) | Epoch=076, loss=7.7841, this epoch 1.4473, total 112.4969
(T) | Epoch=077, loss=7.7808, this epoch 1.5260, total 114.0229
(T) | Epoch=078, loss=7.7223, this epoch 1.5670, total 115.5899
+++model saved ! 2016.pth
(T) | Epoch=079, loss=7.7388, this epoch 1.5315, total 117.1214
(T) | Epoch=080, loss=7.7078, this epoch 1.4702, total 118.5916
+++model saved ! 2016.pth
(T) | Epoch=081, loss=7.7200, this epoch 1.4977, total 120.0893
(T) | Epoch=082, loss=7.7504, this epoch 1.4970, total 121.5863
(T) | Epoch=083, loss=7.7256, this epoch 1.5225, total 123.1088
(T) | Epoch=084, loss=7.7405, this epoch 1.5083, total 124.6171
(T) | Epoch=085, loss=7.9853, this epoch 1.4343, total 126.0514
(T) | Epoch=086, loss=7.7604, this epoch 1.4801, total 127.5315
(T) | Epoch=087, loss=7.6827, this epoch 1.4815, total 129.0130
+++model saved ! 2016.pth
(T) | Epoch=088, loss=7.6728, this epoch 1.4776, total 130.4906
+++model saved ! 2016.pth
(T) | Epoch=089, loss=7.7032, this epoch 1.4279, total 131.9185
(T) | Epoch=090, loss=7.6639, this epoch 1.4807, total 133.3992
+++model saved ! 2016.pth
(T) | Epoch=091, loss=7.7604, this epoch 1.5131, total 134.9123
(T) | Epoch=092, loss=7.7533, this epoch 1.4757, total 136.3880
(T) | Epoch=093, loss=7.6967, this epoch 1.4811, total 137.8692
(T) | Epoch=094, loss=7.6565, this epoch 1.4869, total 139.3561
+++model saved ! 2016.pth
(T) | Epoch=095, loss=7.7766, this epoch 1.4962, total 140.8523
(T) | Epoch=096, loss=7.7297, this epoch 1.5106, total 142.3629
(T) | Epoch=097, loss=7.6908, this epoch 1.4479, total 143.8108
(T) | Epoch=098, loss=7.7321, this epoch 1.4638, total 145.2746
(T) | Epoch=099, loss=7.6779, this epoch 1.4791, total 146.7537
(T) | Epoch=100, loss=7.6936, this epoch 1.4645, total 148.2182
(T) | Epoch=101, loss=7.7002, this epoch 1.4534, total 149.6716
(T) | Epoch=102, loss=7.6657, this epoch 1.4678, total 151.1395
(T) | Epoch=103, loss=7.6628, this epoch 1.4582, total 152.5977
(T) | Epoch=104, loss=7.6648, this epoch 1.4774, total 154.0751
(T) | Epoch=105, loss=7.8314, this epoch 1.4705, total 155.5456
(T) | Epoch=106, loss=7.6576, this epoch 1.5166, total 157.0621
(T) | Epoch=107, loss=7.9045, this epoch 1.4926, total 158.5548
(T) | Epoch=108, loss=7.6804, this epoch 1.4517, total 160.0065
(T) | Epoch=109, loss=7.6935, this epoch 1.5243, total 161.5308
(T) | Epoch=110, loss=7.6577, this epoch 1.5379, total 163.0687
(T) | Epoch=111, loss=7.6718, this epoch 1.4951, total 164.5638
(T) | Epoch=112, loss=8.1404, this epoch 1.4290, total 165.9928
(T) | Epoch=113, loss=7.6954, this epoch 1.4970, total 167.4898
(T) | Epoch=114, loss=7.7348, this epoch 1.4442, total 168.9340
(T) | Epoch=115, loss=7.6981, this epoch 1.4849, total 170.4189
(T) | Epoch=116, loss=7.9329, this epoch 1.4935, total 171.9124
(T) | Epoch=117, loss=7.9696, this epoch 1.4817, total 173.3941
(T) | Epoch=118, loss=7.7740, this epoch 1.4491, total 174.8433
(T) | Epoch=119, loss=7.7467, this epoch 1.5013, total 176.3446
(T) | Epoch=120, loss=7.8114, this epoch 1.5182, total 177.8628
(T) | Epoch=121, loss=7.7820, this epoch 1.4988, total 179.3616
(T) | Epoch=122, loss=7.8905, this epoch 1.5303, total 180.8920
(T) | Epoch=123, loss=7.8847, this epoch 1.4694, total 182.3613
(T) | Epoch=124, loss=7.7380, this epoch 1.4277, total 183.7890
(T) | Epoch=125, loss=7.7314, this epoch 1.5120, total 185.3010
(T) | Epoch=126, loss=7.7788, this epoch 1.4950, total 186.7961
(T) | Epoch=127, loss=7.7780, this epoch 1.4845, total 188.2806
(T) | Epoch=128, loss=7.7069, this epoch 1.4837, total 189.7642
(T) | Epoch=129, loss=7.6958, this epoch 1.4686, total 191.2329
(T) | Epoch=130, loss=7.6921, this epoch 1.4512, total 192.6841
(T) | Epoch=131, loss=7.7212, this epoch 1.4633, total 194.1474
(T) | Epoch=132, loss=7.6800, this epoch 1.5038, total 195.6513
(T) | Epoch=133, loss=7.6819, this epoch 1.4706, total 197.1219
(T) | Epoch=134, loss=7.7725, this epoch 1.5020, total 198.6239
(T) | Epoch=135, loss=8.0296, this epoch 1.5079, total 200.1318
(T) | Epoch=136, loss=7.7016, this epoch 1.5216, total 201.6534
(T) | Epoch=137, loss=7.7240, this epoch 1.4866, total 203.1400
(T) | Epoch=138, loss=7.6803, this epoch 1.4725, total 204.6125
(T) | Epoch=139, loss=7.6857, this epoch 1.4805, total 206.0929
(T) | Epoch=140, loss=7.7758, this epoch 1.4738, total 207.5667
(T) | Epoch=141, loss=7.6878, this epoch 1.5021, total 209.0689
(T) | Epoch=142, loss=7.6937, this epoch 1.4443, total 210.5132
(T) | Epoch=143, loss=7.6953, this epoch 1.4692, total 211.9824
(T) | Epoch=144, loss=7.6950, this epoch 1.4803, total 213.4627
(T) | Epoch=145, loss=7.6916, this epoch 1.4532, total 214.9160
(T) | Epoch=146, loss=7.8707, this epoch 1.4491, total 216.3651
(T) | Epoch=147, loss=7.9587, this epoch 1.5200, total 217.8851
(T) | Epoch=148, loss=7.7416, this epoch 1.5522, total 219.4373
(T) | Epoch=149, loss=7.6807, this epoch 1.5326, total 220.9699
(T) | Epoch=150, loss=7.6698, this epoch 1.5191, total 222.4890
(T) | Epoch=151, loss=7.7041, this epoch 1.4841, total 223.9731
(T) | Epoch=152, loss=7.6688, this epoch 1.5323, total 225.5054
(T) | Epoch=153, loss=7.6618, this epoch 1.5088, total 227.0142
(T) | Epoch=154, loss=7.7258, this epoch 1.5276, total 228.5418
(T) | Epoch=155, loss=7.6446, this epoch 1.5067, total 230.0485
+++model saved ! 2016.pth
(T) | Epoch=156, loss=7.6350, this epoch 1.4757, total 231.5242
+++model saved ! 2016.pth
(T) | Epoch=157, loss=7.6350, this epoch 1.4984, total 233.0227
+++model saved ! 2016.pth
(T) | Epoch=158, loss=7.6295, this epoch 1.4493, total 234.4720
+++model saved ! 2016.pth
(T) | Epoch=159, loss=7.9806, this epoch 1.4425, total 235.9145
(T) | Epoch=160, loss=7.6389, this epoch 1.4840, total 237.3984
(T) | Epoch=161, loss=7.7209, this epoch 1.5258, total 238.9242
(T) | Epoch=162, loss=7.7459, this epoch 1.4847, total 240.4089
(T) | Epoch=163, loss=7.7678, this epoch 1.4815, total 241.8904
(T) | Epoch=164, loss=7.7021, this epoch 1.4836, total 243.3739
(T) | Epoch=165, loss=7.7692, this epoch 1.4684, total 244.8424
(T) | Epoch=166, loss=7.9191, this epoch 1.4690, total 246.3113
(T) | Epoch=167, loss=7.7395, this epoch 1.4648, total 247.7761
(T) | Epoch=168, loss=7.8386, this epoch 1.4889, total 249.2650
(T) | Epoch=169, loss=7.7457, this epoch 1.4399, total 250.7050
(T) | Epoch=170, loss=7.8090, this epoch 1.4174, total 252.1223
(T) | Epoch=171, loss=7.8330, this epoch 1.4430, total 253.5654
(T) | Epoch=172, loss=7.7389, this epoch 1.4599, total 255.0253
(T) | Epoch=173, loss=7.7918, this epoch 1.4529, total 256.4781
(T) | Epoch=174, loss=7.7982, this epoch 1.4940, total 257.9722
(T) | Epoch=175, loss=7.7083, this epoch 1.4814, total 259.4536
(T) | Epoch=176, loss=7.7061, this epoch 1.4753, total 260.9289
(T) | Epoch=177, loss=7.6974, this epoch 1.4986, total 262.4275
(T) | Epoch=178, loss=7.7521, this epoch 1.5099, total 263.9374
(T) | Epoch=179, loss=7.7281, this epoch 1.5207, total 265.4581
(T) | Epoch=180, loss=7.7283, this epoch 1.5705, total 267.0286
(T) | Epoch=181, loss=7.7021, this epoch 1.5248, total 268.5534
(T) | Epoch=182, loss=7.6751, this epoch 1.4668, total 270.0202
(T) | Epoch=183, loss=7.7060, this epoch 1.4615, total 271.4817
(T) | Epoch=184, loss=7.6700, this epoch 1.4563, total 272.9380
(T) | Epoch=185, loss=7.7197, this epoch 1.5164, total 274.4543
(T) | Epoch=186, loss=7.6522, this epoch 1.5149, total 275.9692
(T) | Epoch=187, loss=7.7096, this epoch 1.5416, total 277.5108
(T) | Epoch=188, loss=7.6425, this epoch 1.4696, total 278.9804
(T) | Epoch=189, loss=7.6440, this epoch 1.4752, total 280.4556
(T) | Epoch=190, loss=7.6702, this epoch 1.4690, total 281.9247
(T) | Epoch=191, loss=7.6335, this epoch 1.4911, total 283.4158
(T) | Epoch=192, loss=7.6225, this epoch 1.4468, total 284.8626
+++model saved ! 2016.pth
(T) | Epoch=193, loss=7.6214, this epoch 1.5228, total 286.3854
+++model saved ! 2016.pth
(T) | Epoch=194, loss=7.6194, this epoch 1.4924, total 287.8778
+++model saved ! 2016.pth
(T) | Epoch=195, loss=7.6500, this epoch 1.5262, total 289.4040
(T) | Epoch=196, loss=7.6092, this epoch 1.4497, total 290.8537
+++model saved ! 2016.pth
(T) | Epoch=197, loss=7.5999, this epoch 1.5102, total 292.3639
+++model saved ! 2016.pth
(T) | Epoch=198, loss=7.6908, this epoch 1.5249, total 293.8888
(T) | Epoch=199, loss=7.6461, this epoch 1.4916, total 295.3804
(T) | Epoch=200, loss=7.6914, this epoch 1.4735, total 296.8539
(T) | Epoch=201, loss=7.7265, this epoch 1.4727, total 298.3265
(T) | Epoch=202, loss=7.6334, this epoch 1.5253, total 299.8519
(T) | Epoch=203, loss=7.6409, this epoch 1.4639, total 301.3158
(T) | Epoch=204, loss=7.6671, this epoch 1.4678, total 302.7836
(T) | Epoch=205, loss=7.6712, this epoch 1.4743, total 304.2579
(T) | Epoch=206, loss=7.6833, this epoch 1.5015, total 305.7594
(T) | Epoch=207, loss=7.7145, this epoch 1.4694, total 307.2288
(T) | Epoch=208, loss=7.6406, this epoch 1.4365, total 308.6653
(T) | Epoch=209, loss=7.6699, this epoch 1.4742, total 310.1395
(T) | Epoch=210, loss=7.6666, this epoch 1.4911, total 311.6306
(T) | Epoch=211, loss=7.6291, this epoch 1.4734, total 313.1039
(T) | Epoch=212, loss=7.7429, this epoch 1.4553, total 314.5592
(T) | Epoch=213, loss=7.6209, this epoch 1.4343, total 315.9936
(T) | Epoch=214, loss=7.6179, this epoch 1.4457, total 317.4393
(T) | Epoch=215, loss=7.6074, this epoch 1.4781, total 318.9174
(T) | Epoch=216, loss=7.7853, this epoch 1.5129, total 320.4303
(T) | Epoch=217, loss=7.6730, this epoch 1.4898, total 321.9201
(T) | Epoch=218, loss=7.6273, this epoch 1.5003, total 323.4203
(T) | Epoch=219, loss=7.6329, this epoch 1.4647, total 324.8851
(T) | Epoch=220, loss=7.6359, this epoch 1.4694, total 326.3545
(T) | Epoch=221, loss=7.7191, this epoch 1.5006, total 327.8551
(T) | Epoch=222, loss=7.6743, this epoch 1.4662, total 329.3213
(T) | Epoch=223, loss=7.6385, this epoch 1.4648, total 330.7861
(T) | Epoch=224, loss=7.6429, this epoch 1.4599, total 332.2460
(T) | Epoch=225, loss=7.6478, this epoch 1.4563, total 333.7023
(T) | Epoch=226, loss=7.6565, this epoch 1.4749, total 335.1773
(T) | Epoch=227, loss=7.7008, this epoch 1.4584, total 336.6357
(T) | Epoch=228, loss=7.6370, this epoch 1.4596, total 338.0953
(T) | Epoch=229, loss=7.6407, this epoch 1.4875, total 339.5827
(T) | Epoch=230, loss=7.6681, this epoch 1.4477, total 341.0304
(T) | Epoch=231, loss=7.6297, this epoch 1.4724, total 342.5029
(T) | Epoch=232, loss=7.6327, this epoch 1.4577, total 343.9606
(T) | Epoch=233, loss=7.6318, this epoch 1.4820, total 345.4426
(T) | Epoch=234, loss=7.6505, this epoch 1.4486, total 346.8912
(T) | Epoch=235, loss=7.6506, this epoch 1.4512, total 348.3423
(T) | Epoch=236, loss=7.6233, this epoch 1.5006, total 349.8429
(T) | Epoch=237, loss=7.6656, this epoch 1.5319, total 351.3748
(T) | Epoch=238, loss=7.6206, this epoch 1.5075, total 352.8823
(T) | Epoch=239, loss=7.6069, this epoch 1.4993, total 354.3816
(T) | Epoch=240, loss=7.6020, this epoch 1.4587, total 355.8403
(T) | Epoch=241, loss=7.6014, this epoch 1.4633, total 357.3036
(T) | Epoch=242, loss=7.5974, this epoch 1.4580, total 358.7617
+++model saved ! 2016.pth
(T) | Epoch=243, loss=7.7122, this epoch 1.4316, total 360.1933
(T) | Epoch=244, loss=7.5825, this epoch 1.4505, total 361.6438
+++model saved ! 2016.pth
(T) | Epoch=245, loss=7.6316, this epoch 1.4125, total 363.0564
(T) | Epoch=246, loss=7.6575, this epoch 1.4871, total 364.5435
(T) | Epoch=247, loss=7.6442, this epoch 1.4731, total 366.0166
(T) | Epoch=248, loss=7.6549, this epoch 1.4361, total 367.4527
(T) | Epoch=249, loss=7.6258, this epoch 1.4537, total 368.9064
(T) | Epoch=250, loss=7.6161, this epoch 1.4500, total 370.3565
(T) | Epoch=251, loss=7.6353, this epoch 1.4445, total 371.8009
(T) | Epoch=252, loss=7.6374, this epoch 1.5095, total 373.3105
(T) | Epoch=253, loss=7.6581, this epoch 1.4002, total 374.7106
(T) | Epoch=254, loss=7.6275, this epoch 1.4607, total 376.1713
(T) | Epoch=255, loss=7.6231, this epoch 1.4563, total 377.6276
(T) | Epoch=256, loss=7.6265, this epoch 1.4842, total 379.1118
(T) | Epoch=257, loss=7.6247, this epoch 1.4347, total 380.5465
(T) | Epoch=258, loss=7.6480, this epoch 1.4809, total 382.0274
(T) | Epoch=259, loss=7.6726, this epoch 1.4665, total 383.4940
(T) | Epoch=260, loss=7.7362, this epoch 1.5173, total 385.0113
(T) | Epoch=261, loss=7.6112, this epoch 1.4619, total 386.4731
(T) | Epoch=262, loss=7.5926, this epoch 1.4772, total 387.9503
(T) | Epoch=263, loss=7.6275, this epoch 1.5054, total 389.4556
(T) | Epoch=264, loss=7.5969, this epoch 1.5014, total 390.9571
(T) | Epoch=265, loss=7.5855, this epoch 1.4415, total 392.3985
(T) | Epoch=266, loss=7.5856, this epoch 1.5351, total 393.9336
(T) | Epoch=267, loss=7.5699, this epoch 1.5110, total 395.4446
+++model saved ! 2016.pth
(T) | Epoch=268, loss=7.6041, this epoch 1.5168, total 396.9614
(T) | Epoch=269, loss=7.5493, this epoch 1.4915, total 398.4529
+++model saved ! 2016.pth
(T) | Epoch=270, loss=7.5517, this epoch 1.5296, total 399.9825
(T) | Epoch=271, loss=7.5466, this epoch 1.4881, total 401.4707
+++model saved ! 2016.pth
(T) | Epoch=272, loss=8.0264, this epoch 1.4961, total 402.9667
(T) | Epoch=273, loss=7.6125, this epoch 1.4359, total 404.4026
(T) | Epoch=274, loss=7.6927, this epoch 1.5569, total 405.9595
(T) | Epoch=275, loss=7.7581, this epoch 1.4657, total 407.4252
(T) | Epoch=276, loss=7.6928, this epoch 1.4852, total 408.9104
(T) | Epoch=277, loss=7.7092, this epoch 1.4257, total 410.3361
(T) | Epoch=278, loss=7.7423, this epoch 1.4488, total 411.7849
(T) | Epoch=279, loss=7.7906, this epoch 1.4197, total 413.2046
(T) | Epoch=280, loss=7.9144, this epoch 1.4429, total 414.6475
(T) | Epoch=281, loss=7.8076, this epoch 1.4369, total 416.0845
(T) | Epoch=282, loss=7.8167, this epoch 1.4485, total 417.5329
(T) | Epoch=283, loss=7.7798, this epoch 1.4420, total 418.9750
(T) | Epoch=284, loss=7.8240, this epoch 1.4339, total 420.4088
(T) | Epoch=285, loss=7.7826, this epoch 1.4446, total 421.8535
(T) | Epoch=286, loss=7.9037, this epoch 1.4248, total 423.2783
(T) | Epoch=287, loss=7.9312, this epoch 1.5036, total 424.7819
(T) | Epoch=288, loss=7.9051, this epoch 1.4867, total 426.2686
(T) | Epoch=289, loss=7.9388, this epoch 1.4698, total 427.7384
(T) | Epoch=290, loss=7.8462, this epoch 1.5060, total 429.2443
(T) | Epoch=291, loss=7.7874, this epoch 1.4650, total 430.7094
(T) | Epoch=292, loss=7.7891, this epoch 1.4486, total 432.1580
(T) | Epoch=293, loss=7.9099, this epoch 1.4454, total 433.6034
(T) | Epoch=294, loss=7.7901, this epoch 1.5046, total 435.1079
(T) | Epoch=295, loss=7.7890, this epoch 1.5066, total 436.6145
(T) | Epoch=296, loss=7.7900, this epoch 1.4886, total 438.1032
(T) | Epoch=297, loss=7.8543, this epoch 1.4730, total 439.5762
(T) | Epoch=298, loss=8.0895, this epoch 1.4898, total 441.0660
(T) | Epoch=299, loss=7.7896, this epoch 1.4396, total 442.5055
(T) | Epoch=300, loss=7.9353, this epoch 1.4693, total 443.9748
(T) | Epoch=301, loss=7.7884, this epoch 1.4703, total 445.4452
(T) | Epoch=302, loss=7.9216, this epoch 1.4434, total 446.8886
(T) | Epoch=303, loss=7.7897, this epoch 1.4575, total 448.3461
(T) | Epoch=304, loss=7.7893, this epoch 1.4436, total 449.7897
(T) | Epoch=305, loss=7.7887, this epoch 1.4540, total 451.2437
(T) | Epoch=306, loss=7.9136, this epoch 1.4626, total 452.7063
(T) | Epoch=307, loss=7.9213, this epoch 1.4777, total 454.1840
(T) | Epoch=308, loss=7.7882, this epoch 1.4435, total 455.6276
(T) | Epoch=309, loss=7.8438, this epoch 1.4516, total 457.0792
(T) | Epoch=310, loss=7.7876, this epoch 1.4852, total 458.5643
(T) | Epoch=311, loss=7.7879, this epoch 1.4553, total 460.0196
(T) | Epoch=312, loss=7.7863, this epoch 1.4350, total 461.4546
(T) | Epoch=313, loss=7.7874, this epoch 1.4511, total 462.9057
(T) | Epoch=314, loss=7.8393, this epoch 1.5051, total 464.4108
(T) | Epoch=315, loss=7.8384, this epoch 1.5117, total 465.9225
(T) | Epoch=316, loss=7.7841, this epoch 1.5167, total 467.4392
(T) | Epoch=317, loss=7.8805, this epoch 1.4585, total 468.8977
(T) | Epoch=318, loss=7.7858, this epoch 1.4609, total 470.3587
(T) | Epoch=319, loss=7.7838, this epoch 1.4718, total 471.8305
(T) | Epoch=320, loss=7.7820, this epoch 1.4387, total 473.2692
(T) | Epoch=321, loss=7.8743, this epoch 1.4324, total 474.7016
(T) | Epoch=322, loss=7.8903, this epoch 1.4473, total 476.1490
(T) | Epoch=323, loss=7.8891, this epoch 1.4720, total 477.6210
(T) | Epoch=324, loss=7.7777, this epoch 1.4727, total 479.0937
(T) | Epoch=325, loss=7.8938, this epoch 1.5045, total 480.5982
(T) | Epoch=326, loss=7.7803, this epoch 1.4526, total 482.0508
(T) | Epoch=327, loss=7.7766, this epoch 1.4762, total 483.5270
(T) | Epoch=328, loss=7.7773, this epoch 1.5043, total 485.0313
(T) | Epoch=329, loss=7.7776, this epoch 1.4575, total 486.4888
(T) | Epoch=330, loss=7.8904, this epoch 1.5253, total 488.0141
(T) | Epoch=331, loss=7.8145, this epoch 1.4310, total 489.4451
(T) | Epoch=332, loss=7.7720, this epoch 1.4517, total 490.8968
(T) | Epoch=333, loss=7.8152, this epoch 1.4742, total 492.3710
(T) | Epoch=334, loss=7.7734, this epoch 1.4656, total 493.8366
(T) | Epoch=335, loss=7.7695, this epoch 1.4900, total 495.3267
(T) | Epoch=336, loss=7.7696, this epoch 1.5336, total 496.8603
(T) | Epoch=337, loss=7.8588, this epoch 1.4984, total 498.3587
(T) | Epoch=338, loss=7.8664, this epoch 1.4992, total 499.8579
(T) | Epoch=339, loss=7.8717, this epoch 1.4460, total 501.3039
(T) | Epoch=340, loss=7.8483, this epoch 1.4662, total 502.7702
(T) | Epoch=341, loss=7.7959, this epoch 1.4540, total 504.2242
(T) | Epoch=342, loss=7.7556, this epoch 1.4649, total 505.6891
(T) | Epoch=343, loss=7.9333, this epoch 1.5005, total 507.1896
(T) | Epoch=344, loss=7.8531, this epoch 1.4528, total 508.6424
(T) | Epoch=345, loss=7.9464, this epoch 1.5404, total 510.1828
(T) | Epoch=346, loss=7.7494, this epoch 1.5489, total 511.7317
(T) | Epoch=347, loss=7.8198, this epoch 1.4305, total 513.1622
(T) | Epoch=348, loss=7.8303, this epoch 1.5026, total 514.6647
(T) | Epoch=349, loss=7.8083, this epoch 1.4890, total 516.1537
(T) | Epoch=350, loss=7.7799, this epoch 1.4993, total 517.6530
(T) | Epoch=351, loss=7.7349, this epoch 1.5103, total 519.1634
(T) | Epoch=352, loss=7.8048, this epoch 1.4756, total 520.6390
(T) | Epoch=353, loss=7.7255, this epoch 1.4734, total 522.1124
(T) | Epoch=354, loss=7.7234, this epoch 1.5000, total 523.6123
(T) | Epoch=355, loss=7.7990, this epoch 1.4255, total 525.0379
(T) | Epoch=356, loss=7.7167, this epoch 1.5116, total 526.5495
(T) | Epoch=357, loss=7.7518, this epoch 1.5021, total 528.0516
(T) | Epoch=358, loss=7.7096, this epoch 1.5549, total 529.6065
(T) | Epoch=359, loss=7.7178, this epoch 1.5157, total 531.1222
(T) | Epoch=360, loss=7.7386, this epoch 1.5178, total 532.6400
(T) | Epoch=361, loss=7.6995, this epoch 1.5703, total 534.2103
(T) | Epoch=362, loss=7.7580, this epoch 1.5831, total 535.7933
(T) | Epoch=363, loss=7.7012, this epoch 1.4822, total 537.2756
(T) | Epoch=364, loss=7.7008, this epoch 1.4681, total 538.7437
(T) | Epoch=365, loss=7.7237, this epoch 1.4983, total 540.2420
(T) | Epoch=366, loss=7.6910, this epoch 1.5223, total 541.7643
(T) | Epoch=367, loss=7.7336, this epoch 1.4493, total 543.2136
(T) | Epoch=368, loss=7.6818, this epoch 1.4658, total 544.6795
(T) | Epoch=369, loss=7.8351, this epoch 1.4796, total 546.1590
(T) | Epoch=370, loss=7.7580, this epoch 1.4936, total 547.6527
(T) | Epoch=371, loss=7.6759, this epoch 1.5166, total 549.1693
(T) | Epoch=372, loss=7.6740, this epoch 1.4701, total 550.6394
(T) | Epoch=373, loss=7.6948, this epoch 1.4413, total 552.0806
(T) | Epoch=374, loss=7.6699, this epoch 1.5074, total 553.5880
(T) | Epoch=375, loss=7.6610, this epoch 1.4884, total 555.0764
(T) | Epoch=376, loss=7.8949, this epoch 1.5066, total 556.5830
(T) | Epoch=377, loss=7.6607, this epoch 1.5052, total 558.0882
(T) | Epoch=378, loss=7.6540, this epoch 1.4961, total 559.5843
(T) | Epoch=379, loss=7.6588, this epoch 1.5185, total 561.1028
(T) | Epoch=380, loss=7.6600, this epoch 1.4495, total 562.5523
(T) | Epoch=381, loss=7.6927, this epoch 1.5420, total 564.0942
(T) | Epoch=382, loss=7.6894, this epoch 1.5179, total 565.6121
(T) | Epoch=383, loss=7.6950, this epoch 1.4700, total 567.0822
(T) | Epoch=384, loss=7.6705, this epoch 1.4910, total 568.5731
(T) | Epoch=385, loss=7.6422, this epoch 1.4845, total 570.0577
(T) | Epoch=386, loss=7.6419, this epoch 1.4819, total 571.5396
(T) | Epoch=387, loss=7.6368, this epoch 1.5494, total 573.0890
(T) | Epoch=388, loss=7.6347, this epoch 1.4887, total 574.5776
(T) | Epoch=389, loss=7.6315, this epoch 1.5213, total 576.0990
(T) | Epoch=390, loss=7.6342, this epoch 1.4679, total 577.5669
(T) | Epoch=391, loss=7.6326, this epoch 1.4847, total 579.0516
(T) | Epoch=392, loss=7.6625, this epoch 1.5003, total 580.5519
(T) | Epoch=393, loss=7.6289, this epoch 1.5016, total 582.0535
(T) | Epoch=394, loss=7.6257, this epoch 1.4837, total 583.5372
(T) | Epoch=395, loss=7.6796, this epoch 1.4833, total 585.0205
(T) | Epoch=396, loss=7.6172, this epoch 1.4613, total 586.4818
(T) | Epoch=397, loss=7.6148, this epoch 1.5125, total 587.9943
(T) | Epoch=398, loss=7.6550, this epoch 1.4772, total 589.4715
(T) | Epoch=399, loss=7.6177, this epoch 1.5242, total 590.9956
(T) | Epoch=400, loss=7.6535, this epoch 1.5059, total 592.5015
(T) | Epoch=401, loss=7.6070, this epoch 1.5284, total 594.0299
(T) | Epoch=402, loss=7.6853, this epoch 1.5277, total 595.5576
(T) | Epoch=403, loss=7.7738, this epoch 1.5446, total 597.1021
(T) | Epoch=404, loss=7.6505, this epoch 1.4749, total 598.5770
(T) | Epoch=405, loss=7.6475, this epoch 1.5021, total 600.0791
(T) | Epoch=406, loss=7.6837, this epoch 1.4724, total 601.5515
(T) | Epoch=407, loss=7.6217, this epoch 1.4674, total 603.0189
(T) | Epoch=408, loss=7.6186, this epoch 1.4673, total 604.4862
(T) | Epoch=409, loss=7.6610, this epoch 1.4694, total 605.9556
(T) | Epoch=410, loss=7.6358, this epoch 1.4815, total 607.4370
(T) | Epoch=411, loss=7.6181, this epoch 1.4555, total 608.8925
(T) | Epoch=412, loss=7.6446, this epoch 1.5131, total 610.4056
(T) | Epoch=413, loss=7.6595, this epoch 1.4500, total 611.8556
(T) | Epoch=414, loss=7.6464, this epoch 1.5034, total 613.3590
(T) | Epoch=415, loss=7.6406, this epoch 1.4913, total 614.8503
(T) | Epoch=416, loss=7.6049, this epoch 1.4896, total 616.3399
(T) | Epoch=417, loss=7.6072, this epoch 1.4708, total 617.8108
(T) | Epoch=418, loss=7.6069, this epoch 1.4961, total 619.3069
(T) | Epoch=419, loss=7.6294, this epoch 1.4721, total 620.7790
(T) | Epoch=420, loss=7.5997, this epoch 1.4866, total 622.2656
(T) | Epoch=421, loss=7.6541, this epoch 1.5068, total 623.7724
(T) | Epoch=422, loss=7.6473, this epoch 1.4882, total 625.2606
(T) | Epoch=423, loss=7.6440, this epoch 1.4930, total 626.7536
(T) | Epoch=424, loss=7.6294, this epoch 1.4710, total 628.2245
(T) | Epoch=425, loss=7.5835, this epoch 1.5319, total 629.7564
(T) | Epoch=426, loss=7.6154, this epoch 1.4972, total 631.2536
(T) | Epoch=427, loss=7.5951, this epoch 1.4739, total 632.7274
(T) | Epoch=428, loss=7.6772, this epoch 1.4583, total 634.1857
(T) | Epoch=429, loss=7.5929, this epoch 1.4708, total 635.6566
(T) | Epoch=430, loss=7.5881, this epoch 1.4733, total 637.1299
(T) | Epoch=431, loss=7.5790, this epoch 1.4680, total 638.5978
(T) | Epoch=432, loss=7.6305, this epoch 1.4104, total 640.0082
(T) | Epoch=433, loss=7.5647, this epoch 1.4266, total 641.4349
(T) | Epoch=434, loss=7.5661, this epoch 1.4455, total 642.8803
(T) | Epoch=435, loss=8.8555, this epoch 1.4790, total 644.3594
(T) | Epoch=436, loss=7.5970, this epoch 1.4372, total 645.7966
(T) | Epoch=437, loss=7.6141, this epoch 1.4732, total 647.2698
(T) | Epoch=438, loss=7.6272, this epoch 1.4855, total 648.7553
(T) | Epoch=439, loss=7.6476, this epoch 1.4828, total 650.2381
(T) | Epoch=440, loss=7.6558, this epoch 1.5105, total 651.7486
(T) | Epoch=441, loss=7.6616, this epoch 1.5103, total 653.2589
(T) | Epoch=442, loss=7.7359, this epoch 1.6335, total 654.8924
(T) | Epoch=443, loss=7.6688, this epoch 1.5239, total 656.4163
(T) | Epoch=444, loss=7.7759, this epoch 1.5721, total 657.9883
(T) | Epoch=445, loss=7.6888, this epoch 1.4704, total 659.4587
(T) | Epoch=446, loss=7.6943, this epoch 1.5105, total 660.9692
(T) | Epoch=447, loss=7.6920, this epoch 1.4768, total 662.4459
(T) | Epoch=448, loss=7.7044, this epoch 1.4743, total 663.9203
(T) | Epoch=449, loss=7.7629, this epoch 1.5291, total 665.4494
(T) | Epoch=450, loss=7.7834, this epoch 1.4760, total 666.9254
(T) | Epoch=451, loss=7.7058, this epoch 1.4637, total 668.3891
(T) | Epoch=452, loss=7.7130, this epoch 1.4803, total 669.8694
(T) | Epoch=453, loss=7.7006, this epoch 1.4683, total 671.3376
(T) | Epoch=454, loss=7.8071, this epoch 1.4763, total 672.8139
(T) | Epoch=455, loss=7.7116, this epoch 1.4885, total 674.3024
(T) | Epoch=456, loss=7.6989, this epoch 1.5208, total 675.8232
(T) | Epoch=457, loss=7.7952, this epoch 1.5242, total 677.3474
(T) | Epoch=458, loss=7.7380, this epoch 1.5423, total 678.8897
(T) | Epoch=459, loss=7.6934, this epoch 1.5125, total 680.4022
(T) | Epoch=460, loss=7.7487, this epoch 1.5351, total 681.9373
(T) | Epoch=461, loss=7.6262, this epoch 1.4889, total 683.4262
(T) | Epoch=462, loss=7.6856, this epoch 1.5310, total 684.9572
(T) | Epoch=463, loss=7.7327, this epoch 1.4383, total 686.3955
(T) | Epoch=464, loss=7.7354, this epoch 1.4631, total 687.8586
(T) | Epoch=465, loss=7.6769, this epoch 1.4441, total 689.3027
(T) | Epoch=466, loss=7.6861, this epoch 1.4689, total 690.7717
(T) | Epoch=467, loss=7.6730, this epoch 1.4738, total 692.2455
(T) | Epoch=468, loss=7.7102, this epoch 1.4894, total 693.7349
(T) | Epoch=469, loss=7.7181, this epoch 1.4796, total 695.2145
(T) | Epoch=470, loss=7.6789, this epoch 1.4690, total 696.6835
(T) | Epoch=471, loss=7.6631, this epoch 1.4466, total 698.1301
(T) | Epoch=472, loss=7.6953, this epoch 1.5030, total 699.6331
(T) | Epoch=473, loss=7.6945, this epoch 1.5047, total 701.1378
(T) | Epoch=474, loss=7.6669, this epoch 1.5109, total 702.6487
(T) | Epoch=475, loss=7.7069, this epoch 1.4617, total 704.1104
(T) | Epoch=476, loss=7.6591, this epoch 1.4585, total 705.5690
(T) | Epoch=477, loss=7.8788, this epoch 1.4806, total 707.0496
(T) | Epoch=478, loss=7.6518, this epoch 1.5284, total 708.5780
(T) | Epoch=479, loss=7.6897, this epoch 1.4765, total 710.0545
(T) | Epoch=480, loss=7.6492, this epoch 1.4488, total 711.5033
(T) | Epoch=481, loss=7.6469, this epoch 1.5061, total 713.0094
(T) | Epoch=482, loss=7.6953, this epoch 1.4786, total 714.4880
(T) | Epoch=483, loss=7.6424, this epoch 1.4521, total 715.9401
(T) | Epoch=484, loss=7.6413, this epoch 1.4302, total 717.3703
(T) | Epoch=485, loss=7.6418, this epoch 1.4420, total 718.8123
(T) | Epoch=486, loss=7.6354, this epoch 1.4640, total 720.2763
(T) | Epoch=487, loss=7.6761, this epoch 1.4704, total 721.7467
(T) | Epoch=488, loss=7.6678, this epoch 1.4613, total 723.2080
(T) | Epoch=489, loss=7.6692, this epoch 1.4352, total 724.6432
(T) | Epoch=490, loss=7.6350, this epoch 1.4402, total 726.0834
(T) | Epoch=491, loss=7.7432, this epoch 1.4667, total 727.5501
(T) | Epoch=492, loss=7.6367, this epoch 1.4551, total 729.0052
(T) | Epoch=493, loss=7.6636, this epoch 1.4614, total 730.4666
(T) | Epoch=494, loss=7.6340, this epoch 1.4566, total 731.9232
(T) | Epoch=495, loss=7.7777, this epoch 1.4144, total 733.3376
(T) | Epoch=496, loss=7.6382, this epoch 1.4774, total 734.8150
(T) | Epoch=497, loss=7.6332, this epoch 1.4841, total 736.2991
(T) | Epoch=498, loss=7.6808, this epoch 1.4837, total 737.7828
(T) | Epoch=499, loss=7.6218, this epoch 1.4733, total 739.2561
(T) | Epoch=500, loss=7.6256, this epoch 1.4640, total 740.7201
(T) | Epoch=501, loss=7.6146, this epoch 1.4391, total 742.1592
(T) | Epoch=502, loss=7.6180, this epoch 1.4575, total 743.6167
(T) | Epoch=503, loss=7.6192, this epoch 1.4148, total 745.0315
(T) | Epoch=504, loss=7.6568, this epoch 1.4351, total 746.4666
(T) | Epoch=505, loss=7.6222, this epoch 1.4617, total 747.9283
(T) | Epoch=506, loss=7.6270, this epoch 1.4501, total 749.3785
(T) | Epoch=507, loss=7.7611, this epoch 1.4547, total 750.8332
(T) | Epoch=508, loss=7.6683, this epoch 1.4491, total 752.2823
(T) | Epoch=509, loss=7.6216, this epoch 1.4785, total 753.7608
(T) | Epoch=510, loss=7.6057, this epoch 1.4505, total 755.2113
(T) | Epoch=511, loss=7.5973, this epoch 1.4781, total 756.6894
(T) | Epoch=512, loss=7.6436, this epoch 1.4919, total 758.1813
(T) | Epoch=513, loss=7.6028, this epoch 1.4219, total 759.6032
(T) | Epoch=514, loss=7.6421, this epoch 1.4527, total 761.0559
(T) | Epoch=515, loss=7.6110, this epoch 1.4611, total 762.5170
(T) | Epoch=516, loss=7.5971, this epoch 1.4736, total 763.9906
(T) | Epoch=517, loss=7.5968, this epoch 1.4510, total 765.4416
(T) | Epoch=518, loss=7.5976, this epoch 1.4886, total 766.9303
(T) | Epoch=519, loss=7.6575, this epoch 1.5032, total 768.4334
(T) | Epoch=520, loss=7.5918, this epoch 1.4979, total 769.9313
(T) | Epoch=521, loss=7.8254, this epoch 1.5207, total 771.4520
(T) | Epoch=522, loss=7.5873, this epoch 1.4523, total 772.9043
(T) | Epoch=523, loss=7.6075, this epoch 1.4626, total 774.3669
(T) | Epoch=524, loss=7.5892, this epoch 1.4708, total 775.8377
(T) | Epoch=525, loss=7.5842, this epoch 1.4610, total 777.2987
(T) | Epoch=526, loss=7.5914, this epoch 1.4947, total 778.7934
(T) | Epoch=527, loss=7.5829, this epoch 1.5088, total 780.3022
(T) | Epoch=528, loss=7.6304, this epoch 1.5508, total 781.8530
(T) | Epoch=529, loss=7.6188, this epoch 1.5268, total 783.3798
(T) | Epoch=530, loss=7.5749, this epoch 1.4916, total 784.8714
(T) | Epoch=531, loss=7.5748, this epoch 1.4720, total 786.3434
(T) | Epoch=532, loss=7.5735, this epoch 1.4418, total 787.7852
(T) | Epoch=533, loss=7.5773, this epoch 1.4472, total 789.2324
(T) | Epoch=534, loss=7.5610, this epoch 1.5129, total 790.7453
(T) | Epoch=535, loss=7.5635, this epoch 1.5442, total 792.2895
(T) | Epoch=536, loss=7.5694, this epoch 1.4870, total 793.7765
(T) | Epoch=537, loss=7.5600, this epoch 1.4754, total 795.2519
(T) | Epoch=538, loss=7.5680, this epoch 1.4666, total 796.7185
(T) | Epoch=539, loss=7.6645, this epoch 1.4698, total 798.1883
(T) | Epoch=540, loss=7.5580, this epoch 1.4721, total 799.6604
(T) | Epoch=541, loss=7.6246, this epoch 1.4478, total 801.1082
(T) | Epoch=542, loss=7.6086, this epoch 1.4310, total 802.5392
(T) | Epoch=543, loss=7.5702, this epoch 1.4417, total 803.9809
(T) | Epoch=544, loss=7.5940, this epoch 1.4876, total 805.4685
(T) | Epoch=545, loss=7.6040, this epoch 1.5650, total 807.0335
(T) | Epoch=546, loss=7.6244, this epoch 1.5470, total 808.5806
(T) | Epoch=547, loss=7.5849, this epoch 1.4451, total 810.0257
(T) | Epoch=548, loss=7.6130, this epoch 1.5141, total 811.5398
(T) | Epoch=549, loss=7.5738, this epoch 1.5353, total 813.0751
(T) | Epoch=550, loss=7.5826, this epoch 1.5060, total 814.5810
(T) | Epoch=551, loss=7.6111, this epoch 1.5174, total 816.0984
(T) | Epoch=552, loss=7.5868, this epoch 1.4606, total 817.5590
(T) | Epoch=553, loss=7.5866, this epoch 1.5222, total 819.0812
(T) | Epoch=554, loss=7.5646, this epoch 1.5122, total 820.5934
(T) | Epoch=555, loss=7.5692, this epoch 1.4439, total 822.0373
(T) | Epoch=556, loss=7.6203, this epoch 1.5016, total 823.5389
(T) | Epoch=557, loss=7.5584, this epoch 1.4793, total 825.0182
(T) | Epoch=558, loss=7.5464, this epoch 1.4592, total 826.4774
+++model saved ! 2016.pth
(T) | Epoch=559, loss=7.5851, this epoch 1.5193, total 827.9967
(T) | Epoch=560, loss=7.6410, this epoch 1.4399, total 829.4366
(T) | Epoch=561, loss=7.6574, this epoch 1.4569, total 830.8935
(T) | Epoch=562, loss=7.6625, this epoch 1.4486, total 832.3421
(T) | Epoch=563, loss=7.5632, this epoch 1.4606, total 833.8027
(T) | Epoch=564, loss=7.5562, this epoch 1.4565, total 835.2592
(T) | Epoch=565, loss=7.5854, this epoch 1.4822, total 836.7413
(T) | Epoch=566, loss=7.6233, this epoch 1.4923, total 838.2336
(T) | Epoch=567, loss=7.6616, this epoch 1.4552, total 839.6888
(T) | Epoch=568, loss=7.6135, this epoch 1.4585, total 841.1473
(T) | Epoch=569, loss=7.5859, this epoch 1.4234, total 842.5707
(T) | Epoch=570, loss=7.6316, this epoch 1.4257, total 843.9964
(T) | Epoch=571, loss=7.6261, this epoch 1.5305, total 845.5269
(T) | Epoch=572, loss=7.5836, this epoch 1.4181, total 846.9451
(T) | Epoch=573, loss=7.5971, this epoch 1.4259, total 848.3710
(T) | Epoch=574, loss=7.5732, this epoch 1.4564, total 849.8274
(T) | Epoch=575, loss=7.6232, this epoch 1.4285, total 851.2559
(T) | Epoch=576, loss=7.5735, this epoch 1.5336, total 852.7895
(T) | Epoch=577, loss=7.6003, this epoch 1.5070, total 854.2965
(T) | Epoch=578, loss=7.5478, this epoch 1.5518, total 855.8484
(T) | Epoch=579, loss=7.5747, this epoch 1.4791, total 857.3275
(T) | Epoch=580, loss=7.5865, this epoch 1.4447, total 858.7722
(T) | Epoch=581, loss=7.5419, this epoch 1.4153, total 860.1875
+++model saved ! 2016.pth
(T) | Epoch=582, loss=7.5377, this epoch 1.4608, total 861.6483
+++model saved ! 2016.pth
(T) | Epoch=583, loss=7.5394, this epoch 1.4685, total 863.1169
(T) | Epoch=584, loss=7.5436, this epoch 1.5374, total 864.6543
(T) | Epoch=585, loss=7.6660, this epoch 1.4823, total 866.1365
(T) | Epoch=586, loss=7.6943, this epoch 1.4941, total 867.6307
(T) | Epoch=587, loss=7.5414, this epoch 1.4624, total 869.0930
(T) | Epoch=588, loss=7.5496, this epoch 1.4567, total 870.5497
(T) | Epoch=589, loss=7.6015, this epoch 1.4495, total 871.9992
(T) | Epoch=590, loss=7.5909, this epoch 1.4783, total 873.4775
(T) | Epoch=591, loss=7.5737, this epoch 1.4789, total 874.9564
(T) | Epoch=592, loss=7.5666, this epoch 1.4737, total 876.4301
(T) | Epoch=593, loss=7.6187, this epoch 1.4735, total 877.9036
(T) | Epoch=594, loss=7.5772, this epoch 1.4257, total 879.3293
(T) | Epoch=595, loss=7.5701, this epoch 1.4914, total 880.8207
(T) | Epoch=596, loss=7.5685, this epoch 1.5270, total 882.3477
(T) | Epoch=597, loss=7.7428, this epoch 1.4809, total 883.8286
(T) | Epoch=598, loss=7.6816, this epoch 1.4514, total 885.2801
(T) | Epoch=599, loss=7.5591, this epoch 1.4488, total 886.7289
(T) | Epoch=600, loss=7.5543, this epoch 1.5335, total 888.2623
(T) | Epoch=601, loss=7.5813, this epoch 1.4936, total 889.7559
(T) | Epoch=602, loss=7.5786, this epoch 1.4892, total 891.2451
(T) | Epoch=603, loss=7.5394, this epoch 1.4787, total 892.7238
(T) | Epoch=604, loss=7.5837, this epoch 1.4826, total 894.2064
(T) | Epoch=605, loss=7.6497, this epoch 1.4875, total 895.6939
(T) | Epoch=606, loss=7.6468, this epoch 1.4356, total 897.1295
(T) | Epoch=607, loss=7.5772, this epoch 1.4789, total 898.6084
(T) | Epoch=608, loss=7.5956, this epoch 1.4724, total 900.0808
(T) | Epoch=609, loss=7.5851, this epoch 1.4467, total 901.5274
(T) | Epoch=610, loss=7.5447, this epoch 1.4728, total 903.0002
(T) | Epoch=611, loss=7.5901, this epoch 1.4590, total 904.4593
(T) | Epoch=612, loss=7.5597, this epoch 1.4235, total 905.8827
(T) | Epoch=613, loss=7.5652, this epoch 1.4077, total 907.2905
(T) | Epoch=614, loss=7.6075, this epoch 1.3986, total 908.6890
(T) | Epoch=615, loss=7.5424, this epoch 1.4595, total 910.1485
(T) | Epoch=616, loss=7.5880, this epoch 1.4786, total 911.6271
(T) | Epoch=617, loss=7.5568, this epoch 1.4896, total 913.1167
(T) | Epoch=618, loss=7.9246, this epoch 1.4883, total 914.6050
(T) | Epoch=619, loss=7.5886, this epoch 1.4592, total 916.0642
(T) | Epoch=620, loss=7.5779, this epoch 1.4973, total 917.5615
(T) | Epoch=621, loss=7.6063, this epoch 1.4698, total 919.0313
(T) | Epoch=622, loss=7.7040, this epoch 1.4552, total 920.4865
(T) | Epoch=623, loss=7.6573, this epoch 1.4800, total 921.9666
(T) | Epoch=624, loss=7.6209, this epoch 1.5522, total 923.5187
(T) | Epoch=625, loss=7.7462, this epoch 1.5235, total 925.0422
(T) | Epoch=626, loss=7.7053, this epoch 1.4295, total 926.4718
(T) | Epoch=627, loss=7.7241, this epoch 1.5391, total 928.0109
(T) | Epoch=628, loss=7.6715, this epoch 1.5552, total 929.5661
(T) | Epoch=629, loss=7.7613, this epoch 1.4476, total 931.0137
(T) | Epoch=630, loss=7.6643, this epoch 1.4628, total 932.4765
(T) | Epoch=631, loss=7.6158, this epoch 1.4400, total 933.9165
(T) | Epoch=632, loss=7.6885, this epoch 1.4548, total 935.3713
(T) | Epoch=633, loss=7.7158, this epoch 1.4594, total 936.8306
(T) | Epoch=634, loss=7.6617, this epoch 1.4248, total 938.2554
(T) | Epoch=635, loss=7.6372, this epoch 1.4685, total 939.7239
(T) | Epoch=636, loss=7.5970, this epoch 1.4319, total 941.1559
(T) | Epoch=637, loss=7.5732, this epoch 1.4549, total 942.6108
(T) | Epoch=638, loss=7.5922, this epoch 1.4490, total 944.0598
(T) | Epoch=639, loss=7.5744, this epoch 1.4587, total 945.5185
(T) | Epoch=640, loss=7.5679, this epoch 1.4606, total 946.9790
(T) | Epoch=641, loss=7.5585, this epoch 1.4946, total 948.4736
(T) | Epoch=642, loss=7.5652, this epoch 1.5464, total 950.0200
(T) | Epoch=643, loss=7.5579, this epoch 1.5180, total 951.5380
(T) | Epoch=644, loss=7.5418, this epoch 1.6232, total 953.1612
(T) | Epoch=645, loss=7.5535, this epoch 1.6175, total 954.7788
(T) | Epoch=646, loss=7.6644, this epoch 1.6175, total 956.3963
(T) | Epoch=647, loss=7.5938, this epoch 1.5795, total 957.9758
(T) | Epoch=648, loss=7.5303, this epoch 1.5960, total 959.5718
+++model saved ! 2016.pth
(T) | Epoch=649, loss=7.5369, this epoch 1.6138, total 961.1856
(T) | Epoch=650, loss=7.5302, this epoch 1.4999, total 962.6855
+++model saved ! 2016.pth
(T) | Epoch=651, loss=7.7184, this epoch 1.5163, total 964.2018
(T) | Epoch=652, loss=7.5743, this epoch 1.5030, total 965.7048
(T) | Epoch=653, loss=7.5901, this epoch 1.5554, total 967.2602
(T) | Epoch=654, loss=7.5366, this epoch 1.4974, total 968.7575
(T) | Epoch=655, loss=7.5400, this epoch 1.5040, total 970.2615
(T) | Epoch=656, loss=7.5406, this epoch 1.5201, total 971.7816
(T) | Epoch=657, loss=7.5509, this epoch 1.5037, total 973.2853
(T) | Epoch=658, loss=7.5420, this epoch 1.4760, total 974.7613
(T) | Epoch=659, loss=7.5411, this epoch 1.5131, total 976.2744
(T) | Epoch=660, loss=7.5595, this epoch 1.4731, total 977.7475
(T) | Epoch=661, loss=7.5507, this epoch 1.4646, total 979.2121
(T) | Epoch=662, loss=7.5366, this epoch 1.5476, total 980.7597
(T) | Epoch=663, loss=7.5315, this epoch 1.4674, total 982.2272
(T) | Epoch=664, loss=7.5941, this epoch 1.4776, total 983.7047
(T) | Epoch=665, loss=7.5324, this epoch 1.4989, total 985.2036
(T) | Epoch=666, loss=7.5218, this epoch 1.4711, total 986.6748
+++model saved ! 2016.pth
(T) | Epoch=667, loss=7.5428, this epoch 1.5356, total 988.2103
(T) | Epoch=668, loss=7.5979, this epoch 1.4456, total 989.6559
(T) | Epoch=669, loss=7.5203, this epoch 1.5273, total 991.1832
+++model saved ! 2016.pth
(T) | Epoch=670, loss=7.5354, this epoch 1.4366, total 992.6198
(T) | Epoch=671, loss=7.5235, this epoch 1.4267, total 994.0465
(T) | Epoch=672, loss=7.5090, this epoch 1.4153, total 995.4619
+++model saved ! 2016.pth
(T) | Epoch=673, loss=7.6084, this epoch 1.4615, total 996.9234
(T) | Epoch=674, loss=7.6919, this epoch 1.4805, total 998.4039
(T) | Epoch=675, loss=7.6284, this epoch 1.5221, total 999.9260
(T) | Epoch=676, loss=7.5683, this epoch 1.4737, total 1001.3997
(T) | Epoch=677, loss=7.5386, this epoch 1.4712, total 1002.8709
(T) | Epoch=678, loss=7.5375, this epoch 1.4828, total 1004.3537
(T) | Epoch=679, loss=7.6165, this epoch 1.4371, total 1005.7908
(T) | Epoch=680, loss=7.5592, this epoch 1.4706, total 1007.2614
(T) | Epoch=681, loss=7.5891, this epoch 1.5413, total 1008.8027
(T) | Epoch=682, loss=7.6065, this epoch 1.4881, total 1010.2908
(T) | Epoch=683, loss=7.5375, this epoch 1.4443, total 1011.7351
(T) | Epoch=684, loss=7.5520, this epoch 1.4477, total 1013.1828
(T) | Epoch=685, loss=7.5973, this epoch 1.4842, total 1014.6670
(T) | Epoch=686, loss=7.5370, this epoch 1.4940, total 1016.1610
(T) | Epoch=687, loss=7.5928, this epoch 1.4583, total 1017.6193
(T) | Epoch=688, loss=7.5405, this epoch 1.4416, total 1019.0609
(T) | Epoch=689, loss=7.5507, this epoch 1.4426, total 1020.5035
(T) | Epoch=690, loss=7.5025, this epoch 1.5636, total 1022.0671
+++model saved ! 2016.pth
(T) | Epoch=691, loss=7.5100, this epoch 1.4486, total 1023.5157
(T) | Epoch=692, loss=7.5161, this epoch 1.4350, total 1024.9507
(T) | Epoch=693, loss=7.5075, this epoch 1.4119, total 1026.3625
(T) | Epoch=694, loss=7.5037, this epoch 1.4355, total 1027.7980
(T) | Epoch=695, loss=7.6855, this epoch 1.4822, total 1029.2803
(T) | Epoch=696, loss=7.6191, this epoch 1.4865, total 1030.7667
(T) | Epoch=697, loss=7.4859, this epoch 1.4651, total 1032.2318
+++model saved ! 2016.pth
(T) | Epoch=698, loss=7.4998, this epoch 1.5100, total 1033.7418
(T) | Epoch=699, loss=7.6113, this epoch 1.4619, total 1035.2038
(T) | Epoch=700, loss=7.5673, this epoch 1.4869, total 1036.6906
(T) | Epoch=701, loss=7.6014, this epoch 1.4893, total 1038.1800
(T) | Epoch=702, loss=7.5841, this epoch 1.5016, total 1039.6816
(T) | Epoch=703, loss=7.5623, this epoch 1.5115, total 1041.1931
(T) | Epoch=704, loss=7.5112, this epoch 1.4771, total 1042.6701
(T) | Epoch=705, loss=7.5196, this epoch 1.5115, total 1044.1816
(T) | Epoch=706, loss=7.5050, this epoch 1.5532, total 1045.7349
(T) | Epoch=707, loss=7.5091, this epoch 1.5060, total 1047.2409
(T) | Epoch=708, loss=7.5170, this epoch 1.5068, total 1048.7477
(T) | Epoch=709, loss=7.4916, this epoch 1.4791, total 1050.2268
(T) | Epoch=710, loss=7.4894, this epoch 1.4910, total 1051.7178
(T) | Epoch=711, loss=7.4918, this epoch 1.5074, total 1053.2251
(T) | Epoch=712, loss=7.4801, this epoch 1.5217, total 1054.7469
+++model saved ! 2016.pth
(T) | Epoch=713, loss=7.4883, this epoch 1.4588, total 1056.2056
(T) | Epoch=714, loss=7.5451, this epoch 1.4757, total 1057.6813
(T) | Epoch=715, loss=7.5167, this epoch 1.4603, total 1059.1415
(T) | Epoch=716, loss=7.7756, this epoch 1.4193, total 1060.5608
(T) | Epoch=717, loss=7.5639, this epoch 1.4964, total 1062.0572
(T) | Epoch=718, loss=7.5330, this epoch 1.4773, total 1063.5345
(T) | Epoch=719, loss=7.4973, this epoch 1.5346, total 1065.0691
(T) | Epoch=720, loss=7.5384, this epoch 1.5435, total 1066.6127
(T) | Epoch=721, loss=7.5110, this epoch 1.5459, total 1068.1586
(T) | Epoch=722, loss=7.5924, this epoch 1.5394, total 1069.6980
(T) | Epoch=723, loss=7.6101, this epoch 1.5164, total 1071.2144
(T) | Epoch=724, loss=7.6637, this epoch 1.5156, total 1072.7300
(T) | Epoch=725, loss=7.5691, this epoch 1.4889, total 1074.2189
(T) | Epoch=726, loss=7.5562, this epoch 1.5156, total 1075.7346
(T) | Epoch=727, loss=7.5286, this epoch 1.5286, total 1077.2632
(T) | Epoch=728, loss=7.6278, this epoch 1.4864, total 1078.7496
(T) | Epoch=729, loss=7.4906, this epoch 1.4836, total 1080.2332
(T) | Epoch=730, loss=7.5124, this epoch 1.4890, total 1081.7222
(T) | Epoch=731, loss=7.5713, this epoch 1.4604, total 1083.1826
(T) | Epoch=732, loss=7.5772, this epoch 1.5024, total 1084.6849
(T) | Epoch=733, loss=7.4962, this epoch 1.4696, total 1086.1545
(T) | Epoch=734, loss=7.6442, this epoch 1.4892, total 1087.6438
(T) | Epoch=735, loss=7.5288, this epoch 1.4757, total 1089.1194
(T) | Epoch=736, loss=7.6173, this epoch 1.5289, total 1090.6483
(T) | Epoch=737, loss=7.4920, this epoch 1.4694, total 1092.1178
(T) | Epoch=738, loss=7.4910, this epoch 1.4522, total 1093.5699
(T) | Epoch=739, loss=7.4957, this epoch 1.4714, total 1095.0413
(T) | Epoch=740, loss=7.5523, this epoch 1.4717, total 1096.5129
(T) | Epoch=741, loss=7.4833, this epoch 1.4393, total 1097.9522
(T) | Epoch=742, loss=7.4919, this epoch 1.4830, total 1099.4352
(T) | Epoch=743, loss=7.5959, this epoch 1.5009, total 1100.9361
(T) | Epoch=744, loss=7.5345, this epoch 1.4859, total 1102.4220
(T) | Epoch=745, loss=7.5529, this epoch 1.5012, total 1103.9232
(T) | Epoch=746, loss=7.6533, this epoch 1.4803, total 1105.4035
(T) | Epoch=747, loss=7.5002, this epoch 1.4920, total 1106.8955
(T) | Epoch=748, loss=7.6444, this epoch 1.4854, total 1108.3809
(T) | Epoch=749, loss=7.5042, this epoch 1.4658, total 1109.8467
(T) | Epoch=750, loss=7.4901, this epoch 1.4846, total 1111.3313
(T) | Epoch=751, loss=7.4888, this epoch 1.4804, total 1112.8117
(T) | Epoch=752, loss=7.4750, this epoch 1.4596, total 1114.2712
+++model saved ! 2016.pth
(T) | Epoch=753, loss=7.5470, this epoch 1.4818, total 1115.7531
(T) | Epoch=754, loss=7.5540, this epoch 1.4483, total 1117.2014
(T) | Epoch=755, loss=7.4729, this epoch 1.4462, total 1118.6476
+++model saved ! 2016.pth
(T) | Epoch=756, loss=7.5052, this epoch 1.4919, total 1120.1395
(T) | Epoch=757, loss=7.4819, this epoch 1.5133, total 1121.6528
(T) | Epoch=758, loss=7.6917, this epoch 1.5090, total 1123.1618
(T) | Epoch=759, loss=7.5317, this epoch 1.4930, total 1124.6547
(T) | Epoch=760, loss=7.4822, this epoch 1.4351, total 1126.0899
(T) | Epoch=761, loss=7.4894, this epoch 1.4852, total 1127.5750
(T) | Epoch=762, loss=7.5406, this epoch 1.5216, total 1129.0966
(T) | Epoch=763, loss=7.5323, this epoch 1.4812, total 1130.5778
(T) | Epoch=764, loss=7.6193, this epoch 1.4230, total 1132.0008
(T) | Epoch=765, loss=7.5374, this epoch 1.4912, total 1133.4920
(T) | Epoch=766, loss=7.5751, this epoch 1.5619, total 1135.0539
(T) | Epoch=767, loss=7.5834, this epoch 1.5516, total 1136.6055
(T) | Epoch=768, loss=7.7935, this epoch 1.5192, total 1138.1247
(T) | Epoch=769, loss=7.5117, this epoch 1.4828, total 1139.6075
(T) | Epoch=770, loss=7.5153, this epoch 1.4590, total 1141.0665
(T) | Epoch=771, loss=7.5309, this epoch 1.4972, total 1142.5637
(T) | Epoch=772, loss=7.6173, this epoch 1.4732, total 1144.0369
(T) | Epoch=773, loss=7.5414, this epoch 1.4928, total 1145.5297
(T) | Epoch=774, loss=7.5881, this epoch 1.5094, total 1147.0391
(T) | Epoch=775, loss=7.4857, this epoch 1.5085, total 1148.5476
(T) | Epoch=776, loss=7.4799, this epoch 1.4792, total 1150.0268
(T) | Epoch=777, loss=7.5660, this epoch 1.5314, total 1151.5582
(T) | Epoch=778, loss=7.4705, this epoch 1.5402, total 1153.0984
+++model saved ! 2016.pth
(T) | Epoch=779, loss=7.5744, this epoch 1.5134, total 1154.6118
(T) | Epoch=780, loss=7.4698, this epoch 1.5081, total 1156.1199
+++model saved ! 2016.pth
(T) | Epoch=781, loss=7.5405, this epoch 1.5012, total 1157.6211
(T) | Epoch=782, loss=7.4797, this epoch 1.5036, total 1159.1247
(T) | Epoch=783, loss=7.6766, this epoch 1.4987, total 1160.6235
(T) | Epoch=784, loss=7.4664, this epoch 1.4797, total 1162.1032
+++model saved ! 2016.pth
(T) | Epoch=785, loss=7.5509, this epoch 1.5002, total 1163.6034
(T) | Epoch=786, loss=7.4781, this epoch 1.4831, total 1165.0865
(T) | Epoch=787, loss=7.4693, this epoch 1.4496, total 1166.5360
(T) | Epoch=788, loss=7.4842, this epoch 1.4585, total 1167.9946
(T) | Epoch=789, loss=7.5137, this epoch 1.4782, total 1169.4727
(T) | Epoch=790, loss=7.5145, this epoch 1.4826, total 1170.9553
(T) | Epoch=791, loss=7.6304, this epoch 1.5309, total 1172.4862
(T) | Epoch=792, loss=7.5807, this epoch 1.5032, total 1173.9894
(T) | Epoch=793, loss=7.5676, this epoch 1.4868, total 1175.4762
(T) | Epoch=794, loss=7.5561, this epoch 1.4988, total 1176.9749
(T) | Epoch=795, loss=7.4869, this epoch 1.4916, total 1178.4666
(T) | Epoch=796, loss=7.4931, this epoch 1.5544, total 1180.0210
(T) | Epoch=797, loss=7.5260, this epoch 1.5606, total 1181.5817
(T) | Epoch=798, loss=7.4712, this epoch 1.5674, total 1183.1491
(T) | Epoch=799, loss=7.5479, this epoch 1.5586, total 1184.7077
(T) | Epoch=800, loss=7.4716, this epoch 1.4966, total 1186.2042
(T) | Epoch=801, loss=7.5732, this epoch 1.4642, total 1187.6685
(T) | Epoch=802, loss=7.4689, this epoch 1.4549, total 1189.1234
(T) | Epoch=803, loss=7.4664, this epoch 1.4855, total 1190.6089
+++model saved ! 2016.pth
(T) | Epoch=804, loss=7.4937, this epoch 1.4915, total 1192.1003
(T) | Epoch=805, loss=7.4592, this epoch 1.4567, total 1193.5570
+++model saved ! 2016.pth
(T) | Epoch=806, loss=7.4727, this epoch 1.4444, total 1195.0014
(T) | Epoch=807, loss=7.4544, this epoch 1.4629, total 1196.4643
+++model saved ! 2016.pth
(T) | Epoch=808, loss=7.4552, this epoch 1.4962, total 1197.9605
(T) | Epoch=809, loss=7.4606, this epoch 1.4522, total 1199.4127
(T) | Epoch=810, loss=7.5576, this epoch 1.4720, total 1200.8847
(T) | Epoch=811, loss=7.4504, this epoch 1.5234, total 1202.4081
+++model saved ! 2016.pth
(T) | Epoch=812, loss=7.4696, this epoch 1.4971, total 1203.9052
(T) | Epoch=813, loss=7.4539, this epoch 1.4576, total 1205.3627
(T) | Epoch=814, loss=7.5601, this epoch 1.4414, total 1206.8042
(T) | Epoch=815, loss=7.4713, this epoch 1.4520, total 1208.2562
(T) | Epoch=816, loss=7.4558, this epoch 1.4360, total 1209.6922
(T) | Epoch=817, loss=7.4746, this epoch 1.4613, total 1211.1535
(T) | Epoch=818, loss=7.4876, this epoch 1.4512, total 1212.6047
(T) | Epoch=819, loss=7.4837, this epoch 1.4292, total 1214.0339
(T) | Epoch=820, loss=7.4662, this epoch 1.4598, total 1215.4938
(T) | Epoch=821, loss=7.4507, this epoch 1.5602, total 1217.0540
(T) | Epoch=822, loss=7.5482, this epoch 1.5486, total 1218.6026
(T) | Epoch=823, loss=8.8821, this epoch 1.4812, total 1220.0838
(T) | Epoch=824, loss=7.4677, this epoch 1.5355, total 1221.6193
(T) | Epoch=825, loss=7.5743, this epoch 1.5247, total 1223.1440
(T) | Epoch=826, loss=7.6496, this epoch 1.5355, total 1224.6795
(T) | Epoch=827, loss=7.6390, this epoch 1.5369, total 1226.2163
(T) | Epoch=828, loss=7.5075, this epoch 1.5312, total 1227.7475
(T) | Epoch=829, loss=7.8442, this epoch 1.5261, total 1229.2736
(T) | Epoch=830, loss=7.8455, this epoch 1.5235, total 1230.7971
(T) | Epoch=831, loss=7.6267, this epoch 1.5404, total 1232.3375
(T) | Epoch=832, loss=7.8708, this epoch 1.4698, total 1233.8072
(T) | Epoch=833, loss=7.6390, this epoch 1.4768, total 1235.2841
(T) | Epoch=834, loss=7.6361, this epoch 1.4617, total 1236.7458
(T) | Epoch=835, loss=7.6419, this epoch 1.4532, total 1238.1990
(T) | Epoch=836, loss=7.6333, this epoch 1.4790, total 1239.6781
(T) | Epoch=837, loss=7.6403, this epoch 1.4414, total 1241.1195
(T) | Epoch=838, loss=7.6808, this epoch 1.4369, total 1242.5564
(T) | Epoch=839, loss=7.6436, this epoch 1.4539, total 1244.0103
(T) | Epoch=840, loss=7.8465, this epoch 1.5266, total 1245.5369
(T) | Epoch=841, loss=7.6376, this epoch 1.5689, total 1247.1058
(T) | Epoch=842, loss=7.9338, this epoch 1.4622, total 1248.5681
(T) | Epoch=843, loss=7.6396, this epoch 1.5460, total 1250.1140
(T) | Epoch=844, loss=7.8279, this epoch 1.4792, total 1251.5932
(T) | Epoch=845, loss=7.6721, this epoch 1.4736, total 1253.0668
(T) | Epoch=846, loss=7.6277, this epoch 1.4631, total 1254.5299
(T) | Epoch=847, loss=7.6734, this epoch 1.4612, total 1255.9911
(T) | Epoch=848, loss=7.6208, this epoch 1.5183, total 1257.5094
(T) | Epoch=849, loss=7.6283, this epoch 1.5398, total 1259.0492
(T) | Epoch=850, loss=7.6647, this epoch 1.4520, total 1260.5012
(T) | Epoch=851, loss=7.7263, this epoch 1.4376, total 1261.9388
(T) | Epoch=852, loss=7.6009, this epoch 1.4648, total 1263.4036
(T) | Epoch=853, loss=7.6208, this epoch 1.4336, total 1264.8372
(T) | Epoch=854, loss=7.6169, this epoch 1.5240, total 1266.3612
(T) | Epoch=855, loss=7.5946, this epoch 1.5633, total 1267.9246
(T) | Epoch=856, loss=7.7374, this epoch 1.5891, total 1269.5137
(T) | Epoch=857, loss=7.6576, this epoch 1.5225, total 1271.0362
(T) | Epoch=858, loss=7.5710, this epoch 1.4794, total 1272.5156
(T) | Epoch=859, loss=7.5684, this epoch 1.4615, total 1273.9770
(T) | Epoch=860, loss=7.5653, this epoch 1.4566, total 1275.4336
(T) | Epoch=861, loss=7.6460, this epoch 1.4628, total 1276.8964
(T) | Epoch=862, loss=7.6017, this epoch 1.4927, total 1278.3891
(T) | Epoch=863, loss=7.5994, this epoch 1.4620, total 1279.8511
(T) | Epoch=864, loss=7.5516, this epoch 1.4430, total 1281.2941
(T) | Epoch=865, loss=7.6192, this epoch 1.4420, total 1282.7360
(T) | Epoch=866, loss=7.5535, this epoch 1.4558, total 1284.1919
(T) | Epoch=867, loss=7.6017, this epoch 1.4902, total 1285.6821
(T) | Epoch=868, loss=7.5214, this epoch 1.5305, total 1287.2126
(T) | Epoch=869, loss=7.6487, this epoch 1.4668, total 1288.6794
(T) | Epoch=870, loss=7.5530, this epoch 1.5069, total 1290.1863
(T) | Epoch=871, loss=7.5214, this epoch 1.5151, total 1291.7014
(T) | Epoch=872, loss=7.5198, this epoch 1.5058, total 1293.2071
(T) | Epoch=873, loss=7.5563, this epoch 1.4616, total 1294.6688
(T) | Epoch=874, loss=7.5525, this epoch 1.4231, total 1296.0918
(T) | Epoch=875, loss=7.5070, this epoch 1.4793, total 1297.5711
(T) | Epoch=876, loss=7.5476, this epoch 1.4711, total 1299.0422
(T) | Epoch=877, loss=7.5215, this epoch 1.5125, total 1300.5547
(T) | Epoch=878, loss=7.5331, this epoch 1.4592, total 1302.0139
(T) | Epoch=879, loss=7.5423, this epoch 1.4522, total 1303.4661
(T) | Epoch=880, loss=7.5014, this epoch 1.4386, total 1304.9047
(T) | Epoch=881, loss=7.4963, this epoch 1.4592, total 1306.3639
(T) | Epoch=882, loss=7.4871, this epoch 1.5210, total 1307.8849
(T) | Epoch=883, loss=7.5394, this epoch 1.5065, total 1309.3914
(T) | Epoch=884, loss=7.5239, this epoch 1.5071, total 1310.8985
(T) | Epoch=885, loss=7.6077, this epoch 1.4458, total 1312.3442
(T) | Epoch=886, loss=7.4911, this epoch 1.5151, total 1313.8593
(T) | Epoch=887, loss=7.4757, this epoch 1.4711, total 1315.3304
(T) | Epoch=888, loss=7.5696, this epoch 1.4484, total 1316.7788
(T) | Epoch=889, loss=7.4795, this epoch 1.4227, total 1318.2015
(T) | Epoch=890, loss=7.5284, this epoch 1.4919, total 1319.6934
(T) | Epoch=891, loss=7.4869, this epoch 1.4783, total 1321.1718
(T) | Epoch=892, loss=7.4715, this epoch 1.4647, total 1322.6364
(T) | Epoch=893, loss=7.5612, this epoch 1.4634, total 1324.0999
(T) | Epoch=894, loss=7.5040, this epoch 1.4509, total 1325.5507
(T) | Epoch=895, loss=7.4730, this epoch 1.4306, total 1326.9814
(T) | Epoch=896, loss=7.5167, this epoch 1.5528, total 1328.5342
(T) | Epoch=897, loss=7.4823, this epoch 1.5146, total 1330.0488
(T) | Epoch=898, loss=7.5450, this epoch 1.4969, total 1331.5457
(T) | Epoch=899, loss=7.5723, this epoch 1.4669, total 1333.0126
(T) | Epoch=900, loss=7.4532, this epoch 1.5109, total 1334.5235
(T) | Epoch=901, loss=7.5013, this epoch 1.5152, total 1336.0387
(T) | Epoch=902, loss=7.4777, this epoch 1.5066, total 1337.5453
(T) | Epoch=903, loss=8.0184, this epoch 1.4623, total 1339.0076
(T) | Epoch=904, loss=7.5532, this epoch 1.4719, total 1340.4795
(T) | Epoch=905, loss=7.4580, this epoch 1.4830, total 1341.9625
(T) | Epoch=906, loss=7.4644, this epoch 1.5120, total 1343.4745
(T) | Epoch=907, loss=7.4955, this epoch 1.4937, total 1344.9681
(T) | Epoch=908, loss=7.4946, this epoch 1.4652, total 1346.4333
(T) | Epoch=909, loss=7.4934, this epoch 1.4449, total 1347.8782
(T) | Epoch=910, loss=7.5578, this epoch 1.4366, total 1349.3148
(T) | Epoch=911, loss=7.5203, this epoch 1.4316, total 1350.7464
(T) | Epoch=912, loss=7.4988, this epoch 1.4754, total 1352.2218
(T) | Epoch=913, loss=7.4927, this epoch 1.4623, total 1353.6841
(T) | Epoch=914, loss=7.5270, this epoch 1.4505, total 1355.1346
(T) | Epoch=915, loss=7.4781, this epoch 1.4581, total 1356.5928
(T) | Epoch=916, loss=7.5513, this epoch 1.4524, total 1358.0451
(T) | Epoch=917, loss=7.4864, this epoch 1.5334, total 1359.5785
(T) | Epoch=918, loss=7.5257, this epoch 1.5315, total 1361.1100
(T) | Epoch=919, loss=7.4888, this epoch 1.4938, total 1362.6038
(T) | Epoch=920, loss=7.4680, this epoch 1.4711, total 1364.0749
(T) | Epoch=921, loss=7.5015, this epoch 1.5147, total 1365.5896
(T) | Epoch=922, loss=7.5331, this epoch 1.5428, total 1367.1324
(T) | Epoch=923, loss=7.6029, this epoch 1.5444, total 1368.6768
(T) | Epoch=924, loss=7.4840, this epoch 1.5409, total 1370.2177
(T) | Epoch=925, loss=7.4960, this epoch 1.5516, total 1371.7693
(T) | Epoch=926, loss=7.4580, this epoch 1.4878, total 1373.2571
(T) | Epoch=927, loss=7.4708, this epoch 1.5207, total 1374.7778
(T) | Epoch=928, loss=7.4672, this epoch 1.4982, total 1376.2760
(T) | Epoch=929, loss=7.4598, this epoch 1.5239, total 1377.7999
(T) | Epoch=930, loss=7.5423, this epoch 1.5588, total 1379.3587
(T) | Epoch=931, loss=7.4580, this epoch 1.4556, total 1380.8143
(T) | Epoch=932, loss=7.5290, this epoch 1.4817, total 1382.2959
(T) | Epoch=933, loss=7.5099, this epoch 1.4602, total 1383.7562
(T) | Epoch=934, loss=7.4667, this epoch 1.4776, total 1385.2338
(T) | Epoch=935, loss=7.4673, this epoch 1.4882, total 1386.7220
(T) | Epoch=936, loss=7.4829, this epoch 1.5330, total 1388.2550
(T) | Epoch=937, loss=7.5459, this epoch 1.4944, total 1389.7494
(T) | Epoch=938, loss=7.4652, this epoch 1.4613, total 1391.2106
(T) | Epoch=939, loss=7.4647, this epoch 1.4714, total 1392.6821
(T) | Epoch=940, loss=7.5075, this epoch 1.4670, total 1394.1491
(T) | Epoch=941, loss=7.4650, this epoch 1.4758, total 1395.6249
(T) | Epoch=942, loss=7.4524, this epoch 1.4941, total 1397.1190
(T) | Epoch=943, loss=7.5999, this epoch 1.4606, total 1398.5796
(T) | Epoch=944, loss=7.4609, this epoch 1.4487, total 1400.0283
(T) | Epoch=945, loss=7.4690, this epoch 1.4869, total 1401.5152
(T) | Epoch=946, loss=7.4562, this epoch 1.4752, total 1402.9903
(T) | Epoch=947, loss=7.5178, this epoch 1.4895, total 1404.4798
(T) | Epoch=948, loss=7.4710, this epoch 1.5179, total 1405.9977
(T) | Epoch=949, loss=7.4487, this epoch 1.5277, total 1407.5255
+++model saved ! 2016.pth
(T) | Epoch=950, loss=7.4660, this epoch 1.4620, total 1408.9875
(T) | Epoch=951, loss=7.4575, this epoch 1.4386, total 1410.4260
(T) | Epoch=952, loss=7.4778, this epoch 1.4697, total 1411.8957
(T) | Epoch=953, loss=7.4722, this epoch 1.4786, total 1413.3743
(T) | Epoch=954, loss=7.4648, this epoch 1.5250, total 1414.8993
(T) | Epoch=955, loss=7.4652, this epoch 1.4498, total 1416.3491
(T) | Epoch=956, loss=7.4660, this epoch 1.5051, total 1417.8543
(T) | Epoch=957, loss=7.4707, this epoch 1.4819, total 1419.3361
(T) | Epoch=958, loss=7.4581, this epoch 1.4576, total 1420.7937
(T) | Epoch=959, loss=7.4598, this epoch 1.4990, total 1422.2927
(T) | Epoch=960, loss=7.5022, this epoch 1.4950, total 1423.7877
(T) | Epoch=961, loss=7.4473, this epoch 1.5155, total 1425.3033
+++model saved ! 2016.pth
(T) | Epoch=962, loss=7.4643, this epoch 1.5513, total 1426.8546
(T) | Epoch=963, loss=7.4682, this epoch 1.5205, total 1428.3751
(T) | Epoch=964, loss=7.4684, this epoch 1.5319, total 1429.9070
(T) | Epoch=965, loss=7.5104, this epoch 1.5164, total 1431.4233
(T) | Epoch=966, loss=7.4625, this epoch 1.5043, total 1432.9277
(T) | Epoch=967, loss=7.4656, this epoch 1.4326, total 1434.3602
(T) | Epoch=968, loss=7.5318, this epoch 1.4384, total 1435.7986
(T) | Epoch=969, loss=7.4919, this epoch 1.4489, total 1437.2475
(T) | Epoch=970, loss=7.4457, this epoch 1.4309, total 1438.6784
+++model saved ! 2016.pth
(T) | Epoch=971, loss=7.4625, this epoch 1.5030, total 1440.1814
(T) | Epoch=972, loss=7.5440, this epoch 1.4918, total 1441.6731
(T) | Epoch=973, loss=8.0167, this epoch 1.5002, total 1443.1733
(T) | Epoch=974, loss=7.4560, this epoch 1.4974, total 1444.6707
(T) | Epoch=975, loss=7.4629, this epoch 1.5072, total 1446.1779
(T) | Epoch=976, loss=7.5489, this epoch 1.4754, total 1447.6533
(T) | Epoch=977, loss=7.4848, this epoch 1.4943, total 1449.1476
(T) | Epoch=978, loss=7.5312, this epoch 1.5349, total 1450.6825
(T) | Epoch=979, loss=7.6225, this epoch 1.5196, total 1452.2021
(T) | Epoch=980, loss=7.5117, this epoch 1.4641, total 1453.6662
(T) | Epoch=981, loss=7.6630, this epoch 1.4878, total 1455.1540
(T) | Epoch=982, loss=7.5768, this epoch 1.4727, total 1456.6267
(T) | Epoch=983, loss=7.5710, this epoch 1.4973, total 1458.1240
(T) | Epoch=984, loss=7.5170, this epoch 1.5052, total 1459.6292
(T) | Epoch=985, loss=7.5734, this epoch 1.5044, total 1461.1336
(T) | Epoch=986, loss=7.5337, this epoch 1.5123, total 1462.6459
(T) | Epoch=987, loss=7.5398, this epoch 1.4593, total 1464.1052
(T) | Epoch=988, loss=7.5815, this epoch 1.5130, total 1465.6182
(T) | Epoch=989, loss=7.5682, this epoch 1.5370, total 1467.1552
(T) | Epoch=990, loss=7.5196, this epoch 1.4846, total 1468.6398
(T) | Epoch=991, loss=7.4958, this epoch 1.4655, total 1470.1054
(T) | Epoch=992, loss=7.7848, this epoch 1.5236, total 1471.6289
(T) | Epoch=993, loss=7.4975, this epoch 1.4589, total 1473.0879
(T) | Epoch=994, loss=7.5405, this epoch 1.4739, total 1474.5618
(T) | Epoch=995, loss=7.5651, this epoch 1.4939, total 1476.0557
(T) | Epoch=996, loss=7.5036, this epoch 1.4718, total 1477.5275
(T) | Epoch=997, loss=7.4839, this epoch 1.4780, total 1479.0056
(T) | Epoch=998, loss=7.4669, this epoch 1.4306, total 1480.4362
(T) | Epoch=999, loss=7.5263, this epoch 1.4551, total 1481.8913
(T) | Epoch=1000, loss=7.4754, this epoch 1.5032, total 1483.3945
=== Final ===

==============================
LoRA FINE-TUNING
==============================
Random seed set to 3
Epoch: 0, loss: 35.6021, train_acc: 0.0000, train_recall: 0.0000, train_f1: 0.0000, val_acc: 0.000000, val_recall: 0.000000, val_f1: 0.000000
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed3.csv
Epoch: 1, loss: 31.5033, train_acc: 0.0000, train_recall: 0.0000, train_f1: 0.0000, val_acc: 0.000000, val_recall: 0.000000, val_f1: 0.000000
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed3.csv
Epoch: 2, loss: 28.3797, train_acc: 0.2389, train_recall: 0.2500, train_f1: 0.0964, val_acc: 0.141026, val_recall: 0.250000, val_f1: 0.061798
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed3.csv
Epoch: 3, loss: 43.5369, train_acc: 0.3333, train_recall: 0.2459, train_f1: 0.1250, val_acc: 0.487179, val_recall: 0.250000, val_f1: 0.163793
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed3.csv
Epoch: 4, loss: 78.4488, train_acc: 0.4111, train_recall: 0.2500, train_f1: 0.1457, val_acc: 0.333333, val_recall: 0.250000, val_f1: 0.125000
Epoch: 5, loss: 114.6999, train_acc: 0.0111, train_recall: 0.2500, train_f1: 0.0055, val_acc: 0.038462, val_recall: 0.250000, val_f1: 0.018519
Epoch: 6, loss: 87.9933, train_acc: 0.4111, train_recall: 0.2500, train_f1: 0.1457, val_acc: 0.333333, val_recall: 0.250000, val_f1: 0.125000
Epoch: 7, loss: 76.0902, train_acc: 0.4111, train_recall: 0.2500, train_f1: 0.1457, val_acc: 0.333333, val_recall: 0.250000, val_f1: 0.125000
Epoch: 8, loss: 52.5150, train_acc: 0.4111, train_recall: 0.2500, train_f1: 0.1462, val_acc: 0.333333, val_recall: 0.250000, val_f1: 0.125000
Epoch: 9, loss: 65.9703, train_acc: 0.3389, train_recall: 0.2500, train_f1: 0.1266, val_acc: 0.487179, val_recall: 0.250000, val_f1: 0.163793
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed3.csv
Epoch: 10, loss: 70.1071, train_acc: 0.3389, train_recall: 0.2500, train_f1: 0.1266, val_acc: 0.487179, val_recall: 0.250000, val_f1: 0.163793
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed3.csv
Epoch: 11, loss: 62.0270, train_acc: 0.3722, train_recall: 0.3089, train_f1: 0.2306, val_acc: 0.474359, val_recall: 0.356459, val_f1: 0.267270
Epoch: 12, loss: 63.3349, train_acc: 0.2944, train_recall: 0.2584, train_f1: 0.1866, val_acc: 0.307692, val_recall: 0.270933, val_f1: 0.181253
Epoch: 13, loss: 63.9356, train_acc: 0.1778, train_recall: 0.1488, train_f1: 0.0696, val_acc: 0.102564, val_recall: 0.145455, val_f1: 0.050794
Epoch: 14, loss: 52.0698, train_acc: 0.1778, train_recall: 0.1488, train_f1: 0.0696, val_acc: 0.102564, val_recall: 0.145455, val_f1: 0.050794
Epoch: 15, loss: 34.3368, train_acc: 0.3500, train_recall: 0.2778, train_f1: 0.2505, val_acc: 0.320513, val_recall: 0.310913, val_f1: 0.243830
Epoch: 16, loss: 37.7068, train_acc: 0.4111, train_recall: 0.2500, train_f1: 0.1457, val_acc: 0.333333, val_recall: 0.250000, val_f1: 0.125000
Epoch: 17, loss: 41.9872, train_acc: 0.4111, train_recall: 0.2500, train_f1: 0.1457, val_acc: 0.333333, val_recall: 0.250000, val_f1: 0.125000
Epoch: 18, loss: 41.4553, train_acc: 0.3500, train_recall: 0.2467, train_f1: 0.1876, val_acc: 0.461538, val_recall: 0.255061, val_f1: 0.221725
Epoch: 19, loss: 45.4030, train_acc: 0.3389, train_recall: 0.2500, train_f1: 0.1266, val_acc: 0.487179, val_recall: 0.250000, val_f1: 0.163793
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed3.csv
Epoch: 20, loss: 43.0929, train_acc: 0.4167, train_recall: 0.2541, train_f1: 0.1543, val_acc: 0.346154, val_recall: 0.256579, val_f1: 0.139034
Epoch: 21, loss: 39.9455, train_acc: 0.3722, train_recall: 0.2595, train_f1: 0.2054, val_acc: 0.474359, val_recall: 0.267713, val_f1: 0.237772
Epoch: 22, loss: 39.3559, train_acc: 0.3389, train_recall: 0.2500, train_f1: 0.1271, val_acc: 0.487179, val_recall: 0.250000, val_f1: 0.163793
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed3.csv
Epoch: 23, loss: 34.9779, train_acc: 0.4111, train_recall: 0.2500, train_f1: 0.1457, val_acc: 0.346154, val_recall: 0.256579, val_f1: 0.139034
Epoch: 24, loss: 28.6035, train_acc: 0.3778, train_recall: 0.2621, train_f1: 0.2103, val_acc: 0.474359, val_recall: 0.267713, val_f1: 0.237772
Epoch: 25, loss: 33.3691, train_acc: 0.2667, train_recall: 0.2688, train_f1: 0.1407, val_acc: 0.205128, val_recall: 0.282895, val_f1: 0.121941
Epoch: 26, loss: 35.4101, train_acc: 0.2444, train_recall: 0.2421, train_f1: 0.1379, val_acc: 0.217949, val_recall: 0.273325, val_f1: 0.134935
Epoch: 27, loss: 36.3815, train_acc: 0.3722, train_recall: 0.3089, train_f1: 0.2306, val_acc: 0.474359, val_recall: 0.356459, val_f1: 0.267270
Epoch: 28, loss: 36.5788, train_acc: 0.3389, train_recall: 0.2500, train_f1: 0.1266, val_acc: 0.487179, val_recall: 0.250000, val_f1: 0.163793
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed3.csv
Epoch: 29, loss: 28.6166, train_acc: 0.3500, train_recall: 0.2467, train_f1: 0.1876, val_acc: 0.461538, val_recall: 0.255061, val_f1: 0.221725
Epoch: 30, loss: 38.0233, train_acc: 0.4111, train_recall: 0.2500, train_f1: 0.1457, val_acc: 0.333333, val_recall: 0.250000, val_f1: 0.125000
Epoch: 31, loss: 41.8116, train_acc: 0.4111, train_recall: 0.2500, train_f1: 0.1457, val_acc: 0.333333, val_recall: 0.250000, val_f1: 0.125000
Epoch: 32, loss: 39.3245, train_acc: 0.4111, train_recall: 0.2500, train_f1: 0.1457, val_acc: 0.333333, val_recall: 0.250000, val_f1: 0.125000
Epoch: 33, loss: 37.6532, train_acc: 0.2389, train_recall: 0.2500, train_f1: 0.0973, val_acc: 0.153846, val_recall: 0.259615, val_f1: 0.081076
Epoch: 34, loss: 35.2553, train_acc: 0.2389, train_recall: 0.2476, train_f1: 0.1023, val_acc: 0.166667, val_recall: 0.269231, val_f1: 0.098436
Epoch: 35, loss: 30.4540, train_acc: 0.4333, train_recall: 0.3122, train_f1: 0.2497, val_acc: 0.346154, val_recall: 0.351399, val_f1: 0.229857
Epoch: 36, loss: 32.0337, train_acc: 0.3389, train_recall: 0.2500, train_f1: 0.1266, val_acc: 0.487179, val_recall: 0.250000, val_f1: 0.163793
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed3.csv
Epoch: 37, loss: 32.9273, train_acc: 0.3722, train_recall: 0.2681, train_f1: 0.1789, val_acc: 0.512821, val_recall: 0.278340, val_f1: 0.234980
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed3.csv
Epoch: 38, loss: 37.9366, train_acc: 0.4111, train_recall: 0.2500, train_f1: 0.1457, val_acc: 0.333333, val_recall: 0.250000, val_f1: 0.125000
Epoch: 39, loss: 38.5544, train_acc: 0.4111, train_recall: 0.2500, train_f1: 0.1457, val_acc: 0.333333, val_recall: 0.250000, val_f1: 0.125000
Epoch: 40, loss: 33.5060, train_acc: 0.4000, train_recall: 0.2533, train_f1: 0.2052, val_acc: 0.358974, val_recall: 0.244939, val_f1: 0.188279
Epoch: 41, loss: 35.1949, train_acc: 0.3389, train_recall: 0.2500, train_f1: 0.1266, val_acc: 0.487179, val_recall: 0.250000, val_f1: 0.163793
Epoch: 42, loss: 32.5583, train_acc: 0.3667, train_recall: 0.2979, train_f1: 0.2247, val_acc: 0.500000, val_recall: 0.337321, val_f1: 0.272157
Epoch: 43, loss: 29.4713, train_acc: 0.4222, train_recall: 0.3055, train_f1: 0.2450, val_acc: 0.346154, val_recall: 0.351399, val_f1: 0.228012
Epoch: 44, loss: 34.4280, train_acc: 0.2556, train_recall: 0.2577, train_f1: 0.1226, val_acc: 0.192308, val_recall: 0.288462, val_f1: 0.125893
Epoch: 45, loss: 33.5367, train_acc: 0.4167, train_recall: 0.3045, train_f1: 0.2424, val_acc: 0.333333, val_recall: 0.341783, val_f1: 0.218750
Epoch: 46, loss: 32.8349, train_acc: 0.4333, train_recall: 0.3122, train_f1: 0.2497, val_acc: 0.346154, val_recall: 0.351399, val_f1: 0.228012
Epoch: 47, loss: 29.8400, train_acc: 0.4333, train_recall: 0.3122, train_f1: 0.2497, val_acc: 0.346154, val_recall: 0.351399, val_f1: 0.228012
Epoch: 48, loss: 30.7224, train_acc: 0.3722, train_recall: 0.3089, train_f1: 0.2306, val_acc: 0.474359, val_recall: 0.356459, val_f1: 0.267270
Epoch: 49, loss: 31.6596, train_acc: 0.3722, train_recall: 0.3038, train_f1: 0.2288, val_acc: 0.512821, val_recall: 0.360048, val_f1: 0.290049
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed3.csv
Epoch: 50, loss: 28.2804, train_acc: 0.3722, train_recall: 0.3038, train_f1: 0.2295, val_acc: 0.500000, val_recall: 0.337321, val_f1: 0.272157
Epoch: 51, loss: 31.0158, train_acc: 0.4111, train_recall: 0.2500, train_f1: 0.1457, val_acc: 0.333333, val_recall: 0.250000, val_f1: 0.125000
Epoch: 52, loss: 32.2897, train_acc: 0.4111, train_recall: 0.2500, train_f1: 0.1457, val_acc: 0.333333, val_recall: 0.250000, val_f1: 0.125000
Epoch: 53, loss: 33.1344, train_acc: 0.2389, train_recall: 0.2500, train_f1: 0.0964, val_acc: 0.141026, val_recall: 0.250000, val_f1: 0.061798
Epoch: 54, loss: 30.1743, train_acc: 0.2389, train_recall: 0.2500, train_f1: 0.0964, val_acc: 0.141026, val_recall: 0.250000, val_f1: 0.061798
Epoch: 55, loss: 31.0300, train_acc: 0.4111, train_recall: 0.2500, train_f1: 0.1457, val_acc: 0.333333, val_recall: 0.250000, val_f1: 0.125000
Epoch: 56, loss: 29.3379, train_acc: 0.3611, train_recall: 0.2534, train_f1: 0.1959, val_acc: 0.461538, val_recall: 0.258097, val_f1: 0.227343
Epoch: 57, loss: 34.3713, train_acc: 0.3389, train_recall: 0.2500, train_f1: 0.1266, val_acc: 0.487179, val_recall: 0.250000, val_f1: 0.163793
Epoch: 58, loss: 31.4901, train_acc: 0.3333, train_recall: 0.2394, train_f1: 0.1653, val_acc: 0.474359, val_recall: 0.252530, val_f1: 0.203510
Epoch: 59, loss: 37.1052, train_acc: 0.4111, train_recall: 0.2500, train_f1: 0.1457, val_acc: 0.333333, val_recall: 0.250000, val_f1: 0.125000
Epoch: 60, loss: 38.3503, train_acc: 0.4111, train_recall: 0.2500, train_f1: 0.1462, val_acc: 0.333333, val_recall: 0.250000, val_f1: 0.125000
Epoch: 61, loss: 34.6360, train_acc: 0.3722, train_recall: 0.2264, train_f1: 0.1523, val_acc: 0.294872, val_recall: 0.221154, val_f1: 0.125000
Epoch: 62, loss: 26.3389, train_acc: 0.4444, train_recall: 0.3409, train_f1: 0.2670, val_acc: 0.320513, val_recall: 0.358392, val_f1: 0.213768
Epoch: 63, loss: 33.6280, train_acc: 0.2389, train_recall: 0.2500, train_f1: 0.0964, val_acc: 0.141026, val_recall: 0.250000, val_f1: 0.061798
Epoch: 64, loss: 32.2977, train_acc: 0.3833, train_recall: 0.3222, train_f1: 0.2387, val_acc: 0.448718, val_recall: 0.343301, val_f1: 0.250150
Epoch: 65, loss: 35.2768, train_acc: 0.3333, train_recall: 0.2459, train_f1: 0.1250, val_acc: 0.474359, val_recall: 0.243421, val_f1: 0.160870
Epoch: 66, loss: 30.7356, train_acc: 0.3833, train_recall: 0.3222, train_f1: 0.2387, val_acc: 0.448718, val_recall: 0.343301, val_f1: 0.250150
Epoch: 67, loss: 30.3102, train_acc: 0.2389, train_recall: 0.2500, train_f1: 0.0964, val_acc: 0.141026, val_recall: 0.250000, val_f1: 0.061798
Epoch: 68, loss: 29.9295, train_acc: 0.4111, train_recall: 0.2500, train_f1: 0.1462, val_acc: 0.333333, val_recall: 0.250000, val_f1: 0.125000
Epoch: 69, loss: 32.7007, train_acc: 0.4111, train_recall: 0.2500, train_f1: 0.1462, val_acc: 0.333333, val_recall: 0.250000, val_f1: 0.125000
Epoch: 70, loss: 30.3062, train_acc: 0.4111, train_recall: 0.2500, train_f1: 0.1462, val_acc: 0.333333, val_recall: 0.250000, val_f1: 0.125000
Epoch: 71, loss: 26.6901, train_acc: 0.3333, train_recall: 0.2459, train_f1: 0.1250, val_acc: 0.474359, val_recall: 0.243421, val_f1: 0.160870
Epoch: 72, loss: 25.8355, train_acc: 0.4000, train_recall: 0.3483, train_f1: 0.2548, val_acc: 0.435897, val_recall: 0.369019, val_f1: 0.254202
Epoch: 73, loss: 25.5221, train_acc: 0.3722, train_recall: 0.3300, train_f1: 0.2530, val_acc: 0.358974, val_recall: 0.351767, val_f1: 0.243237
Epoch: 74, loss: 25.8256, train_acc: 0.4000, train_recall: 0.2550, train_f1: 0.2129, val_acc: 0.358974, val_recall: 0.244939, val_f1: 0.188279
Epoch: 75, loss: 28.8071, train_acc: 0.3389, train_recall: 0.2500, train_f1: 0.1266, val_acc: 0.487179, val_recall: 0.250000, val_f1: 0.163793
Epoch: 76, loss: 27.2005, train_acc: 0.3944, train_recall: 0.2492, train_f1: 0.2012, val_acc: 0.358974, val_recall: 0.244939, val_f1: 0.188279
Epoch: 77, loss: 26.1780, train_acc: 0.3944, train_recall: 0.2492, train_f1: 0.2012, val_acc: 0.358974, val_recall: 0.244939, val_f1: 0.188279
Epoch: 78, loss: 25.0323, train_acc: 0.4333, train_recall: 0.3122, train_f1: 0.2497, val_acc: 0.346154, val_recall: 0.351399, val_f1: 0.228012
Epoch: 79, loss: 25.6844, train_acc: 0.3889, train_recall: 0.3423, train_f1: 0.2622, val_acc: 0.384615, val_recall: 0.364925, val_f1: 0.256244
Epoch: 80, loss: 25.6545, train_acc: 0.3833, train_recall: 0.3222, train_f1: 0.2387, val_acc: 0.448718, val_recall: 0.343301, val_f1: 0.250150
Epoch: 81, loss: 24.5949, train_acc: 0.4222, train_recall: 0.3055, train_f1: 0.2439, val_acc: 0.333333, val_recall: 0.341783, val_f1: 0.217475
Epoch: 82, loss: 24.8661, train_acc: 0.4333, train_recall: 0.3122, train_f1: 0.2497, val_acc: 0.346154, val_recall: 0.351399, val_f1: 0.228012
Epoch: 83, loss: 25.5660, train_acc: 0.3389, train_recall: 0.2517, train_f1: 0.1366, val_acc: 0.474359, val_recall: 0.243421, val_f1: 0.160870
Epoch: 84, loss: 25.4808, train_acc: 0.4167, train_recall: 0.3094, train_f1: 0.2457, val_acc: 0.320513, val_recall: 0.332168, val_f1: 0.209979
Epoch: 85, loss: 26.2158, train_acc: 0.4500, train_recall: 0.3492, train_f1: 0.2714, val_acc: 0.346154, val_recall: 0.377622, val_f1: 0.230431
Epoch: 86, loss: 25.0079, train_acc: 0.4167, train_recall: 0.3118, train_f1: 0.2471, val_acc: 0.320513, val_recall: 0.332168, val_f1: 0.209979
Epoch: 87, loss: 24.5299, train_acc: 0.3389, train_recall: 0.2517, train_f1: 0.1369, val_acc: 0.461538, val_recall: 0.236842, val_f1: 0.160714
Epoch: 88, loss: 23.8571, train_acc: 0.4222, train_recall: 0.3152, train_f1: 0.2512, val_acc: 0.320513, val_recall: 0.332168, val_f1: 0.209979
Epoch: 89, loss: 24.5823, train_acc: 0.3667, train_recall: 0.3271, train_f1: 0.2346, val_acc: 0.384615, val_recall: 0.358852, val_f1: 0.229579
Epoch: 90, loss: 26.2738, train_acc: 0.3333, train_recall: 0.2459, train_f1: 0.1255, val_acc: 0.474359, val_recall: 0.243421, val_f1: 0.160870
Epoch: 91, loss: 26.8690, train_acc: 0.2889, train_recall: 0.1757, train_f1: 0.1347, val_acc: 0.230769, val_recall: 0.173077, val_f1: 0.112500
Epoch: 92, loss: 27.5666, train_acc: 0.4111, train_recall: 0.2500, train_f1: 0.1462, val_acc: 0.333333, val_recall: 0.250000, val_f1: 0.126214
Epoch: 93, loss: 24.3477, train_acc: 0.3500, train_recall: 0.2651, train_f1: 0.1653, val_acc: 0.461538, val_recall: 0.236842, val_f1: 0.162162
Epoch: 94, loss: 26.0571, train_acc: 0.2389, train_recall: 0.2500, train_f1: 0.0968, val_acc: 0.141026, val_recall: 0.250000, val_f1: 0.061798
Epoch: 95, loss: 24.6184, train_acc: 0.4056, train_recall: 0.2564, train_f1: 0.1789, val_acc: 0.320513, val_recall: 0.237348, val_f1: 0.135269
Epoch: 96, loss: 26.3909, train_acc: 0.3500, train_recall: 0.2651, train_f1: 0.1654, val_acc: 0.461538, val_recall: 0.236842, val_f1: 0.162162
Epoch: 97, loss: 25.5104, train_acc: 0.2611, train_recall: 0.2647, train_f1: 0.1338, val_acc: 0.205128, val_recall: 0.282895, val_f1: 0.122249
Epoch: 98, loss: 24.6857, train_acc: 0.4500, train_recall: 0.2844, train_f1: 0.2309, val_acc: 0.371795, val_recall: 0.248482, val_f1: 0.203491
Epoch: 99, loss: 25.5422, train_acc: 0.4500, train_recall: 0.2844, train_f1: 0.2309, val_acc: 0.371795, val_recall: 0.248482, val_f1: 0.200599
Epoch: 100, loss: 26.3519, train_acc: 0.3333, train_recall: 0.2459, train_f1: 0.1250, val_acc: 0.474359, val_recall: 0.243421, val_f1: 0.160870
Epoch: 101, loss: 27.0858, train_acc: 0.4111, train_recall: 0.2500, train_f1: 0.1462, val_acc: 0.346154, val_recall: 0.256579, val_f1: 0.141533
Epoch: 102, loss: 26.9394, train_acc: 0.2556, train_recall: 0.2601, train_f1: 0.1174, val_acc: 0.153846, val_recall: 0.259615, val_f1: 0.081076
Epoch: 103, loss: 25.9181, train_acc: 0.4000, train_recall: 0.2505, train_f1: 0.1698, val_acc: 0.320513, val_recall: 0.237348, val_f1: 0.135269
Epoch: 104, loss: 25.4030, train_acc: 0.3500, train_recall: 0.2651, train_f1: 0.1653, val_acc: 0.461538, val_recall: 0.236842, val_f1: 0.162162
Epoch: 105, loss: 25.6630, train_acc: 0.2611, train_recall: 0.2647, train_f1: 0.1338, val_acc: 0.205128, val_recall: 0.282895, val_f1: 0.122249
Epoch: 106, loss: 24.5398, train_acc: 0.4333, train_recall: 0.2845, train_f1: 0.2586, val_acc: 0.371795, val_recall: 0.239372, val_f1: 0.211283
Epoch: 107, loss: 27.0152, train_acc: 0.4278, train_recall: 0.2623, train_f1: 0.1714, val_acc: 0.346154, val_recall: 0.253543, val_f1: 0.149390
Epoch: 108, loss: 25.4734, train_acc: 0.3389, train_recall: 0.2493, train_f1: 0.1326, val_acc: 0.487179, val_recall: 0.253036, val_f1: 0.180799
Epoch: 109, loss: 24.3624, train_acc: 0.4278, train_recall: 0.2735, train_f1: 0.2179, val_acc: 0.320513, val_recall: 0.244387, val_f1: 0.177633
Epoch: 110, loss: 26.8401, train_acc: 0.2611, train_recall: 0.2647, train_f1: 0.1338, val_acc: 0.205128, val_recall: 0.282895, val_f1: 0.122249
Epoch: 111, loss: 26.9582, train_acc: 0.3556, train_recall: 0.2760, train_f1: 0.1906, val_acc: 0.461538, val_recall: 0.269139, val_f1: 0.207996
Epoch: 112, loss: 24.0877, train_acc: 0.4167, train_recall: 0.3606, train_f1: 0.2650, val_acc: 0.435897, val_recall: 0.369019, val_f1: 0.254202
Epoch: 113, loss: 26.2245, train_acc: 0.4111, train_recall: 0.2580, train_f1: 0.1804, val_acc: 0.320513, val_recall: 0.237348, val_f1: 0.135269
Epoch: 114, loss: 24.4232, train_acc: 0.4556, train_recall: 0.3532, train_f1: 0.2812, val_acc: 0.346154, val_recall: 0.371550, val_f1: 0.246894
Epoch: 115, loss: 25.8216, train_acc: 0.4111, train_recall: 0.3565, train_f1: 0.2616, val_acc: 0.423077, val_recall: 0.362440, val_f1: 0.246851
Epoch: 116, loss: 29.1473, train_acc: 0.3389, train_recall: 0.2517, train_f1: 0.1366, val_acc: 0.474359, val_recall: 0.243421, val_f1: 0.160870
Epoch: 117, loss: 27.5299, train_acc: 0.3389, train_recall: 0.2517, train_f1: 0.1366, val_acc: 0.474359, val_recall: 0.243421, val_f1: 0.160870
Epoch: 118, loss: 25.6024, train_acc: 0.4222, train_recall: 0.2687, train_f1: 0.2042, val_acc: 0.307692, val_recall: 0.224696, val_f1: 0.139599
Epoch: 119, loss: 27.3123, train_acc: 0.2222, train_recall: 0.4636, train_f1: 0.1571, val_acc: 0.153846, val_recall: 0.320221, val_f1: 0.119540
Epoch: 120, loss: 27.0897, train_acc: 0.4278, train_recall: 0.3454, train_f1: 0.2588, val_acc: 0.269231, val_recall: 0.333042, val_f1: 0.182698
Epoch: 121, loss: 27.1200, train_acc: 0.4222, train_recall: 0.2599, train_f1: 0.1666, val_acc: 0.346154, val_recall: 0.256579, val_f1: 0.142821
Epoch: 122, loss: 23.9764, train_acc: 0.3778, train_recall: 0.3123, train_f1: 0.2380, val_acc: 0.448718, val_recall: 0.343301, val_f1: 0.252174
Epoch: 123, loss: 26.5415, train_acc: 0.3667, train_recall: 0.3031, train_f1: 0.2267, val_acc: 0.448718, val_recall: 0.327153, val_f1: 0.246382
Epoch: 124, loss: 24.5455, train_acc: 0.4167, train_recall: 0.3606, train_f1: 0.2650, val_acc: 0.435897, val_recall: 0.369019, val_f1: 0.254202
Epoch: 125, loss: 25.8622, train_acc: 0.4500, train_recall: 0.3492, train_f1: 0.2707, val_acc: 0.333333, val_recall: 0.364971, val_f1: 0.228763
Epoch: 126, loss: 26.5162, train_acc: 0.4222, train_recall: 0.3176, train_f1: 0.2503, val_acc: 0.333333, val_recall: 0.354895, val_f1: 0.220000
Epoch: 127, loss: 25.2742, train_acc: 0.4389, train_recall: 0.3299, train_f1: 0.2770, val_acc: 0.333333, val_recall: 0.348822, val_f1: 0.237485
Epoch: 128, loss: 24.5047, train_acc: 0.3889, train_recall: 0.3281, train_f1: 0.2432, val_acc: 0.448718, val_recall: 0.343301, val_f1: 0.251068
Epoch: 129, loss: 25.7156, train_acc: 0.3722, train_recall: 0.3089, train_f1: 0.2306, val_acc: 0.461538, val_recall: 0.349880, val_f1: 0.260316
Epoch: 130, loss: 24.5505, train_acc: 0.4556, train_recall: 0.2945, train_f1: 0.2369, val_acc: 0.371795, val_recall: 0.322184, val_f1: 0.258333
Epoch: 131, loss: 24.9037, train_acc: 0.4389, train_recall: 0.3540, train_f1: 0.2967, val_acc: 0.320513, val_recall: 0.356321, val_f1: 0.241871
Epoch: 132, loss: 23.5402, train_acc: 0.4778, train_recall: 0.3768, train_f1: 0.3491, val_acc: 0.371795, val_recall: 0.366489, val_f1: 0.287689
Epoch: 133, loss: 24.5303, train_acc: 0.4500, train_recall: 0.2844, train_f1: 0.2309, val_acc: 0.358974, val_recall: 0.241903, val_f1: 0.192044
Epoch: 134, loss: 26.4747, train_acc: 0.3444, train_recall: 0.2558, train_f1: 0.1384, val_acc: 0.487179, val_recall: 0.250000, val_f1: 0.163793
Epoch: 135, loss: 23.5925, train_acc: 0.4556, train_recall: 0.3585, train_f1: 0.3430, val_acc: 0.410256, val_recall: 0.341783, val_f1: 0.295393
Epoch: 136, loss: 25.0400, train_acc: 0.4556, train_recall: 0.3525, train_f1: 0.2741, val_acc: 0.333333, val_recall: 0.368007, val_f1: 0.221429
Epoch: 137, loss: 26.0842, train_acc: 0.4444, train_recall: 0.3482, train_f1: 0.2680, val_acc: 0.320513, val_recall: 0.371503, val_f1: 0.215677
Epoch: 138, loss: 25.0803, train_acc: 0.4556, train_recall: 0.3525, train_f1: 0.2741, val_acc: 0.333333, val_recall: 0.368007, val_f1: 0.221429
Epoch: 139, loss: 23.4267, train_acc: 0.4778, train_recall: 0.3783, train_f1: 0.3527, val_acc: 0.371795, val_recall: 0.366489, val_f1: 0.288841
Epoch: 140, loss: 24.7749, train_acc: 0.3722, train_recall: 0.3020, train_f1: 0.2279, val_acc: 0.487179, val_recall: 0.330742, val_f1: 0.264361
Epoch: 141, loss: 24.9736, train_acc: 0.4278, train_recall: 0.2647, train_f1: 0.1816, val_acc: 0.333333, val_recall: 0.257039, val_f1: 0.179494
Epoch: 142, loss: 24.6625, train_acc: 0.4278, train_recall: 0.3465, train_f1: 0.2859, val_acc: 0.294872, val_recall: 0.343163, val_f1: 0.217716
Epoch: 143, loss: 23.6530, train_acc: 0.4500, train_recall: 0.3399, train_f1: 0.3334, val_acc: 0.410256, val_recall: 0.373114, val_f1: 0.317700
Epoch: 144, loss: 24.2601, train_acc: 0.4556, train_recall: 0.2885, train_f1: 0.2356, val_acc: 0.384615, val_recall: 0.255061, val_f1: 0.208951
Epoch: 145, loss: 24.3900, train_acc: 0.3778, train_recall: 0.3130, train_f1: 0.2332, val_acc: 0.461538, val_recall: 0.349880, val_f1: 0.260316
Epoch: 146, loss: 24.3750, train_acc: 0.4389, train_recall: 0.3519, train_f1: 0.2823, val_acc: 0.294872, val_recall: 0.346200, val_f1: 0.213095
Epoch: 147, loss: 25.7450, train_acc: 0.4278, train_recall: 0.2640, train_f1: 0.1750, val_acc: 0.358974, val_recall: 0.279306, val_f1: 0.177467
Epoch: 148, loss: 23.6424, train_acc: 0.4833, train_recall: 0.3795, train_f1: 0.3496, val_acc: 0.371795, val_recall: 0.369525, val_f1: 0.288703
Epoch: 149, loss: 24.0494, train_acc: 0.3889, train_recall: 0.3246, train_f1: 0.2422, val_acc: 0.461538, val_recall: 0.366029, val_f1: 0.266460
Epoch: 150, loss: 23.0812, train_acc: 0.4778, train_recall: 0.3620, train_f1: 0.3497, val_acc: 0.410256, val_recall: 0.373114, val_f1: 0.316138
Epoch: 151, loss: 24.4561, train_acc: 0.4444, train_recall: 0.3236, train_f1: 0.2754, val_acc: 0.346154, val_recall: 0.358438, val_f1: 0.251878
Epoch: 152, loss: 25.5380, train_acc: 0.2611, train_recall: 0.2647, train_f1: 0.1338, val_acc: 0.179487, val_recall: 0.269737, val_f1: 0.100356
Epoch: 153, loss: 24.9693, train_acc: 0.3778, train_recall: 0.3130, train_f1: 0.2332, val_acc: 0.461538, val_recall: 0.349880, val_f1: 0.260316
Epoch: 154, loss: 23.1862, train_acc: 0.4722, train_recall: 0.3523, train_f1: 0.3469, val_acc: 0.384615, val_recall: 0.362992, val_f1: 0.307635
Epoch: 155, loss: 25.5284, train_acc: 0.4333, train_recall: 0.2681, train_f1: 0.1828, val_acc: 0.333333, val_recall: 0.246964, val_f1: 0.138458
Epoch: 156, loss: 23.6672, train_acc: 0.4833, train_recall: 0.3795, train_f1: 0.3496, val_acc: 0.371795, val_recall: 0.369525, val_f1: 0.288703
Epoch: 157, loss: 24.3696, train_acc: 0.4222, train_recall: 0.3647, train_f1: 0.2683, val_acc: 0.448718, val_recall: 0.375598, val_f1: 0.261925
Epoch: 158, loss: 24.1335, train_acc: 0.3833, train_recall: 0.3188, train_f1: 0.2370, val_acc: 0.461538, val_recall: 0.366029, val_f1: 0.264957
Epoch: 159, loss: 24.3840, train_acc: 0.4556, train_recall: 0.3311, train_f1: 0.2867, val_acc: 0.346154, val_recall: 0.358438, val_f1: 0.251878
Epoch: 160, loss: 23.5066, train_acc: 0.4944, train_recall: 0.3877, train_f1: 0.3613, val_acc: 0.384615, val_recall: 0.376104, val_f1: 0.298707
Epoch: 161, loss: 23.8750, train_acc: 0.4833, train_recall: 0.3838, train_f1: 0.3612, val_acc: 0.384615, val_recall: 0.373068, val_f1: 0.298506
Epoch: 162, loss: 23.4288, train_acc: 0.5000, train_recall: 0.3911, train_f1: 0.3652, val_acc: 0.384615, val_recall: 0.376104, val_f1: 0.298707
Epoch: 163, loss: 23.5236, train_acc: 0.4611, train_recall: 0.3338, train_f1: 0.3317, val_acc: 0.410256, val_recall: 0.330742, val_f1: 0.311619
Epoch: 164, loss: 23.3009, train_acc: 0.4944, train_recall: 0.3860, train_f1: 0.3630, val_acc: 0.384615, val_recall: 0.376104, val_f1: 0.298707
Epoch: 165, loss: 23.5839, train_acc: 0.4889, train_recall: 0.3836, train_f1: 0.3555, val_acc: 0.371795, val_recall: 0.369525, val_f1: 0.288703
Epoch: 166, loss: 23.3008, train_acc: 0.4944, train_recall: 0.3877, train_f1: 0.3613, val_acc: 0.384615, val_recall: 0.376104, val_f1: 0.298707
Epoch: 167, loss: 23.3618, train_acc: 0.5000, train_recall: 0.3738, train_f1: 0.3679, val_acc: 0.435897, val_recall: 0.376196, val_f1: 0.339202
Epoch: 168, loss: 23.2631, train_acc: 0.4944, train_recall: 0.3860, train_f1: 0.3630, val_acc: 0.384615, val_recall: 0.376104, val_f1: 0.298707
Epoch: 169, loss: 23.5955, train_acc: 0.4667, train_recall: 0.3622, train_f1: 0.3038, val_acc: 0.333333, val_recall: 0.361934, val_f1: 0.242777
Epoch: 170, loss: 23.0970, train_acc: 0.4889, train_recall: 0.3836, train_f1: 0.3555, val_acc: 0.371795, val_recall: 0.369525, val_f1: 0.288703
Epoch: 171, loss: 24.1004, train_acc: 0.4056, train_recall: 0.2815, train_f1: 0.2392, val_acc: 0.423077, val_recall: 0.251472, val_f1: 0.242060
Epoch: 172, loss: 26.1312, train_acc: 0.4278, train_recall: 0.2640, train_f1: 0.1746, val_acc: 0.358974, val_recall: 0.279306, val_f1: 0.179527
Epoch: 173, loss: 25.7313, train_acc: 0.4333, train_recall: 0.3495, train_f1: 0.2677, val_acc: 0.269231, val_recall: 0.333042, val_f1: 0.182698
Epoch: 174, loss: 25.4123, train_acc: 0.4333, train_recall: 0.2681, train_f1: 0.1828, val_acc: 0.358974, val_recall: 0.279306, val_f1: 0.179527
Epoch: 175, loss: 24.6798, train_acc: 0.3444, train_recall: 0.2558, train_f1: 0.1384, val_acc: 0.487179, val_recall: 0.266148, val_f1: 0.200742
Epoch: 176, loss: 24.1858, train_acc: 0.2722, train_recall: 0.4958, train_f1: 0.2344, val_acc: 0.256410, val_recall: 0.353668, val_f1: 0.211169
Epoch: 177, loss: 23.0055, train_acc: 0.4889, train_recall: 0.3836, train_f1: 0.3555, val_acc: 0.371795, val_recall: 0.369525, val_f1: 0.288703
Epoch: 178, loss: 24.2087, train_acc: 0.4444, train_recall: 0.2770, train_f1: 0.2038, val_acc: 0.358974, val_recall: 0.270197, val_f1: 0.202848
Epoch: 179, loss: 27.3172, train_acc: 0.3389, train_recall: 0.2500, train_f1: 0.1266, val_acc: 0.487179, val_recall: 0.250000, val_f1: 0.163793
Epoch: 180, loss: 25.1268, train_acc: 0.3389, train_recall: 0.2500, train_f1: 0.1266, val_acc: 0.461538, val_recall: 0.236842, val_f1: 0.157895
Epoch: 181, loss: 26.2679, train_acc: 0.4278, train_recall: 0.3454, train_f1: 0.2588, val_acc: 0.269231, val_recall: 0.333042, val_f1: 0.182698
Epoch: 182, loss: 29.4118, train_acc: 0.4556, train_recall: 0.2989, train_f1: 0.2310, val_acc: 0.371795, val_recall: 0.318182, val_f1: 0.211842
Epoch: 183, loss: 30.3774, train_acc: 0.4222, train_recall: 0.3420, train_f1: 0.2555, val_acc: 0.269231, val_recall: 0.333042, val_f1: 0.183333
Epoch: 184, loss: 30.2777, train_acc: 0.4167, train_recall: 0.2558, train_f1: 0.1583, val_acc: 0.346154, val_recall: 0.272727, val_f1: 0.162563
Epoch: 185, loss: 26.8807, train_acc: 0.4333, train_recall: 0.2733, train_f1: 0.1900, val_acc: 0.346154, val_recall: 0.272727, val_f1: 0.162563
Epoch: 186, loss: 25.9707, train_acc: 0.4222, train_recall: 0.3647, train_f1: 0.2683, val_acc: 0.448718, val_recall: 0.375598, val_f1: 0.261925
Epoch: 187, loss: 32.1270, train_acc: 0.3389, train_recall: 0.2500, train_f1: 0.1266, val_acc: 0.487179, val_recall: 0.250000, val_f1: 0.163793
Epoch: 188, loss: 30.9354, train_acc: 0.3722, train_recall: 0.2866, train_f1: 0.1946, val_acc: 0.474359, val_recall: 0.259569, val_f1: 0.193412
Epoch: 189, loss: 32.3840, train_acc: 0.2389, train_recall: 0.2500, train_f1: 0.0964, val_acc: 0.141026, val_recall: 0.250000, val_f1: 0.061798
Epoch: 190, loss: 28.8627, train_acc: 0.2556, train_recall: 0.2601, train_f1: 0.1174, val_acc: 0.153846, val_recall: 0.259615, val_f1: 0.081076
Epoch: 191, loss: 30.3090, train_acc: 0.4111, train_recall: 0.2500, train_f1: 0.1462, val_acc: 0.333333, val_recall: 0.250000, val_f1: 0.126214
Epoch: 192, loss: 32.8193, train_acc: 0.4111, train_recall: 0.2500, train_f1: 0.1457, val_acc: 0.333333, val_recall: 0.250000, val_f1: 0.125000
Epoch: 193, loss: 30.4660, train_acc: 0.4333, train_recall: 0.2664, train_f1: 0.1788, val_acc: 0.358974, val_recall: 0.260121, val_f1: 0.161146
Epoch: 194, loss: 32.8659, train_acc: 0.3389, train_recall: 0.2500, train_f1: 0.1266, val_acc: 0.487179, val_recall: 0.250000, val_f1: 0.163793
Epoch: 195, loss: 34.5268, train_acc: 0.3389, train_recall: 0.2500, train_f1: 0.1266, val_acc: 0.487179, val_recall: 0.250000, val_f1: 0.163793
Epoch: 196, loss: 30.0622, train_acc: 0.3389, train_recall: 0.2500, train_f1: 0.1266, val_acc: 0.487179, val_recall: 0.250000, val_f1: 0.163793
Epoch: 197, loss: 25.5752, train_acc: 0.4500, train_recall: 0.3221, train_f1: 0.2743, val_acc: 0.320513, val_recall: 0.312983, val_f1: 0.225478
Epoch: 198, loss: 27.8538, train_acc: 0.4500, train_recall: 0.3516, train_f1: 0.2711, val_acc: 0.320513, val_recall: 0.371503, val_f1: 0.215677
Epoch: 199, loss: 32.5636, train_acc: 0.2389, train_recall: 0.2500, train_f1: 0.0968, val_acc: 0.141026, val_recall: 0.250000, val_f1: 0.061798
Epoch: 200, loss: 28.4120, train_acc: 0.4500, train_recall: 0.3516, train_f1: 0.2711, val_acc: 0.320513, val_recall: 0.371503, val_f1: 0.215677
Epoch: 201, loss: 26.1992, train_acc: 0.4500, train_recall: 0.3287, train_f1: 0.2737, val_acc: 0.371795, val_recall: 0.380705, val_f1: 0.252137
Epoch: 202, loss: 24.5290, train_acc: 0.4333, train_recall: 0.2803, train_f1: 0.2427, val_acc: 0.410256, val_recall: 0.259109, val_f1: 0.225531
Epoch: 203, loss: 30.3554, train_acc: 0.3389, train_recall: 0.2500, train_f1: 0.1266, val_acc: 0.487179, val_recall: 0.250000, val_f1: 0.163793
Epoch: 204, loss: 30.1843, train_acc: 0.3389, train_recall: 0.2500, train_f1: 0.1266, val_acc: 0.487179, val_recall: 0.250000, val_f1: 0.163793
Epoch: 205, loss: 25.5060, train_acc: 0.4278, train_recall: 0.2717, train_f1: 0.2348, val_acc: 0.371795, val_recall: 0.242409, val_f1: 0.210203
Epoch: 206, loss: 26.8539, train_acc: 0.4333, train_recall: 0.2681, train_f1: 0.1831, val_acc: 0.333333, val_recall: 0.243927, val_f1: 0.145705
Epoch: 207, loss: 25.1187, train_acc: 0.4444, train_recall: 0.3347, train_f1: 0.3356, val_acc: 0.358974, val_recall: 0.346798, val_f1: 0.290058
Epoch: 208, loss: 25.6707, train_acc: 0.4111, train_recall: 0.3565, train_f1: 0.2616, val_acc: 0.435897, val_recall: 0.369019, val_f1: 0.254202
Epoch: 209, loss: 26.3241, train_acc: 0.4111, train_recall: 0.3565, train_f1: 0.2616, val_acc: 0.435897, val_recall: 0.369019, val_f1: 0.254202
Epoch: 210, loss: 25.4219, train_acc: 0.4167, train_recall: 0.3606, train_f1: 0.2650, val_acc: 0.435897, val_recall: 0.369019, val_f1: 0.255105
Epoch: 211, loss: 24.3555, train_acc: 0.4722, train_recall: 0.3491, train_f1: 0.3358, val_acc: 0.397436, val_recall: 0.372608, val_f1: 0.308826
Epoch: 212, loss: 25.0172, train_acc: 0.4778, train_recall: 0.3142, train_f1: 0.2883, val_acc: 0.384615, val_recall: 0.268173, val_f1: 0.239800
Epoch: 213, loss: 27.3647, train_acc: 0.3389, train_recall: 0.2500, train_f1: 0.1266, val_acc: 0.487179, val_recall: 0.250000, val_f1: 0.163793
Epoch: 214, loss: 24.7230, train_acc: 0.4556, train_recall: 0.3326, train_f1: 0.3341, val_acc: 0.448718, val_recall: 0.347442, val_f1: 0.334879
Epoch: 215, loss: 26.8742, train_acc: 0.4333, train_recall: 0.3147, train_f1: 0.2512, val_acc: 0.358974, val_recall: 0.374126, val_f1: 0.236313
Epoch: 216, loss: 28.6258, train_acc: 0.4556, train_recall: 0.3525, train_f1: 0.2738, val_acc: 0.320513, val_recall: 0.358392, val_f1: 0.213768
Epoch: 217, loss: 29.3953, train_acc: 0.4278, train_recall: 0.3454, train_f1: 0.2588, val_acc: 0.269231, val_recall: 0.333042, val_f1: 0.182698
Epoch: 218, loss: 27.9481, train_acc: 0.4278, train_recall: 0.3454, train_f1: 0.2588, val_acc: 0.269231, val_recall: 0.333042, val_f1: 0.182698
Epoch: 219, loss: 25.2516, train_acc: 0.4556, train_recall: 0.3501, train_f1: 0.2741, val_acc: 0.307692, val_recall: 0.348776, val_f1: 0.208120
Epoch: 220, loss: 26.7170, train_acc: 0.3389, train_recall: 0.2500, train_f1: 0.1266, val_acc: 0.474359, val_recall: 0.243421, val_f1: 0.160870
Epoch: 221, loss: 27.1355, train_acc: 0.3389, train_recall: 0.2500, train_f1: 0.1266, val_acc: 0.487179, val_recall: 0.250000, val_f1: 0.163793
Epoch: 222, loss: 26.9049, train_acc: 0.4278, train_recall: 0.2630, train_f1: 0.1770, val_acc: 0.307692, val_recall: 0.227733, val_f1: 0.131420
Epoch: 223, loss: 25.8737, train_acc: 0.4556, train_recall: 0.2955, train_f1: 0.2329, val_acc: 0.320513, val_recall: 0.266608, val_f1: 0.171828
Epoch: 224, loss: 27.7009, train_acc: 0.2444, train_recall: 0.2541, train_f1: 0.1059, val_acc: 0.153846, val_recall: 0.256579, val_f1: 0.075718
Epoch: 225, loss: 26.8212, train_acc: 0.4167, train_recall: 0.3588, train_f1: 0.2647, val_acc: 0.435897, val_recall: 0.369019, val_f1: 0.255105
Epoch: 226, loss: 27.6877, train_acc: 0.3444, train_recall: 0.2558, train_f1: 0.1384, val_acc: 0.474359, val_recall: 0.259569, val_f1: 0.195006
Epoch: 227, loss: 23.8214, train_acc: 0.4778, train_recall: 0.3776, train_f1: 0.3506, val_acc: 0.358974, val_recall: 0.359910, val_f1: 0.277938
Epoch: 228, loss: 27.8225, train_acc: 0.4500, train_recall: 0.2907, train_f1: 0.2165, val_acc: 0.346154, val_recall: 0.285839, val_f1: 0.180598
Epoch: 229, loss: 27.6409, train_acc: 0.4500, train_recall: 0.2931, train_f1: 0.2223, val_acc: 0.320513, val_recall: 0.266608, val_f1: 0.168040
Epoch: 230, loss: 30.2294, train_acc: 0.2389, train_recall: 0.2500, train_f1: 0.0964, val_acc: 0.141026, val_recall: 0.250000, val_f1: 0.061798
Epoch: 231, loss: 27.3481, train_acc: 0.2556, train_recall: 0.2601, train_f1: 0.1174, val_acc: 0.153846, val_recall: 0.259615, val_f1: 0.081076
Epoch: 232, loss: 24.7451, train_acc: 0.4500, train_recall: 0.2960, train_f1: 0.2540, val_acc: 0.397436, val_recall: 0.246457, val_f1: 0.220916
Epoch: 233, loss: 27.4452, train_acc: 0.3667, train_recall: 0.2619, train_f1: 0.1860, val_acc: 0.512821, val_recall: 0.275304, val_f1: 0.225744
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed3.csv
Epoch: 234, loss: 28.9221, train_acc: 0.4556, train_recall: 0.2885, train_f1: 0.2359, val_acc: 0.384615, val_recall: 0.255061, val_f1: 0.207678
Epoch: 235, loss: 28.4131, train_acc: 0.4556, train_recall: 0.2885, train_f1: 0.2356, val_acc: 0.397436, val_recall: 0.261640, val_f1: 0.215645
Epoch: 236, loss: 29.4813, train_acc: 0.3444, train_recall: 0.2534, train_f1: 0.1337, val_acc: 0.500000, val_recall: 0.259615, val_f1: 0.183736
Epoch: 237, loss: 26.4821, train_acc: 0.4500, train_recall: 0.2880, train_f1: 0.2405, val_acc: 0.423077, val_recall: 0.271761, val_f1: 0.232057
Epoch: 238, loss: 25.6899, train_acc: 0.4556, train_recall: 0.2885, train_f1: 0.2359, val_acc: 0.371795, val_recall: 0.248482, val_f1: 0.202061
Epoch: 239, loss: 24.1530, train_acc: 0.4833, train_recall: 0.3817, train_f1: 0.3562, val_acc: 0.384615, val_recall: 0.373068, val_f1: 0.298506
Epoch: 240, loss: 27.8564, train_acc: 0.2611, train_recall: 0.2647, train_f1: 0.1340, val_acc: 0.166667, val_recall: 0.263158, val_f1: 0.088515
Epoch: 241, loss: 24.6258, train_acc: 0.4944, train_recall: 0.3877, train_f1: 0.3608, val_acc: 0.371795, val_recall: 0.369525, val_f1: 0.288703
Epoch: 242, loss: 24.1638, train_acc: 0.4833, train_recall: 0.3201, train_f1: 0.2970, val_acc: 0.397436, val_recall: 0.307048, val_f1: 0.283575
Epoch: 243, loss: 24.4159, train_acc: 0.4444, train_recall: 0.2897, train_f1: 0.2471, val_acc: 0.448718, val_recall: 0.272773, val_f1: 0.244384
Epoch: 244, loss: 24.2604, train_acc: 0.4611, train_recall: 0.2934, train_f1: 0.2414, val_acc: 0.410256, val_recall: 0.268219, val_f1: 0.224969
Epoch: 245, loss: 24.1059, train_acc: 0.2611, train_recall: 0.4630, train_f1: 0.2570, val_acc: 0.256410, val_recall: 0.327444, val_f1: 0.236876
Epoch: 246, loss: 23.3385, train_acc: 0.5056, train_recall: 0.3959, train_f1: 0.3726, val_acc: 0.384615, val_recall: 0.376104, val_f1: 0.298707
Epoch: 247, loss: 23.6935, train_acc: 0.4944, train_recall: 0.3870, train_f1: 0.3590, val_acc: 0.384615, val_recall: 0.376104, val_f1: 0.298707
Epoch: 248, loss: 23.6967, train_acc: 0.5000, train_recall: 0.3918, train_f1: 0.3673, val_acc: 0.384615, val_recall: 0.376104, val_f1: 0.298707
Epoch: 249, loss: 23.4516, train_acc: 0.5056, train_recall: 0.3959, train_f1: 0.3726, val_acc: 0.384615, val_recall: 0.376104, val_f1: 0.298707
Epoch: 250, loss: 23.6427, train_acc: 0.4833, train_recall: 0.3573, train_f1: 0.3463, val_acc: 0.423077, val_recall: 0.385766, val_f1: 0.326681
Epoch: 251, loss: 23.5356, train_acc: 0.4556, train_recall: 0.3125, train_f1: 0.3029, val_acc: 0.435897, val_recall: 0.311603, val_f1: 0.309866
Epoch: 252, loss: 23.2995, train_acc: 0.5000, train_recall: 0.3918, train_f1: 0.3673, val_acc: 0.384615, val_recall: 0.376104, val_f1: 0.298707
Epoch: 253, loss: 23.4141, train_acc: 0.5000, train_recall: 0.3918, train_f1: 0.3668, val_acc: 0.384615, val_recall: 0.376104, val_f1: 0.298707
Epoch: 254, loss: 23.2419, train_acc: 0.5000, train_recall: 0.3918, train_f1: 0.3669, val_acc: 0.384615, val_recall: 0.376104, val_f1: 0.298707
Epoch: 255, loss: 23.0744, train_acc: 0.5000, train_recall: 0.3918, train_f1: 0.3673, val_acc: 0.384615, val_recall: 0.376104, val_f1: 0.298707
Epoch: 256, loss: 23.1368, train_acc: 0.4722, train_recall: 0.3290, train_f1: 0.3252, val_acc: 0.410256, val_recall: 0.279260, val_f1: 0.276622
Epoch: 257, loss: 23.1072, train_acc: 0.5000, train_recall: 0.3901, train_f1: 0.3683, val_acc: 0.384615, val_recall: 0.376104, val_f1: 0.298707
Epoch: 258, loss: 22.9557, train_acc: 0.4889, train_recall: 0.3836, train_f1: 0.3555, val_acc: 0.371795, val_recall: 0.369525, val_f1: 0.288703
Epoch: 259, loss: 23.1208, train_acc: 0.4889, train_recall: 0.3836, train_f1: 0.3552, val_acc: 0.371795, val_recall: 0.369525, val_f1: 0.288703
Epoch: 260, loss: 22.9152, train_acc: 0.4944, train_recall: 0.3877, train_f1: 0.3615, val_acc: 0.371795, val_recall: 0.369525, val_f1: 0.288703
Epoch: 261, loss: 22.8935, train_acc: 0.4611, train_recall: 0.3339, train_f1: 0.3332, val_acc: 0.358974, val_recall: 0.282205, val_f1: 0.263168
Epoch: 262, loss: 22.8483, train_acc: 0.4944, train_recall: 0.3877, train_f1: 0.3612, val_acc: 0.371795, val_recall: 0.369525, val_f1: 0.288703
Epoch: 263, loss: 23.0294, train_acc: 0.4722, train_recall: 0.3663, train_f1: 0.3109, val_acc: 0.320513, val_recall: 0.352319, val_f1: 0.228929
Epoch: 264, loss: 22.7375, train_acc: 0.4833, train_recall: 0.3795, train_f1: 0.3496, val_acc: 0.371795, val_recall: 0.369525, val_f1: 0.288703
Epoch: 265, loss: 23.3515, train_acc: 0.3778, train_recall: 0.2958, train_f1: 0.2126, val_acc: 0.448718, val_recall: 0.262560, val_f1: 0.204206
Epoch: 266, loss: 22.8332, train_acc: 0.4833, train_recall: 0.3715, train_f1: 0.3451, val_acc: 0.358974, val_recall: 0.365983, val_f1: 0.276609
Epoch: 267, loss: 23.2414, train_acc: 0.4778, train_recall: 0.3771, train_f1: 0.3409, val_acc: 0.346154, val_recall: 0.372516, val_f1: 0.265207
Epoch: 268, loss: 22.9889, train_acc: 0.4889, train_recall: 0.3367, train_f1: 0.3285, val_acc: 0.423077, val_recall: 0.288876, val_f1: 0.286277
/home/ADS/cyang314/ucr_work/HINI_Baseline/GraphLoRA/model/GraphLoRA.py:218: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.
Consider using tensor.detach() first. (Triggered internally at /pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:836.)
  f.write(f'{pre_dataset} to {downstream_dataset}: seed: %d, epoch: %d, train_loss: %f, train_acc: %f, train_recall: %f, train_f1: %f, val_acc: %f, val_recall: %f, val_f1: %f\n' %
Epoch: 269, loss: 23.5665, train_acc: 0.4722, train_recall: 0.3265, train_f1: 0.3204, val_acc: 0.435897, val_recall: 0.295455, val_f1: 0.295285
Epoch: 270, loss: 24.5319, train_acc: 0.4667, train_recall: 0.3037, train_f1: 0.2491, val_acc: 0.346154, val_recall: 0.282803, val_f1: 0.196493
Epoch: 271, loss: 23.6998, train_acc: 0.4556, train_recall: 0.3659, train_f1: 0.3014, val_acc: 0.294872, val_recall: 0.346200, val_f1: 0.213095
Epoch: 272, loss: 23.2001, train_acc: 0.4722, train_recall: 0.3265, train_f1: 0.3199, val_acc: 0.435897, val_recall: 0.295455, val_f1: 0.295285
Epoch: 273, loss: 23.7079, train_acc: 0.4889, train_recall: 0.3323, train_f1: 0.3227, val_acc: 0.448718, val_recall: 0.308106, val_f1: 0.305714
Epoch: 274, loss: 23.5371, train_acc: 0.4722, train_recall: 0.3265, train_f1: 0.3204, val_acc: 0.448718, val_recall: 0.302033, val_f1: 0.304159
Epoch: 275, loss: 22.9111, train_acc: 0.5111, train_recall: 0.3905, train_f1: 0.3700, val_acc: 0.423077, val_recall: 0.401914, val_f1: 0.328877
Epoch: 276, loss: 25.2165, train_acc: 0.2833, train_recall: 0.2806, train_f1: 0.1575, val_acc: 0.179487, val_recall: 0.272773, val_f1: 0.107932
Epoch: 277, loss: 23.3528, train_acc: 0.4611, train_recall: 0.3406, train_f1: 0.3298, val_acc: 0.423077, val_recall: 0.302954, val_f1: 0.291240
Epoch: 278, loss: 24.1233, train_acc: 0.4889, train_recall: 0.3201, train_f1: 0.2796, val_acc: 0.397436, val_recall: 0.306082, val_f1: 0.254213
Epoch: 279, loss: 22.8994, train_acc: 0.4944, train_recall: 0.3502, train_f1: 0.3445, val_acc: 0.397436, val_recall: 0.330236, val_f1: 0.295418
Epoch: 280, loss: 23.7608, train_acc: 0.4222, train_recall: 0.3647, train_f1: 0.2687, val_acc: 0.461538, val_recall: 0.382177, val_f1: 0.268794
Epoch: 281, loss: 24.8455, train_acc: 0.3000, train_recall: 0.2882, train_f1: 0.1835, val_acc: 0.230769, val_recall: 0.279904, val_f1: 0.141851
Epoch: 282, loss: 22.9162, train_acc: 0.5111, train_recall: 0.3912, train_f1: 0.3722, val_acc: 0.423077, val_recall: 0.401914, val_f1: 0.328877
Epoch: 283, loss: 24.9071, train_acc: 0.4556, train_recall: 0.2852, train_f1: 0.2179, val_acc: 0.384615, val_recall: 0.283355, val_f1: 0.225962
Epoch: 284, loss: 23.9878, train_acc: 0.4389, train_recall: 0.2899, train_f1: 0.2494, val_acc: 0.448718, val_recall: 0.269737, val_f1: 0.243447
Epoch: 285, loss: 23.3991, train_acc: 0.4778, train_recall: 0.3323, train_f1: 0.3295, val_acc: 0.448718, val_recall: 0.302033, val_f1: 0.304159
Epoch: 286, loss: 23.0833, train_acc: 0.5000, train_recall: 0.3830, train_f1: 0.3606, val_acc: 0.397436, val_recall: 0.388756, val_f1: 0.308367
Epoch: 287, loss: 23.9816, train_acc: 0.4833, train_recall: 0.3795, train_f1: 0.3493, val_acc: 0.346154, val_recall: 0.356367, val_f1: 0.267088
Epoch: 288, loss: 23.0173, train_acc: 0.5167, train_recall: 0.3953, train_f1: 0.3779, val_acc: 0.423077, val_recall: 0.401914, val_f1: 0.328877
Epoch: 289, loss: 23.7609, train_acc: 0.4444, train_recall: 0.2957, train_f1: 0.2615, val_acc: 0.461538, val_recall: 0.292464, val_f1: 0.286654
Epoch: 290, loss: 24.3458, train_acc: 0.4611, train_recall: 0.2944, train_f1: 0.2479, val_acc: 0.410256, val_recall: 0.284367, val_f1: 0.258951
Epoch: 291, loss: 23.7314, train_acc: 0.4556, train_recall: 0.3164, train_f1: 0.3116, val_acc: 0.435897, val_recall: 0.276270, val_f1: 0.269805
Epoch: 292, loss: 23.7682, train_acc: 0.4889, train_recall: 0.3836, train_f1: 0.3555, val_acc: 0.358974, val_recall: 0.362946, val_f1: 0.278571
Epoch: 293, loss: 24.7138, train_acc: 0.4500, train_recall: 0.3618, train_f1: 0.2935, val_acc: 0.294872, val_recall: 0.346200, val_f1: 0.213095
Epoch: 294, loss: 23.3417, train_acc: 0.4944, train_recall: 0.3797, train_f1: 0.3572, val_acc: 0.384615, val_recall: 0.379141, val_f1: 0.298590
Epoch: 295, loss: 24.0186, train_acc: 0.4222, train_recall: 0.2868, train_f1: 0.2600, val_acc: 0.448718, val_recall: 0.279812, val_f1: 0.275702
Epoch: 296, loss: 24.3878, train_acc: 0.4667, train_recall: 0.3002, train_f1: 0.2594, val_acc: 0.397436, val_recall: 0.277788, val_f1: 0.248966
Epoch: 297, loss: 22.8534, train_acc: 0.5167, train_recall: 0.3953, train_f1: 0.3779, val_acc: 0.423077, val_recall: 0.401914, val_f1: 0.328877
Epoch: 298, loss: 23.5004, train_acc: 0.4889, train_recall: 0.3937, train_f1: 0.3696, val_acc: 0.384615, val_recall: 0.360922, val_f1: 0.289377
Epoch: 299, loss: 23.4179, train_acc: 0.4889, train_recall: 0.3836, train_f1: 0.3552, val_acc: 0.371795, val_recall: 0.369525, val_f1: 0.288703
epoch: 234, train_acc: 0.366667, val_acc: 0.512821, val_recall: 0.275304, val_f1: 0.225744
Running: year=2016 â†’ downstream_year=2017, seed=4
Random seed set to 42

==============================
PRE-TRAINING
==============================
create PreTrain instance...
pre-training...
(T) | Epoch=001, loss=7.8972, this epoch 1.5704, total 1.5704
+++model saved ! 2016.pth
(T) | Epoch=002, loss=7.8960, this epoch 1.4957, total 3.0661
+++model saved ! 2016.pth
(T) | Epoch=003, loss=7.8942, this epoch 1.4653, total 4.5314
+++model saved ! 2016.pth
(T) | Epoch=004, loss=7.8973, this epoch 1.4526, total 5.9840
(T) | Epoch=005, loss=7.8891, this epoch 1.4630, total 7.4470
+++model saved ! 2016.pth
(T) | Epoch=006, loss=7.8855, this epoch 1.4699, total 8.9169
+++model saved ! 2016.pth
(T) | Epoch=007, loss=7.8808, this epoch 1.5285, total 10.4454
+++model saved ! 2016.pth
(T) | Epoch=008, loss=7.8751, this epoch 1.4850, total 11.9304
+++model saved ! 2016.pth
(T) | Epoch=009, loss=7.8690, this epoch 1.5055, total 13.4359
+++model saved ! 2016.pth
(T) | Epoch=010, loss=7.8849, this epoch 1.4809, total 14.9168
(T) | Epoch=011, loss=7.8509, this epoch 1.4940, total 16.4108
+++model saved ! 2016.pth
(T) | Epoch=012, loss=7.8414, this epoch 1.4681, total 17.8790
+++model saved ! 2016.pth
(T) | Epoch=013, loss=7.8309, this epoch 1.4708, total 19.3497
+++model saved ! 2016.pth
(T) | Epoch=014, loss=7.8687, this epoch 1.5296, total 20.8793
(T) | Epoch=015, loss=7.8100, this epoch 1.4435, total 22.3228
+++model saved ! 2016.pth
(T) | Epoch=016, loss=8.9353, this epoch 1.4848, total 23.8076
(T) | Epoch=017, loss=7.7978, this epoch 1.4753, total 25.2830
+++model saved ! 2016.pth
(T) | Epoch=018, loss=7.8477, this epoch 1.4976, total 26.7805
(T) | Epoch=019, loss=8.3957, this epoch 1.4761, total 28.2567
(T) | Epoch=020, loss=7.8094, this epoch 1.4644, total 29.7211
(T) | Epoch=021, loss=8.1407, this epoch 1.4590, total 31.1801
(T) | Epoch=022, loss=8.0648, this epoch 1.4674, total 32.6475
(T) | Epoch=023, loss=7.8291, this epoch 1.4355, total 34.0830
(T) | Epoch=024, loss=7.8650, this epoch 1.4801, total 35.5631
(T) | Epoch=025, loss=7.9818, this epoch 1.4504, total 37.0135
(T) | Epoch=026, loss=7.8394, this epoch 1.4803, total 38.4938
(T) | Epoch=027, loss=7.9440, this epoch 1.4720, total 39.9657
(T) | Epoch=028, loss=7.9133, this epoch 1.4670, total 41.4327
(T) | Epoch=029, loss=7.8532, this epoch 1.4669, total 42.8996
(T) | Epoch=030, loss=7.8385, this epoch 1.4841, total 44.3837
(T) | Epoch=031, loss=7.8355, this epoch 1.4693, total 45.8529
(T) | Epoch=032, loss=7.8339, this epoch 1.4468, total 47.2997
(T) | Epoch=033, loss=7.8334, this epoch 1.4903, total 48.7901
(T) | Epoch=034, loss=7.8813, this epoch 1.4706, total 50.2606
(T) | Epoch=035, loss=7.8675, this epoch 1.4305, total 51.6912
(T) | Epoch=036, loss=7.8694, this epoch 1.4114, total 53.1026
(T) | Epoch=037, loss=7.8295, this epoch 1.4772, total 54.5798
(T) | Epoch=038, loss=7.8461, this epoch 1.5115, total 56.0914
(T) | Epoch=039, loss=7.8733, this epoch 1.4718, total 57.5631
(T) | Epoch=040, loss=7.9009, this epoch 1.5150, total 59.0781
(T) | Epoch=041, loss=7.8444, this epoch 1.4857, total 60.5639
(T) | Epoch=042, loss=7.8238, this epoch 1.4886, total 62.0524
(T) | Epoch=043, loss=7.8633, this epoch 1.5119, total 63.5644
(T) | Epoch=044, loss=7.8205, this epoch 1.4285, total 64.9929
(T) | Epoch=045, loss=7.8970, this epoch 1.4791, total 66.4720
(T) | Epoch=046, loss=7.8833, this epoch 1.4819, total 67.9539
(T) | Epoch=047, loss=7.8183, this epoch 1.5101, total 69.4640
(T) | Epoch=048, loss=7.8181, this epoch 1.4416, total 70.9056
(T) | Epoch=049, loss=7.8869, this epoch 1.4657, total 72.3713
(T) | Epoch=050, loss=7.8172, this epoch 1.4908, total 73.8621
(T) | Epoch=051, loss=7.8158, this epoch 1.5581, total 75.4202
(T) | Epoch=052, loss=7.8338, this epoch 1.5265, total 76.9467
(T) | Epoch=053, loss=7.8124, this epoch 1.4998, total 78.4466
(T) | Epoch=054, loss=7.8297, this epoch 1.4934, total 79.9399
(T) | Epoch=055, loss=7.8500, this epoch 1.4793, total 81.4192
(T) | Epoch=056, loss=7.8050, this epoch 1.4856, total 82.9048
(T) | Epoch=057, loss=7.8455, this epoch 1.4664, total 84.3712
(T) | Epoch=058, loss=7.7608, this epoch 1.5241, total 85.8953
+++model saved ! 2016.pth
(T) | Epoch=059, loss=7.7941, this epoch 1.5015, total 87.3968
(T) | Epoch=060, loss=7.7920, this epoch 1.4852, total 88.8820
(T) | Epoch=061, loss=7.8088, this epoch 1.4385, total 90.3205
(T) | Epoch=062, loss=7.7835, this epoch 1.5179, total 91.8384
(T) | Epoch=063, loss=7.8721, this epoch 1.5313, total 93.3697
(T) | Epoch=064, loss=7.8320, this epoch 1.5124, total 94.8822
(T) | Epoch=065, loss=7.8550, this epoch 1.5753, total 96.4575
(T) | Epoch=066, loss=7.7719, this epoch 1.5644, total 98.0219
(T) | Epoch=067, loss=7.7767, this epoch 1.4812, total 99.5031
(T) | Epoch=068, loss=7.8208, this epoch 1.5124, total 101.0155
(T) | Epoch=069, loss=7.7684, this epoch 1.5208, total 102.5363
(T) | Epoch=070, loss=7.7887, this epoch 1.5359, total 104.0722
(T) | Epoch=071, loss=7.8050, this epoch 1.5432, total 105.6154
(T) | Epoch=072, loss=7.7990, this epoch 1.5582, total 107.1736
(T) | Epoch=073, loss=7.7964, this epoch 1.5335, total 108.7071
(T) | Epoch=074, loss=7.8045, this epoch 1.4761, total 110.1833
(T) | Epoch=075, loss=7.7370, this epoch 1.4790, total 111.6623
+++model saved ! 2016.pth
(T) | Epoch=076, loss=7.7841, this epoch 1.5048, total 113.1672
(T) | Epoch=077, loss=7.7808, this epoch 1.4632, total 114.6304
(T) | Epoch=078, loss=7.7223, this epoch 1.4948, total 116.1252
+++model saved ! 2016.pth
(T) | Epoch=079, loss=7.7388, this epoch 1.4915, total 117.6168
(T) | Epoch=080, loss=7.7078, this epoch 1.5136, total 119.1303
+++model saved ! 2016.pth
(T) | Epoch=081, loss=7.7200, this epoch 1.5230, total 120.6533
(T) | Epoch=082, loss=7.7504, this epoch 1.5024, total 122.1557
(T) | Epoch=083, loss=7.7256, this epoch 1.4594, total 123.6150
(T) | Epoch=084, loss=7.7405, this epoch 1.5108, total 125.1259
(T) | Epoch=085, loss=7.9853, this epoch 1.4947, total 126.6206
(T) | Epoch=086, loss=7.7604, this epoch 1.4898, total 128.1104
(T) | Epoch=087, loss=7.6827, this epoch 1.4948, total 129.6052
+++model saved ! 2016.pth
(T) | Epoch=088, loss=7.6728, this epoch 1.4799, total 131.0851
+++model saved ! 2016.pth
(T) | Epoch=089, loss=7.7032, this epoch 1.4892, total 132.5743
(T) | Epoch=090, loss=7.6639, this epoch 1.4446, total 134.0190
+++model saved ! 2016.pth
(T) | Epoch=091, loss=7.7604, this epoch 1.4461, total 135.4651
(T) | Epoch=092, loss=7.7533, this epoch 1.4648, total 136.9299
(T) | Epoch=093, loss=7.6967, this epoch 1.5285, total 138.4584
(T) | Epoch=094, loss=7.6565, this epoch 1.4522, total 139.9105
+++model saved ! 2016.pth
(T) | Epoch=095, loss=7.7766, this epoch 1.4915, total 141.4020
(T) | Epoch=096, loss=7.7297, this epoch 1.4604, total 142.8624
(T) | Epoch=097, loss=7.6908, this epoch 1.4538, total 144.3162
(T) | Epoch=098, loss=7.7321, this epoch 1.4889, total 145.8050
(T) | Epoch=099, loss=7.6779, this epoch 1.4770, total 147.2821
(T) | Epoch=100, loss=7.6936, this epoch 1.4724, total 148.7545
(T) | Epoch=101, loss=7.7002, this epoch 1.4906, total 150.2451
(T) | Epoch=102, loss=7.6657, this epoch 1.4681, total 151.7132
(T) | Epoch=103, loss=7.6628, this epoch 1.5108, total 153.2240
(T) | Epoch=104, loss=7.6648, this epoch 1.4871, total 154.7111
(T) | Epoch=105, loss=7.8314, this epoch 1.4856, total 156.1968
(T) | Epoch=106, loss=7.6576, this epoch 1.4790, total 157.6757
(T) | Epoch=107, loss=7.9045, this epoch 1.4904, total 159.1662
(T) | Epoch=108, loss=7.6804, this epoch 1.4510, total 160.6172
(T) | Epoch=109, loss=7.6935, this epoch 1.4740, total 162.0911
(T) | Epoch=110, loss=7.6577, this epoch 1.4374, total 163.5286
(T) | Epoch=111, loss=7.6718, this epoch 1.4706, total 164.9991
(T) | Epoch=112, loss=8.1404, this epoch 1.4873, total 166.4864
(T) | Epoch=113, loss=7.6954, this epoch 1.4759, total 167.9623
(T) | Epoch=114, loss=7.7348, this epoch 1.5271, total 169.4894
(T) | Epoch=115, loss=7.6981, this epoch 1.4744, total 170.9638
(T) | Epoch=116, loss=7.9329, this epoch 1.5407, total 172.5045
(T) | Epoch=117, loss=7.9696, this epoch 1.5312, total 174.0357
(T) | Epoch=118, loss=7.7740, this epoch 1.5274, total 175.5631
(T) | Epoch=119, loss=7.7467, this epoch 1.4619, total 177.0250
(T) | Epoch=120, loss=7.8114, this epoch 1.4561, total 178.4811
(T) | Epoch=121, loss=7.7820, this epoch 1.5486, total 180.0297
(T) | Epoch=122, loss=7.8905, this epoch 1.4710, total 181.5006
(T) | Epoch=123, loss=7.8847, this epoch 1.4719, total 182.9726
(T) | Epoch=124, loss=7.7380, this epoch 1.4585, total 184.4311
(T) | Epoch=125, loss=7.7314, this epoch 1.4861, total 185.9171
(T) | Epoch=126, loss=7.7788, this epoch 1.4493, total 187.3665
(T) | Epoch=127, loss=7.7780, this epoch 1.4769, total 188.8433
(T) | Epoch=128, loss=7.7069, this epoch 1.4711, total 190.3144
(T) | Epoch=129, loss=7.6958, this epoch 1.4887, total 191.8031
(T) | Epoch=130, loss=7.6921, this epoch 1.4661, total 193.2693
(T) | Epoch=131, loss=7.7212, this epoch 1.4829, total 194.7522
(T) | Epoch=132, loss=7.6800, this epoch 1.4883, total 196.2405
(T) | Epoch=133, loss=7.6819, this epoch 1.5215, total 197.7620
(T) | Epoch=134, loss=7.7725, this epoch 1.5030, total 199.2650
(T) | Epoch=135, loss=8.0296, this epoch 1.4910, total 200.7560
(T) | Epoch=136, loss=7.7016, this epoch 1.4907, total 202.2467
(T) | Epoch=137, loss=7.7240, this epoch 1.5121, total 203.7588
(T) | Epoch=138, loss=7.6803, this epoch 1.5354, total 205.2942
(T) | Epoch=139, loss=7.6857, this epoch 1.4768, total 206.7709
(T) | Epoch=140, loss=7.7758, this epoch 1.5013, total 208.2723
(T) | Epoch=141, loss=7.6878, this epoch 1.4788, total 209.7511
(T) | Epoch=142, loss=7.6937, this epoch 1.4980, total 211.2490
(T) | Epoch=143, loss=7.6953, this epoch 1.4970, total 212.7460
(T) | Epoch=144, loss=7.6950, this epoch 1.4812, total 214.2273
(T) | Epoch=145, loss=7.6916, this epoch 1.5418, total 215.7691
(T) | Epoch=146, loss=7.8707, this epoch 1.5196, total 217.2887
(T) | Epoch=147, loss=7.9587, this epoch 1.5215, total 218.8102
(T) | Epoch=148, loss=7.7416, this epoch 1.4903, total 220.3005
(T) | Epoch=149, loss=7.6807, this epoch 1.5010, total 221.8015
(T) | Epoch=150, loss=7.6698, this epoch 1.5659, total 223.3674
(T) | Epoch=151, loss=7.7041, this epoch 1.5036, total 224.8710
(T) | Epoch=152, loss=7.6688, this epoch 1.5259, total 226.3969
(T) | Epoch=153, loss=7.6618, this epoch 1.5009, total 227.8978
(T) | Epoch=154, loss=7.7258, this epoch 1.4800, total 229.3778
(T) | Epoch=155, loss=7.6446, this epoch 1.4516, total 230.8294
+++model saved ! 2016.pth
(T) | Epoch=156, loss=7.6350, this epoch 1.4332, total 232.2626
+++model saved ! 2016.pth
(T) | Epoch=157, loss=7.6350, this epoch 1.4719, total 233.7345
+++model saved ! 2016.pth
(T) | Epoch=158, loss=7.6295, this epoch 1.4671, total 235.2016
+++model saved ! 2016.pth
(T) | Epoch=159, loss=7.9806, this epoch 1.4868, total 236.6884
(T) | Epoch=160, loss=7.6389, this epoch 1.4683, total 238.1566
(T) | Epoch=161, loss=7.7209, this epoch 1.4400, total 239.5966
(T) | Epoch=162, loss=7.7459, this epoch 1.4184, total 241.0150
(T) | Epoch=163, loss=7.7678, this epoch 1.4092, total 242.4242
(T) | Epoch=164, loss=7.7021, this epoch 1.5548, total 243.9790
(T) | Epoch=165, loss=7.7692, this epoch 1.4623, total 245.4413
(T) | Epoch=166, loss=7.9191, this epoch 1.4578, total 246.8992
(T) | Epoch=167, loss=7.7395, this epoch 1.5144, total 248.4136
(T) | Epoch=168, loss=7.8386, this epoch 1.4802, total 249.8938
(T) | Epoch=169, loss=7.7457, this epoch 1.5018, total 251.3956
(T) | Epoch=170, loss=7.8090, this epoch 1.4632, total 252.8588
(T) | Epoch=171, loss=7.8330, this epoch 1.4707, total 254.3295
(T) | Epoch=172, loss=7.7389, this epoch 1.4486, total 255.7781
(T) | Epoch=173, loss=7.7918, this epoch 1.4483, total 257.2265
(T) | Epoch=174, loss=7.7982, this epoch 1.4423, total 258.6687
(T) | Epoch=175, loss=7.7083, this epoch 1.4978, total 260.1666
(T) | Epoch=176, loss=7.7061, this epoch 1.5174, total 261.6840
(T) | Epoch=177, loss=7.6974, this epoch 1.5021, total 263.1861
(T) | Epoch=178, loss=7.7521, this epoch 1.4978, total 264.6839
(T) | Epoch=179, loss=7.7281, this epoch 1.4960, total 266.1798
(T) | Epoch=180, loss=7.7283, this epoch 1.4835, total 267.6633
(T) | Epoch=181, loss=7.7021, this epoch 1.4376, total 269.1009
(T) | Epoch=182, loss=7.6751, this epoch 1.4395, total 270.5404
(T) | Epoch=183, loss=7.7060, this epoch 1.4584, total 271.9988
(T) | Epoch=184, loss=7.6700, this epoch 1.5428, total 273.5415
(T) | Epoch=185, loss=7.7197, this epoch 1.5263, total 275.0679
(T) | Epoch=186, loss=7.6522, this epoch 1.5360, total 276.6039
(T) | Epoch=187, loss=7.7096, this epoch 1.5565, total 278.1604
(T) | Epoch=188, loss=7.6425, this epoch 1.4331, total 279.5935
(T) | Epoch=189, loss=7.6440, this epoch 1.4546, total 281.0481
(T) | Epoch=190, loss=7.6702, this epoch 1.5135, total 282.5615
(T) | Epoch=191, loss=7.6335, this epoch 1.5271, total 284.0887
(T) | Epoch=192, loss=7.6225, this epoch 1.4672, total 285.5559
+++model saved ! 2016.pth
(T) | Epoch=193, loss=7.6214, this epoch 1.4591, total 287.0150
+++model saved ! 2016.pth
(T) | Epoch=194, loss=7.6194, this epoch 1.4563, total 288.4713
+++model saved ! 2016.pth
(T) | Epoch=195, loss=7.6500, this epoch 1.4293, total 289.9006
(T) | Epoch=196, loss=7.6092, this epoch 1.4536, total 291.3542
+++model saved ! 2016.pth
(T) | Epoch=197, loss=7.5999, this epoch 1.4397, total 292.7938
+++model saved ! 2016.pth
(T) | Epoch=198, loss=7.6908, this epoch 1.4580, total 294.2518
(T) | Epoch=199, loss=7.6461, this epoch 1.4758, total 295.7277
(T) | Epoch=200, loss=7.6914, this epoch 1.4659, total 297.1936
(T) | Epoch=201, loss=7.7265, this epoch 1.4861, total 298.6796
(T) | Epoch=202, loss=7.6334, this epoch 1.4927, total 300.1723
(T) | Epoch=203, loss=7.6409, this epoch 1.5057, total 301.6780
(T) | Epoch=204, loss=7.6671, this epoch 1.4756, total 303.1536
(T) | Epoch=205, loss=7.6712, this epoch 1.4665, total 304.6200
(T) | Epoch=206, loss=7.6833, this epoch 1.4770, total 306.0971
(T) | Epoch=207, loss=7.7145, this epoch 1.5348, total 307.6319
(T) | Epoch=208, loss=7.6406, this epoch 1.4717, total 309.1037
(T) | Epoch=209, loss=7.6699, this epoch 1.4551, total 310.5588
(T) | Epoch=210, loss=7.6666, this epoch 1.5133, total 312.0721
(T) | Epoch=211, loss=7.6291, this epoch 1.5075, total 313.5796
(T) | Epoch=212, loss=7.7429, this epoch 1.4977, total 315.0772
(T) | Epoch=213, loss=7.6209, this epoch 1.4777, total 316.5549
(T) | Epoch=214, loss=7.6179, this epoch 1.4885, total 318.0434
(T) | Epoch=215, loss=7.6074, this epoch 1.4778, total 319.5211
(T) | Epoch=216, loss=7.7853, this epoch 1.4729, total 320.9941
(T) | Epoch=217, loss=7.6730, this epoch 1.5081, total 322.5022
(T) | Epoch=218, loss=7.6273, this epoch 1.4546, total 323.9568
(T) | Epoch=219, loss=7.6329, this epoch 1.4736, total 325.4303
(T) | Epoch=220, loss=7.6359, this epoch 1.5024, total 326.9327
(T) | Epoch=221, loss=7.7191, this epoch 1.4864, total 328.4191
(T) | Epoch=222, loss=7.6743, this epoch 1.4631, total 329.8822
(T) | Epoch=223, loss=7.6385, this epoch 1.4529, total 331.3351
(T) | Epoch=224, loss=7.6429, this epoch 1.4313, total 332.7664
(T) | Epoch=225, loss=7.6478, this epoch 1.5337, total 334.3001
(T) | Epoch=226, loss=7.6565, this epoch 1.4501, total 335.7501
(T) | Epoch=227, loss=7.7008, this epoch 1.5170, total 337.2672
(T) | Epoch=228, loss=7.6370, this epoch 1.5047, total 338.7719
(T) | Epoch=229, loss=7.6407, this epoch 1.4567, total 340.2286
(T) | Epoch=230, loss=7.6681, this epoch 1.5123, total 341.7409
(T) | Epoch=231, loss=7.6297, this epoch 1.5021, total 343.2430
(T) | Epoch=232, loss=7.6327, this epoch 1.5000, total 344.7429
(T) | Epoch=233, loss=7.6318, this epoch 1.4972, total 346.2401
(T) | Epoch=234, loss=7.6505, this epoch 1.4764, total 347.7165
(T) | Epoch=235, loss=7.6506, this epoch 1.5538, total 349.2703
(T) | Epoch=236, loss=7.6233, this epoch 1.4776, total 350.7479
(T) | Epoch=237, loss=7.6656, this epoch 1.4644, total 352.2123
(T) | Epoch=238, loss=7.6206, this epoch 1.4726, total 353.6849
(T) | Epoch=239, loss=7.6069, this epoch 1.4673, total 355.1522
(T) | Epoch=240, loss=7.6020, this epoch 1.5300, total 356.6822
(T) | Epoch=241, loss=7.6014, this epoch 1.4978, total 358.1799
(T) | Epoch=242, loss=7.5974, this epoch 1.4613, total 359.6412
+++model saved ! 2016.pth
(T) | Epoch=243, loss=7.7122, this epoch 1.5187, total 361.1599
(T) | Epoch=244, loss=7.5825, this epoch 1.5096, total 362.6695
+++model saved ! 2016.pth
(T) | Epoch=245, loss=7.6316, this epoch 1.4498, total 364.1193
(T) | Epoch=246, loss=7.6575, this epoch 1.4556, total 365.5749
(T) | Epoch=247, loss=7.6442, this epoch 1.4870, total 367.0619
(T) | Epoch=248, loss=7.6549, this epoch 1.4667, total 368.5287
(T) | Epoch=249, loss=7.6258, this epoch 1.4938, total 370.0225
(T) | Epoch=250, loss=7.6161, this epoch 1.5367, total 371.5592
(T) | Epoch=251, loss=7.6353, this epoch 1.5807, total 373.1399
(T) | Epoch=252, loss=7.6374, this epoch 1.6058, total 374.7457
(T) | Epoch=253, loss=7.6581, this epoch 1.5252, total 376.2709
(T) | Epoch=254, loss=7.6275, this epoch 1.5163, total 377.7872
(T) | Epoch=255, loss=7.6231, this epoch 1.4899, total 379.2771
(T) | Epoch=256, loss=7.6265, this epoch 1.4889, total 380.7661
(T) | Epoch=257, loss=7.6247, this epoch 1.4778, total 382.2439
(T) | Epoch=258, loss=7.6480, this epoch 1.4443, total 383.6882
(T) | Epoch=259, loss=7.6726, this epoch 1.4783, total 385.1665
(T) | Epoch=260, loss=7.7362, this epoch 1.4770, total 386.6435
(T) | Epoch=261, loss=7.6112, this epoch 1.5382, total 388.1817
(T) | Epoch=262, loss=7.5926, this epoch 1.5275, total 389.7092
(T) | Epoch=263, loss=7.6275, this epoch 1.5219, total 391.2311
(T) | Epoch=264, loss=7.5969, this epoch 1.4992, total 392.7303
(T) | Epoch=265, loss=7.5855, this epoch 1.5043, total 394.2345
(T) | Epoch=266, loss=7.5856, this epoch 1.5032, total 395.7377
(T) | Epoch=267, loss=7.5699, this epoch 1.5555, total 397.2932
+++model saved ! 2016.pth
(T) | Epoch=268, loss=7.6041, this epoch 1.5073, total 398.8005
(T) | Epoch=269, loss=7.5493, this epoch 1.4834, total 400.2839
+++model saved ! 2016.pth
(T) | Epoch=270, loss=7.5517, this epoch 1.5454, total 401.8293
(T) | Epoch=271, loss=7.5466, this epoch 1.5447, total 403.3740
+++model saved ! 2016.pth
(T) | Epoch=272, loss=8.0264, this epoch 1.5613, total 404.9353
(T) | Epoch=273, loss=7.6125, this epoch 1.5359, total 406.4712
(T) | Epoch=274, loss=7.6927, this epoch 1.5430, total 408.0142
(T) | Epoch=275, loss=7.7581, this epoch 1.4988, total 409.5131
(T) | Epoch=276, loss=7.6928, this epoch 1.5273, total 411.0404
(T) | Epoch=277, loss=7.7092, this epoch 1.5664, total 412.6068
(T) | Epoch=278, loss=7.7423, this epoch 1.5576, total 414.1644
(T) | Epoch=279, loss=7.7906, this epoch 1.5285, total 415.6930
(T) | Epoch=280, loss=7.9144, this epoch 1.5371, total 417.2301
(T) | Epoch=281, loss=7.8076, this epoch 1.5594, total 418.7895
(T) | Epoch=282, loss=7.8167, this epoch 1.5449, total 420.3344
(T) | Epoch=283, loss=7.7798, this epoch 1.5528, total 421.8872
(T) | Epoch=284, loss=7.8240, this epoch 1.5768, total 423.4639
(T) | Epoch=285, loss=7.7826, this epoch 1.4633, total 424.9272
(T) | Epoch=286, loss=7.9037, this epoch 1.4628, total 426.3900
(T) | Epoch=287, loss=7.9312, this epoch 1.5618, total 427.9519
(T) | Epoch=288, loss=7.9051, this epoch 1.5808, total 429.5327
(T) | Epoch=289, loss=7.9388, this epoch 1.4834, total 431.0160
(T) | Epoch=290, loss=7.8462, this epoch 1.5319, total 432.5479
(T) | Epoch=291, loss=7.7874, this epoch 1.5950, total 434.1428
(T) | Epoch=292, loss=7.7891, this epoch 1.5502, total 435.6930
(T) | Epoch=293, loss=7.9099, this epoch 1.5498, total 437.2429
(T) | Epoch=294, loss=7.7901, this epoch 1.5626, total 438.8054
(T) | Epoch=295, loss=7.7890, this epoch 1.5370, total 440.3425
(T) | Epoch=296, loss=7.7900, this epoch 1.5242, total 441.8666
(T) | Epoch=297, loss=7.8543, this epoch 1.5625, total 443.4292
(T) | Epoch=298, loss=8.0895, this epoch 1.5757, total 445.0048
(T) | Epoch=299, loss=7.7896, this epoch 1.5660, total 446.5709
(T) | Epoch=300, loss=7.9353, this epoch 1.4897, total 448.0606
(T) | Epoch=301, loss=7.7884, this epoch 1.5210, total 449.5816
(T) | Epoch=302, loss=7.9216, this epoch 1.5540, total 451.1356
(T) | Epoch=303, loss=7.7897, this epoch 1.5473, total 452.6830
(T) | Epoch=304, loss=7.7893, this epoch 1.5155, total 454.1985
(T) | Epoch=305, loss=7.7887, this epoch 1.4907, total 455.6892
(T) | Epoch=306, loss=7.9136, this epoch 1.5253, total 457.2145
(T) | Epoch=307, loss=7.9213, this epoch 1.5476, total 458.7621
(T) | Epoch=308, loss=7.7882, this epoch 1.4816, total 460.2437
(T) | Epoch=309, loss=7.8438, this epoch 1.5107, total 461.7544
(T) | Epoch=310, loss=7.7876, this epoch 1.5224, total 463.2768
(T) | Epoch=311, loss=7.7879, this epoch 1.5132, total 464.7900
(T) | Epoch=312, loss=7.7863, this epoch 1.4779, total 466.2679
(T) | Epoch=313, loss=7.7874, this epoch 1.4473, total 467.7152
(T) | Epoch=314, loss=7.8393, this epoch 1.5225, total 469.2377
(T) | Epoch=315, loss=7.8384, this epoch 1.4725, total 470.7102
(T) | Epoch=316, loss=7.7841, this epoch 1.5181, total 472.2283
(T) | Epoch=317, loss=7.8805, this epoch 1.4966, total 473.7249
(T) | Epoch=318, loss=7.7858, this epoch 1.4542, total 475.1790
(T) | Epoch=319, loss=7.7838, this epoch 1.5442, total 476.7232
(T) | Epoch=320, loss=7.7820, this epoch 1.5402, total 478.2634
(T) | Epoch=321, loss=7.8743, this epoch 1.5000, total 479.7634
(T) | Epoch=322, loss=7.8903, this epoch 1.4782, total 481.2416
(T) | Epoch=323, loss=7.8891, this epoch 1.4550, total 482.6966
(T) | Epoch=324, loss=7.7777, this epoch 1.5025, total 484.1991
(T) | Epoch=325, loss=7.8938, this epoch 1.4871, total 485.6862
(T) | Epoch=326, loss=7.7803, this epoch 1.4603, total 487.1464
(T) | Epoch=327, loss=7.7766, this epoch 1.5228, total 488.6692
(T) | Epoch=328, loss=7.7773, this epoch 1.4813, total 490.1505
(T) | Epoch=329, loss=7.7776, this epoch 1.4963, total 491.6468
(T) | Epoch=330, loss=7.8904, this epoch 1.5135, total 493.1603
(T) | Epoch=331, loss=7.8145, this epoch 1.5176, total 494.6780
(T) | Epoch=332, loss=7.7720, this epoch 1.4602, total 496.1382
(T) | Epoch=333, loss=7.8152, this epoch 1.4510, total 497.5892
(T) | Epoch=334, loss=7.7734, this epoch 1.4146, total 499.0038
(T) | Epoch=335, loss=7.7695, this epoch 1.4771, total 500.4809
(T) | Epoch=336, loss=7.7696, this epoch 1.4507, total 501.9316
(T) | Epoch=337, loss=7.8588, this epoch 1.4804, total 503.4119
(T) | Epoch=338, loss=7.8664, this epoch 1.4580, total 504.8700
(T) | Epoch=339, loss=7.8717, this epoch 1.4450, total 506.3150
(T) | Epoch=340, loss=7.8483, this epoch 1.4541, total 507.7691
(T) | Epoch=341, loss=7.7959, this epoch 1.5240, total 509.2931
(T) | Epoch=342, loss=7.7556, this epoch 1.5272, total 510.8203
(T) | Epoch=343, loss=7.9333, this epoch 1.4982, total 512.3185
(T) | Epoch=344, loss=7.8531, this epoch 1.5117, total 513.8302
(T) | Epoch=345, loss=7.9464, this epoch 1.5388, total 515.3690
(T) | Epoch=346, loss=7.7494, this epoch 1.4841, total 516.8531
(T) | Epoch=347, loss=7.8198, this epoch 1.4783, total 518.3314
(T) | Epoch=348, loss=7.8303, this epoch 1.4633, total 519.7947
(T) | Epoch=349, loss=7.8083, this epoch 1.4616, total 521.2562
(T) | Epoch=350, loss=7.7799, this epoch 1.5146, total 522.7708
(T) | Epoch=351, loss=7.7349, this epoch 1.4744, total 524.2452
(T) | Epoch=352, loss=7.8048, this epoch 1.5364, total 525.7816
(T) | Epoch=353, loss=7.7255, this epoch 1.5246, total 527.3062
(T) | Epoch=354, loss=7.7234, this epoch 1.5249, total 528.8311
(T) | Epoch=355, loss=7.7990, this epoch 1.4572, total 530.2883
(T) | Epoch=356, loss=7.7167, this epoch 1.5165, total 531.8048
(T) | Epoch=357, loss=7.7518, this epoch 1.5469, total 533.3517
(T) | Epoch=358, loss=7.7096, this epoch 1.5149, total 534.8666
(T) | Epoch=359, loss=7.7178, this epoch 1.4725, total 536.3391
(T) | Epoch=360, loss=7.7386, this epoch 1.5148, total 537.8540
(T) | Epoch=361, loss=7.6995, this epoch 1.4970, total 539.3510
(T) | Epoch=362, loss=7.7580, this epoch 1.5275, total 540.8785
(T) | Epoch=363, loss=7.7012, this epoch 1.5002, total 542.3787
(T) | Epoch=364, loss=7.7008, this epoch 1.4701, total 543.8488
(T) | Epoch=365, loss=7.7237, this epoch 1.4756, total 545.3244
(T) | Epoch=366, loss=7.6910, this epoch 1.4914, total 546.8158
(T) | Epoch=367, loss=7.7336, this epoch 1.4728, total 548.2886
(T) | Epoch=368, loss=7.6818, this epoch 1.4835, total 549.7722
(T) | Epoch=369, loss=7.8351, this epoch 1.4621, total 551.2342
(T) | Epoch=370, loss=7.7580, this epoch 1.5049, total 552.7391
(T) | Epoch=371, loss=7.6759, this epoch 1.5352, total 554.2743
(T) | Epoch=372, loss=7.6740, this epoch 1.4690, total 555.7433
(T) | Epoch=373, loss=7.6948, this epoch 1.5432, total 557.2865
(T) | Epoch=374, loss=7.6699, this epoch 1.5622, total 558.8487
(T) | Epoch=375, loss=7.6610, this epoch 1.5100, total 560.3587
(T) | Epoch=376, loss=7.8949, this epoch 1.5069, total 561.8656
(T) | Epoch=377, loss=7.6607, this epoch 1.5223, total 563.3878
(T) | Epoch=378, loss=7.6540, this epoch 1.5422, total 564.9300
(T) | Epoch=379, loss=7.6588, this epoch 1.4830, total 566.4130
(T) | Epoch=380, loss=7.6600, this epoch 1.4926, total 567.9056
(T) | Epoch=381, loss=7.6927, this epoch 1.5076, total 569.4132
(T) | Epoch=382, loss=7.6894, this epoch 1.5292, total 570.9424
(T) | Epoch=383, loss=7.6950, this epoch 1.5094, total 572.4519
(T) | Epoch=384, loss=7.6705, this epoch 1.4915, total 573.9433
(T) | Epoch=385, loss=7.6422, this epoch 1.5215, total 575.4648
(T) | Epoch=386, loss=7.6419, this epoch 1.5165, total 576.9813
(T) | Epoch=387, loss=7.6368, this epoch 1.5302, total 578.5114
(T) | Epoch=388, loss=7.6347, this epoch 1.5252, total 580.0366
(T) | Epoch=389, loss=7.6315, this epoch 1.5388, total 581.5754
(T) | Epoch=390, loss=7.6342, this epoch 1.4736, total 583.0490
(T) | Epoch=391, loss=7.6326, this epoch 1.5017, total 584.5507
(T) | Epoch=392, loss=7.6625, this epoch 1.5253, total 586.0760
(T) | Epoch=393, loss=7.6289, this epoch 1.5444, total 587.6205
(T) | Epoch=394, loss=7.6257, this epoch 1.4799, total 589.1004
(T) | Epoch=395, loss=7.6796, this epoch 1.4816, total 590.5819
(T) | Epoch=396, loss=7.6172, this epoch 1.5213, total 592.1032
(T) | Epoch=397, loss=7.6148, this epoch 1.5328, total 593.6360
(T) | Epoch=398, loss=7.6550, this epoch 1.4740, total 595.1100
(T) | Epoch=399, loss=7.6177, this epoch 1.5025, total 596.6125
(T) | Epoch=400, loss=7.6535, this epoch 1.4714, total 598.0839
(T) | Epoch=401, loss=7.6070, this epoch 1.5212, total 599.6051
(T) | Epoch=402, loss=7.6853, this epoch 1.5249, total 601.1301
(T) | Epoch=403, loss=7.7738, this epoch 1.4349, total 602.5650
(T) | Epoch=404, loss=7.6505, this epoch 1.4484, total 604.0134
(T) | Epoch=405, loss=7.6475, this epoch 1.4823, total 605.4957
(T) | Epoch=406, loss=7.6837, this epoch 1.4649, total 606.9606
(T) | Epoch=407, loss=7.6217, this epoch 1.4844, total 608.4450
(T) | Epoch=408, loss=7.6186, this epoch 1.4970, total 609.9421
(T) | Epoch=409, loss=7.6610, this epoch 1.4598, total 611.4019
(T) | Epoch=410, loss=7.6358, this epoch 1.4923, total 612.8942
(T) | Epoch=411, loss=7.6181, this epoch 1.4804, total 614.3746
(T) | Epoch=412, loss=7.6446, this epoch 1.5121, total 615.8866
(T) | Epoch=413, loss=7.6595, this epoch 1.4501, total 617.3367
(T) | Epoch=414, loss=7.6464, this epoch 1.4581, total 618.7948
(T) | Epoch=415, loss=7.6406, this epoch 1.4606, total 620.2554
(T) | Epoch=416, loss=7.6049, this epoch 1.5368, total 621.7923
(T) | Epoch=417, loss=7.6072, this epoch 1.4952, total 623.2874
(T) | Epoch=418, loss=7.6069, this epoch 1.4730, total 624.7604
(T) | Epoch=419, loss=7.6294, this epoch 1.4859, total 626.2463
(T) | Epoch=420, loss=7.5997, this epoch 1.4980, total 627.7443
(T) | Epoch=421, loss=7.6541, this epoch 1.4922, total 629.2365
(T) | Epoch=422, loss=7.6473, this epoch 1.4702, total 630.7067
(T) | Epoch=423, loss=7.6440, this epoch 1.5056, total 632.2122
(T) | Epoch=424, loss=7.6294, this epoch 1.5247, total 633.7369
(T) | Epoch=425, loss=7.5835, this epoch 1.5291, total 635.2660
(T) | Epoch=426, loss=7.6154, this epoch 1.5073, total 636.7733
(T) | Epoch=427, loss=7.5951, this epoch 1.5058, total 638.2791
(T) | Epoch=428, loss=7.6772, this epoch 1.4409, total 639.7200
(T) | Epoch=429, loss=7.5929, this epoch 1.5336, total 641.2536
(T) | Epoch=430, loss=7.5881, this epoch 1.5304, total 642.7840
(T) | Epoch=431, loss=7.5790, this epoch 1.5501, total 644.3341
(T) | Epoch=432, loss=7.6305, this epoch 1.5293, total 645.8633
(T) | Epoch=433, loss=7.5647, this epoch 1.4999, total 647.3632
(T) | Epoch=434, loss=7.5661, this epoch 1.5159, total 648.8791
(T) | Epoch=435, loss=8.8555, this epoch 1.4986, total 650.3777
(T) | Epoch=436, loss=7.5970, this epoch 1.5039, total 651.8816
(T) | Epoch=437, loss=7.6141, this epoch 1.5159, total 653.3975
(T) | Epoch=438, loss=7.6272, this epoch 1.5213, total 654.9188
(T) | Epoch=439, loss=7.6476, this epoch 1.5278, total 656.4466
(T) | Epoch=440, loss=7.6558, this epoch 1.5030, total 657.9496
(T) | Epoch=441, loss=7.6616, this epoch 1.4872, total 659.4367
(T) | Epoch=442, loss=7.7359, this epoch 1.5220, total 660.9587
(T) | Epoch=443, loss=7.6688, this epoch 1.4775, total 662.4362
(T) | Epoch=444, loss=7.7759, this epoch 1.4405, total 663.8767
(T) | Epoch=445, loss=7.6888, this epoch 1.4802, total 665.3569
(T) | Epoch=446, loss=7.6943, this epoch 1.5194, total 666.8763
(T) | Epoch=447, loss=7.6920, this epoch 1.5024, total 668.3787
(T) | Epoch=448, loss=7.7044, this epoch 1.4797, total 669.8584
(T) | Epoch=449, loss=7.7629, this epoch 1.4696, total 671.3280
(T) | Epoch=450, loss=7.7834, this epoch 1.4749, total 672.8029
(T) | Epoch=451, loss=7.7058, this epoch 1.5143, total 674.3171
(T) | Epoch=452, loss=7.7130, this epoch 1.5380, total 675.8552
(T) | Epoch=453, loss=7.7006, this epoch 1.5481, total 677.4033
(T) | Epoch=454, loss=7.8071, this epoch 1.5078, total 678.9111
(T) | Epoch=455, loss=7.7116, this epoch 1.5126, total 680.4237
(T) | Epoch=456, loss=7.6989, this epoch 1.5400, total 681.9637
(T) | Epoch=457, loss=7.7952, this epoch 1.5245, total 683.4882
(T) | Epoch=458, loss=7.7380, this epoch 1.5297, total 685.0178
(T) | Epoch=459, loss=7.6934, this epoch 1.5547, total 686.5725
(T) | Epoch=460, loss=7.7487, this epoch 1.5711, total 688.1436
(T) | Epoch=461, loss=7.6262, this epoch 1.5185, total 689.6621
(T) | Epoch=462, loss=7.6856, this epoch 1.5219, total 691.1840
(T) | Epoch=463, loss=7.7327, this epoch 1.5625, total 692.7465
(T) | Epoch=464, loss=7.7354, this epoch 1.5361, total 694.2826
(T) | Epoch=465, loss=7.6769, this epoch 1.4711, total 695.7537
(T) | Epoch=466, loss=7.6861, this epoch 1.5262, total 697.2799
(T) | Epoch=467, loss=7.6730, this epoch 1.5478, total 698.8277
(T) | Epoch=468, loss=7.7102, this epoch 1.5192, total 700.3469
(T) | Epoch=469, loss=7.7181, this epoch 1.5170, total 701.8639
(T) | Epoch=470, loss=7.6789, this epoch 1.5081, total 703.3720
(T) | Epoch=471, loss=7.6631, this epoch 1.5105, total 704.8825
(T) | Epoch=472, loss=7.6953, this epoch 1.5399, total 706.4224
(T) | Epoch=473, loss=7.6945, this epoch 1.5709, total 707.9932
(T) | Epoch=474, loss=7.6669, this epoch 1.5476, total 709.5408
(T) | Epoch=475, loss=7.7069, this epoch 1.5168, total 711.0576
(T) | Epoch=476, loss=7.6591, this epoch 1.5192, total 712.5768
(T) | Epoch=477, loss=7.8788, this epoch 1.5102, total 714.0870
(T) | Epoch=478, loss=7.6518, this epoch 1.4599, total 715.5469
(T) | Epoch=479, loss=7.6897, this epoch 1.5253, total 717.0722
(T) | Epoch=480, loss=7.6492, this epoch 1.4790, total 718.5512
(T) | Epoch=481, loss=7.6469, this epoch 1.4933, total 720.0445
(T) | Epoch=482, loss=7.6953, this epoch 1.5114, total 721.5560
(T) | Epoch=483, loss=7.6424, this epoch 1.4475, total 723.0035
(T) | Epoch=484, loss=7.6413, this epoch 1.4474, total 724.4509
(T) | Epoch=485, loss=7.6418, this epoch 1.4842, total 725.9351
(T) | Epoch=486, loss=7.6354, this epoch 1.4784, total 727.4135
(T) | Epoch=487, loss=7.6761, this epoch 1.5579, total 728.9714
(T) | Epoch=488, loss=7.6678, this epoch 1.4694, total 730.4408
(T) | Epoch=489, loss=7.6692, this epoch 1.5299, total 731.9708
(T) | Epoch=490, loss=7.6350, this epoch 1.4803, total 733.4511
(T) | Epoch=491, loss=7.7432, this epoch 1.5284, total 734.9795
(T) | Epoch=492, loss=7.6367, this epoch 1.5078, total 736.4873
(T) | Epoch=493, loss=7.6636, this epoch 1.5056, total 737.9929
(T) | Epoch=494, loss=7.6340, this epoch 1.4738, total 739.4666
(T) | Epoch=495, loss=7.7777, this epoch 1.5406, total 741.0073
(T) | Epoch=496, loss=7.6382, this epoch 1.5014, total 742.5087
(T) | Epoch=497, loss=7.6332, this epoch 1.4800, total 743.9887
(T) | Epoch=498, loss=7.6808, this epoch 1.4867, total 745.4754
(T) | Epoch=499, loss=7.6218, this epoch 1.5010, total 746.9764
(T) | Epoch=500, loss=7.6256, this epoch 1.5125, total 748.4889
(T) | Epoch=501, loss=7.6146, this epoch 1.4995, total 749.9884
(T) | Epoch=502, loss=7.6180, this epoch 1.4876, total 751.4760
(T) | Epoch=503, loss=7.6192, this epoch 1.4774, total 752.9534
(T) | Epoch=504, loss=7.6568, this epoch 1.5015, total 754.4548
(T) | Epoch=505, loss=7.6222, this epoch 1.5329, total 755.9877
(T) | Epoch=506, loss=7.6270, this epoch 1.5097, total 757.4974
(T) | Epoch=507, loss=7.7611, this epoch 1.4943, total 758.9917
(T) | Epoch=508, loss=7.6683, this epoch 1.5142, total 760.5059
(T) | Epoch=509, loss=7.6216, this epoch 1.4440, total 761.9499
(T) | Epoch=510, loss=7.6057, this epoch 1.4988, total 763.4487
(T) | Epoch=511, loss=7.5973, this epoch 1.4981, total 764.9468
(T) | Epoch=512, loss=7.6436, this epoch 1.4721, total 766.4189
(T) | Epoch=513, loss=7.6028, this epoch 1.4694, total 767.8883
(T) | Epoch=514, loss=7.6421, this epoch 1.5320, total 769.4203
(T) | Epoch=515, loss=7.6110, this epoch 1.5254, total 770.9457
(T) | Epoch=516, loss=7.5971, this epoch 1.5141, total 772.4598
(T) | Epoch=517, loss=7.5968, this epoch 1.4782, total 773.9380
(T) | Epoch=518, loss=7.5976, this epoch 1.4772, total 775.4152
(T) | Epoch=519, loss=7.6575, this epoch 1.4765, total 776.8917
(T) | Epoch=520, loss=7.5918, this epoch 1.4509, total 778.3426
(T) | Epoch=521, loss=7.8254, this epoch 1.5317, total 779.8742
(T) | Epoch=522, loss=7.5873, this epoch 1.5385, total 781.4127
(T) | Epoch=523, loss=7.6075, this epoch 1.5049, total 782.9177
(T) | Epoch=524, loss=7.5892, this epoch 1.4716, total 784.3893
(T) | Epoch=525, loss=7.5842, this epoch 1.4754, total 785.8647
(T) | Epoch=526, loss=7.5914, this epoch 1.4757, total 787.3404
(T) | Epoch=527, loss=7.5829, this epoch 1.4749, total 788.8152
(T) | Epoch=528, loss=7.6304, this epoch 1.5427, total 790.3579
(T) | Epoch=529, loss=7.6188, this epoch 1.5229, total 791.8808
(T) | Epoch=530, loss=7.5749, this epoch 1.4852, total 793.3660
(T) | Epoch=531, loss=7.5748, this epoch 1.5130, total 794.8790
(T) | Epoch=532, loss=7.5735, this epoch 1.4882, total 796.3671
(T) | Epoch=533, loss=7.5773, this epoch 1.4753, total 797.8425
(T) | Epoch=534, loss=7.5610, this epoch 1.4410, total 799.2835
(T) | Epoch=535, loss=7.5635, this epoch 1.4781, total 800.7615
(T) | Epoch=536, loss=7.5694, this epoch 1.4902, total 802.2517
(T) | Epoch=537, loss=7.5600, this epoch 1.4583, total 803.7101
(T) | Epoch=538, loss=7.5680, this epoch 1.5281, total 805.2381
(T) | Epoch=539, loss=7.6645, this epoch 1.5406, total 806.7787
(T) | Epoch=540, loss=7.5580, this epoch 1.4927, total 808.2714
(T) | Epoch=541, loss=7.6246, this epoch 1.5216, total 809.7931
(T) | Epoch=542, loss=7.6086, this epoch 1.5189, total 811.3119
(T) | Epoch=543, loss=7.5702, this epoch 1.5271, total 812.8390
(T) | Epoch=544, loss=7.5940, this epoch 1.5130, total 814.3520
(T) | Epoch=545, loss=7.6040, this epoch 1.4708, total 815.8228
(T) | Epoch=546, loss=7.6244, this epoch 1.5005, total 817.3233
(T) | Epoch=547, loss=7.5849, this epoch 1.4806, total 818.8040
(T) | Epoch=548, loss=7.6130, this epoch 1.4919, total 820.2958
(T) | Epoch=549, loss=7.5738, this epoch 1.5008, total 821.7966
(T) | Epoch=550, loss=7.5826, this epoch 1.5292, total 823.3259
(T) | Epoch=551, loss=7.6111, this epoch 1.4894, total 824.8153
(T) | Epoch=552, loss=7.5868, this epoch 1.5110, total 826.3263
(T) | Epoch=553, loss=7.5866, this epoch 1.4561, total 827.7824
(T) | Epoch=554, loss=7.5646, this epoch 1.4617, total 829.2441
(T) | Epoch=555, loss=7.5692, this epoch 1.5309, total 830.7750
(T) | Epoch=556, loss=7.6203, this epoch 1.5230, total 832.2980
(T) | Epoch=557, loss=7.5584, this epoch 1.4748, total 833.7728
(T) | Epoch=558, loss=7.5464, this epoch 1.4768, total 835.2496
+++model saved ! 2016.pth
(T) | Epoch=559, loss=7.5851, this epoch 1.5308, total 836.7804
(T) | Epoch=560, loss=7.6410, this epoch 1.5215, total 838.3019
(T) | Epoch=561, loss=7.6574, this epoch 1.5166, total 839.8185
(T) | Epoch=562, loss=7.6625, this epoch 1.5025, total 841.3209
(T) | Epoch=563, loss=7.5632, this epoch 1.4896, total 842.8106
(T) | Epoch=564, loss=7.5562, this epoch 1.5331, total 844.3437
(T) | Epoch=565, loss=7.5854, this epoch 1.5517, total 845.8953
(T) | Epoch=566, loss=7.6233, this epoch 1.5298, total 847.4251
(T) | Epoch=567, loss=7.6616, this epoch 1.5280, total 848.9531
(T) | Epoch=568, loss=7.6135, this epoch 1.5004, total 850.4534
(T) | Epoch=569, loss=7.5859, this epoch 1.5062, total 851.9596
(T) | Epoch=570, loss=7.6316, this epoch 1.4688, total 853.4285
(T) | Epoch=571, loss=7.6261, this epoch 1.4471, total 854.8755
(T) | Epoch=572, loss=7.5836, this epoch 1.5222, total 856.3977
(T) | Epoch=573, loss=7.5971, this epoch 1.5743, total 857.9720
(T) | Epoch=574, loss=7.5732, this epoch 1.5107, total 859.4827
(T) | Epoch=575, loss=7.6232, this epoch 1.4789, total 860.9616
(T) | Epoch=576, loss=7.5735, this epoch 1.4867, total 862.4483
(T) | Epoch=577, loss=7.6003, this epoch 1.5365, total 863.9848
(T) | Epoch=578, loss=7.5478, this epoch 1.4749, total 865.4597
(T) | Epoch=579, loss=7.5747, this epoch 1.4896, total 866.9493
(T) | Epoch=580, loss=7.5865, this epoch 1.5001, total 868.4495
(T) | Epoch=581, loss=7.5419, this epoch 1.4964, total 869.9459
+++model saved ! 2016.pth
(T) | Epoch=582, loss=7.5377, this epoch 1.4598, total 871.4057
+++model saved ! 2016.pth
(T) | Epoch=583, loss=7.5394, this epoch 1.4603, total 872.8660
(T) | Epoch=584, loss=7.5436, this epoch 1.5249, total 874.3909
(T) | Epoch=585, loss=7.6660, this epoch 1.4775, total 875.8683
(T) | Epoch=586, loss=7.6943, this epoch 1.5684, total 877.4368
(T) | Epoch=587, loss=7.5414, this epoch 1.4784, total 878.9152
(T) | Epoch=588, loss=7.5496, this epoch 1.5308, total 880.4459
(T) | Epoch=589, loss=7.6015, this epoch 1.5653, total 882.0112
(T) | Epoch=590, loss=7.5909, this epoch 1.5259, total 883.5371
(T) | Epoch=591, loss=7.5737, this epoch 1.5253, total 885.0624
(T) | Epoch=592, loss=7.5666, this epoch 1.4410, total 886.5034
(T) | Epoch=593, loss=7.6187, this epoch 1.4653, total 887.9687
(T) | Epoch=594, loss=7.5772, this epoch 1.5104, total 889.4791
(T) | Epoch=595, loss=7.5701, this epoch 1.5673, total 891.0464
(T) | Epoch=596, loss=7.5685, this epoch 1.5993, total 892.6457
(T) | Epoch=597, loss=7.7428, this epoch 1.4864, total 894.1322
(T) | Epoch=598, loss=7.6816, this epoch 1.4729, total 895.6051
(T) | Epoch=599, loss=7.5591, this epoch 1.5487, total 897.1538
(T) | Epoch=600, loss=7.5543, this epoch 1.5288, total 898.6826
(T) | Epoch=601, loss=7.5813, this epoch 1.5517, total 900.2342
(T) | Epoch=602, loss=7.5786, this epoch 1.5196, total 901.7539
(T) | Epoch=603, loss=7.5394, this epoch 1.5167, total 903.2706
(T) | Epoch=604, loss=7.5837, this epoch 1.4774, total 904.7479
(T) | Epoch=605, loss=7.6497, this epoch 1.5281, total 906.2760
(T) | Epoch=606, loss=7.6468, this epoch 1.4739, total 907.7499
(T) | Epoch=607, loss=7.5772, this epoch 1.5179, total 909.2677
(T) | Epoch=608, loss=7.5956, this epoch 1.4842, total 910.7519
(T) | Epoch=609, loss=7.5851, this epoch 1.5846, total 912.3365
(T) | Epoch=610, loss=7.5447, this epoch 1.4743, total 913.8108
(T) | Epoch=611, loss=7.5901, this epoch 1.5416, total 915.3523
(T) | Epoch=612, loss=7.5597, this epoch 1.4824, total 916.8348
(T) | Epoch=613, loss=7.5652, this epoch 1.5317, total 918.3665
(T) | Epoch=614, loss=7.6075, this epoch 1.5200, total 919.8865
(T) | Epoch=615, loss=7.5424, this epoch 1.5016, total 921.3881
(T) | Epoch=616, loss=7.5880, this epoch 1.5307, total 922.9187
(T) | Epoch=617, loss=7.5568, this epoch 1.5215, total 924.4402
(T) | Epoch=618, loss=7.9246, this epoch 1.4599, total 925.9001
(T) | Epoch=619, loss=7.5886, this epoch 1.5231, total 927.4232
(T) | Epoch=620, loss=7.5779, this epoch 1.5137, total 928.9369
(T) | Epoch=621, loss=7.6063, this epoch 1.4907, total 930.4276
(T) | Epoch=622, loss=7.7040, this epoch 1.5527, total 931.9804
(T) | Epoch=623, loss=7.6573, this epoch 1.5284, total 933.5087
(T) | Epoch=624, loss=7.6209, this epoch 1.4541, total 934.9628
(T) | Epoch=625, loss=7.7462, this epoch 1.5367, total 936.4995
(T) | Epoch=626, loss=7.7053, this epoch 1.4936, total 937.9932
(T) | Epoch=627, loss=7.7241, this epoch 1.5565, total 939.5497
(T) | Epoch=628, loss=7.6715, this epoch 1.5506, total 941.1003
(T) | Epoch=629, loss=7.7613, this epoch 1.4786, total 942.5789
(T) | Epoch=630, loss=7.6643, this epoch 1.5429, total 944.1219
(T) | Epoch=631, loss=7.6158, this epoch 1.5933, total 945.7152
(T) | Epoch=632, loss=7.6885, this epoch 1.5762, total 947.2913
(T) | Epoch=633, loss=7.7158, this epoch 1.5595, total 948.8508
(T) | Epoch=634, loss=7.6617, this epoch 1.4791, total 950.3299
(T) | Epoch=635, loss=7.6372, this epoch 1.5472, total 951.8771
(T) | Epoch=636, loss=7.5970, this epoch 1.5090, total 953.3861
(T) | Epoch=637, loss=7.5732, this epoch 1.5024, total 954.8885
(T) | Epoch=638, loss=7.5922, this epoch 1.5126, total 956.4011
(T) | Epoch=639, loss=7.5744, this epoch 1.5014, total 957.9026
(T) | Epoch=640, loss=7.5679, this epoch 1.5009, total 959.4035
(T) | Epoch=641, loss=7.5585, this epoch 1.4903, total 960.8938
(T) | Epoch=642, loss=7.5652, this epoch 1.4838, total 962.3776
(T) | Epoch=643, loss=7.5579, this epoch 1.5174, total 963.8950
(T) | Epoch=644, loss=7.5418, this epoch 1.4781, total 965.3730
(T) | Epoch=645, loss=7.5535, this epoch 1.4654, total 966.8384
(T) | Epoch=646, loss=7.6644, this epoch 1.5191, total 968.3575
(T) | Epoch=647, loss=7.5938, this epoch 1.5587, total 969.9162
(T) | Epoch=648, loss=7.5303, this epoch 1.5582, total 971.4744
+++model saved ! 2016.pth
(T) | Epoch=649, loss=7.5369, this epoch 1.5522, total 973.0265
(T) | Epoch=650, loss=7.5302, this epoch 1.5466, total 974.5731
+++model saved ! 2016.pth
(T) | Epoch=651, loss=7.7184, this epoch 1.5435, total 976.1166
(T) | Epoch=652, loss=7.5743, this epoch 1.5104, total 977.6270
(T) | Epoch=653, loss=7.5901, this epoch 1.4712, total 979.0983
(T) | Epoch=654, loss=7.5366, this epoch 1.5306, total 980.6288
(T) | Epoch=655, loss=7.5400, this epoch 1.5523, total 982.1811
(T) | Epoch=656, loss=7.5406, this epoch 1.5372, total 983.7183
(T) | Epoch=657, loss=7.5509, this epoch 1.5807, total 985.2990
(T) | Epoch=658, loss=7.5420, this epoch 1.5528, total 986.8518
(T) | Epoch=659, loss=7.5411, this epoch 1.5631, total 988.4149
(T) | Epoch=660, loss=7.5595, this epoch 1.5698, total 989.9847
(T) | Epoch=661, loss=7.5507, this epoch 1.5419, total 991.5266
(T) | Epoch=662, loss=7.5366, this epoch 1.5102, total 993.0367
(T) | Epoch=663, loss=7.5315, this epoch 1.5020, total 994.5387
(T) | Epoch=664, loss=7.5941, this epoch 1.4722, total 996.0109
(T) | Epoch=665, loss=7.5324, this epoch 1.4801, total 997.4911
(T) | Epoch=666, loss=7.5218, this epoch 1.4688, total 998.9599
+++model saved ! 2016.pth
(T) | Epoch=667, loss=7.5428, this epoch 1.4847, total 1000.4446
(T) | Epoch=668, loss=7.5979, this epoch 1.4795, total 1001.9241
(T) | Epoch=669, loss=7.5203, this epoch 1.5031, total 1003.4272
+++model saved ! 2016.pth
(T) | Epoch=670, loss=7.5354, this epoch 1.4732, total 1004.9004
(T) | Epoch=671, loss=7.5235, this epoch 1.4930, total 1006.3934
(T) | Epoch=672, loss=7.5090, this epoch 1.5176, total 1007.9110
+++model saved ! 2016.pth
(T) | Epoch=673, loss=7.6084, this epoch 1.4834, total 1009.3945
(T) | Epoch=674, loss=7.6919, this epoch 1.5284, total 1010.9229
(T) | Epoch=675, loss=7.6284, this epoch 1.5514, total 1012.4743
(T) | Epoch=676, loss=7.5683, this epoch 1.5107, total 1013.9850
(T) | Epoch=677, loss=7.5386, this epoch 1.5057, total 1015.4907
(T) | Epoch=678, loss=7.5375, this epoch 1.4271, total 1016.9178
(T) | Epoch=679, loss=7.6165, this epoch 1.5054, total 1018.4231
(T) | Epoch=680, loss=7.5592, this epoch 1.4566, total 1019.8797
(T) | Epoch=681, loss=7.5891, this epoch 1.5078, total 1021.3875
(T) | Epoch=682, loss=7.6065, this epoch 1.4307, total 1022.8181
(T) | Epoch=683, loss=7.5375, this epoch 1.4385, total 1024.2566
(T) | Epoch=684, loss=7.5520, this epoch 1.5263, total 1025.7829
(T) | Epoch=685, loss=7.5973, this epoch 1.5182, total 1027.3011
(T) | Epoch=686, loss=7.5370, this epoch 1.4949, total 1028.7960
(T) | Epoch=687, loss=7.5928, this epoch 1.4903, total 1030.2863
(T) | Epoch=688, loss=7.5405, this epoch 1.4636, total 1031.7499
(T) | Epoch=689, loss=7.5507, this epoch 1.4751, total 1033.2250
(T) | Epoch=690, loss=7.5025, this epoch 1.4924, total 1034.7174
+++model saved ! 2016.pth
(T) | Epoch=691, loss=7.5100, this epoch 1.5319, total 1036.2492
(T) | Epoch=692, loss=7.5161, this epoch 1.5275, total 1037.7767
(T) | Epoch=693, loss=7.5075, this epoch 1.5613, total 1039.3380
(T) | Epoch=694, loss=7.5037, this epoch 1.5095, total 1040.8475
(T) | Epoch=695, loss=7.6855, this epoch 1.4826, total 1042.3301
(T) | Epoch=696, loss=7.6191, this epoch 1.5172, total 1043.8473
(T) | Epoch=697, loss=7.4859, this epoch 1.4946, total 1045.3419
+++model saved ! 2016.pth
(T) | Epoch=698, loss=7.4998, this epoch 1.4820, total 1046.8239
(T) | Epoch=699, loss=7.6113, this epoch 1.5126, total 1048.3366
(T) | Epoch=700, loss=7.5673, this epoch 1.4804, total 1049.8169
(T) | Epoch=701, loss=7.6014, this epoch 1.4986, total 1051.3156
(T) | Epoch=702, loss=7.5841, this epoch 1.4797, total 1052.7953
(T) | Epoch=703, loss=7.5623, this epoch 1.4842, total 1054.2794
(T) | Epoch=704, loss=7.5112, this epoch 1.4689, total 1055.7483
(T) | Epoch=705, loss=7.5196, this epoch 1.5273, total 1057.2756
(T) | Epoch=706, loss=7.5050, this epoch 1.5543, total 1058.8299
(T) | Epoch=707, loss=7.5091, this epoch 1.4982, total 1060.3281
(T) | Epoch=708, loss=7.5170, this epoch 1.4474, total 1061.7755
(T) | Epoch=709, loss=7.4916, this epoch 1.5123, total 1063.2878
(T) | Epoch=710, loss=7.4894, this epoch 1.4927, total 1064.7805
(T) | Epoch=711, loss=7.4918, this epoch 1.5031, total 1066.2836
(T) | Epoch=712, loss=7.4801, this epoch 1.4658, total 1067.7494
+++model saved ! 2016.pth
(T) | Epoch=713, loss=7.4883, this epoch 1.5135, total 1069.2629
(T) | Epoch=714, loss=7.5451, this epoch 1.5026, total 1070.7655
(T) | Epoch=715, loss=7.5167, this epoch 1.5148, total 1072.2803
(T) | Epoch=716, loss=7.7756, this epoch 1.5479, total 1073.8282
(T) | Epoch=717, loss=7.5639, this epoch 1.5313, total 1075.3595
(T) | Epoch=718, loss=7.5330, this epoch 1.4984, total 1076.8578
(T) | Epoch=719, loss=7.4973, this epoch 1.4606, total 1078.3184
(T) | Epoch=720, loss=7.5384, this epoch 1.4703, total 1079.7887
(T) | Epoch=721, loss=7.5110, this epoch 1.4947, total 1081.2834
(T) | Epoch=722, loss=7.5924, this epoch 1.5226, total 1082.8059
(T) | Epoch=723, loss=7.6101, this epoch 1.4649, total 1084.2709
(T) | Epoch=724, loss=7.6637, this epoch 1.5459, total 1085.8168
(T) | Epoch=725, loss=7.5691, this epoch 1.4790, total 1087.2958
(T) | Epoch=726, loss=7.5562, this epoch 1.4799, total 1088.7757
(T) | Epoch=727, loss=7.5286, this epoch 1.4999, total 1090.2756
(T) | Epoch=728, loss=7.6278, this epoch 1.5149, total 1091.7905
(T) | Epoch=729, loss=7.4906, this epoch 1.4733, total 1093.2638
(T) | Epoch=730, loss=7.5124, this epoch 1.4592, total 1094.7230
(T) | Epoch=731, loss=7.5713, this epoch 1.5129, total 1096.2359
(T) | Epoch=732, loss=7.5772, this epoch 1.4944, total 1097.7302
(T) | Epoch=733, loss=7.4962, this epoch 1.4760, total 1099.2062
(T) | Epoch=734, loss=7.6442, this epoch 1.4961, total 1100.7023
(T) | Epoch=735, loss=7.5288, this epoch 1.4817, total 1102.1840
(T) | Epoch=736, loss=7.6173, this epoch 1.4477, total 1103.6317
(T) | Epoch=737, loss=7.4920, this epoch 1.4951, total 1105.1268
(T) | Epoch=738, loss=7.4910, this epoch 1.4646, total 1106.5914
(T) | Epoch=739, loss=7.4957, this epoch 1.4668, total 1108.0582
(T) | Epoch=740, loss=7.5523, this epoch 1.4402, total 1109.4984
(T) | Epoch=741, loss=7.4833, this epoch 1.5033, total 1111.0017
(T) | Epoch=742, loss=7.4919, this epoch 1.4849, total 1112.4866
(T) | Epoch=743, loss=7.5959, this epoch 1.4974, total 1113.9840
(T) | Epoch=744, loss=7.5345, this epoch 1.5452, total 1115.5292
(T) | Epoch=745, loss=7.5529, this epoch 1.5532, total 1117.0824
(T) | Epoch=746, loss=7.6533, this epoch 1.5350, total 1118.6173
(T) | Epoch=747, loss=7.5002, this epoch 1.4968, total 1120.1142
(T) | Epoch=748, loss=7.6444, this epoch 1.5177, total 1121.6319
(T) | Epoch=749, loss=7.5042, this epoch 1.5167, total 1123.1486
(T) | Epoch=750, loss=7.4901, this epoch 1.5547, total 1124.7033
(T) | Epoch=751, loss=7.4888, this epoch 1.5597, total 1126.2630
(T) | Epoch=752, loss=7.4750, this epoch 1.5360, total 1127.7990
+++model saved ! 2016.pth
(T) | Epoch=753, loss=7.5470, this epoch 1.5357, total 1129.3347
(T) | Epoch=754, loss=7.5540, this epoch 1.5368, total 1130.8715
(T) | Epoch=755, loss=7.4729, this epoch 1.5285, total 1132.3999
+++model saved ! 2016.pth
(T) | Epoch=756, loss=7.5052, this epoch 1.5248, total 1133.9248
(T) | Epoch=757, loss=7.4819, this epoch 1.5115, total 1135.4363
(T) | Epoch=758, loss=7.6917, this epoch 1.4860, total 1136.9223
(T) | Epoch=759, loss=7.5317, this epoch 1.4819, total 1138.4042
(T) | Epoch=760, loss=7.4822, this epoch 1.5000, total 1139.9042
(T) | Epoch=761, loss=7.4894, this epoch 1.4878, total 1141.3920
(T) | Epoch=762, loss=7.5406, this epoch 1.5169, total 1142.9089
(T) | Epoch=763, loss=7.5323, this epoch 1.4580, total 1144.3669
(T) | Epoch=764, loss=7.6193, this epoch 1.4441, total 1145.8110
(T) | Epoch=765, loss=7.5374, this epoch 1.5068, total 1147.3178
(T) | Epoch=766, loss=7.5751, this epoch 1.5301, total 1148.8479
(T) | Epoch=767, loss=7.5834, this epoch 1.4914, total 1150.3393
(T) | Epoch=768, loss=7.7935, this epoch 1.4624, total 1151.8017
(T) | Epoch=769, loss=7.5117, this epoch 1.5173, total 1153.3190
(T) | Epoch=770, loss=7.5153, this epoch 1.4824, total 1154.8014
(T) | Epoch=771, loss=7.5309, this epoch 1.4166, total 1156.2180
(T) | Epoch=772, loss=7.6173, this epoch 1.4563, total 1157.6743
(T) | Epoch=773, loss=7.5414, this epoch 1.5074, total 1159.1817
(T) | Epoch=774, loss=7.5881, this epoch 1.4174, total 1160.5991
(T) | Epoch=775, loss=7.4857, this epoch 1.4451, total 1162.0442
(T) | Epoch=776, loss=7.4799, this epoch 1.4353, total 1163.4796
(T) | Epoch=777, loss=7.5660, this epoch 1.5138, total 1164.9933
(T) | Epoch=778, loss=7.4705, this epoch 1.5166, total 1166.5099
+++model saved ! 2016.pth
(T) | Epoch=779, loss=7.5744, this epoch 1.5503, total 1168.0602
(T) | Epoch=780, loss=7.4698, this epoch 1.5225, total 1169.5827
+++model saved ! 2016.pth
(T) | Epoch=781, loss=7.5405, this epoch 1.4860, total 1171.0687
(T) | Epoch=782, loss=7.4797, this epoch 1.5547, total 1172.6234
(T) | Epoch=783, loss=7.6766, this epoch 1.5080, total 1174.1314
(T) | Epoch=784, loss=7.4664, this epoch 1.4962, total 1175.6276
+++model saved ! 2016.pth
(T) | Epoch=785, loss=7.5509, this epoch 1.4783, total 1177.1060
(T) | Epoch=786, loss=7.4781, this epoch 1.5041, total 1178.6101
(T) | Epoch=787, loss=7.4693, this epoch 1.4707, total 1180.0808
(T) | Epoch=788, loss=7.4842, this epoch 1.4819, total 1181.5626
(T) | Epoch=789, loss=7.5137, this epoch 1.5037, total 1183.0664
(T) | Epoch=790, loss=7.5145, this epoch 1.6073, total 1184.6736
(T) | Epoch=791, loss=7.6304, this epoch 1.5925, total 1186.2661
(T) | Epoch=792, loss=7.5807, this epoch 1.6388, total 1187.9049
(T) | Epoch=793, loss=7.5676, this epoch 1.6540, total 1189.5589
(T) | Epoch=794, loss=7.5561, this epoch 1.6335, total 1191.1924
(T) | Epoch=795, loss=7.4869, this epoch 1.5580, total 1192.7504
(T) | Epoch=796, loss=7.4931, this epoch 1.4937, total 1194.2441
(T) | Epoch=797, loss=7.5260, this epoch 1.5225, total 1195.7666
(T) | Epoch=798, loss=7.4712, this epoch 1.4682, total 1197.2348
(T) | Epoch=799, loss=7.5479, this epoch 1.4476, total 1198.6824
(T) | Epoch=800, loss=7.4716, this epoch 1.5969, total 1200.2793
(T) | Epoch=801, loss=7.5732, this epoch 1.5786, total 1201.8579
(T) | Epoch=802, loss=7.4689, this epoch 1.6295, total 1203.4874
(T) | Epoch=803, loss=7.4664, this epoch 1.5784, total 1205.0658
+++model saved ! 2016.pth
(T) | Epoch=804, loss=7.4937, this epoch 1.6432, total 1206.7091
(T) | Epoch=805, loss=7.4592, this epoch 1.6158, total 1208.3249
+++model saved ! 2016.pth
(T) | Epoch=806, loss=7.4727, this epoch 1.6628, total 1209.9876
(T) | Epoch=807, loss=7.4544, this epoch 1.6499, total 1211.6376
+++model saved ! 2016.pth
(T) | Epoch=808, loss=7.4552, this epoch 1.6562, total 1213.2938
(T) | Epoch=809, loss=7.4606, this epoch 1.6167, total 1214.9104
(T) | Epoch=810, loss=7.5576, this epoch 1.6125, total 1216.5229
(T) | Epoch=811, loss=7.4504, this epoch 1.6110, total 1218.1339
+++model saved ! 2016.pth
(T) | Epoch=812, loss=7.4696, this epoch 1.5987, total 1219.7326
(T) | Epoch=813, loss=7.4539, this epoch 1.5770, total 1221.3096
(T) | Epoch=814, loss=7.5601, this epoch 1.5678, total 1222.8774
(T) | Epoch=815, loss=7.4713, this epoch 1.5580, total 1224.4353
(T) | Epoch=816, loss=7.4558, this epoch 1.5551, total 1225.9905
(T) | Epoch=817, loss=7.4746, this epoch 1.5505, total 1227.5410
(T) | Epoch=818, loss=7.4876, this epoch 1.5952, total 1229.1362
(T) | Epoch=819, loss=7.4837, this epoch 1.5897, total 1230.7258
(T) | Epoch=820, loss=7.4662, this epoch 1.5927, total 1232.3186
(T) | Epoch=821, loss=7.4507, this epoch 1.6305, total 1233.9491
(T) | Epoch=822, loss=7.5482, this epoch 1.6081, total 1235.5571
(T) | Epoch=823, loss=8.8821, this epoch 1.5825, total 1237.1397
(T) | Epoch=824, loss=7.4677, this epoch 1.6134, total 1238.7531
(T) | Epoch=825, loss=7.5743, this epoch 1.5647, total 1240.3179
(T) | Epoch=826, loss=7.6496, this epoch 1.6201, total 1241.9380
(T) | Epoch=827, loss=7.6390, this epoch 1.5554, total 1243.4934
(T) | Epoch=828, loss=7.5075, this epoch 1.6381, total 1245.1315
(T) | Epoch=829, loss=7.8442, this epoch 1.5976, total 1246.7291
(T) | Epoch=830, loss=7.8455, this epoch 1.5835, total 1248.3126
(T) | Epoch=831, loss=7.6267, this epoch 1.6131, total 1249.9257
(T) | Epoch=832, loss=7.8708, this epoch 1.6311, total 1251.5568
(T) | Epoch=833, loss=7.6390, this epoch 1.6151, total 1253.1719
(T) | Epoch=834, loss=7.6361, this epoch 1.6278, total 1254.7997
(T) | Epoch=835, loss=7.6419, this epoch 1.6471, total 1256.4468
(T) | Epoch=836, loss=7.6333, this epoch 1.6599, total 1258.1067
(T) | Epoch=837, loss=7.6403, this epoch 1.6604, total 1259.7671
(T) | Epoch=838, loss=7.6808, this epoch 1.6021, total 1261.3693
(T) | Epoch=839, loss=7.6436, this epoch 1.6102, total 1262.9794
(T) | Epoch=840, loss=7.8465, this epoch 1.6267, total 1264.6062
(T) | Epoch=841, loss=7.6376, this epoch 1.5612, total 1266.1674
(T) | Epoch=842, loss=7.9338, this epoch 1.5338, total 1267.7012
(T) | Epoch=843, loss=7.6396, this epoch 1.5745, total 1269.2757
(T) | Epoch=844, loss=7.8279, this epoch 1.6222, total 1270.8979
(T) | Epoch=845, loss=7.6721, this epoch 1.6653, total 1272.5632
(T) | Epoch=846, loss=7.6277, this epoch 1.6569, total 1274.2201
(T) | Epoch=847, loss=7.6734, this epoch 1.6435, total 1275.8636
(T) | Epoch=848, loss=7.6208, this epoch 1.6295, total 1277.4931
(T) | Epoch=849, loss=7.6283, this epoch 1.6257, total 1279.1188
(T) | Epoch=850, loss=7.6647, this epoch 1.6351, total 1280.7539
(T) | Epoch=851, loss=7.7263, this epoch 1.6690, total 1282.4229
(T) | Epoch=852, loss=7.6009, this epoch 1.6380, total 1284.0609
(T) | Epoch=853, loss=7.6208, this epoch 1.6429, total 1285.7038
(T) | Epoch=854, loss=7.6169, this epoch 1.6602, total 1287.3640
(T) | Epoch=855, loss=7.5946, this epoch 1.6475, total 1289.0114
(T) | Epoch=856, loss=7.7374, this epoch 1.6561, total 1290.6676
(T) | Epoch=857, loss=7.6576, this epoch 1.5597, total 1292.2273
(T) | Epoch=858, loss=7.5710, this epoch 1.6063, total 1293.8336
(T) | Epoch=859, loss=7.5684, this epoch 1.6424, total 1295.4760
(T) | Epoch=860, loss=7.5653, this epoch 1.6691, total 1297.1451
(T) | Epoch=861, loss=7.6460, this epoch 1.6760, total 1298.8210
(T) | Epoch=862, loss=7.6017, this epoch 1.6635, total 1300.4846
(T) | Epoch=863, loss=7.5994, this epoch 1.6758, total 1302.1604
(T) | Epoch=864, loss=7.5516, this epoch 1.6494, total 1303.8098
(T) | Epoch=865, loss=7.6192, this epoch 1.6335, total 1305.4434
(T) | Epoch=866, loss=7.5535, this epoch 1.5996, total 1307.0430
(T) | Epoch=867, loss=7.6017, this epoch 1.6402, total 1308.6832
(T) | Epoch=868, loss=7.5214, this epoch 1.6499, total 1310.3331
(T) | Epoch=869, loss=7.6487, this epoch 1.6546, total 1311.9877
(T) | Epoch=870, loss=7.5530, this epoch 1.6481, total 1313.6358
(T) | Epoch=871, loss=7.5214, this epoch 1.6225, total 1315.2583
(T) | Epoch=872, loss=7.5198, this epoch 1.6301, total 1316.8884
(T) | Epoch=873, loss=7.5563, this epoch 1.5904, total 1318.4788
(T) | Epoch=874, loss=7.5525, this epoch 1.5243, total 1320.0031
(T) | Epoch=875, loss=7.5070, this epoch 1.5794, total 1321.5825
(T) | Epoch=876, loss=7.5476, this epoch 1.5617, total 1323.1442
(T) | Epoch=877, loss=7.5215, this epoch 1.6198, total 1324.7640
(T) | Epoch=878, loss=7.5331, this epoch 1.6187, total 1326.3827
(T) | Epoch=879, loss=7.5423, this epoch 1.5488, total 1327.9315
(T) | Epoch=880, loss=7.5014, this epoch 1.5735, total 1329.5050
(T) | Epoch=881, loss=7.4963, this epoch 1.5989, total 1331.1039
(T) | Epoch=882, loss=7.4871, this epoch 1.5742, total 1332.6781
(T) | Epoch=883, loss=7.5394, this epoch 1.6540, total 1334.3321
(T) | Epoch=884, loss=7.5239, this epoch 1.5972, total 1335.9293
(T) | Epoch=885, loss=7.6077, this epoch 1.6341, total 1337.5634
(T) | Epoch=886, loss=7.4911, this epoch 1.6399, total 1339.2033
(T) | Epoch=887, loss=7.4757, this epoch 1.5690, total 1340.7723
(T) | Epoch=888, loss=7.5696, this epoch 1.6015, total 1342.3737
(T) | Epoch=889, loss=7.4795, this epoch 1.5978, total 1343.9715
(T) | Epoch=890, loss=7.5284, this epoch 1.5043, total 1345.4758
(T) | Epoch=891, loss=7.4869, this epoch 1.5243, total 1347.0001
(T) | Epoch=892, loss=7.4715, this epoch 1.5405, total 1348.5406
(T) | Epoch=893, loss=7.5612, this epoch 1.5572, total 1350.0978
(T) | Epoch=894, loss=7.5040, this epoch 1.5598, total 1351.6575
(T) | Epoch=895, loss=7.4730, this epoch 1.5777, total 1353.2352
(T) | Epoch=896, loss=7.5167, this epoch 1.5855, total 1354.8207
(T) | Epoch=897, loss=7.4823, this epoch 1.5767, total 1356.3974
(T) | Epoch=898, loss=7.5450, this epoch 1.5749, total 1357.9723
(T) | Epoch=899, loss=7.5723, this epoch 1.5481, total 1359.5204
(T) | Epoch=900, loss=7.4532, this epoch 1.4703, total 1360.9907
(T) | Epoch=901, loss=7.5013, this epoch 1.4607, total 1362.4514
(T) | Epoch=902, loss=7.4777, this epoch 1.4678, total 1363.9191
(T) | Epoch=903, loss=8.0184, this epoch 1.5280, total 1365.4471
(T) | Epoch=904, loss=7.5532, this epoch 1.5443, total 1366.9915
(T) | Epoch=905, loss=7.4580, this epoch 1.5979, total 1368.5894
(T) | Epoch=906, loss=7.4644, this epoch 1.5311, total 1370.1205
(T) | Epoch=907, loss=7.4955, this epoch 1.4958, total 1371.6163
(T) | Epoch=908, loss=7.4946, this epoch 1.5272, total 1373.1435
(T) | Epoch=909, loss=7.4934, this epoch 1.4549, total 1374.5984
(T) | Epoch=910, loss=7.5578, this epoch 1.5178, total 1376.1162
(T) | Epoch=911, loss=7.5203, this epoch 1.4915, total 1377.6077
(T) | Epoch=912, loss=7.4988, this epoch 1.4965, total 1379.1042
(T) | Epoch=913, loss=7.4927, this epoch 1.4565, total 1380.5607
(T) | Epoch=914, loss=7.5270, this epoch 1.5304, total 1382.0911
(T) | Epoch=915, loss=7.4781, this epoch 1.5947, total 1383.6858
(T) | Epoch=916, loss=7.5513, this epoch 1.5400, total 1385.2258
(T) | Epoch=917, loss=7.4864, this epoch 1.5226, total 1386.7484
(T) | Epoch=918, loss=7.5257, this epoch 1.4553, total 1388.2037
(T) | Epoch=919, loss=7.4888, this epoch 1.5253, total 1389.7290
(T) | Epoch=920, loss=7.4680, this epoch 1.5431, total 1391.2721
(T) | Epoch=921, loss=7.5015, this epoch 1.4999, total 1392.7720
(T) | Epoch=922, loss=7.5331, this epoch 1.4928, total 1394.2648
(T) | Epoch=923, loss=7.6029, this epoch 1.4665, total 1395.7313
(T) | Epoch=924, loss=7.4840, this epoch 1.5280, total 1397.2592
(T) | Epoch=925, loss=7.4960, this epoch 1.5525, total 1398.8117
(T) | Epoch=926, loss=7.4580, this epoch 1.4915, total 1400.3032
(T) | Epoch=927, loss=7.4708, this epoch 1.4918, total 1401.7951
(T) | Epoch=928, loss=7.4672, this epoch 1.4725, total 1403.2675
(T) | Epoch=929, loss=7.4598, this epoch 1.5422, total 1404.8097
(T) | Epoch=930, loss=7.5423, this epoch 1.5130, total 1406.3227
(T) | Epoch=931, loss=7.4580, this epoch 1.4818, total 1407.8046
(T) | Epoch=932, loss=7.5290, this epoch 1.4640, total 1409.2686
(T) | Epoch=933, loss=7.5099, this epoch 1.4547, total 1410.7233
(T) | Epoch=934, loss=7.4667, this epoch 1.4653, total 1412.1886
(T) | Epoch=935, loss=7.4673, this epoch 1.5195, total 1413.7081
(T) | Epoch=936, loss=7.4829, this epoch 1.5027, total 1415.2107
(T) | Epoch=937, loss=7.5459, this epoch 1.4966, total 1416.7073
(T) | Epoch=938, loss=7.4652, this epoch 1.4699, total 1418.1772
(T) | Epoch=939, loss=7.4647, this epoch 1.4640, total 1419.6413
(T) | Epoch=940, loss=7.5075, this epoch 1.4812, total 1421.1225
(T) | Epoch=941, loss=7.4650, this epoch 1.5569, total 1422.6793
(T) | Epoch=942, loss=7.4524, this epoch 1.4843, total 1424.1637
(T) | Epoch=943, loss=7.5999, this epoch 1.4584, total 1425.6221
(T) | Epoch=944, loss=7.4609, this epoch 1.5048, total 1427.1269
(T) | Epoch=945, loss=7.4690, this epoch 1.4795, total 1428.6064
(T) | Epoch=946, loss=7.4562, this epoch 1.5298, total 1430.1362
(T) | Epoch=947, loss=7.5178, this epoch 1.4913, total 1431.6275
(T) | Epoch=948, loss=7.4710, this epoch 1.4947, total 1433.1222
(T) | Epoch=949, loss=7.4487, this epoch 1.5031, total 1434.6253
+++model saved ! 2016.pth
(T) | Epoch=950, loss=7.4660, this epoch 1.5559, total 1436.1812
(T) | Epoch=951, loss=7.4575, this epoch 1.5246, total 1437.7058
(T) | Epoch=952, loss=7.4778, this epoch 1.5059, total 1439.2117
(T) | Epoch=953, loss=7.4722, this epoch 1.4784, total 1440.6901
(T) | Epoch=954, loss=7.4648, this epoch 1.4914, total 1442.1815
(T) | Epoch=955, loss=7.4652, this epoch 1.5017, total 1443.6831
(T) | Epoch=956, loss=7.4660, this epoch 1.5060, total 1445.1891
(T) | Epoch=957, loss=7.4707, this epoch 1.5171, total 1446.7062
(T) | Epoch=958, loss=7.4581, this epoch 1.5039, total 1448.2100
(T) | Epoch=959, loss=7.4598, this epoch 1.4926, total 1449.7026
(T) | Epoch=960, loss=7.5022, this epoch 1.4931, total 1451.1957
(T) | Epoch=961, loss=7.4473, this epoch 1.5013, total 1452.6971
+++model saved ! 2016.pth
(T) | Epoch=962, loss=7.4643, this epoch 1.5069, total 1454.2040
(T) | Epoch=963, loss=7.4682, this epoch 1.4600, total 1455.6640
(T) | Epoch=964, loss=7.4684, this epoch 1.4923, total 1457.1563
(T) | Epoch=965, loss=7.5104, this epoch 1.4974, total 1458.6537
(T) | Epoch=966, loss=7.4625, this epoch 1.4855, total 1460.1393
(T) | Epoch=967, loss=7.4656, this epoch 1.4756, total 1461.6149
(T) | Epoch=968, loss=7.5318, this epoch 1.5398, total 1463.1547
(T) | Epoch=969, loss=7.4919, this epoch 1.5104, total 1464.6651
(T) | Epoch=970, loss=7.4457, this epoch 1.4878, total 1466.1529
+++model saved ! 2016.pth
(T) | Epoch=971, loss=7.4625, this epoch 1.5484, total 1467.7013
(T) | Epoch=972, loss=7.5440, this epoch 1.4886, total 1469.1900
(T) | Epoch=973, loss=8.0167, this epoch 1.5118, total 1470.7017
(T) | Epoch=974, loss=7.4560, this epoch 1.4432, total 1472.1450
(T) | Epoch=975, loss=7.4629, this epoch 1.4630, total 1473.6080
(T) | Epoch=976, loss=7.5489, this epoch 1.5396, total 1475.1475
(T) | Epoch=977, loss=7.4848, this epoch 1.4882, total 1476.6357
(T) | Epoch=978, loss=7.5312, this epoch 1.5066, total 1478.1423
(T) | Epoch=979, loss=7.6225, this epoch 1.4913, total 1479.6336
(T) | Epoch=980, loss=7.5117, this epoch 1.4602, total 1481.0939
(T) | Epoch=981, loss=7.6630, this epoch 1.5222, total 1482.6160
(T) | Epoch=982, loss=7.5768, this epoch 1.4794, total 1484.0954
(T) | Epoch=983, loss=7.5710, this epoch 1.5006, total 1485.5960
(T) | Epoch=984, loss=7.5170, this epoch 1.4461, total 1487.0421
(T) | Epoch=985, loss=7.5734, this epoch 1.5236, total 1488.5658
(T) | Epoch=986, loss=7.5337, this epoch 1.4796, total 1490.0454
(T) | Epoch=987, loss=7.5398, this epoch 1.5578, total 1491.6032
(T) | Epoch=988, loss=7.5815, this epoch 1.5689, total 1493.1721
(T) | Epoch=989, loss=7.5682, this epoch 1.4926, total 1494.6647
(T) | Epoch=990, loss=7.5196, this epoch 1.4898, total 1496.1544
(T) | Epoch=991, loss=7.4958, this epoch 1.5379, total 1497.6923
(T) | Epoch=992, loss=7.7848, this epoch 1.5617, total 1499.2540
(T) | Epoch=993, loss=7.4975, this epoch 1.5501, total 1500.8042
(T) | Epoch=994, loss=7.5405, this epoch 1.5100, total 1502.3142
(T) | Epoch=995, loss=7.5651, this epoch 1.5463, total 1503.8604
(T) | Epoch=996, loss=7.5036, this epoch 1.5406, total 1505.4011
(T) | Epoch=997, loss=7.4839, this epoch 1.5020, total 1506.9030
(T) | Epoch=998, loss=7.4669, this epoch 1.5005, total 1508.4035
(T) | Epoch=999, loss=7.5263, this epoch 1.5723, total 1509.9757
(T) | Epoch=1000, loss=7.4754, this epoch 1.5550, total 1511.5308
=== Final ===

==============================
LoRA FINE-TUNING
==============================
Random seed set to 4
Epoch: 0, loss: 33.4954, train_acc: 0.0000, train_recall: 0.0000, train_f1: 0.0000, val_acc: 0.000000, val_recall: 0.000000, val_f1: 0.000000
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed4.csv
Epoch: 1, loss: 26.0946, train_acc: 0.4000, train_recall: 0.2000, train_f1: 0.1157, val_acc: 0.358974, val_recall: 0.200000, val_f1: 0.106667
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed4.csv
Epoch: 2, loss: 56.0150, train_acc: 0.3389, train_recall: 0.3253, train_f1: 0.2089, val_acc: 0.461538, val_recall: 0.243243, val_f1: 0.157895
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed4.csv
Epoch: 3, loss: 98.1835, train_acc: 0.2444, train_recall: 0.3333, train_f1: 0.1811, val_acc: 0.141026, val_recall: 0.250000, val_f1: 0.062500
Epoch: 4, loss: 136.9284, train_acc: 0.0167, train_recall: 0.2500, train_f1: 0.0082, val_acc: 0.025641, val_recall: 0.250000, val_f1: 0.012500
Epoch: 5, loss: 93.0488, train_acc: 0.0167, train_recall: 0.2500, train_f1: 0.0082, val_acc: 0.025641, val_recall: 0.250000, val_f1: 0.012500
Epoch: 6, loss: 97.4469, train_acc: 0.0000, train_recall: 0.0000, train_f1: 0.0000, val_acc: 0.000000, val_recall: 0.000000, val_f1: 0.000000
Epoch: 7, loss: 83.2255, train_acc: 0.0000, train_recall: 0.0000, train_f1: 0.0000, val_acc: 0.000000, val_recall: 0.000000, val_f1: 0.000000
Epoch: 8, loss: 57.8947, train_acc: 0.0000, train_recall: 0.0000, train_f1: 0.0000, val_acc: 0.000000, val_recall: 0.000000, val_f1: 0.000000
Epoch: 9, loss: 32.5393, train_acc: 0.3389, train_recall: 0.2460, train_f1: 0.1266, val_acc: 0.461538, val_recall: 0.243243, val_f1: 0.157895
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed4.csv
Epoch: 10, loss: 40.3594, train_acc: 0.4000, train_recall: 0.2500, train_f1: 0.1429, val_acc: 0.358974, val_recall: 0.250000, val_f1: 0.132075
Epoch: 11, loss: 42.0036, train_acc: 0.4000, train_recall: 0.2500, train_f1: 0.1429, val_acc: 0.358974, val_recall: 0.250000, val_f1: 0.132075
Epoch: 12, loss: 34.2290, train_acc: 0.4000, train_recall: 0.2500, train_f1: 0.1429, val_acc: 0.358974, val_recall: 0.250000, val_f1: 0.132075
Epoch: 13, loss: 43.8626, train_acc: 0.2389, train_recall: 0.2500, train_f1: 0.0964, val_acc: 0.141026, val_recall: 0.250000, val_f1: 0.061798
Epoch: 14, loss: 39.9518, train_acc: 0.2500, train_recall: 0.2545, train_f1: 0.1244, val_acc: 0.166667, val_recall: 0.247543, val_f1: 0.095125
Epoch: 15, loss: 53.4938, train_acc: 0.3444, train_recall: 0.2500, train_f1: 0.1281, val_acc: 0.474359, val_recall: 0.250000, val_f1: 0.160870
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed4.csv
Epoch: 16, loss: 56.6317, train_acc: 0.3444, train_recall: 0.2500, train_f1: 0.1281, val_acc: 0.474359, val_recall: 0.250000, val_f1: 0.160870
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed4.csv
Epoch: 17, loss: 50.8621, train_acc: 0.3444, train_recall: 0.2500, train_f1: 0.1281, val_acc: 0.474359, val_recall: 0.250000, val_f1: 0.160870
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed4.csv
Epoch: 18, loss: 37.7364, train_acc: 0.3444, train_recall: 0.2500, train_f1: 0.1281, val_acc: 0.474359, val_recall: 0.250000, val_f1: 0.160870
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed4.csv
Epoch: 19, loss: 44.7201, train_acc: 0.4000, train_recall: 0.2500, train_f1: 0.1429, val_acc: 0.358974, val_recall: 0.250000, val_f1: 0.132075
Epoch: 20, loss: 51.0736, train_acc: 0.4000, train_recall: 0.2500, train_f1: 0.1429, val_acc: 0.358974, val_recall: 0.250000, val_f1: 0.132075
Epoch: 21, loss: 50.6117, train_acc: 0.4000, train_recall: 0.2500, train_f1: 0.1429, val_acc: 0.358974, val_recall: 0.250000, val_f1: 0.132075
Epoch: 22, loss: 44.4337, train_acc: 0.4222, train_recall: 0.3177, train_f1: 0.2509, val_acc: 0.384615, val_recall: 0.392045, val_f1: 0.250000
Epoch: 23, loss: 49.9821, train_acc: 0.2389, train_recall: 0.2500, train_f1: 0.0964, val_acc: 0.141026, val_recall: 0.250000, val_f1: 0.061798
Epoch: 24, loss: 44.4776, train_acc: 0.2389, train_recall: 0.2500, train_f1: 0.0968, val_acc: 0.141026, val_recall: 0.250000, val_f1: 0.062500
Epoch: 25, loss: 40.2926, train_acc: 0.4000, train_recall: 0.2500, train_f1: 0.1429, val_acc: 0.358974, val_recall: 0.250000, val_f1: 0.132075
Epoch: 26, loss: 36.9199, train_acc: 0.4000, train_recall: 0.2500, train_f1: 0.1429, val_acc: 0.358974, val_recall: 0.250000, val_f1: 0.132075
Epoch: 27, loss: 35.9466, train_acc: 0.3444, train_recall: 0.2500, train_f1: 0.1281, val_acc: 0.474359, val_recall: 0.250000, val_f1: 0.160870
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed4.csv
Epoch: 28, loss: 41.0022, train_acc: 0.3444, train_recall: 0.2500, train_f1: 0.1281, val_acc: 0.474359, val_recall: 0.250000, val_f1: 0.160870
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed4.csv
Epoch: 29, loss: 39.1845, train_acc: 0.3444, train_recall: 0.2500, train_f1: 0.1281, val_acc: 0.474359, val_recall: 0.250000, val_f1: 0.160870
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed4.csv
Epoch: 30, loss: 31.4886, train_acc: 0.3444, train_recall: 0.2500, train_f1: 0.1281, val_acc: 0.474359, val_recall: 0.250000, val_f1: 0.160870
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed4.csv
Epoch: 31, loss: 37.7552, train_acc: 0.4000, train_recall: 0.2500, train_f1: 0.1429, val_acc: 0.358974, val_recall: 0.250000, val_f1: 0.132075
Epoch: 32, loss: 39.6098, train_acc: 0.4000, train_recall: 0.2500, train_f1: 0.1429, val_acc: 0.358974, val_recall: 0.250000, val_f1: 0.132075
Epoch: 33, loss: 40.1283, train_acc: 0.2667, train_recall: 0.2627, train_f1: 0.1389, val_acc: 0.153846, val_recall: 0.245130, val_f1: 0.089827
Epoch: 34, loss: 39.9113, train_acc: 0.2667, train_recall: 0.2627, train_f1: 0.1388, val_acc: 0.153846, val_recall: 0.245130, val_f1: 0.089653
Epoch: 35, loss: 39.8151, train_acc: 0.4000, train_recall: 0.2500, train_f1: 0.1429, val_acc: 0.358974, val_recall: 0.250000, val_f1: 0.132075
Epoch: 36, loss: 37.7364, train_acc: 0.4000, train_recall: 0.2500, train_f1: 0.1429, val_acc: 0.358974, val_recall: 0.250000, val_f1: 0.132075
Epoch: 37, loss: 30.6676, train_acc: 0.4167, train_recall: 0.3096, train_f1: 0.2451, val_acc: 0.371795, val_recall: 0.369318, val_f1: 0.239358
Epoch: 38, loss: 29.5303, train_acc: 0.4056, train_recall: 0.3442, train_f1: 0.2572, val_acc: 0.423077, val_recall: 0.366708, val_f1: 0.252035
Epoch: 39, loss: 35.8652, train_acc: 0.3444, train_recall: 0.2500, train_f1: 0.1281, val_acc: 0.474359, val_recall: 0.250000, val_f1: 0.162281
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed4.csv
Epoch: 40, loss: 35.5304, train_acc: 0.3611, train_recall: 0.3450, train_f1: 0.2771, val_acc: 0.500000, val_recall: 0.295455, val_f1: 0.233333
âœ“ New best val_acc.
âœ“ Predictions and labels saved to predictions&true_seed4.csv
Epoch: 41, loss: 35.7300, train_acc: 0.2500, train_recall: 0.3374, train_f1: 0.2305, val_acc: 0.141026, val_recall: 0.250000, val_f1: 0.063218
Epoch: 42, loss: 32.1751, train_acc: 0.2389, train_recall: 0.3257, train_f1: 0.2286, val_acc: 0.141026, val_recall: 0.234029, val_f1: 0.071719
Epoch: 43, loss: 26.4849, train_acc: 0.4333, train_recall: 0.4199, train_f1: 0.4401, val_acc: 0.384615, val_recall: 0.336982, val_f1: 0.273000
Epoch: 44, loss: 32.1344, train_acc: 0.4056, train_recall: 0.3333, train_f1: 0.2684, val_acc: 0.358974, val_recall: 0.250000, val_f1: 0.133333
Epoch: 45, loss: 33.2684, train_acc: 0.4056, train_recall: 0.3333, train_f1: 0.2684, val_acc: 0.358974, val_recall: 0.250000, val_f1: 0.133333
Epoch: 46, loss: 30.8905, train_acc: 0.3889, train_recall: 0.3576, train_f1: 0.3018, val_acc: 0.474359, val_recall: 0.254344, val_f1: 0.190341
Epoch: 47, loss: 30.2292, train_acc: 0.3889, train_recall: 0.3520, train_f1: 0.3308, val_acc: 0.410256, val_recall: 0.222732, val_f1: 0.179238
Epoch: 48, loss: 30.7094, train_acc: 0.4056, train_recall: 0.3333, train_f1: 0.2684, val_acc: 0.358974, val_recall: 0.250000, val_f1: 0.133333
Epoch: 49, loss: 27.0762, train_acc: 0.3889, train_recall: 0.3538, train_f1: 0.3406, val_acc: 0.423077, val_recall: 0.245459, val_f1: 0.220487
Epoch: 50, loss: 26.8914, train_acc: 0.4167, train_recall: 0.4316, train_f1: 0.3863, val_acc: 0.410256, val_recall: 0.359951, val_f1: 0.247794
Epoch: 51, loss: 28.7735, train_acc: 0.3278, train_recall: 0.3696, train_f1: 0.3251, val_acc: 0.179487, val_recall: 0.262987, val_f1: 0.115187
Epoch: 52, loss: 28.8486, train_acc: 0.3222, train_recall: 0.3656, train_f1: 0.2922, val_acc: 0.179487, val_recall: 0.262987, val_f1: 0.115187
Epoch: 53, loss: 26.6883, train_acc: 0.4556, train_recall: 0.4302, train_f1: 0.3733, val_acc: 0.384615, val_recall: 0.392045, val_f1: 0.250658
Epoch: 54, loss: 25.6243, train_acc: 0.4111, train_recall: 0.4082, train_f1: 0.3556, val_acc: 0.435897, val_recall: 0.345867, val_f1: 0.277869
Epoch: 55, loss: 27.1356, train_acc: 0.3778, train_recall: 0.3501, train_f1: 0.2529, val_acc: 0.461538, val_recall: 0.245415, val_f1: 0.175220
Epoch: 56, loss: 30.2413, train_acc: 0.4000, train_recall: 0.3299, train_f1: 0.2259, val_acc: 0.358974, val_recall: 0.250000, val_f1: 0.133333
Epoch: 57, loss: 29.7723, train_acc: 0.4000, train_recall: 0.3299, train_f1: 0.2420, val_acc: 0.358974, val_recall: 0.250000, val_f1: 0.133333
Epoch: 58, loss: 24.9706, train_acc: 0.4000, train_recall: 0.3996, train_f1: 0.3711, val_acc: 0.435897, val_recall: 0.348039, val_f1: 0.288235
Epoch: 59, loss: 27.1749, train_acc: 0.4056, train_recall: 0.4218, train_f1: 0.3550, val_acc: 0.410256, val_recall: 0.359951, val_f1: 0.247794
Epoch: 60, loss: 28.3989, train_acc: 0.2944, train_recall: 0.3536, train_f1: 0.2796, val_acc: 0.269231, val_recall: 0.301597, val_f1: 0.171429
Epoch: 61, loss: 26.2705, train_acc: 0.4111, train_recall: 0.4276, train_f1: 0.3587, val_acc: 0.410256, val_recall: 0.359951, val_f1: 0.249405
Epoch: 62, loss: 24.8103, train_acc: 0.4222, train_recall: 0.3935, train_f1: 0.3525, val_acc: 0.346154, val_recall: 0.337662, val_f1: 0.223805
Epoch: 63, loss: 26.0395, train_acc: 0.4111, train_recall: 0.3397, train_f1: 0.2624, val_acc: 0.346154, val_recall: 0.254870, val_f1: 0.165913
Epoch: 64, loss: 27.3535, train_acc: 0.3556, train_recall: 0.3391, train_f1: 0.2411, val_acc: 0.474359, val_recall: 0.265971, val_f1: 0.202381
Epoch: 65, loss: 24.4053, train_acc: 0.4056, train_recall: 0.4095, train_f1: 0.3735, val_acc: 0.397436, val_recall: 0.339395, val_f1: 0.252839
Epoch: 66, loss: 26.9032, train_acc: 0.4444, train_recall: 0.4232, train_f1: 0.3662, val_acc: 0.371795, val_recall: 0.383117, val_f1: 0.244129
Epoch: 67, loss: 28.8585, train_acc: 0.3556, train_recall: 0.4264, train_f1: 0.2593, val_acc: 0.346154, val_recall: 0.337662, val_f1: 0.236275
Epoch: 68, loss: 28.8443, train_acc: 0.4000, train_recall: 0.4729, train_f1: 0.2942, val_acc: 0.358974, val_recall: 0.374188, val_f1: 0.250181
Epoch: 69, loss: 26.4597, train_acc: 0.4444, train_recall: 0.4232, train_f1: 0.3662, val_acc: 0.371795, val_recall: 0.383117, val_f1: 0.244129
Epoch: 70, loss: 24.2575, train_acc: 0.3778, train_recall: 0.3458, train_f1: 0.3365, val_acc: 0.410256, val_recall: 0.238702, val_f1: 0.213209
Epoch: 71, loss: 27.2865, train_acc: 0.3500, train_recall: 0.3333, train_f1: 0.2536, val_acc: 0.461538, val_recall: 0.243243, val_f1: 0.157895
Epoch: 72, loss: 24.8109, train_acc: 0.3944, train_recall: 0.3371, train_f1: 0.3412, val_acc: 0.448718, val_recall: 0.298065, val_f1: 0.276508
Epoch: 73, loss: 25.8179, train_acc: 0.4556, train_recall: 0.4278, train_f1: 0.3964, val_acc: 0.371795, val_recall: 0.369318, val_f1: 0.238928
Epoch: 74, loss: 25.9721, train_acc: 0.4278, train_recall: 0.4175, train_f1: 0.3825, val_acc: 0.384615, val_recall: 0.405844, val_f1: 0.251956
Epoch: 75, loss: 24.5237, train_acc: 0.4500, train_recall: 0.4267, train_f1: 0.3947, val_acc: 0.397436, val_recall: 0.398802, val_f1: 0.264806
Epoch: 76, loss: 26.3478, train_acc: 0.3833, train_recall: 0.3559, train_f1: 0.3056, val_acc: 0.474359, val_recall: 0.268142, val_f1: 0.215141
Epoch: 77, loss: 25.1146, train_acc: 0.4167, train_recall: 0.3432, train_f1: 0.2892, val_acc: 0.358974, val_recall: 0.263799, val_f1: 0.169529
Epoch: 78, loss: 24.4781, train_acc: 0.4556, train_recall: 0.4278, train_f1: 0.3960, val_acc: 0.397436, val_recall: 0.398802, val_f1: 0.265658
Epoch: 79, loss: 24.7323, train_acc: 0.4056, train_recall: 0.4248, train_f1: 0.3823, val_acc: 0.397436, val_recall: 0.369165, val_f1: 0.243266
Epoch: 80, loss: 24.7557, train_acc: 0.4167, train_recall: 0.4335, train_f1: 0.3995, val_acc: 0.397436, val_recall: 0.371336, val_f1: 0.256885
Epoch: 81, loss: 24.1543, train_acc: 0.4556, train_recall: 0.4278, train_f1: 0.3960, val_acc: 0.384615, val_recall: 0.392045, val_f1: 0.250658
Epoch: 82, loss: 24.6258, train_acc: 0.4167, train_recall: 0.3561, train_f1: 0.3656, val_acc: 0.307692, val_recall: 0.206366, val_f1: 0.202113
Epoch: 83, loss: 26.0378, train_acc: 0.3833, train_recall: 0.3559, train_f1: 0.3056, val_acc: 0.474359, val_recall: 0.268142, val_f1: 0.215141
Epoch: 84, loss: 23.9225, train_acc: 0.4556, train_recall: 0.4278, train_f1: 0.3960, val_acc: 0.371795, val_recall: 0.383117, val_f1: 0.245833
Epoch: 85, loss: 25.3550, train_acc: 0.3833, train_recall: 0.4696, train_f1: 0.2851, val_acc: 0.358974, val_recall: 0.387987, val_f1: 0.251736
Epoch: 86, loss: 24.7461, train_acc: 0.4556, train_recall: 0.4278, train_f1: 0.3960, val_acc: 0.371795, val_recall: 0.383117, val_f1: 0.245833
Epoch: 87, loss: 23.5625, train_acc: 0.4500, train_recall: 0.4439, train_f1: 0.4588, val_acc: 0.358974, val_recall: 0.339439, val_f1: 0.257338
Epoch: 88, loss: 25.6979, train_acc: 0.3444, train_recall: 0.3382, train_f1: 0.2913, val_acc: 0.448718, val_recall: 0.284398, val_f1: 0.222080
Epoch: 89, loss: 24.2669, train_acc: 0.4333, train_recall: 0.4209, train_f1: 0.3856, val_acc: 0.397436, val_recall: 0.412601, val_f1: 0.267639
Epoch: 90, loss: 25.0149, train_acc: 0.4556, train_recall: 0.4278, train_f1: 0.3956, val_acc: 0.384615, val_recall: 0.392045, val_f1: 0.250658
Epoch: 91, loss: 24.2012, train_acc: 0.4556, train_recall: 0.4278, train_f1: 0.3956, val_acc: 0.384615, val_recall: 0.392045, val_f1: 0.249756
Epoch: 92, loss: 24.2109, train_acc: 0.4111, train_recall: 0.4294, train_f1: 0.3813, val_acc: 0.397436, val_recall: 0.369165, val_f1: 0.243266
Epoch: 93, loss: 24.0287, train_acc: 0.4333, train_recall: 0.4386, train_f1: 0.4193, val_acc: 0.384615, val_recall: 0.332639, val_f1: 0.244178
Epoch: 94, loss: 25.5503, train_acc: 0.4167, train_recall: 0.3432, train_f1: 0.2892, val_acc: 0.358974, val_recall: 0.263799, val_f1: 0.169529
Epoch: 95, loss: 25.4884, train_acc: 0.2444, train_recall: 0.3333, train_f1: 0.2218, val_acc: 0.141026, val_recall: 0.250000, val_f1: 0.063218
Epoch: 96, loss: 24.3149, train_acc: 0.4167, train_recall: 0.3432, train_f1: 0.2892, val_acc: 0.346154, val_recall: 0.254870, val_f1: 0.165913
Epoch: 97, loss: 26.4911, train_acc: 0.3444, train_recall: 0.3311, train_f1: 0.1949, val_acc: 0.461538, val_recall: 0.259214, val_f1: 0.197552
Epoch: 98, loss: 26.4591, train_acc: 0.3778, train_recall: 0.4669, train_f1: 0.3073, val_acc: 0.333333, val_recall: 0.289641, val_f1: 0.226108
Epoch: 99, loss: 26.8257, train_acc: 0.4000, train_recall: 0.4729, train_f1: 0.2938, val_acc: 0.358974, val_recall: 0.374188, val_f1: 0.249462
Epoch: 100, loss: 26.9646, train_acc: 0.4500, train_recall: 0.4243, train_f1: 0.3925, val_acc: 0.371795, val_recall: 0.383117, val_f1: 0.244891
Epoch: 101, loss: 27.1048, train_acc: 0.4556, train_recall: 0.4302, train_f1: 0.3967, val_acc: 0.384615, val_recall: 0.392045, val_f1: 0.249756
Epoch: 102, loss: 25.6695, train_acc: 0.4611, train_recall: 0.4342, train_f1: 0.4058, val_acc: 0.384615, val_recall: 0.392045, val_f1: 0.249756
Epoch: 103, loss: 23.8507, train_acc: 0.4389, train_recall: 0.4415, train_f1: 0.4293, val_acc: 0.410256, val_recall: 0.364294, val_f1: 0.272256
Epoch: 104, loss: 26.9157, train_acc: 0.3833, train_recall: 0.3559, train_f1: 0.3056, val_acc: 0.474359, val_recall: 0.268142, val_f1: 0.216887
Epoch: 105, loss: 24.9773, train_acc: 0.4056, train_recall: 0.3553, train_f1: 0.3650, val_acc: 0.346154, val_recall: 0.220121, val_f1: 0.224402
Epoch: 106, loss: 24.3468, train_acc: 0.3944, train_recall: 0.3528, train_f1: 0.3551, val_acc: 0.371795, val_recall: 0.227119, val_f1: 0.227413
Epoch: 107, loss: 24.1710, train_acc: 0.4500, train_recall: 0.4463, train_f1: 0.4580, val_acc: 0.358974, val_recall: 0.339439, val_f1: 0.257338
Epoch: 108, loss: 24.3051, train_acc: 0.4500, train_recall: 0.4463, train_f1: 0.4580, val_acc: 0.358974, val_recall: 0.339439, val_f1: 0.257338
Epoch: 109, loss: 24.2782, train_acc: 0.3944, train_recall: 0.3528, train_f1: 0.3551, val_acc: 0.371795, val_recall: 0.227119, val_f1: 0.227413
Epoch: 110, loss: 25.0961, train_acc: 0.4000, train_recall: 0.3580, train_f1: 0.3551, val_acc: 0.410256, val_recall: 0.249561, val_f1: 0.248747
Epoch: 111, loss: 25.2116, train_acc: 0.4167, train_recall: 0.3432, train_f1: 0.2889, val_acc: 0.384615, val_recall: 0.279484, val_f1: 0.190747
Epoch: 112, loss: 24.7310, train_acc: 0.3889, train_recall: 0.4221, train_f1: 0.3679, val_acc: 0.333333, val_recall: 0.335381, val_f1: 0.206667
Epoch: 113, loss: 24.6125, train_acc: 0.4056, train_recall: 0.4253, train_f1: 0.3780, val_acc: 0.397436, val_recall: 0.369165, val_f1: 0.243266
Epoch: 114, loss: 24.1288, train_acc: 0.4167, train_recall: 0.3669, train_f1: 0.3875, val_acc: 0.333333, val_recall: 0.211193, val_f1: 0.214751
Epoch: 115, loss: 23.8897, train_acc: 0.4278, train_recall: 0.3900, train_f1: 0.4198, val_acc: 0.384615, val_recall: 0.236048, val_f1: 0.234761
Epoch: 116, loss: 23.6748, train_acc: 0.4500, train_recall: 0.4439, train_f1: 0.4592, val_acc: 0.346154, val_recall: 0.316712, val_f1: 0.246570
Epoch: 117, loss: 24.1979, train_acc: 0.3944, train_recall: 0.4256, train_f1: 0.3757, val_acc: 0.333333, val_recall: 0.335381, val_f1: 0.208108
Epoch: 118, loss: 23.4923, train_acc: 0.4333, train_recall: 0.4380, train_f1: 0.4230, val_acc: 0.384615, val_recall: 0.332639, val_f1: 0.245759
Epoch: 119, loss: 24.7624, train_acc: 0.4278, train_recall: 0.3548, train_f1: 0.3113, val_acc: 0.346154, val_recall: 0.254870, val_f1: 0.165913
Epoch: 120, loss: 23.3331, train_acc: 0.4556, train_recall: 0.4498, train_f1: 0.4629, val_acc: 0.333333, val_recall: 0.323754, val_f1: 0.238812
Epoch: 121, loss: 24.9121, train_acc: 0.3722, train_recall: 0.4804, train_f1: 0.2862, val_acc: 0.307692, val_recall: 0.305897, val_f1: 0.206003
Epoch: 122, loss: 24.2931, train_acc: 0.3944, train_recall: 0.4256, train_f1: 0.3757, val_acc: 0.333333, val_recall: 0.335381, val_f1: 0.208108
Epoch: 123, loss: 23.4327, train_acc: 0.4611, train_recall: 0.4318, train_f1: 0.4051, val_acc: 0.384615, val_recall: 0.389874, val_f1: 0.259900
Epoch: 124, loss: 24.6282, train_acc: 0.4500, train_recall: 0.3827, train_f1: 0.3610, val_acc: 0.346154, val_recall: 0.268669, val_f1: 0.188763
Epoch: 125, loss: 23.9429, train_acc: 0.3889, train_recall: 0.3538, train_f1: 0.3412, val_acc: 0.410256, val_recall: 0.236530, val_f1: 0.205360
Epoch: 126, loss: 24.2497, train_acc: 0.4500, train_recall: 0.4463, train_f1: 0.4580, val_acc: 0.320513, val_recall: 0.316997, val_f1: 0.230681
Epoch: 127, loss: 23.5106, train_acc: 0.4444, train_recall: 0.4405, train_f1: 0.4544, val_acc: 0.333333, val_recall: 0.307783, val_f1: 0.233694
Epoch: 128, loss: 23.6830, train_acc: 0.3833, train_recall: 0.3569, train_f1: 0.3679, val_acc: 0.371795, val_recall: 0.248201, val_f1: 0.224192
Epoch: 129, loss: 23.5631, train_acc: 0.4611, train_recall: 0.4318, train_f1: 0.4051, val_acc: 0.384615, val_recall: 0.389874, val_f1: 0.260793
Epoch: 130, loss: 23.3861, train_acc: 0.4500, train_recall: 0.4439, train_f1: 0.4592, val_acc: 0.333333, val_recall: 0.323754, val_f1: 0.239523
Epoch: 131, loss: 23.8816, train_acc: 0.4389, train_recall: 0.4426, train_f1: 0.4227, val_acc: 0.384615, val_recall: 0.332639, val_f1: 0.248323
Epoch: 132, loss: 24.1234, train_acc: 0.3778, train_recall: 0.4770, train_f1: 0.3161, val_acc: 0.320513, val_recall: 0.314825, val_f1: 0.230052
Epoch: 133, loss: 23.6411, train_acc: 0.4611, train_recall: 0.4318, train_f1: 0.4051, val_acc: 0.358974, val_recall: 0.360390, val_f1: 0.234143
Epoch: 134, loss: 23.2359, train_acc: 0.4500, train_recall: 0.4439, train_f1: 0.4592, val_acc: 0.346154, val_recall: 0.330511, val_f1: 0.244891
Epoch: 135, loss: 23.6580, train_acc: 0.4389, train_recall: 0.4426, train_f1: 0.4222, val_acc: 0.397436, val_recall: 0.355366, val_f1: 0.255515
Epoch: 136, loss: 23.9695, train_acc: 0.4500, train_recall: 0.3827, train_f1: 0.3610, val_acc: 0.358974, val_recall: 0.277597, val_f1: 0.188209
Epoch: 137, loss: 24.0979, train_acc: 0.4167, train_recall: 0.4348, train_f1: 0.4122, val_acc: 0.320513, val_recall: 0.314825, val_f1: 0.211230
Epoch: 138, loss: 25.7378, train_acc: 0.3778, train_recall: 0.3519, train_f1: 0.3034, val_acc: 0.474359, val_recall: 0.270314, val_f1: 0.229377
Epoch: 139, loss: 27.3384, train_acc: 0.4111, train_recall: 0.3374, train_f1: 0.2769, val_acc: 0.358974, val_recall: 0.250000, val_f1: 0.133333
Epoch: 140, loss: 27.0760, train_acc: 0.4111, train_recall: 0.3374, train_f1: 0.2769, val_acc: 0.358974, val_recall: 0.250000, val_f1: 0.133333
Epoch: 141, loss: 26.2671, train_acc: 0.3833, train_recall: 0.3559, train_f1: 0.3054, val_acc: 0.474359, val_recall: 0.268142, val_f1: 0.216887
Epoch: 142, loss: 24.3564, train_acc: 0.4222, train_recall: 0.4428, train_f1: 0.3891, val_acc: 0.384615, val_recall: 0.362408, val_f1: 0.236111
Epoch: 143, loss: 24.8174, train_acc: 0.4556, train_recall: 0.4307, train_f1: 0.4027, val_acc: 0.384615, val_recall: 0.392045, val_f1: 0.249035
Epoch: 144, loss: 26.9040, train_acc: 0.4167, train_recall: 0.3432, train_f1: 0.2889, val_acc: 0.371795, val_recall: 0.272727, val_f1: 0.176282
Epoch: 145, loss: 24.5011, train_acc: 0.4167, train_recall: 0.3669, train_f1: 0.3878, val_acc: 0.333333, val_recall: 0.211193, val_f1: 0.214751
Epoch: 146, loss: 27.8898, train_acc: 0.4056, train_recall: 0.4200, train_f1: 0.3783, val_acc: 0.397436, val_recall: 0.321253, val_f1: 0.230958
Epoch: 147, loss: 29.0389, train_acc: 0.4111, train_recall: 0.4294, train_f1: 0.3813, val_acc: 0.397436, val_recall: 0.369165, val_f1: 0.243266
Epoch: 148, loss: 29.6508, train_acc: 0.3889, train_recall: 0.4221, train_f1: 0.3679, val_acc: 0.320513, val_recall: 0.328624, val_f1: 0.199391
Epoch: 149, loss: 26.9679, train_acc: 0.4111, train_recall: 0.4294, train_f1: 0.3813, val_acc: 0.397436, val_recall: 0.369165, val_f1: 0.243266
Epoch: 150, loss: 23.9968, train_acc: 0.4389, train_recall: 0.3948, train_f1: 0.4345, val_acc: 0.320513, val_recall: 0.213891, val_f1: 0.218245
Epoch: 151, loss: 28.9939, train_acc: 0.4111, train_recall: 0.3391, train_f1: 0.2804, val_acc: 0.371795, val_recall: 0.272727, val_f1: 0.176282
Epoch: 152, loss: 29.5911, train_acc: 0.4056, train_recall: 0.3333, train_f1: 0.2684, val_acc: 0.346154, val_recall: 0.241071, val_f1: 0.129808
Epoch: 153, loss: 25.9203, train_acc: 0.4167, train_recall: 0.3432, train_f1: 0.2889, val_acc: 0.358974, val_recall: 0.263799, val_f1: 0.172735
Epoch: 154, loss: 30.2206, train_acc: 0.3556, train_recall: 0.3391, train_f1: 0.2655, val_acc: 0.474359, val_recall: 0.265971, val_f1: 0.200624
Epoch: 155, loss: 30.3020, train_acc: 0.3556, train_recall: 0.3498, train_f1: 0.3065, val_acc: 0.448718, val_recall: 0.284398, val_f1: 0.223633
Epoch: 156, loss: 29.8355, train_acc: 0.3889, train_recall: 0.4221, train_f1: 0.3679, val_acc: 0.307692, val_recall: 0.321867, val_f1: 0.195261
Epoch: 157, loss: 30.1973, train_acc: 0.2000, train_recall: 0.2868, train_f1: 0.1181, val_acc: 0.128205, val_recall: 0.227273, val_f1: 0.084746
Epoch: 158, loss: 32.4071, train_acc: 0.1833, train_recall: 0.4226, train_f1: 0.1478, val_acc: 0.153846, val_recall: 0.477273, val_f1: 0.134913
Epoch: 159, loss: 25.1319, train_acc: 0.4611, train_recall: 0.4318, train_f1: 0.4051, val_acc: 0.371795, val_recall: 0.369318, val_f1: 0.238928
Epoch: 160, loss: 26.4752, train_acc: 0.4333, train_recall: 0.3617, train_f1: 0.3347, val_acc: 0.371795, val_recall: 0.284354, val_f1: 0.209825
Epoch: 161, loss: 27.9278, train_acc: 0.3833, train_recall: 0.3554, train_f1: 0.3098, val_acc: 0.487179, val_recall: 0.277071, val_f1: 0.231749
Epoch: 162, loss: 26.6760, train_acc: 0.4000, train_recall: 0.3726, train_f1: 0.3860, val_acc: 0.410256, val_recall: 0.270643, val_f1: 0.244983
Epoch: 163, loss: 27.9870, train_acc: 0.4611, train_recall: 0.4318, train_f1: 0.4051, val_acc: 0.371795, val_recall: 0.369318, val_f1: 0.237179
Epoch: 164, loss: 29.2983, train_acc: 0.4556, train_recall: 0.3485, train_f1: 0.2794, val_acc: 0.384615, val_recall: 0.392045, val_f1: 0.248864
Epoch: 165, loss: 29.0901, train_acc: 0.4556, train_recall: 0.3485, train_f1: 0.2794, val_acc: 0.384615, val_recall: 0.392045, val_f1: 0.248864
Epoch: 166, loss: 28.4800, train_acc: 0.4444, train_recall: 0.3612, train_f1: 0.3317, val_acc: 0.371795, val_recall: 0.346196, val_f1: 0.264295
Epoch: 167, loss: 28.6086, train_acc: 0.4389, train_recall: 0.3627, train_f1: 0.3036, val_acc: 0.410256, val_recall: 0.348324, val_f1: 0.267013
Epoch: 168, loss: 27.2725, train_acc: 0.4500, train_recall: 0.3646, train_f1: 0.3373, val_acc: 0.358974, val_recall: 0.323469, val_f1: 0.254605
Epoch: 169, loss: 29.5987, train_acc: 0.4389, train_recall: 0.2889, train_f1: 0.2149, val_acc: 0.358974, val_recall: 0.263799, val_f1: 0.166782
Epoch: 170, loss: 28.0745, train_acc: 0.4111, train_recall: 0.2990, train_f1: 0.2684, val_acc: 0.423077, val_recall: 0.245459, val_f1: 0.217740
Epoch: 171, loss: 27.2995, train_acc: 0.4444, train_recall: 0.3606, train_f1: 0.3340, val_acc: 0.346154, val_recall: 0.316712, val_f1: 0.249086
Epoch: 172, loss: 27.8486, train_acc: 0.4444, train_recall: 0.3612, train_f1: 0.3320, val_acc: 0.371795, val_recall: 0.346196, val_f1: 0.264295
Epoch: 173, loss: 27.6516, train_acc: 0.4556, train_recall: 0.3705, train_f1: 0.3412, val_acc: 0.358974, val_recall: 0.339439, val_f1: 0.258108
Epoch: 174, loss: 27.2416, train_acc: 0.4500, train_recall: 0.3646, train_f1: 0.3369, val_acc: 0.358974, val_recall: 0.339439, val_f1: 0.258108
Epoch: 175, loss: 27.0437, train_acc: 0.4333, train_recall: 0.3564, train_f1: 0.3034, val_acc: 0.397436, val_recall: 0.323425, val_f1: 0.245514
Epoch: 176, loss: 28.1317, train_acc: 0.4500, train_recall: 0.3005, train_f1: 0.2328, val_acc: 0.358974, val_recall: 0.263799, val_f1: 0.166782
Epoch: 177, loss: 26.5766, train_acc: 0.4389, train_recall: 0.3627, train_f1: 0.3036, val_acc: 0.397436, val_recall: 0.339395, val_f1: 0.250719
Epoch: 178, loss: 26.5018, train_acc: 0.4500, train_recall: 0.3652, train_f1: 0.3355, val_acc: 0.346154, val_recall: 0.314540, val_f1: 0.240047
Epoch: 179, loss: 27.1261, train_acc: 0.4167, train_recall: 0.2891, train_f1: 0.2315, val_acc: 0.346154, val_recall: 0.282468, val_f1: 0.194498
Epoch: 180, loss: 26.5484, train_acc: 0.4222, train_recall: 0.4392, train_f1: 0.3886, val_acc: 0.410256, val_recall: 0.375921, val_f1: 0.250425
Epoch: 181, loss: 26.1314, train_acc: 0.4389, train_recall: 0.4451, train_f1: 0.4406, val_acc: 0.358974, val_recall: 0.351066, val_f1: 0.242407
Epoch: 182, loss: 26.9777, train_acc: 0.4556, train_recall: 0.3839, train_f1: 0.3584, val_acc: 0.358974, val_recall: 0.263799, val_f1: 0.166782
Epoch: 183, loss: 25.8685, train_acc: 0.4167, train_recall: 0.4334, train_f1: 0.3849, val_acc: 0.410256, val_recall: 0.359951, val_f1: 0.246661
Epoch: 184, loss: 25.8949, train_acc: 0.4500, train_recall: 0.4336, train_f1: 0.4178, val_acc: 0.397436, val_recall: 0.412601, val_f1: 0.265114
Epoch: 185, loss: 25.7907, train_acc: 0.4611, train_recall: 0.4318, train_f1: 0.4051, val_acc: 0.371795, val_recall: 0.369318, val_f1: 0.237179
Epoch: 186, loss: 25.3224, train_acc: 0.4389, train_recall: 0.4446, train_f1: 0.4428, val_acc: 0.358974, val_recall: 0.337267, val_f1: 0.246763
Epoch: 187, loss: 25.0777, train_acc: 0.4389, train_recall: 0.4446, train_f1: 0.4428, val_acc: 0.358974, val_recall: 0.337267, val_f1: 0.246763
Epoch: 188, loss: 25.2197, train_acc: 0.4722, train_recall: 0.4518, train_f1: 0.4765, val_acc: 0.333333, val_recall: 0.325158, val_f1: 0.259148
Epoch: 189, loss: 25.1278, train_acc: 0.4333, train_recall: 0.4411, train_f1: 0.4371, val_acc: 0.346154, val_recall: 0.330511, val_f1: 0.239889
Epoch: 190, loss: 25.0437, train_acc: 0.4667, train_recall: 0.4488, train_f1: 0.4765, val_acc: 0.358974, val_recall: 0.336500, val_f1: 0.276702
Epoch: 191, loss: 24.8922, train_acc: 0.4278, train_recall: 0.3957, train_f1: 0.4151, val_acc: 0.397436, val_recall: 0.247916, val_f1: 0.226316
Epoch: 192, loss: 24.8255, train_acc: 0.4611, train_recall: 0.4538, train_f1: 0.4670, val_acc: 0.346154, val_recall: 0.316712, val_f1: 0.246140
Epoch: 193, loss: 24.8958, train_acc: 0.4222, train_recall: 0.3899, train_f1: 0.4094, val_acc: 0.410256, val_recall: 0.238702, val_f1: 0.212807
Epoch: 194, loss: 24.8142, train_acc: 0.4556, train_recall: 0.4480, train_f1: 0.4636, val_acc: 0.346154, val_recall: 0.316712, val_f1: 0.246570
Epoch: 195, loss: 24.8774, train_acc: 0.4500, train_recall: 0.4445, train_f1: 0.4574, val_acc: 0.346154, val_recall: 0.316712, val_f1: 0.246140
Epoch: 196, loss: 24.8393, train_acc: 0.4611, train_recall: 0.4538, train_f1: 0.4674, val_acc: 0.333333, val_recall: 0.309955, val_f1: 0.238762
Epoch: 197, loss: 24.5231, train_acc: 0.4500, train_recall: 0.4000, train_f1: 0.4387, val_acc: 0.346154, val_recall: 0.217949, val_f1: 0.219360
Epoch: 198, loss: 25.2682, train_acc: 0.4167, train_recall: 0.3926, train_f1: 0.3725, val_acc: 0.461538, val_recall: 0.261386, val_f1: 0.209251
Epoch: 199, loss: 25.5791, train_acc: 0.3111, train_recall: 0.3615, train_f1: 0.3123, val_acc: 0.166667, val_recall: 0.254058, val_f1: 0.103576
Epoch: 200, loss: 27.3457, train_acc: 0.4556, train_recall: 0.3839, train_f1: 0.3584, val_acc: 0.371795, val_recall: 0.272727, val_f1: 0.174384
Epoch: 201, loss: 25.6706, train_acc: 0.4556, train_recall: 0.3839, train_f1: 0.3584, val_acc: 0.358974, val_recall: 0.263799, val_f1: 0.168067
Epoch: 202, loss: 26.1104, train_acc: 0.4222, train_recall: 0.4392, train_f1: 0.3886, val_acc: 0.410256, val_recall: 0.375921, val_f1: 0.250425
Epoch: 203, loss: 27.4682, train_acc: 0.4222, train_recall: 0.4392, train_f1: 0.3889, val_acc: 0.410256, val_recall: 0.375921, val_f1: 0.250425
Epoch: 204, loss: 26.7399, train_acc: 0.4222, train_recall: 0.4392, train_f1: 0.3889, val_acc: 0.410256, val_recall: 0.375921, val_f1: 0.250425
Epoch: 205, loss: 24.4765, train_acc: 0.4556, train_recall: 0.4503, train_f1: 0.4618, val_acc: 0.371795, val_recall: 0.346196, val_f1: 0.265307
Epoch: 206, loss: 27.1195, train_acc: 0.4556, train_recall: 0.3839, train_f1: 0.3584, val_acc: 0.371795, val_recall: 0.272727, val_f1: 0.174384
Epoch: 207, loss: 27.7959, train_acc: 0.4556, train_recall: 0.3839, train_f1: 0.3584, val_acc: 0.371795, val_recall: 0.272727, val_f1: 0.174384
Epoch: 208, loss: 24.7589, train_acc: 0.4667, train_recall: 0.4377, train_f1: 0.4099, val_acc: 0.384615, val_recall: 0.392045, val_f1: 0.250658
Epoch: 209, loss: 25.2569, train_acc: 0.4222, train_recall: 0.4386, train_f1: 0.3929, val_acc: 0.423077, val_recall: 0.384850, val_f1: 0.269323
Epoch: 210, loss: 26.9540, train_acc: 0.3778, train_recall: 0.3677, train_f1: 0.3273, val_acc: 0.448718, val_recall: 0.252457, val_f1: 0.187213
Epoch: 211, loss: 24.9117, train_acc: 0.4444, train_recall: 0.4478, train_f1: 0.4303, val_acc: 0.423077, val_recall: 0.371051, val_f1: 0.279552
Epoch: 212, loss: 24.3698, train_acc: 0.4667, train_recall: 0.4377, train_f1: 0.4094, val_acc: 0.384615, val_recall: 0.392045, val_f1: 0.249756
Epoch: 213, loss: 26.5028, train_acc: 0.4556, train_recall: 0.3839, train_f1: 0.3584, val_acc: 0.358974, val_recall: 0.263799, val_f1: 0.168067
Epoch: 214, loss: 25.3239, train_acc: 0.4500, train_recall: 0.3827, train_f1: 0.3610, val_acc: 0.358974, val_recall: 0.277597, val_f1: 0.192500
Epoch: 215, loss: 23.7860, train_acc: 0.4556, train_recall: 0.4503, train_f1: 0.4618, val_acc: 0.358974, val_recall: 0.339439, val_f1: 0.259070
Epoch: 216, loss: 25.7346, train_acc: 0.4167, train_recall: 0.4334, train_f1: 0.3607, val_acc: 0.397436, val_recall: 0.353194, val_f1: 0.241289
Epoch: 217, loss: 25.6031, train_acc: 0.3889, train_recall: 0.4221, train_f1: 0.3438, val_acc: 0.333333, val_recall: 0.335381, val_f1: 0.208108
Epoch: 218, loss: 24.0895, train_acc: 0.4444, train_recall: 0.4410, train_f1: 0.4288, val_acc: 0.346154, val_recall: 0.314540, val_f1: 0.243421
Epoch: 219, loss: 24.5139, train_acc: 0.4556, train_recall: 0.4284, train_f1: 0.3787, val_acc: 0.358974, val_recall: 0.360390, val_f1: 0.235133
Epoch: 220, loss: 25.4542, train_acc: 0.4556, train_recall: 0.4284, train_f1: 0.3792, val_acc: 0.358974, val_recall: 0.360390, val_f1: 0.236842
Epoch: 221, loss: 25.0594, train_acc: 0.3722, train_recall: 0.4561, train_f1: 0.2830, val_acc: 0.346154, val_recall: 0.365260, val_f1: 0.250962
Epoch: 222, loss: 24.3813, train_acc: 0.3611, train_recall: 0.4706, train_f1: 0.2795, val_acc: 0.320513, val_recall: 0.312654, val_f1: 0.220608
Epoch: 223, loss: 24.3896, train_acc: 0.4389, train_recall: 0.4426, train_f1: 0.3984, val_acc: 0.397436, val_recall: 0.339395, val_f1: 0.252839
Epoch: 224, loss: 23.8249, train_acc: 0.4500, train_recall: 0.4463, train_f1: 0.4580, val_acc: 0.346154, val_recall: 0.332683, val_f1: 0.252740
Epoch: 225, loss: 23.7175, train_acc: 0.4667, train_recall: 0.4377, train_f1: 0.4094, val_acc: 0.397436, val_recall: 0.398802, val_f1: 0.265658
Epoch: 226, loss: 24.0503, train_acc: 0.4056, train_recall: 0.3452, train_f1: 0.3490, val_acc: 0.448718, val_recall: 0.298065, val_f1: 0.276508
Epoch: 227, loss: 24.2029, train_acc: 0.4111, train_recall: 0.3498, train_f1: 0.3544, val_acc: 0.461538, val_recall: 0.304822, val_f1: 0.286154
Epoch: 228, loss: 23.8046, train_acc: 0.4611, train_recall: 0.4324, train_f1: 0.4120, val_acc: 0.397436, val_recall: 0.382832, val_f1: 0.270263
Epoch: 229, loss: 23.7456, train_acc: 0.4722, train_recall: 0.4607, train_f1: 0.4764, val_acc: 0.346154, val_recall: 0.332683, val_f1: 0.252740
Epoch: 230, loss: 23.9628, train_acc: 0.4333, train_recall: 0.4411, train_f1: 0.4366, val_acc: 0.371795, val_recall: 0.359995, val_f1: 0.258184
Epoch: 231, loss: 23.7639, train_acc: 0.4611, train_recall: 0.4520, train_f1: 0.4660, val_acc: 0.371795, val_recall: 0.330226, val_f1: 0.261620
Epoch: 232, loss: 24.6554, train_acc: 0.4611, train_recall: 0.3879, train_f1: 0.3653, val_acc: 0.346154, val_recall: 0.254870, val_f1: 0.161250
Epoch: 233, loss: 24.0828, train_acc: 0.4389, train_recall: 0.4273, train_f1: 0.3954, val_acc: 0.384615, val_recall: 0.405844, val_f1: 0.253265
Epoch: 234, loss: 23.4161, train_acc: 0.4667, train_recall: 0.4578, train_f1: 0.4704, val_acc: 0.371795, val_recall: 0.346196, val_f1: 0.265307
Epoch: 235, loss: 24.6246, train_acc: 0.3889, train_recall: 0.3600, train_f1: 0.3075, val_acc: 0.474359, val_recall: 0.268142, val_f1: 0.215141
Epoch: 236, loss: 24.5808, train_acc: 0.4611, train_recall: 0.3879, train_f1: 0.3653, val_acc: 0.346154, val_recall: 0.254870, val_f1: 0.161250
Epoch: 237, loss: 24.9952, train_acc: 0.4000, train_recall: 0.4124, train_f1: 0.3719, val_acc: 0.371795, val_recall: 0.396916, val_f1: 0.244193
Epoch: 238, loss: 23.9412, train_acc: 0.4500, train_recall: 0.3833, train_f1: 0.3664, val_acc: 0.358974, val_recall: 0.275426, val_f1: 0.197579
Epoch: 239, loss: 24.4578, train_acc: 0.4111, train_recall: 0.3957, train_f1: 0.3806, val_acc: 0.435897, val_recall: 0.279813, val_f1: 0.231690
Epoch: 240, loss: 23.5240, train_acc: 0.4556, train_recall: 0.4485, train_f1: 0.4612, val_acc: 0.358974, val_recall: 0.337267, val_f1: 0.251369
Epoch: 241, loss: 23.7704, train_acc: 0.4778, train_recall: 0.4463, train_f1: 0.4321, val_acc: 0.410256, val_recall: 0.405559, val_f1: 0.280033
Epoch: 242, loss: 23.8804, train_acc: 0.4778, train_recall: 0.4463, train_f1: 0.4321, val_acc: 0.410256, val_recall: 0.405559, val_f1: 0.280033
Epoch: 243, loss: 23.6051, train_acc: 0.4556, train_recall: 0.4485, train_f1: 0.4612, val_acc: 0.358974, val_recall: 0.337267, val_f1: 0.251369
Epoch: 244, loss: 23.6904, train_acc: 0.4444, train_recall: 0.4461, train_f1: 0.4294, val_acc: 0.397436, val_recall: 0.339395, val_f1: 0.252335
Epoch: 245, loss: 23.5319, train_acc: 0.4444, train_recall: 0.4021, train_f1: 0.4291, val_acc: 0.371795, val_recall: 0.224947, val_f1: 0.219942
Epoch: 246, loss: 23.4619, train_acc: 0.4500, train_recall: 0.4006, train_f1: 0.4373, val_acc: 0.320513, val_recall: 0.200092, val_f1: 0.200469
Epoch: 247, loss: 23.5461, train_acc: 0.4389, train_recall: 0.4426, train_f1: 0.4227, val_acc: 0.410256, val_recall: 0.362123, val_f1: 0.262775
Epoch: 248, loss: 23.1696, train_acc: 0.4667, train_recall: 0.4578, train_f1: 0.4704, val_acc: 0.371795, val_recall: 0.346196, val_f1: 0.265307
Epoch: 249, loss: 23.1110, train_acc: 0.4722, train_recall: 0.4405, train_f1: 0.4278, val_acc: 0.410256, val_recall: 0.405559, val_f1: 0.280033
Epoch: 250, loss: 22.8484, train_acc: 0.4611, train_recall: 0.4520, train_f1: 0.4660, val_acc: 0.358974, val_recall: 0.337267, val_f1: 0.251369
Epoch: 251, loss: 22.8904, train_acc: 0.4667, train_recall: 0.4560, train_f1: 0.4698, val_acc: 0.371795, val_recall: 0.346196, val_f1: 0.265307
Epoch: 252, loss: 23.0767, train_acc: 0.4611, train_recall: 0.4324, train_f1: 0.4120, val_acc: 0.371795, val_recall: 0.383117, val_f1: 0.245833
Epoch: 253, loss: 23.1088, train_acc: 0.4722, train_recall: 0.4440, train_f1: 0.4189, val_acc: 0.371795, val_recall: 0.383117, val_f1: 0.244891
Epoch: 254, loss: 23.0377, train_acc: 0.4000, train_recall: 0.3613, train_f1: 0.3471, val_acc: 0.423077, val_recall: 0.243287, val_f1: 0.212547
Epoch: 255, loss: 23.2296, train_acc: 0.3500, train_recall: 0.4105, train_f1: 0.2184, val_acc: 0.397436, val_recall: 0.227602, val_f1: 0.199953
Epoch: 256, loss: 23.1591, train_acc: 0.4167, train_recall: 0.4892, train_f1: 0.3168, val_acc: 0.358974, val_recall: 0.374188, val_f1: 0.252206
Epoch: 257, loss: 23.7341, train_acc: 0.4556, train_recall: 0.4284, train_f1: 0.3782, val_acc: 0.358974, val_recall: 0.360390, val_f1: 0.235897
Epoch: 258, loss: 23.8351, train_acc: 0.2500, train_recall: 0.3374, train_f1: 0.2057, val_acc: 0.141026, val_recall: 0.250000, val_f1: 0.064706
Epoch: 259, loss: 24.2931, train_acc: 0.3833, train_recall: 0.3542, train_f1: 0.2711, val_acc: 0.461538, val_recall: 0.245415, val_f1: 0.175220
Epoch: 260, loss: 25.3631, train_acc: 0.4278, train_recall: 0.3495, train_f1: 0.3024, val_acc: 0.358974, val_recall: 0.247828, val_f1: 0.144226
Epoch: 261, loss: 24.4870, train_acc: 0.4333, train_recall: 0.3759, train_f1: 0.3703, val_acc: 0.371795, val_recall: 0.208977, val_f1: 0.187500
Epoch: 262, loss: 24.9743, train_acc: 0.3889, train_recall: 0.3600, train_f1: 0.2831, val_acc: 0.461538, val_recall: 0.261386, val_f1: 0.215203
Epoch: 263, loss: 23.3008, train_acc: 0.4056, train_recall: 0.4775, train_f1: 0.3099, val_acc: 0.358974, val_recall: 0.374188, val_f1: 0.254291
Epoch: 264, loss: 24.5083, train_acc: 0.4389, train_recall: 0.4226, train_f1: 0.3942, val_acc: 0.371795, val_recall: 0.396916, val_f1: 0.250082
Epoch: 265, loss: 25.0452, train_acc: 0.4389, train_recall: 0.4273, train_f1: 0.3954, val_acc: 0.371795, val_recall: 0.396916, val_f1: 0.250082
Epoch: 266, loss: 23.7156, train_acc: 0.4611, train_recall: 0.4318, train_f1: 0.4047, val_acc: 0.371795, val_recall: 0.383117, val_f1: 0.247635
Epoch: 267, loss: 23.0871, train_acc: 0.4389, train_recall: 0.4426, train_f1: 0.4227, val_acc: 0.397436, val_recall: 0.339395, val_f1: 0.255539
/home/ADS/cyang314/ucr_work/HINI_Baseline/GraphLoRA/model/GraphLoRA.py:218: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.
Consider using tensor.detach() first. (Triggered internally at /pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:836.)
  f.write(f'{pre_dataset} to {downstream_dataset}: seed: %d, epoch: %d, train_loss: %f, train_acc: %f, train_recall: %f, train_f1: %f, val_acc: %f, val_recall: %f, val_f1: %f\n' %
Epoch: 268, loss: 23.4750, train_acc: 0.4167, train_recall: 0.3823, train_f1: 0.3939, val_acc: 0.384615, val_recall: 0.223017, val_f1: 0.196032
Epoch: 269, loss: 24.7994, train_acc: 0.4611, train_recall: 0.3879, train_f1: 0.3667, val_acc: 0.346154, val_recall: 0.254870, val_f1: 0.165714
Epoch: 270, loss: 23.3151, train_acc: 0.4667, train_recall: 0.4359, train_f1: 0.4136, val_acc: 0.371795, val_recall: 0.383117, val_f1: 0.247635
Epoch: 271, loss: 23.7708, train_acc: 0.3778, train_recall: 0.4862, train_f1: 0.2900, val_acc: 0.333333, val_recall: 0.335381, val_f1: 0.224387
Epoch: 272, loss: 23.9322, train_acc: 0.3778, train_recall: 0.4845, train_f1: 0.2896, val_acc: 0.320513, val_recall: 0.328624, val_f1: 0.219019
Epoch: 273, loss: 22.8879, train_acc: 0.4444, train_recall: 0.4461, train_f1: 0.4049, val_acc: 0.397436, val_recall: 0.355366, val_f1: 0.259879
Epoch: 274, loss: 24.6387, train_acc: 0.4611, train_recall: 0.3879, train_f1: 0.3667, val_acc: 0.358974, val_recall: 0.263799, val_f1: 0.172125
Epoch: 275, loss: 23.2490, train_acc: 0.4556, train_recall: 0.4040, train_f1: 0.4420, val_acc: 0.333333, val_recall: 0.211193, val_f1: 0.216882
Epoch: 276, loss: 25.6004, train_acc: 0.3667, train_recall: 0.3508, train_f1: 0.2879, val_acc: 0.461538, val_recall: 0.259214, val_f1: 0.197552
Epoch: 277, loss: 25.1969, train_acc: 0.3944, train_recall: 0.4262, train_f1: 0.3714, val_acc: 0.333333, val_recall: 0.335381, val_f1: 0.210149
Epoch: 278, loss: 23.7034, train_acc: 0.3944, train_recall: 0.4262, train_f1: 0.3714, val_acc: 0.333333, val_recall: 0.335381, val_f1: 0.210149
Epoch: 279, loss: 24.1022, train_acc: 0.4611, train_recall: 0.3879, train_f1: 0.3667, val_acc: 0.358974, val_recall: 0.263799, val_f1: 0.172125
Epoch: 280, loss: 23.9200, train_acc: 0.4222, train_recall: 0.3674, train_f1: 0.3737, val_acc: 0.346154, val_recall: 0.217949, val_f1: 0.225312
Epoch: 281, loss: 24.9527, train_acc: 0.3889, train_recall: 0.3600, train_f1: 0.3075, val_acc: 0.461538, val_recall: 0.261386, val_f1: 0.215203
Epoch: 282, loss: 24.8282, train_acc: 0.4500, train_recall: 0.3763, train_f1: 0.3488, val_acc: 0.358974, val_recall: 0.263799, val_f1: 0.172125
Epoch: 283, loss: 23.1440, train_acc: 0.4667, train_recall: 0.4359, train_f1: 0.4136, val_acc: 0.371795, val_recall: 0.383117, val_f1: 0.249486
Epoch: 284, loss: 24.6056, train_acc: 0.4000, train_recall: 0.4320, train_f1: 0.3509, val_acc: 0.307692, val_recall: 0.321867, val_f1: 0.197222
Epoch: 285, loss: 24.3105, train_acc: 0.4333, train_recall: 0.4386, train_f1: 0.3494, val_acc: 0.371795, val_recall: 0.341852, val_f1: 0.250424
Epoch: 286, loss: 24.2019, train_acc: 0.3778, train_recall: 0.4455, train_f1: 0.2829, val_acc: 0.371795, val_recall: 0.230059, val_f1: 0.208527
Epoch: 287, loss: 24.8914, train_acc: 0.4556, train_recall: 0.3839, train_f1: 0.3584, val_acc: 0.358974, val_recall: 0.263799, val_f1: 0.173462
Epoch: 288, loss: 24.6223, train_acc: 0.4556, train_recall: 0.3839, train_f1: 0.3584, val_acc: 0.346154, val_recall: 0.254870, val_f1: 0.165714
Epoch: 289, loss: 23.6137, train_acc: 0.4389, train_recall: 0.4426, train_f1: 0.4227, val_acc: 0.397436, val_recall: 0.355366, val_f1: 0.259879
Epoch: 290, loss: 24.1827, train_acc: 0.4444, train_recall: 0.4484, train_f1: 0.4269, val_acc: 0.397436, val_recall: 0.355366, val_f1: 0.259879
Epoch: 291, loss: 23.4614, train_acc: 0.4611, train_recall: 0.4538, train_f1: 0.4670, val_acc: 0.346154, val_recall: 0.330511, val_f1: 0.247635
Epoch: 292, loss: 23.5545, train_acc: 0.4667, train_recall: 0.4359, train_f1: 0.4136, val_acc: 0.371795, val_recall: 0.383117, val_f1: 0.247635
Epoch: 293, loss: 23.7162, train_acc: 0.4667, train_recall: 0.4359, train_f1: 0.4146, val_acc: 0.358974, val_recall: 0.360390, val_f1: 0.236842
Epoch: 294, loss: 22.8000, train_acc: 0.4611, train_recall: 0.4520, train_f1: 0.4664, val_acc: 0.346154, val_recall: 0.330511, val_f1: 0.247635
Epoch: 295, loss: 23.0149, train_acc: 0.4500, train_recall: 0.4501, train_f1: 0.4325, val_acc: 0.397436, val_recall: 0.355366, val_f1: 0.259879
Epoch: 296, loss: 23.0486, train_acc: 0.4667, train_recall: 0.4382, train_f1: 0.4162, val_acc: 0.371795, val_recall: 0.383117, val_f1: 0.246742
Epoch: 297, loss: 23.3945, train_acc: 0.4667, train_recall: 0.4359, train_f1: 0.4140, val_acc: 0.371795, val_recall: 0.383117, val_f1: 0.247635
Epoch: 298, loss: 22.6127, train_acc: 0.4722, train_recall: 0.4422, train_f1: 0.4247, val_acc: 0.371795, val_recall: 0.383117, val_f1: 0.249486
Epoch: 299, loss: 23.2623, train_acc: 0.3778, train_recall: 0.4437, train_f1: 0.2793, val_acc: 0.371795, val_recall: 0.214088, val_f1: 0.188207
epoch: 41, train_acc: 0.361111, val_acc: 0.500000, val_recall: 0.295455, val_f1: 0.233333
